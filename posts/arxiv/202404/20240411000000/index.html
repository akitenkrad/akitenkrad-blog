<!doctype html><html><head><title>arXiv @ 2024.04.11</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.11"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cond-mat.stat-mech (1) cs.AI (8) cs.CL (42) cs.CR (10) cs.CV (70) cs.CY (2) cs.DB (4) cs.DC (5) cs.DM (1) cs.DS (3) cs.GR (1) cs.HC (4) cs.IR (4) cs.IT (2) cs.LG (39) cs.LO (2) cs.MA (1) cs.NE (8) cs.NI (4) cs.PL (1) cs.RO (12) cs.SD (2) cs.SE (8) cs.SI (2) eess.AS (2) eess.IV (3) eess.SY (7) math.CO (1) math.CT (1) math.DG (1) math.NA (3) math.OC (1) physics.comp-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240411000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-11T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.11"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240411000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Apr 11, 2024</p></div><div class=title><h1>arXiv @ 2024.04.11</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cond-matstat-mech-1>cond-mat.stat-mech (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csai-8>cs.AI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cscl-42>cs.CL (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cscr-10>cs.CR (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cscv-70>cs.CV (70)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csdb-4>cs.DB (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cslg-39>cs.LG (39)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cslo-2>cs.LO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csne-8>cs.NE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csni-4>cs.NI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csro-12>cs.RO (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#csse-8>cs.SE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#mathct-1>math.CT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#mathdg-1>math.DG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#physicsdata-an-1>physics.data-an (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/#quant-ph-2>quant-ph (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Augmented Reality (AR)</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Bard</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>13</td><td></td><td>13</td><td>11</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Chain-of-thought</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Claude</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>3</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Convolutional Neural Network</td><td>2</td><td></td><td>14</td><td></td><td>2</td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Deep Neural Network</td><td>1</td><td></td><td>6</td><td>3</td><td>1</td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Domain Adaptation</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td></td><td>5</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>5</td><td>1</td><td>11</td><td>6</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>GPT</td><td>7</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>GPT-3</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>7</td><td></td><td>1</td></tr><tr><td>Graph</td><td></td><td></td><td>1</td><td>5</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>1</td><td>3</td><td>2</td></tr><tr><td>Grounding</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>3</td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Information Retrieval</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>2</td><td></td><td>6</td><td>4</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>37</td><td>2</td><td>6</td><td>8</td><td>4</td></tr><tr><td>Low-Resource</td><td>4</td><td>1</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Masked Language Model</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mixed Reality (MR)</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>11</td><td></td><td>8</td><td>2</td><td>2</td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>3</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Optical Character Recognition</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>Outlier Detection</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Prompt</td><td>4</td><td></td><td>6</td><td>1</td><td>2</td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td>7</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>5</td><td></td><td>4</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td></td><td>6</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Rerank</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval Augmentation</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Security</td><td></td><td>6</td><td>2</td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>7</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td></td><td>3</td><td>4</td></tr><tr><td>Simulator</td><td></td><td></td><td></td><td>3</td><td>4</td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Summarization</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>4</td><td></td><td>5</td><td>1</td><td></td></tr><tr><td>Text Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td>4</td><td></td><td>13</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Virtual Reality (VR)</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td>2</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td></td><td>7</td><td>3</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscr-10>cs.CR (10)</h2><h3 id=110--1264-sandwich-attack-multi-language-mixture-adaptive-attack-on-llms-bibek-upadhayay-et-al-2024>(1/10 | 1/264) Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs (Bibek Upadhayay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bibek Upadhayay, Vahid Behzadan. (2024)<br><strong>Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs</strong><br><button class=copy-to-clipboard title="Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 125<br>Keywords: Black Box, High-Resource, Low-Resource, Bard, Claude, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07242v1.pdf filename=2404.07242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly being developed and applied, but their widespread use faces challenges. These include aligning <b>LLMs&rsquo;</b> responses with human values to prevent harmful outputs, which is addressed through safety training methods. Even so, bad actors and malicious users have succeeded in attempts to manipulate the <b>LLMs</b> to generate misaligned responses for harmful questions such as methods to create a bomb in school labs, recipes for harmful drugs, and ways to evade privacy rights. Another challenge is the multilingual capabilities of <b>LLMs,</b> which enable the model to understand and respond in multiple languages. Consequently, attackers exploit the unbalanced pre-training datasets of <b>LLMs</b> in different languages and the comparatively lower model performance in <b>low-resource</b> languages than <b>high-resource</b> ones. As a result, attackers use a <b>low-resource</b> languages to intentionally manipulate the model to create harmful responses. Many of the similar attack vectors have been patched by model providers, making the <b>LLMs</b> more robust against language-based manipulation. In this paper, we introduce a new <b>black-box</b> <b>attack</b> vector called the \emph{Sandwich attack}: a multi-language mixture attack, which manipulates state-of-the-art <b>LLMs</b> into generating harmful and misaligned responses. Our experiments with five different models, namely Google&rsquo;s <b>Bard,</b> <b>Gemini</b> Pro, <b>LLaMA-2-70-B-Chat,</b> <b>GPT-3.5-Turbo,</b> <b>GPT-4,</b> and <b>Claude-3-OPUS,</b> show that this attack vector can be used by adversaries to generate harmful responses and elicit misaligned responses from these models. By detailing both the mechanism and impact of the Sandwich attack, this paper aims to guide future research and development towards more secure and resilient <b>LLMs,</b> ensuring they serve the public good while minimizing potential for misuse.</p></p class="citation"></blockquote><h3 id=210--2264-flex-flexible-federated-learning-framework-francisco-herrera-et-al-2024>(2/10 | 2/264) FLEX: FLEXible Federated Learning Framework (Francisco Herrera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Herrera, Daniel Jiménez-López, Alberto Argente-Garrido, Nuria Rodríguez-Barroso, Cristina Zuheros, Ignacio Aguilera-Martos, Beatriz Bello, Mario García-Márquez, M. Victoria Luzón. (2024)<br><strong>FLEX: FLEXible Federated Learning Framework</strong><br><button class=copy-to-clipboard title="FLEX: FLEXible Federated Learning Framework" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Federated Learning, Adversarial Attack, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06127v1.pdf filename=2404.06127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of Artificial Intelligence (AI), the need for privacy and <b>security</b> in data processing has become paramount. As AI applications continue to expand, the collection and handling of sensitive data raise concerns about individual privacy protection. <b>Federated</b> <b>Learning</b> (FL) emerges as a promising solution to address these challenges by enabling decentralized model training on local devices, thus preserving data privacy. This paper introduces FLEX: a FLEXible <b>Federated</b> <b>Learning</b> Framework designed to provide maximum flexibility in FL research experiments. By offering customizable features for data distribution, privacy parameters, and communication strategies, FLEX empowers researchers to innovate and develop novel FL techniques. The framework also includes libraries for specific FL implementations including: (1) anomalies, (2) blockchain, (3) <b>adversarial</b> <b>attacks</b> and defences, (4) natural language processing and (5) decision trees, enhancing its versatility and applicability in various domains. Overall, FLEX represents a significant advancement in FL research, facilitating the development of robust and efficient FL applications.</p></p class="citation"></blockquote><h3 id=310--3264-boosting-digital-safeguards-blending-cryptography-and-steganography-anamitra-maiti-et-al-2024>(3/10 | 3/264) Boosting Digital Safeguards: Blending Cryptography and Steganography (Anamitra Maiti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anamitra Maiti, Subham Laha, Rishav Upadhaya, Soumyajit Biswas, Vikas Chaudhary, Biplab Kar, Nikhil Kumar, Jaydip Sen. (2024)<br><strong>Boosting Digital Safeguards: Blending Cryptography and Steganography</strong><br><button class=copy-to-clipboard title="Boosting Digital Safeguards: Blending Cryptography and Steganography" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05985v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05985v2.pdf filename=2404.05985v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data <b>security</b> measures to prevent unauthorized access and exploitation. Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission. Steganography, on the other hand, originates from the Greek term for &ldquo;covered writing&rdquo; and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible. This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> to improve upon traditional steganographic methods. By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes. The application of <b>GANs</b> enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection. By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive <b>security</b> system designed to maintain both the privacy and integrity of information. This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden. This fusion of technologies tackles the core challenges of data <b>security</b> in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information <b>security.</b></p></p class="citation"></blockquote><h3 id=410--4264-deep-learning-based-out-of-distribution-source-code-data-identification-how-far-we-have-gone-van-nguyen-et-al-2024>(4/10 | 4/264) Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone? (Van Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Van Nguyen, Xingliang Yuan, Tingmin Wu, Surya Nepal, Marthie Grobler, Carsten Rudolph. (2024)<br><strong>Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone?</strong><br><button class=copy-to-clipboard title="Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone?" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 25<br>Keywords: Out-of-distribution, Representation Learning, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05964v1.pdf filename=2404.05964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software vulnerabilities (SVs) have become a common, serious, and crucial concern to safety-critical <b>security</b> systems. That leads to significant progress in the use of AI-based methods for software vulnerability detection (SVD). In practice, although AI-based methods have been achieving promising performances in SVD and other domain applications (e.g., computer vision), they are well-known to fail in detecting the ground-truth label of input data (referred to as <b>out-of-distribution,</b> OOD, data) lying far away from the training data distribution (i.e., in-distribution, ID). This drawback leads to serious issues where the models fail to indicate when they are likely mistaken. To address this problem, OOD detectors (i.e., determining whether an input is ID or OOD) have been applied before feeding the input data to the downstream AI-based modules. While OOD detection has been widely designed for computer vision and medical diagnosis applications, automated AI-based techniques for OOD source code data detection have not yet been well-studied and explored. To this end, in this paper, we propose an innovative deep learning-based approach addressing the OOD source code data identification problem. Our method is derived from an information-theoretic perspective with the use of innovative cluster-contrastive learning to effectively learn and leverage source code characteristics, enhancing data <b>representation</b> <b>learning</b> for solving the problem. The rigorous and comprehensive experiments on real-world source code datasets show the effectiveness and advancement of our approach compared to state-of-the-art baselines by a wide margin. In short, on average, our method achieves a significantly higher performance from around 15.27%, 7.39%, and 4.93% on the FPR, AUROC, and AUPR measures, respectively, in comparison with the baselines.</p></p class="citation"></blockquote><h3 id=510--5264-current-affairs-a-measurement-study-of-deployment-and-security-trends-in-ev-charging-infrastructure-marcell-szakály-et-al-2024>(5/10 | 5/264) Current Affairs: A Measurement Study of Deployment and Security Trends in EV Charging Infrastructure (Marcell Szakály et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcell Szakály, Sebastian Köhler, Ivan Martinovic. (2024)<br><strong>Current Affairs: A Measurement Study of Deployment and Security Trends in EV Charging Infrastructure</strong><br><button class=copy-to-clipboard title="Current Affairs: A Measurement Study of Deployment and Security Trends in EV Charging Infrastructure" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06635v1.pdf filename=2404.06635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The deployment of electric vehicle charging infrastructure is occurring at a rapid pace. Simultaneously, existing standards, such as ISO 15118, which defines critical charging communication, are being improved and further developed. In this paper, we conduct a measurement study of already deployed DC charging stations to analyze the current state of deployment for various protocols. We present the adoption of TLS, and various EV charging protocols with a direct <b>security</b> impact, as well as observations about the Signal Level Attenuation Characterization (SLAC) process, and encryption keys. Our results indicate that even recently installed charging stations (December 2023) do not adhere to the latest version of the standard, leaving them vulnerable to attacks. We found that 84% of the surveyed charging stations do not implement Transport Layer <b>Security</b> (TLS), and are thus unable to implement the latest versions of the ISO 15118 protocol, leaving them vulnerable to attacks already demonstrated years ago. Finally, we observe and document anomalous behavior and violations of the standard.</p></p class="citation"></blockquote><h3 id=610--6264-software-based-security-framework-for-edge-and-mobile-iot-josé-cecílio-et-al-2024>(6/10 | 6/264) Software-based Security Framework for Edge and Mobile IoT (José Cecílio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Cecílio, Alan Oliveira de Sá, André Souto. (2024)<br><strong>Software-based Security Framework for Edge and Mobile IoT</strong><br><button class=copy-to-clipboard title="Software-based Security Framework for Edge and Mobile IoT" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: H-0, cs-CR, cs-DC, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06435v1.pdf filename=2404.06435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the proliferation of Internet of Things (IoT) devices, ensuring secure communications has become imperative. Due to their low cost and embedded nature, many of these devices operate with computational and energy constraints, neglecting the potential <b>security</b> vulnerabilities that they may bring. This work-in-progress is focused on designing secure communication among remote servers and embedded IoT devices to balance <b>security</b> robustness and energy efficiency. The proposed approach uses lightweight cryptography, optimizing device performance and <b>security</b> without overburdening their limited resources. Our architecture stands out for integrating Edge servers and a central Name Server, allowing secure and decentralized authentication and efficient connection transitions between different Edge servers. This architecture enhances the scalability of the IoT network and reduces the load on each server, distributing the responsibility for authentication and key management.</p></p class="citation"></blockquote><h3 id=710--7264-towards-robust-domain-generation-algorithm-classification-arthur-drichel-et-al-2024>(7/10 | 7/264) Towards Robust Domain Generation Algorithm Classification (Arthur Drichel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Drichel, Marc Meyer, Ulrike Meyer. (2024)<br><strong>Towards Robust Domain Generation Algorithm Classification</strong><br><button class=copy-to-clipboard title="Towards Robust Domain Generation Algorithm Classification" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06236v1.pdf filename=2404.06236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we conduct a comprehensive study on the robustness of domain generation algorithm (DGA) classifiers. We implement 32 white-box attacks, 19 of which are very effective and induce a false-negative rate (FNR) of $\approx$ 100% on unhardened classifiers. To defend the classifiers, we evaluate different hardening approaches and propose a novel training scheme that leverages <b>adversarial</b> <b>latent</b> space vectors and discretized <b>adversarial</b> <b>domains</b> to significantly improve robustness. In our study, we highlight a pitfall to avoid when hardening classifiers and uncover training biases that can be easily exploited by attackers to bypass detection, but which can be mitigated by <b>adversarial</b> <b>training</b> (AT). In our study, we do not observe any trade-off between robustness and performance, on the contrary, hardening improves a classifier&rsquo;s detection performance for known and unknown DGAs. We implement all attacks and defenses discussed in this paper as a standalone library, which we make publicly available to facilitate hardening of DGA classifiers: <a href=https://gitlab.com/rwth-itsec/robust-dga-detection>https://gitlab.com/rwth-itsec/robust-dga-detection</a></p></p class="citation"></blockquote><h3 id=810--8264-privacy-preserving-scanpath-comparison-for-pervasive-eye-tracking-suleyman-ozdel-et-al-2024>(8/10 | 8/264) Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking (Suleyman Ozdel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suleyman Ozdel, Efe Bozkir, Enkelejda Kasneci. (2024)<br><strong>Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking</strong><br><button class=copy-to-clipboard title="Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06216v1.pdf filename=2404.06216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As eye tracking becomes pervasive with screen-based devices and head-mounted displays, privacy concerns regarding eye-tracking data have escalated. While state-of-the-art approaches for privacy-preserving eye tracking mostly involve <b>differential</b> <b>privacy</b> and empirical data manipulations, previous research has not focused on methods for scanpaths. We introduce a novel privacy-preserving scanpath comparison protocol designed for the widely used Needleman-Wunsch algorithm, a generalized version of the edit distance algorithm. Particularly, by incorporating the Paillier homomorphic encryption scheme, our protocol ensures that no private information is revealed. Furthermore, we introduce a random processing strategy and a multi-layered masking method to obfuscate the values while preserving the original order of encrypted editing operation costs. This minimizes communication overhead, requiring a single communication round for each iteration of the Needleman-Wunsch process. We demonstrate the efficiency and applicability of our protocol on three publicly available datasets with comprehensive computational performance analyses and make our source code publicly accessible.</p></p class="citation"></blockquote><h3 id=910--9264-s-box-security-analysis-of-nist-lightweight-cryptography-candidates-a-critical-empirical-study-mahnoor-naseer-et-al-2024>(9/10 | 9/264) S-box Security Analysis of NIST Lightweight Cryptography Candidates: A Critical Empirical Study (Mahnoor Naseer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahnoor Naseer, Sundas Tariq, Naveed Riaz, Naveed Ahmed, Mureed Hussain. (2024)<br><strong>S-box Security Analysis of NIST Lightweight Cryptography Candidates: A Critical Empirical Study</strong><br><button class=copy-to-clipboard title="S-box Security Analysis of NIST Lightweight Cryptography Candidates: A Critical Empirical Study" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06094v1.pdf filename=2404.06094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the resource-constrained world of the digital landscape, lightweight cryptography plays a critical role in safeguarding information and ensuring the <b>security</b> of various systems, devices, and communication channels. Its efficient and resource-friendly nature makes it the ideal solution for applications where computational power is limited. In response to the growing need for platform-specific implementations, NIST issued a call for standardization of Lightweight cryptography algorithms in 2018. Ascon emerged as the winner of this competition. NIST initially established general evaluation criteria for a standard lightweight scheme including <b>security</b> strength, mitigation against side-channel and fault-injection attacks, and implementation efficiency. To verify the <b>security</b> claims, evaluating the individual components used in any cryptographic algorithm is a crucial step. The quality of a substitution box (S-box) significantly impacts the overall <b>security</b> of a cryptographic primitive. This paper analyzes the S-boxes of six finalists in the NIST Lightweight Cryptography (LWC) standardization process. We evaluate them based on well-established cryptographic properties. Our analysis explores how these properties influence the S-boxes&rsquo; resistance against known cryptanalytic attacks and potential implementation-specific vulnerabilities, thus reflecting on their compliance with NIST&rsquo;s <b>security</b> requirements.</p></p class="citation"></blockquote><h3 id=1010--10264-is-your-ai-truly-yours-leveraging-blockchain-for-copyrights-provenance-and-lineage-yilin-sai-et-al-2024>(10/10 | 10/264) Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage (Yilin Sai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilin Sai, Qin Wang, Guangsheng Yu, H. M. N. Dilum Bandara, Shiping Chen. (2024)<br><strong>Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage</strong><br><button class=copy-to-clipboard title="Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CY, cs.CR<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06077v1.pdf filename=2404.06077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount. AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners. However, existing studies primarily center on safeguarding static copyrights, which simply treats metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory. In this paper, we present \textsc{IBis}, a blockchain-based framework tailored for AI model training workflows. \textsc{IBis} integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants. Our framework addresses concerns regarding data and model provenance and copyright compliance. \textsc{IBis} enables iterative model retraining and <b>fine-tuning,</b> and offers flexible license checks and renewals. Further, \textsc{IBis} provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes. We implement \textsc{IBis} using Daml on the Canton blockchain. Evaluation results showcase the feasibility and scalability of \textsc{IBis} across varying numbers of users, datasets, models, and licenses.</p></p class="citation"></blockquote><h2 id=cscl-42>cs.CL (42)</h2><h3 id=142--11264-characterizing-multimodal-long-form-summarization-a-case-study-on-financial-reports-tianyu-cao-et-al-2024>(1/42 | 11/264) Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports (Tianyu Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Cao, Natraj Raman, Danial Dervovic, Chenhao Tan. (2024)<br><strong>Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports</strong><br><button class=copy-to-clipboard title="Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 96<br>Keywords: Multi-modal, Multi-modal, Claude, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06162v1.pdf filename=2404.06162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> expand the power of natural language processing to handle long inputs, rigorous and systematic analyses are necessary to understand their abilities and behavior. A salient application is <b>summarization,</b> due to its ubiquity and controversy (e.g., researchers have declared the death of <b>summarization).</b> In this paper, we use financial report <b>summarization</b> as a case study because financial reports not only are long but also use numbers and tables extensively. We propose a computational framework for characterizing <b>multimodal</b> long-form <b>summarization</b> and investigate the behavior of <b>Claude</b> 2.0/2.1, <b>GPT-4/3.5,</b> and Command. We find that <b>GPT-3.5</b> and Command fail to perform this <b>summarization</b> task meaningfully. For <b>Claude</b> 2 and <b>GPT-4,</b> we analyze the extractiveness of the summary and identify a position bias in <b>LLMs.</b> This position bias disappears after shuffling the input for <b>Claude,</b> which suggests that <b>Claude</b> has the ability to recognize important information. We also conduct a comprehensive investigation on the use of numeric data in <b>LLM-generated</b> summaries and offer a taxonomy of numeric hallucination. We employ <b>prompt</b> engineering to improve <b>GPT-4&rsquo;s</b> use of numbers with limited success. Overall, our analyses highlight the strong capability of <b>Claude</b> 2 in handling long <b>multimodal</b> inputs compared to <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=242--12264-llms-reading-comprehension-is-affected-by-parametric-knowledge-and-struggles-with-hypothetical-statements-victoria-basmov-et-al-2024>(2/42 | 12/264) LLMs&rsquo; Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements (Victoria Basmov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victoria Basmov, Yoav Goldberg, Reut Tsarfaty. (2024)<br><strong>LLMs&rsquo; Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements</strong><br><button class=copy-to-clipboard title="LLMs' Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: ChatGPT, GPT, GPT-4, LLaMA, Natural Language Understanding, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06283v1.pdf filename=2404.06283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of reading comprehension (RC), often implemented as context-based <b>question</b> <b>answering</b> <b>(QA),</b> provides a primary means to assess language models&rsquo; <b>natural</b> <b>language</b> <b>understanding</b> (NLU) capabilities. Yet, when applied to <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with extensive built-in world knowledge, this method can be deceptive. If the context aligns with the <b>LLMs&rsquo;</b> internal knowledge, it is hard to discern whether the models&rsquo; answers stem from context comprehension or from <b>LLMs&rsquo;</b> internal information. Conversely, using data that conflicts with the models&rsquo; knowledge creates erroneous trends which distort the results. To address this issue, we suggest to use RC on imaginary data, based on fictitious facts and entities. This task is entirely independent of the models&rsquo; world knowledge, enabling us to evaluate <b>LLMs&rsquo;</b> linguistic abilities without the interference of parametric knowledge. Testing <b>ChatGPT,</b> <b>GPT-4,</b> <b>LLaMA</b> 2 and Mixtral on such imaginary data, we uncover a class of linguistic phenomena posing a challenge to current <b>LLMs,</b> involving thinking in terms of alternative, hypothetical scenarios. While all the models handle simple affirmative and negative contexts with high accuracy, they are much more prone to error when dealing with modal and conditional contexts. Crucially, these phenomena also trigger the <b>LLMs&rsquo;</b> vulnerability to knowledge-conflicts again. In particular, while some models prove virtually unaffected by knowledge conflicts in affirmative and negative contexts, when faced with more semantically involved modal and conditional environments, they often fail to separate the text from their internal knowledge.</p></p class="citation"></blockquote><h3 id=342--13264-rar-b-reasoning-as-retrieval-benchmark-chenghao-xiao-et-al-2024>(3/42 | 13/264) RAR-b: Reasoning as Retrieval Benchmark (Chenghao Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed. (2024)<br><strong>RAR-b: Reasoning as Retrieval Benchmark</strong><br><button class=copy-to-clipboard title="RAR-b: Reasoning as Retrieval Benchmark" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Rerank, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06347v1.pdf filename=2404.06347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic textual similartiy (STS) and <b>information</b> <b>retrieval</b> <b>tasks</b> <b>(IR)</b> tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging <b>Retrieval-augmented</b> <b>Generation</b> <b>(RAG)</b> paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the <b>reasoning</b> abilities stored in them. Addressing this, we pose the question: Can retrievers solve <b>reasoning</b> problems? By transforming <b>reasoning</b> tasks into <b>retrieval</b> <b>tasks,</b> <b>we</b> find that without specifically trained for <b>reasoning-level</b> language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting <b>LLMs,</b> especially in <b>reasoning-intensive</b> tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for <b>reasoning</b> tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve <b>reasoning-level</b> language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting <b>reasoning</b> abilities into them through <b>fine-tuning</b> still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by <b>fine-tuning</b> a <b>reranking</b> model. We release <b>Reasoning</b> as <b>Retrieval</b> <b>Benchmark</b> <b>(RAR-b),</b> a holistic suite of tasks and settings to evaluate the <b>reasoning</b> abilities stored in retriever models. RAR-b is available at <a href=https://github.com/gowitheflow-1998/RAR-b>https://github.com/gowitheflow-1998/RAR-b</a>.</p></p class="citation"></blockquote><h3 id=442--14264-llm2vec-large-language-models-are-secretly-powerful-text-encoders-parishad-behnamghader-et-al-2024>(4/42 | 14/264) LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders (Parishad BehnamGhader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, Siva Reddy. (2024)<br><strong>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</strong><br><button class=copy-to-clipboard title="LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Contrastive Learning, Supervised Learning, Unsupervised Learning, GPT, GPT-4, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05961v1.pdf filename=2404.05961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>decoder-only</b> <b>language</b> models <b>(LLMs)</b> are the state-of-the-art models on most of today&rsquo;s NLP tasks and <b>benchmarks.</b> Yet, the community is only slowly adopting these models for <b>text</b> <b>embedding</b> tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple <b>unsupervised</b> approach that can transform any decoder-only <b>LLM</b> into a strong <b>text</b> <b>encoder.</b> LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) <b>unsupervised</b> <b>contrastive</b> <b>learning.</b> We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular <b>LLMs</b> ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a <b>large</b> <b>margin</b> <b>on</b> word-level tasks and reach a new <b>unsupervised</b> state-of-the-art performance on the Massive <b>Text</b> <b>Embeddings</b> <b>Benchmark</b> (MTEB). Moreover, when combining LLM2Vec with <b>supervised</b> <b>contrastive</b> <b>learning,</b> we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that <b>LLMs</b> can be effectively transformed into universal <b>text</b> <b>encoders</b> in a parameter-efficient manner without the need for expensive adaptation or synthetic <b>GPT-4</b> generated data.</p></p class="citation"></blockquote><h3 id=542--15264-privacy-preserving-prompt-engineering-a-survey-kennedy-edemacu-et-al-2024>(5/42 | 15/264) Privacy Preserving Prompt Engineering: A Survey (Kennedy Edemacu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kennedy Edemacu, Xintao Wu. (2024)<br><strong>Privacy Preserving Prompt Engineering: A Survey</strong><br><button class=copy-to-clipboard title="Privacy Preserving Prompt Engineering: A Survey" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06001v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06001v2.pdf filename=2404.06001v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have demonstrated significant proficiency in solving a wide range of general natural language processing (NLP) tasks. Researchers have observed a direct correlation between the performance of these models and their sizes. As a result, the sizes of these models have notably expanded in recent years, persuading researchers to adopt the term <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to characterize the larger-sized <b>PLMs.</b> The size expansion comes with a distinct capability called <b>in-context</b> <b>learning</b> <b>(ICL),</b> which represents a special form of <b>prompting</b> and allows the models to be utilized through the presentation of demonstration examples without modifications to the model parameters. Although interesting, privacy concerns have become a major obstacle in its widespread usage. Multiple studies have examined the privacy risks linked to <b>ICL</b> and <b>prompting</b> in general, and have devised techniques to alleviate these risks. Thus, there is a necessity to organize these mitigation techniques for the benefit of the community. This survey provides a systematic overview of the privacy protection methods employed during <b>ICL</b> and <b>prompting</b> in general. We review, analyze, and compare different methods under this paradigm. Furthermore, we provide a summary of the resources accessible for the development of these frameworks. Finally, we discuss the limitations of these frameworks and offer a detailed examination of the promising areas that necessitate further exploration.</p></p class="citation"></blockquote><h3 id=642--16264-text-based-reasoning-about-vector-graphics-zhenhailong-wang-et-al-2024>(6/42 | 16/264) Text-Based Reasoning About Vector Graphics (Zhenhailong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li, Jiajun Wu, Heng Ji. (2024)<br><strong>Text-Based Reasoning About Vector Graphics</strong><br><button class=copy-to-clipboard title="Text-Based Reasoning About Vector Graphics" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 79<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Zero-shot, GPT, Question Answering, Reasoning, Large Language Model, Pre-trained Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06479v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06479v2.pdf filename=2404.06479v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While large <b>multimodal</b> models excel in broad <b>vision-language</b> <b>benchmarks,</b> they often struggle with tasks requiring precise perception of low-level visual details, such as comparing line lengths or solving simple mazes. In particular, this failure mode persists in <b>question-answering</b> <b>tasks</b> about vector graphics &ndash; images composed purely of 2D objects and shapes. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which performs text-based <b>reasoning</b> about vector graphics. VDLM leverages Scalable Vector Graphics (SVG) for a more precise visual description and first uses an off-the-shelf raster-to-SVG algorithm for encoding. Since existing language models cannot understand raw SVGs in a <b>zero-shot</b> setting, VDLM then bridges SVG with <b>pretrained</b> <b>language</b> <b>models</b> through a newly introduced intermediate symbolic representation, Primal Visual Description (PVD), comprising primitive attributes (e.g., shape, position, measurement) with their corresponding predicted values. PVD is task-agnostic and represents visual primitives that are universal across all vector graphics. It can be learned with procedurally generated (SVG, PVD) pairs and also enables the direct use of <b>LLMs</b> for generalization to complex <b>reasoning</b> tasks. By casting an image to a text-based representation, we can leverage the power of language models to learn alignment from SVG to visual primitives and generalize to unseen <b>question-answering</b> <b>tasks.</b> Empirical results show that VDLM achieves stronger <b>zero-shot</b> performance compared to state-of-the-art LMMs, such as <b>GPT-4V,</b> in various low-level <b>multimodal</b> perception and <b>reasoning</b> tasks on vector graphics. We additionally present extensive analyses on VDLM&rsquo;s performance, demonstrating that our framework offers better interpretability due to its disentangled perception and <b>reasoning</b> processes. Project page: <a href=https://mikewangwzhl.github.io/VDLM/>https://mikewangwzhl.github.io/VDLM/</a></p></p class="citation"></blockquote><h3 id=742--17264-visualwebbench-how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding-junpeng-liu-et-al-2024>(7/42 | 17/264) VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? (Junpeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, Xiang Yue. (2024)<br><strong>VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?</strong><br><button class=copy-to-clipboard title="VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 79<br>Keywords: Optical Character Recognition, Benchmarking, Multi-modal, Multi-modal, Claude, GPT, Gemini, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05955v1.pdf filename=2404.05955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>models</b> (MLLMs) have shown promise in web-related tasks, but evaluating their performance in the web domain remains a challenge due to the lack of comprehensive <b>benchmarks.</b> Existing <b>benchmarks</b> are either designed for general <b>multimodal</b> tasks, failing to capture the unique characteristics of web pages, or focus on end-to-end web agent tasks, unable to measure fine-grained abilities such as <b>OCR,</b> understanding, and <b>grounding.</b> In this paper, we introduce \bench{}, a <b>multimodal</b> <b>benchmark</b> designed to assess the capabilities of MLLMs across a variety of web tasks. \bench{} consists of seven tasks, and comprises 1.5K human-curated instances from 139 real websites, covering 87 sub-domains. We evaluate 14 open-source MLLMs, <b>Gemini</b> Pro, <b>Claude-3</b> series, and <b>GPT-4V(ision)</b> on \bench{}, revealing significant challenges and performance gaps. Further analysis highlights the limitations of current MLLMs, including inadequate <b>grounding</b> in text-rich environments and subpar performance with low-resolution image inputs. We believe \bench{} will serve as a valuable resource for the research community and contribute to the creation of more powerful and versatile MLLMs for web-related applications.</p></p class="citation"></blockquote><h3 id=842--18264-optimization-methods-for-personalizing-large-language-models-through-retrieval-augmentation-alireza-salemi-et-al-2024>(8/42 | 18/264) Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation (Alireza Salemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Salemi, Surya Kallumadi, Hamed Zamani. (2024)<br><strong>Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation</strong><br><button class=copy-to-clipboard title="Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Reinforcement Learning, Large Language Model, Large Language Model, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05970v1.pdf filename=2404.05970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies <b>retrieval-augmented</b> <b>approaches</b> for personalizing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which potentially have a substantial impact on various applications and domains. We propose the first attempt to optimize the <b>retrieval</b> <b>models</b> that deliver a limited number of personal documents to <b>large</b> <b>language</b> <b>models</b> for the purpose of personalized generation. We develop two optimization algorithms that solicit feedback from the downstream personalized generation tasks for <b>retrieval</b> <b>optimization&ndash;one</b> based on <b>reinforcement</b> <b>learning</b> whose reward function is defined using any arbitrary metric for personalized generation and another based on <b>knowledge</b> <b>distillation</b> from the downstream <b>LLM</b> to the <b>retrieval</b> <b>model.</b> This paper also introduces a pre- and post-generation retriever selection model that decides what retriever to choose for each <b>LLM</b> input. Extensive experiments on diverse tasks from the language model personalization (LaMP) <b>benchmark</b> reveal statistically significant improvements in six out of seven datasets.</p></p class="citation"></blockquote><h3 id=942--19264-less-is-more-for-improving-automatic-evaluation-of-factual-consistency-tong-wang-et-al-2024>(9/42 | 19/264) Less is More for Improving Automatic Evaluation of Factual Consistency (Tong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Wang, Ninad Kulkarni, Yanjun Qi. (2024)<br><strong>Less is More for Improving Automatic Evaluation of Factual Consistency</strong><br><button class=copy-to-clipboard title="Less is More for Improving Automatic Evaluation of Factual Consistency" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Automatic Evaluation, Benchmarking, ChatGPT, Language Generation, Natural Language Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06579v1.pdf filename=2404.06579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing the factual consistency of automatically generated texts in relation to source context is crucial for developing reliable <b>natural</b> <b>language</b> <b>generation</b> applications. Recent literature proposes AlignScore which uses a unified alignment model to evaluate factual consistency and substantially outperforms previous methods across many <b>benchmark</b> tasks. In this paper, we take a closer look of datasets used in AlignScore and uncover an unexpected finding: utilizing a smaller number of data points can actually improve performance. We process the original AlignScore training dataset to remove noise, augment with robustness-enhanced samples, and utilize a subset comprising 10% of the data to train an improved factual consistency evaluation model, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates superior performance, consistently outperforming AlignScore and other strong baselines like <b>ChatGPT</b> across four <b>benchmarks</b> (two utilizing traditional <b>natural</b> <b>language</b> <b>generation</b> datasets and two focused on <b>large</b> <b>language</b> <b>model</b> outputs). Our experiments show that LIM-RA achieves the highest score on 24 of the 33 test datasets, while staying competitive on the rest, establishing the new state-of-the-art <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1042--20264-all-in-one-an-empirical-study-of-gpt-for-few-shot-aspect-based-sentiment-anlaysis-baoxing-jiang-2024>(10/42 | 20/264) All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis (Baoxing Jiang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoxing Jiang. (2024)<br><strong>All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis</strong><br><button class=copy-to-clipboard title="All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, GPT, Transformer, Aspect-based Sentiment Analysis, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06063v1.pdf filename=2404.06063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-Based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA) is an indispensable and highly challenging task in natural language processing. Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain. With the development of Generative Pre-trained <b>Transformers</b> <b>(GPTs),</b> there came inspiration for a one-stop solution to <b>sentiment</b> <b>analysis.</b> In this study, we used <b>GPTs</b> for all sub-tasks of <b>few-shot</b> ABSA while defining a general learning paradigm for this application. We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks. In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates. In the second stage, AiO leverages <b>GPT</b> contextual learning capabilities to generate predictions. The study conducted comprehensive comparative and ablation experiments on five <b>benchmark</b> datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with <b>few-shot</b> data.</p></p class="citation"></blockquote><h3 id=1142--21264-latent-distance-guided-alignment-training-for-large-language-models-haotian-luo-et-al-2024>(11/42 | 21/264) Latent Distance Guided Alignment Training for Large Language Models (Haotian Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Luo, Wenhao Zheng, Huaxiu Yao. (2024)<br><strong>Latent Distance Guided Alignment Training for Large Language Models</strong><br><button class=copy-to-clipboard title="Latent Distance Guided Alignment Training for Large Language Models" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06390v1.pdf filename=2404.06390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring alignment with human preferences is a crucial characteristic of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Presently, the primary alignment methods, <b>RLHF</b> and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality <b>supervised</b> <b>fine-tune</b> dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.</p></p class="citation"></blockquote><h3 id=1242--22264-cendol-open-instruction-tuned-generative-large-language-models-for-indonesian-languages-samuel-cahyawijaya-et-al-2024>(12/42 | 22/264) Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages (Samuel Cahyawijaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri, Emmanuel Dave, Jhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil Maulana Akbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta Indra Winata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung. (2024)<br><strong>Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages</strong><br><button class=copy-to-clipboard title="Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Low-Resource, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06138v1.pdf filename=2404.06138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in <b>low-resource</b> languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian <b>LLMs</b> encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol&rsquo;s effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to <b>low-resource</b> languages, such as Indonesian, even without <b>RLHF</b> and safety <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1342--23264-not-understanding-latin-poetic-style-with-deep-learning-ben-nagy-2024>(13/42 | 23/264) (Not) Understanding Latin Poetic Style with Deep Learning (Ben Nagy, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Nagy. (2024)<br><strong>(Not) Understanding Latin Poetic Style with Deep Learning</strong><br><button class=copy-to-clipboard title="(Not) Understanding Latin Poetic Style with Deep Learning" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 45<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, LSTM, Reasoning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06150v1.pdf filename=2404.06150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article <b>summarizes</b> some mostly unsuccessful attempts to understand authorial style by examining the attention of various neural networks <b>(LSTMs</b> and <b>CNNs)</b> trained on a corpus of classical Latin verse that has been encoded to include sonic and metrical features. Carefully configured neural networks are shown to be extremely strong authorship classifiers, so it is hoped that they might therefore teach `traditional&rsquo; readers something about how the authors differ in style. Sadly their <b>reasoning</b> is, so far, inscrutable. While the overall goal has not yet been reached, this work reports some useful findings in terms of effective ways to encode and embed verse, the relative strengths and weaknesses of the neural network families, and useful (and not so useful) techniques for designing and inspecting NN models in this domain. This article suggests that, for poetry, <b>CNNs</b> are better choices than <b>LSTMs</b> &ndash; they train more quickly, have equivalent accuracy, and (potentially) offer better interpretability. Based on a great deal of experimentation, it also suggests that simple, trainable embeddings are more effective than domain-specific schemes, and stresses the importance of techniques to reduce overfitting, like dropout and batch normalization.</p></p class="citation"></blockquote><h3 id=1442--24264-khayyam-challenge-persianmmlu-is-your-llm-truly-wise-to-the-persian-language-omid-ghahroodi-et-al-2024>(14/42 | 24/264) Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language? (Omid Ghahroodi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, Mohammad Hossein Rohban. (2024)<br><strong>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?</strong><br><button class=copy-to-clipboard title="Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Information Retrieval, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06644v1.pdf filename=2404.06644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is challenging due to their generative nature, necessitating precise evaluation methodologies. Additionally, non-English <b>LLM</b> evaluation lags behind English, resulting in the absence or weakness of <b>LLMs</b> for many languages. In response to this necessity, we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously curated collection comprising 20,192 four-choice questions sourced from 38 diverse tasks extracted from Persian examinations, spanning a wide spectrum of subjects, complexities, and ages. The primary objective of the Khayyam Challenge is to facilitate the rigorous evaluation of <b>LLMs</b> that support the Persian language. Distinctive features of the Khayyam Challenge are (i) its comprehensive coverage of various topics, including literary comprehension, mathematics, sciences, logic, intelligence testing, etc., aimed at assessing different facets of <b>LLMs</b> such as language comprehension, <b>reasoning,</b> and <b>information</b> <b>retrieval</b> across various educational stages, from lower primary school to upper secondary school (ii) its inclusion of rich metadata such as human response rates, difficulty levels, and descriptive answers (iii) its utilization of new data to avoid data contamination issues prevalent in existing frameworks (iv) its use of original, non-translated data tailored for Persian speakers, ensuring the framework is free from translation challenges and errors while encompassing cultural nuances (v) its inherent scalability for future data updates and evaluations without requiring special human effort. Previous works lacked an evaluation framework that combined all of these features into a single comprehensive <b>benchmark.</b> Furthermore, we evaluate a wide range of existing <b>LLMs</b> that support the Persian language, with statistical analyses and interpretations of their outputs.</p></p class="citation"></blockquote><h3 id=1542--25264-ada-leval-evaluating-long-context-llms-with-length-adaptable-benchmarks-chonghua-wang-et-al-2024>(15/42 | 25/264) Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks (Chonghua Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen. (2024)<br><strong>Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks</strong><br><button class=copy-to-clipboard title="Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06480v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06480v2.pdf filename=2404.06480v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> community has shown increasing interest in enhancing <b>LLMs&rsquo;</b> capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models&rsquo; long-text capabilities has become increasingly important. Existing long-text evaluation <b>benchmarks,</b> such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on <b>QA</b> and <b>summarization</b> tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest <b>LLMs</b> claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable <b>benchmark</b> for evaluating the long-context understanding of <b>LLMs.</b> Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of <b>LLMs&rsquo;</b> long context capabilities. These <b>benchmarks</b> support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current <b>LLMs,</b> especially in ultra-long-context settings. Our code is available at <a href=https://github.com/open-compass/Ada-LEval>https://github.com/open-compass/Ada-LEval</a>.</p></p class="citation"></blockquote><h3 id=1642--26264-take-a-look-at-it-rethinking-how-to-evaluate-language-model-jailbreak-hongyu-cai-et-al-2024>(16/42 | 26/264) Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak (Hongyu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, Z. Berkay Celik. (2024)<br><strong>Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak</strong><br><button class=copy-to-clipboard title="Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Language Generation, Natural Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06407v1.pdf filename=2404.06407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become increasingly integrated with various applications. To ensure that <b>LLMs</b> do not generate unsafe responses, they are aligned with safeguards that specify what content is restricted. However, such alignment can be bypassed to produce prohibited content using a technique commonly referred to as jailbreak. Different systems have been proposed to perform the jailbreak automatically. These systems rely on evaluation methods to determine whether a jailbreak attempt is successful. However, our analysis reveals that current jailbreak evaluation methods have two limitations. (1) Their objectives lack clarity and do not align with the goal of identifying unsafe responses. (2) They oversimplify the jailbreak result as a binary outcome, successful or not. In this paper, we propose three metrics, safeguard violation, informativeness, and relative truthfulness, to evaluate <b>language</b> <b>model</b> jailbreak. Additionally, we demonstrate how these metrics correlate with the goal of different malicious actors. To compute these metrics, we introduce a multifaceted approach that extends the <b>natural</b> <b>language</b> <b>generation</b> evaluation method after preprocessing the response. We evaluate our metrics on a <b>benchmark</b> dataset produced from three malicious intent datasets and three jailbreak systems. The <b>benchmark</b> dataset is labeled by three annotators. We compare our multifaceted approach with three existing jailbreak evaluation methods. Experiments demonstrate that our multifaceted evaluation outperforms existing methods, with F1 scores improving on average by 17% compared to existing baselines. Our findings motivate the need to move away from the binary view of the jailbreak problem and incorporate a more comprehensive evaluation to ensure the safety of the <b>language</b> <b>model.</b></p></p class="citation"></blockquote><h3 id=1742--27264-clinlinker-medical-entity-linking-of-clinical-concept-mentions-in-spanish-fernando-gallego-et-al-2024>(17/42 | 27/264) ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish (Fernando Gallego et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Gallego, Guillermo López-García, Luis Gasco-Sánchez, Martin Krallinger, Francisco J. Veredas. (2024)<br><strong>ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish</strong><br><button class=copy-to-clipboard title="ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Named Entity Recognition, Text Analysis, Text Mining<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06367v1.pdf filename=2404.06367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in natural language processing techniques, such as <b>named</b> <b>entity</b> <b>recognition</b> and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinical <b>text</b> <b>analysis.</b> This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedical <b>text</b> <b>mining:</b> initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a <b>contrastive-learning</b> <b>strategy</b> to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach&rsquo;s performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous <b>benchmarks</b> by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach&rsquo;s ability to address language-specific nuances and set a new <b>benchmark</b> in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.</p></p class="citation"></blockquote><h3 id=1842--28264-minicpm-unveiling-the-potential-of-small-language-models-with-scalable-training-strategies-shengding-hu-et-al-2024>(18/42 | 28/264) MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies (Shengding Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</strong><br><button class=copy-to-clipboard title="MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Domain Adaptation, Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06395v1.pdf filename=2404.06395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burgeoning interest in developing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with up to trillion parameters has been met with concerns regarding resource efficiency and practical expense, particularly given the immense cost of experimentation. This scenario underscores the importance of exploring the potential of Small Language Models (SLMs) as a resource-efficient alternative. In this context, we introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter variants, not only excel in their respective categories but also demonstrate capabilities on par with 7B-13B <b>LLMs.</b> While focusing on SLMs, our approach exhibits scalability in both model and data dimensions for future <b>LLM</b> research. Regarding model <b>scaling,</b> <b>we</b> employ extensive model wind tunnel experiments for stable and optimal <b>scaling.</b> <b>For</b> data <b>scaling,</b> <b>we</b> introduce a Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to continuous training and <b>domain</b> <b>adaptation.</b> We present an in-depth analysis of the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we are now able to efficiently study data-model <b>scaling</b> <b>law</b> without extensive retraining experiments on both axes of model and data, from which we derive the much higher compute optimal data-model ratio than Chinchilla Optimal. Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE and MiniCPM-128K, whose excellent performance further cementing MiniCPM&rsquo;s foundation in diverse SLM applications. MiniCPM models are available publicly at <a href=https://github.com/OpenBMB/MiniCPM>https://github.com/OpenBMB/MiniCPM</a> .</p></p class="citation"></blockquote><h3 id=1942--29264-low-cost-generation-and-evaluation-of-dictionary-example-sentences-bill-cai-et-al-2024>(19/42 | 29/264) Low-Cost Generation and Evaluation of Dictionary Example Sentences (Bill Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bill Cai, Clarence Boon Liang Ng, Daniel Tan, Shelvia Hotama. (2024)<br><strong>Low-Cost Generation and Evaluation of Dictionary Example Sentences</strong><br><button class=copy-to-clipboard title="Low-Cost Generation and Evaluation of Dictionary Example Sentences" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Automatic Evaluation, Zero-shot, Large Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06224v1.pdf filename=2404.06224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, <b>zero-shot</b> methods for the generation and evaluation of dictionary example sentences. We introduce a new <b>automatic</b> <b>evaluation</b> metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various <b>LLMs</b> and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using <b>masked</b> <b>language</b> <b>models</b> to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.</p></p class="citation"></blockquote><h3 id=2042--30264-vi-ood-a-unified-representation-learning-framework-for-textual-out-of-distribution-detection-li-ming-zhan-et-al-2024>(20/42 | 30/264) VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection (Li-Ming Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li-Ming Zhan, Bo Liu, Xiao-Ming Wu. (2024)<br><strong>VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection</strong><br><button class=copy-to-clipboard title="VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Deep Neural Network, Out-of-distribution, Representation Learning, Transformer, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06217v1.pdf filename=2404.06217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> (OOD) detection plays a crucial role in ensuring the safety and reliability of <b>deep</b> <b>neural</b> <b>networks</b> in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with <b>Transformers.</b> We first identify a key problem prevalent in existing OOD detection methods: the biased <b>representation</b> <b>learned</b> through the maximization of the conditional likelihood $p(y\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the <b>representations</b> <b>of</b> pre-trained <b>Transformers.</b> Through comprehensive experiments on various <b>text</b> <b>classification</b> tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \url{https://github.com/liam0949/LLM-OOD}.</p></p class="citation"></blockquote><h3 id=2142--31264-identifying-shopping-intent-in-product-qa-for-proactive-recommendations-besnik-fetahu-et-al-2024>(21/42 | 31/264) Identifying Shopping Intent in Product QA for Proactive Recommendations (Besnik Fetahu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Besnik Fetahu, Nachshon Cohen, Elad Haramaty, Liane Lewin-Eytan, Oleg Rokhlenko, Shervin Malmasi. (2024)<br><strong>Identifying Shopping Intent in Product QA for Proactive Recommendations</strong><br><button class=copy-to-clipboard title="Identifying Shopping Intent in Product QA for Proactive Recommendations" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Question Answering, Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06017v1.pdf filename=2404.06017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice assistants have become ubiquitous in smart devices allowing users to instantly access information via voice <b>questions.</b> <b>While</b> extensive research has been conducted in <b>question</b> <b>answering</b> for voice search, little attention has been paid on how to enable proactive <b>recommendations</b> from a voice assistant to its users. This is a highly challenging problem that often leads to user friction, mainly due to <b>recommendations</b> provided to the users at the wrong time. We focus on the domain of e-commerce, namely in identifying Shopping Product <b>Questions</b> <b>(SPQs),</b> where the user asking a product-related <b>question</b> <b>may</b> have an underlying shopping need. Identifying a user&rsquo;s shopping need allows voice assistants to enhance shopping experience by determining when to provide <b>recommendations,</b> such as product or deal <b>recommendations,</b> or proactive shopping actions <b>recommendation.</b> Identifying SPQs is a challenging problem and cannot be done from <b>question</b> <b>text</b> alone, and thus requires to infer latent user behavior patterns inferred from user&rsquo;s past shopping history. We propose features that capture the user&rsquo;s latent shopping behavior from their purchase history, and combine them using a novel Mixture-of-Experts (MoE) model. Our evaluation shows that the proposed approach is able to identify SPQs with a high score of F1=0.91. Furthermore, based on an online evaluation with real voice assistant users, we identify SPQs in real-time and recommend shopping actions to users to add the queried product into their shopping list. We demonstrate that we are able to accurately identify SPQs, as indicated by the significantly higher rate of added products to users&rsquo; shopping lists when being <b>prompted</b> after SPQs vs random PQs.</p></p class="citation"></blockquote><h3 id=2242--32264-freeeval-a-modular-framework-for-trustworthy-and-efficient-evaluation-of-large-language-models-zhuohao-yu-et-al-2024>(22/42 | 32/264) FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models (Zhuohao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang. (2024)<br><strong>FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models</strong><br><button class=copy-to-clipboard title="FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Automatic Evaluation, Fairness, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06003v1.pdf filename=2404.06003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> evaluation methodologies and datasets has led to a profound challenge: integrating state-of-the-art evaluation techniques cost-effectively while ensuring reliability, reproducibility, and efficiency. Currently, there is a notable absence of a unified and adaptable framework that seamlessly integrates various evaluation approaches. Moreover, the reliability of evaluation findings is often questionable due to potential data contamination, with the evaluation efficiency commonly overlooked when facing the substantial costs associated with <b>LLM</b> inference. In response to these challenges, we introduce FreeEval, a modular and scalable framework crafted to enable trustworthy and efficient <b>automatic</b> <b>evaluations</b> of <b>LLMs.</b> Firstly, FreeEval&rsquo;s unified abstractions simplify the integration and improve the transparency of diverse evaluation methodologies, encompassing dynamic evaluation that demand sophisticated <b>LLM</b> interactions. Secondly, the framework integrates meta-evaluation techniques like human evaluation and data contamination detection, which, along with dynamic evaluation modules in the platform, enhance the <b>fairness</b> of the evaluation outcomes. Lastly, FreeEval is designed with a high-performance infrastructure, including distributed computation and caching strategies, enabling extensive evaluations across multi-node, multi-GPU clusters for open-source and proprietary <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2342--33264-event-enhanced-retrieval-in-real-time-search-yanan-zhang-et-al-2024>(23/42 | 33/264) Event-enhanced Retrieval in Real-time Search (Yanan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanan Zhang, Xiaoling Bai, Tianhua Zhou. (2024)<br><strong>Event-enhanced Retrieval in Real-time Search</strong><br><button class=copy-to-clipboard title="Event-enhanced Retrieval in Real-time Search" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Information Retrieval, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05989v1.pdf filename=2404.05989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating <b>LLM</b> illusions. However, existing EBR models often face the &ldquo;semantic drift&rdquo; problem and insufficient focus on key <b>information,</b> <b>leading</b> to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event <b>information.</b> <b>To</b> tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate <b>contrastive</b> <b>learning</b> to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event <b>information</b> <b>in</b> events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on <b>prompt-tuning,</b> and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of <b>information</b> <b>retrieval.</b> The codes and dataset are available at <a href=https://github.com/open-event-hub/Event-enhanced_Retrieval>https://github.com/open-event-hub/Event-enhanced_Retrieval</a> .</p></p class="citation"></blockquote><h3 id=2442--34264-interplay-of-machine-translation-diacritics-and-diacritization-wei-rui-chen-et-al-2024>(24/42 | 34/264) Interplay of Machine Translation, Diacritics, and Diacritization (Wei-Rui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei-Rui Chen, Ife Adebara, Muhammad Abdul-Mageed. (2024)<br><strong>Interplay of Machine Translation, Diacritics, and Diacritization</strong><br><button class=copy-to-clipboard title="Interplay of Machine Translation, Diacritics, and Diacritization" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: High-Resource, Low-Resource, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05943v1.pdf filename=2404.05943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate two research questions: (1) how do <b>machine</b> <b>translation</b> <b>(MT)</b> and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on <b>MT</b> performance. We examine these two questions in both <b>high-resource</b> (HR) and <b>low-resource</b> (LR) settings across 55 different languages (36 African languages and 19 European languages). For (1), results show that diacritization significantly benefits <b>MT</b> in the LR scenario, doubling or even tripling performance for some languages, but harms <b>MT</b> in the HR scenario. We find that <b>MT</b> harms diacritization in LR but benefits significantly in HR for some languages. For (2), <b>MT</b> performance is similar regardless of diacritics being kept or removed. In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models. Overall, our work provides insights for developing <b>MT</b> and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate.</p></p class="citation"></blockquote><h3 id=2542--35264-what-is-your-favorite-gender-mlm-gender-bias-evaluation-in-multilingual-masked-language-models-jeongrok-yu-et-al-2024>(25/42 | 35/264) What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models (Jeongrok Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeongrok Yu, Seong Ug Kim, Jacob Choi, Jinho D. Choi. (2024)<br><strong>What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models</strong><br><button class=copy-to-clipboard title="What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Masked Language Model, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06621v1.pdf filename=2404.06621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bias is a disproportionate prejudice in favor of one side against another. Due to the success of <b>transformer-based</b> <b>Masked</b> <b>Language</b> <b>Models</b> <b>(MLMs)</b> and their impact on many NLP tasks, a systematic evaluation of bias in these models is needed more than ever. While many studies have evaluated gender bias in English <b>MLMs,</b> only a few works have been conducted for the task in other languages. This paper proposes a multilingual approach to estimate gender bias in <b>MLMs</b> from 5 languages: Chinese, English, German, Portuguese, and Spanish. Unlike previous work, our approach does not depend on parallel corpora coupled with English to detect gender bias in other languages using multilingual lexicons. Moreover, a novel model-based method is presented to generate sentence pairs for a more robust analysis of gender bias, compared to the traditional lexicon-based method. For each language, both the lexicon-based and model-based methods are applied to create two datasets respectively, which are used to evaluate gender bias in an <b>MLM</b> specifically trained for that language using one existing and 3 new scoring metrics. Our results show that the previous approach is data-sensitive and not stable as it does not remove contextual dependencies irrelevant to gender. In fact, the results often flip when different scoring metrics are used on the same dataset, suggesting that gender bias should be studied on a large dataset using multiple evaluation metrics for best practice.</p></p class="citation"></blockquote><h3 id=2642--36264-comparing-two-model-designs-for-clinical-note-generation-is-an-llm-a-useful-evaluator-of-consistency-nathan-brake-et-al-2024>(26/42 | 36/264) Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency? (Nathan Brake et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Brake, Thomas Schaaf. (2024)<br><strong>Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?</strong><br><button class=copy-to-clipboard title="Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06503v1.pdf filename=2404.06503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X <b>Transformer</b> models and observe that both methods lead to similar <b>ROUGE</b> values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that <b>LLMs</b> like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an <b>LLM</b> to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.</p></p class="citation"></blockquote><h3 id=2742--37264-pitfalls-of-conversational-llms-on-news-debiasing-ipek-baris-schlicht-et-al-2024>(27/42 | 37/264) Pitfalls of Conversational LLMs on News Debiasing (Ipek Baris Schlicht et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek. (2024)<br><strong>Pitfalls of Conversational LLMs on News Debiasing</strong><br><button class=copy-to-clipboard title="Pitfalls of Conversational LLMs on News Debiasing" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06488v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06488v1.pdf filename=2404.06488v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses debiasing in news editing and evaluates the effectiveness of conversational <b>Large</b> <b>Language</b> <b>Models</b> in this task. We designed an evaluation checklist tailored to news editors&rsquo; perspectives, obtained generated texts from three popular conversational models using a subset of a publicly available dataset in media bias, and evaluated the texts according to the designed checklist. Furthermore, we examined the models as evaluator for checking the quality of debiased model outputs. Our findings indicate that none of the <b>LLMs</b> are perfect in debiasing. Notably, some models, including <b>ChatGPT,</b> introduced unnecessary changes that may impact the author&rsquo;s style and create misinformation. Lastly, we show that the models do not perform as proficiently as domain experts in evaluating the quality of debiased outputs.</p></p class="citation"></blockquote><h3 id=2842--38264-clue-instruct-text-based-clue-generation-for-educational-crossword-puzzles-andrea-zugarini-et-al-2024>(28/42 | 38/264) Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles (Andrea Zugarini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Zugarini, Kamyar Zeinalipour, Surya Sai Kadali, Marco Maggini, Marco Gori, Leonardo Rigutini. (2024)<br><strong>Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles</strong><br><button class=copy-to-clipboard title="Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06186v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06186v1.pdf filename=2404.06186v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crossword puzzles are popular linguistic games often used as tools to engage students in learning. Educational crosswords are characterized by less cryptic and more factual clues that distinguish them from traditional crossword puzzles. Despite there exist several publicly available clue-answer pair databases for traditional crosswords, educational clue-answer pairs datasets are missing. In this article, we propose a methodology to build educational clue generation datasets that can be used to instruct <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> By gathering from Wikipedia pages informative content associated with relevant keywords, we use <b>Large</b> <b>Language</b> <b>Models</b> to automatically generate pedagogical clues related to the given input keyword and its context. With such an approach, we created clue-instruct, a dataset containing 44,075 unique examples with text-keyword pairs associated with three distinct crossword clues. We used clue-instruct to instruct different <b>LLMs</b> to generate educational clues from a given input content and keyword. Both human and <b>automatic</b> <b>evaluations</b> confirmed the quality of the generated clues, thus validating the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=2942--39264-smurfcat-at-semeval-2024-task-6-leveraging-synthetic-data-for-hallucination-detection-elisei-rykov-et-al-2024>(29/42 | 39/264) SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection (Elisei Rykov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elisei Rykov, Yana Shishkina, Kseniia Petrushina, Kseniia Titova, Sergey Petrakov, Alexander Panchenko. (2024)<br><strong>SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection</strong><br><button class=copy-to-clipboard title="SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Supervised Learning, Supervised Learning, Hallucination Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06137v1.pdf filename=2404.06137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present our novel systems developed for the SemEval-2024 <b>hallucination</b> <b>detection</b> task. Our investigation spans a range of strategies to compare model predictions with reference standards, encompassing diverse baselines, the refinement of pre-trained encoders through <b>supervised</b> <b>learning,</b> and an ensemble approaches utilizing several high-performing models. Through these explorations, we introduce three distinct methods that exhibit strong performance metrics. To amplify our training data, we generate additional training samples from unlabelled training subset. Furthermore, we provide a detailed comparative analysis of our approaches. Notably, our premier method achieved a commendable 9th place in the competition&rsquo;s model-agnostic track and 17th place in model-aware track, highlighting its effectiveness and potential.</p></p class="citation"></blockquote><h3 id=3042--40264-making-old-kurdish-publications-processable-by-augmenting-available-optical-character-recognition-engines-blnd-yaseen-et-al-2024>(30/42 | 40/264) Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines (Blnd Yaseen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Blnd Yaseen, Hossein Hassani. (2024)<br><strong>Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines</strong><br><button class=copy-to-clipboard title="Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Optical Character Recognition, Optical Character Recognition, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06101v1.pdf filename=2404.06101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kurdish libraries have many historical publications that were printed back in the early days when printing devices were brought to Kurdistan. Having a good <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> to help process these publications and contribute to the Kurdish languages resources which is crucial as Kurdish is considered a <b>low-resource</b> language. Current <b>OCR</b> systems are unable to extract text from historical documents as they have many issues, including being damaged, very fragile, having many marks left on them, and often written in non-standard fonts and more. This is a massive obstacle in processing these documents as currently processing them requires manual typing which is very time-consuming. In this study, we adopt an open-source <b>OCR</b> framework by Google, Tesseract version 5.0, that has been used to extract text for various languages. Currently, there is no public dataset, and we developed our own by collecting historical documents from Zheen Center for Documentation and Research, which were printed before 1950 and resulted in a dataset of 1233 images of lines with transcription of each. Then we used the Arabic model as our base model and trained the model using the dataset. We used different methods to evaluate our model, Tesseracts built-in evaluator lstmeval indicated a Character Error Rate (CER) of 0.755%. Additionally, Ocreval demonstrated an average character accuracy of 84.02%. Finally, we developed a web application to provide an easy- to-use interface for end-users, allowing them to interact with the model by inputting an image of a page and extracting the text. Having an extensive dataset is crucial to develop <b>OCR</b> systems with reasonable accuracy, as currently, no public datasets are available for historical Kurdish documents; this posed a significant challenge in our work. Additionally, the unaligned spaces between characters and words proved another challenge with our work.</p></p class="citation"></blockquote><h3 id=3142--41264-call-for-papers-the-2nd-babylm-challenge-sample-efficient-pretraining-on-a-developmentally-plausible-corpus-leshem-choshen-et-al-2024>(31/42 | 41/264) [Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus (Leshem Choshen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leshem Choshen, Ryan Cotterell, Michael Y. Hu, Tal Linzen, Aaron Mueller, Candace Ross, Alex Warstadt, Ethan Wilcox, Adina Williams, Chengxu Zhuang. (2024)<br><strong>[Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus</strong><br><button class=copy-to-clipboard title="[Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06214v1.pdf filename=2404.06214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>After last year&rsquo;s successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year&rsquo;s competition are as follows: First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired <b>benchmarks,</b> or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a <b>multimodal</b> <b>vision-and-language</b> track, and will release a corpus of 50% text-only and 50% <b>image-text</b> <b>multimodal</b> data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year&rsquo;s challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year&rsquo;s competition, and provide answers to frequently asked questions from last year&rsquo;s challenge.</p></p class="citation"></blockquote><h3 id=3242--42264-ruler-whats-the-real-context-size-of-your-long-context-language-models-cheng-ping-hsieh-et-al-2024>(32/42 | 42/264) RULER: What&rsquo;s the Real Context Size of Your Long-Context Language Models? (Cheng-Ping Hsieh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Boris Ginsburg. (2024)<br><strong>RULER: What&rsquo;s the Real Context Size of Your Long-Context Language Models?</strong><br><button class=copy-to-clipboard title="RULER: What's the Real Context Size of Your Long-Context Language Models?" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06654v1.pdf filename=2404.06654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the &ldquo;needle&rdquo;) from long distractor texts (the &ldquo;haystack&rdquo;), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic <b>benchmark</b> RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate ten long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only four models <b>(GPT-4,</b> Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.</p></p class="citation"></blockquote><h3 id=3342--43264-event-extraction-in-basque-typologically-motivated-cross-lingual-transfer-learning-analysis-mikel-zubillaga-et-al-2024>(33/42 | 43/264) Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis (Mikel Zubillaga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikel Zubillaga, Oscar Sainz, Ainara Estarrona, Oier Lopez de Lacalle, Eneko Agirre. (2024)<br><strong>Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis</strong><br><button class=copy-to-clipboard title="Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Low-Resource, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06392v1.pdf filename=2404.06392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-lingual <b>transfer-learning</b> <b>is</b> widely used in Event Extraction for <b>low-resource</b> languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual <b>transfer,</b> <b>an</b> under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on <b>transfer</b> <b>quality.</b> Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual <b>transfer.</b> <b>In</b> contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.</p></p class="citation"></blockquote><h3 id=3442--44264-surveyagent-a-conversational-system-for-personalized-and-efficient-research-survey-xintao-wang-et-al-2024>(34/42 | 44/264) SurveyAgent: A Conversational System for Personalized and Efficient Research Survey (Xintao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei Shi, Xuyang Ge, Rui Xu, Yanghua Xiao. (2024)<br><strong>SurveyAgent: A Conversational System for Personalized and Efficient Research Survey</strong><br><button class=copy-to-clipboard title="SurveyAgent: A Conversational System for Personalized and Efficient Research Survey" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Recommendation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06364v1.pdf filename=2404.06364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper <b>recommendations,</b> and <b>question-answering,</b> <b>a</b> comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, <b>Recommendation</b> for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent&rsquo;s effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.</p></p class="citation"></blockquote><h3 id=3542--45264-nemo-dataset-of-emotional-speech-in-polish-iwona-christop-2024>(35/42 | 45/264) nEMO: Dataset of Emotional Speech in Polish (Iwona Christop, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iwona Christop. (2024)<br><strong>nEMO: Dataset of Emotional Speech in Polish</strong><br><button class=copy-to-clipboard title="nEMO: Dataset of Emotional Speech in Polish" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Dialogue System, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06292v1.pdf filename=2404.06292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech <b>emotion</b> <b>recognition</b> has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of <b>dialogue</b> <b>systems.</b> However, a major issue in this field is the lack of datasets that adequately represent basic <b>emotional</b> <b>states</b> across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of <b>emotional</b> <b>speech</b> in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six <b>emotional</b> <b>states:</b> anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).</p></p class="citation"></blockquote><h3 id=3642--46264-thoughtsculpt-reasoning-with-intermediate-revision-and-search-yizhou-chi-et-al-2024>(36/42 | 46/264) THOUGHTSCULPT: Reasoning with Intermediate Revision and Search (Yizhou Chi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhou Chi, Kevin Yang, Dan Klein. (2024)<br><strong>THOUGHTSCULPT: Reasoning with Intermediate Revision and Search</strong><br><button class=copy-to-clipboard title="THOUGHTSCULPT: Reasoning with Intermediate Revision and Search" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05966v1.pdf filename=2404.05966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present THOUGHTSCULPT, a general <b>reasoning</b> and search method for tasks with outputs that can be decomposed into components. THOUGHTSCULPT explores a search tree of potential solutions using Monte Carlo Tree Search (MCTS), building solutions one action at a time and evaluating according to any domain-specific heuristic, which in practice is often simply an <b>LLM</b> evaluator. Critically, our action space includes revision actions: THOUGHTSCULPT may choose to revise part of its previous output rather than continuing to build the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art <b>reasoning</b> methods across three challenging tasks: Story Outline Improvement (up to +30% interestingness), Mini-Crosswords Solving (up to +16% word success rate), and Constrained Generation (up to +10% concept coverage).</p></p class="citation"></blockquote><h3 id=3742--47264-exploring-the-necessity-of-visual-modality-in-multimodal-machine-translation-using-authentic-datasets-zi-long-et-al-2024>(37/42 | 47/264) Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets (Zi Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zi Long, Zhenhao Tang, Xianghua Fu, Jian Chen, Shilong Hou, Jinze Lyu. (2024)<br><strong>Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets</strong><br><button class=copy-to-clipboard title="Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06107v1.pdf filename=2404.06107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research in the field of <b>multimodal</b> <b>machine</b> <b>translation</b> (MMT) has indicated that the visual modality is either dispensable or offers only marginal advantages. However, most of these conclusions are drawn from the analysis of experimental results based on a limited set of bilingual sentence-image pairs, such as Multi30k. In these kinds of datasets, the content of one bilingual parallel sentence pair must be well represented by a manually annotated image, which is different from the real-world translation scenario. In this work, we adhere to the universal <b>multimodal</b> <b>machine</b> <b>translation</b> framework proposed by Tang et al. (2022). This approach allows us to delve into the impact of the visual modality on translation efficacy by leveraging real-world translation datasets. Through a comprehensive exploration via probing tasks, we find that the visual modality proves advantageous for the majority of authentic translation datasets. Notably, the translation performance primarily hinges on the alignment and coherence between textual and visual contents. Furthermore, our results suggest that visual information serves a supplementary role in <b>multimodal</b> translation and can be substituted.</p></p class="citation"></blockquote><h3 id=3842--48264-fairpair-a-robust-evaluation-of-biases-in-language-models-through-paired-perturbations-jane-dwivedi-yu-et-al-2024>(38/42 | 48/264) FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations (Jane Dwivedi-Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jane Dwivedi-Yu, Raaz Dwivedi, Timo Schick. (2024)<br><strong>FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations</strong><br><button class=copy-to-clipboard title="FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06619v1.pdf filename=2404.06619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes, and being able to capture biases that appear in typical usage (rather than just extreme, rare cases). Relatedly, bias evaluation should surface not only egregious biases but also ones that are subtle and commonplace, such as a likelihood for talking about appearances with regard to women. We present FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage. FairPair operates through <b>counterfactual</b> pairs, but crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison. Additionally, unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability. We present an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women.</p></p class="citation"></blockquote><h3 id=3942--49264-generalizable-sarcasm-detection-is-just-around-the-corner-of-course-hyewon-jang-et-al-2024>(39/42 | 49/264) Generalizable Sarcasm Detection Is Just Around The Corner, Of Course! (Hyewon Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyewon Jang, Diego Frassinelli. (2024)<br><strong>Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!</strong><br><button class=copy-to-clipboard title="Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06357v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06357v2.pdf filename=2404.06357v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tested the robustness of sarcasm detection models by examining their behavior when <b>fine-tuned</b> on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking). We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset). For intra-dataset predictions, models consistently performed better when <b>fine-tuned</b> with third-party labels rather than with author labels. For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains. Compared to the existing datasets, models <b>fine-tuned</b> on the new dataset we release in this work showed the highest generalizability to other datasets. With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles. We argue that future sarcasm research should take the broad scope of sarcasm into account.</p></p class="citation"></blockquote><h3 id=4042--50264-finding-fake-reviews-in-e-commerce-platforms-by-using-hybrid-algorithms-mathivanan-periasamy-et-al-2024>(40/42 | 50/264) Finding fake reviews in e-commerce platforms by using hybrid algorithms (Mathivanan Periasamy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathivanan Periasamy, Rohith Mahadevan, Bagiya Lakshmi S, Raja CSP Raman, Hasan Kumar S, Jasper Jessiman. (2024)<br><strong>Finding fake reviews in e-commerce platforms by using hybrid algorithms</strong><br><button class=copy-to-clipboard title="Finding fake reviews in e-commerce platforms by using hybrid algorithms" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06339v1.pdf filename=2404.06339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentiment</b> <b>analysis,</b> a vital component in natural language processing, plays a crucial role in understanding the underlying emotions and opinions expressed in textual data. In this paper, we propose an innovative ensemble approach for <b>sentiment</b> <b>analysis</b> for finding fake reviews that amalgamate the predictive capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Decision Tree classifiers. Our ensemble architecture strategically combines these diverse models to capitalize on their strengths while mitigating inherent weaknesses, thereby achieving superior accuracy and robustness in fake review prediction. By combining all the models of our classifiers, the predictive performance is boosted and it also fosters adaptability to varied linguistic patterns and nuances present in real-world datasets. The metrics accounted for on fake reviews demonstrate the efficacy and competitiveness of the proposed ensemble method against traditional single-model approaches. Our findings underscore the potential of ensemble techniques in advancing the state-of-the-art in finding fake reviews using hybrid algorithms, with implications for various applications in different social media and e-platforms to find the best reviews and neglect the fake ones, eliminating puffery and bluffs.</p></p class="citation"></blockquote><h3 id=4142--51264-detection-of-fields-of-applications-in-biomedical-abstracts-with-the-support-of-argumentation-elements-mariana-neves-2024>(41/42 | 51/264) Detection of fields of applications in biomedical abstracts with the support of argumentation elements (Mariana Neves, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mariana Neves. (2024)<br><strong>Detection of fields of applications in biomedical abstracts with the support of argumentation elements</strong><br><button class=copy-to-clipboard title="Detection of fields of applications in biomedical abstracts with the support of argumentation elements" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06121v1.pdf filename=2404.06121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Focusing on particular facts, instead of the complete text, can potentially improve searching for specific information in the scientific literature. In particular, argumentative elements allow focusing on specific parts of a publication, e.g., the background section or the claims from the authors. We evaluated some tools for the extraction of argumentation elements for a specific task in biomedicine, namely, for detecting the fields of the application in a biomedical publication, e.g, whether it addresses the problem of disease diagnosis or drug development. We performed experiments with the PubMedBERT pre-trained model, which was <b>fine-tuned</b> on a specific corpus for the task. We compared the use of title and abstract to restricting to only some argumentative elements. The top F1 scores ranged from 0.22 to 0.84, depending on the field of application. The best argumentative labels were the ones related the conclusion and background sections of an abstract.</p></p class="citation"></blockquote><h3 id=4242--52264-leveraging-interesting-facts-to-enhance-user-engagement-with-conversational-interfaces-nikhita-vedula-et-al-2024>(42/42 | 52/264) Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces (Nikhita Vedula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhita Vedula, Giuseppe Castellucci, Eugene Agichtein, Oleg Rokhlenko, Shervin Malmasi. (2024)<br><strong>Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces</strong><br><button class=copy-to-clipboard title="Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06659v1.pdf filename=2404.06659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational Task Assistants (CTAs) guide users in performing a multitude of activities, such as making recipes. However, ensuring that interactions remain engaging, interesting, and enjoyable for CTA users is not trivial, especially for time-consuming or challenging tasks. Grounded in psychological theories of human interest, we propose to engage users with contextual and interesting statements or facts during interactions with a <b>multi-modal</b> CTA, to reduce fatigue and task abandonment before a task is complete. To operationalize this idea, we train a high-performing classifier (82% F1-score) to automatically identify relevant and interesting facts for users. We use it to create an annotated dataset of task-specific interesting facts for the domain of cooking. Finally, we design and validate a dialogue policy to incorporate the identified relevant and interesting facts into a conversation, to improve user engagement and task completion. Live testing on a leading <b>multi-modal</b> voice assistant shows that 66% of the presented facts were received positively, leading to a 40% gain in the user satisfaction rating, and a 37% increase in conversation length. These findings emphasize that strategically incorporating interesting facts into the CTA experience can promote real-world user participation for guided task interactions.</p></p class="citation"></blockquote><h2 id=cscv-70>cs.CV (70)</h2><h3 id=170--53264-playing-to-vision-foundation-models-strengths-in-stereo-matching-chuang-wei-liu-et-al-2024>(1/70 | 53/264) Playing to Vision Foundation Model&rsquo;s Strengths in Stereo Matching (Chuang-Wei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuang-Wei Liu, Qijun Chen, Rui Fan. (2024)<br><strong>Playing to Vision Foundation Model&rsquo;s Strengths in Stereo Matching</strong><br><button class=copy-to-clipboard title="Playing to Vision Foundation Model's Strengths in Stereo Matching" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 80<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Foundation Model, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06261v1.pdf filename=2404.06261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards <b>vision</b> <b>foundation</b> <b>models</b> (VFM), particularly those developed based on <b>vision</b> <b>Transformers</b> (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric <b>vision</b> <b>tasks.</b> This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.</p></p class="citation"></blockquote><h3 id=270--54264-actnetformer-transformer-resnet-hybrid-method-for-semi-supervised-action-recognition-in-videos-sharana-dharshikgan-suresh-dass-et-al-2024>(2/70 | 54/264) ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos (Sharana Dharshikgan Suresh Dass et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan. (2024)<br><strong>ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos</strong><br><button class=copy-to-clipboard title="ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: Artificial intelligence, Computer vision, Machine learning, Deep
learning, Human-computer Interaction, I-2; I-2-9; I-2-10; I-3-3; I-4-5, cs-AI, cs-CV, cs-HC, cs-LG, cs-MM, cs.CV<br>Keyword Score: 75<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Contrastive Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network, Representation Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06243v1.pdf filename=2404.06243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human action or activity recognition in videos is a fundamental task in computer vision with applications in surveillance and monitoring, self-driving cars, sports analytics, human-robot interaction and many more. Traditional <b>supervised</b> methods require large annotated datasets for training, which are expensive and time-consuming to acquire. This work proposes a novel approach using Cross-Architecture Pseudo-Labeling with <b>contrastive</b> <b>learning</b> for semi-supervised action recognition. Our framework leverages both labeled and unlabelled data to robustly learn action <b>representations</b> <b>in</b> videos, combining pseudo-labeling with <b>contrastive</b> <b>learning</b> for effective learning from both types of samples. We introduce a novel cross-architecture approach where 3D <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (3D <b>CNNs)</b> and video <b>transformers</b> (VIT) are utilised to capture different aspects of action <b>representations;</b> <b>hence</b> we call it ActNetFormer. The 3D <b>CNNs</b> excel at capturing spatial features and local dependencies in the temporal domain, while VIT excels at capturing long-range dependencies across frames. By integrating these complementary architectures within the ActNetFormer framework, our approach can effectively capture both local and global contextual information of an action. This comprehensive <b>representation</b> <b>learning</b> enables the model to achieve better performance in semi-supervised action recognition tasks by leveraging the strengths of each of these architectures. Experimental results on standard action recognition datasets demonstrate that our approach performs better than the existing methods, achieving state-of-the-art performance with only a fraction of labeled data. The official website of this work is available at: <a href=https://github.com/rana2149/ActNetFormer>https://github.com/rana2149/ActNetFormer</a>.</p></p class="citation"></blockquote><h3 id=370--55264-anchor-based-robust-finetuning-of-vision-language-models-jinwei-han-et-al-2024>(3/70 | 55/264) Anchor-based Robust Finetuning of Vision-Language Models (Jinwei Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao, Gui-Song Xia. (2024)<br><strong>Anchor-based Robust Finetuning of Vision-Language Models</strong><br><button class=copy-to-clipboard title="Anchor-based Robust Finetuning of Vision-Language Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Out-of-distribution, Zero-shot, Image2text, Vision-and-Language, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06244v1.pdf filename=2404.06244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We aim at <b>finetuning</b> a <b>vision-language</b> model without hurting its <b>out-of-distribution</b> (OOD) generalization. We address two types of OOD generalization, i.e., i) domain shift such as natural to sketch images, and ii) <b>zero-shot</b> <b>capability</b> to recognize the category that was not contained in the <b>finetune</b> data. Arguably, the diminished OOD generalization after <b>finetuning</b> stems from the excessively simplified <b>finetuning</b> target, which only provides the class information, such as ``a photo of a [CLASS]&rsquo;&rsquo;. This is distinct from the process in that CLIP was pretrained, where there is abundant text supervision with rich semantic information. Therefore, we propose to compensate for the <b>finetune</b> process using auxiliary supervision with rich semantic information, which acts as anchors to preserve the OOD generalization. Specifically, two types of anchors are elaborated in our method, including i) text-compensated anchor which uses the images from the <b>finetune</b> set but enriches the text supervision from a pretrained captioner, ii) <b>image-text-pair</b> anchor which is retrieved from the dataset similar to pretraining data of CLIP according to the downstream task, associating with the original CLIP text with rich semantics. Those anchors are utilized as auxiliary semantic information to maintain the original feature space of CLIP, thereby preserving the OOD generalization capabilities. Comprehensive experiments demonstrate that our method achieves in-distribution performance akin to conventional <b>finetuning</b> while attaining new state-of-the-art results on domain shift and <b>zero-shot</b> <b>learning</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=470--56264-internlm-xcomposer2-4khd-a-pioneering-large-vision-language-model-handling-resolutions-from-336-pixels-to-4k-hd-xiaoyi-dong-et-al-2024>(4/70 | 56/264) InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD (Xiaoyi Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang. (2024)<br><strong>InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD</strong><br><button class=copy-to-clipboard title="InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Vision Transformer, Benchmarking, GPT, Gemini, Transformer, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06512v1.pdf filename=2404.06512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Large <b>Vision-Language</b> <b>Model</b> (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained <b>Vision</b> <b>Transformer</b> (ViT) (336 x 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses <b>GPT-4V</b> and <b>Gemini</b> Pro in 10 of the 16 <b>benchmarks.</b> The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at <a href=https://github.com/InternLM/InternLM-XComposer>https://github.com/InternLM/InternLM-XComposer</a>.</p></p class="citation"></blockquote><h3 id=570--57264-from-barlow-twins-to-triplet-training-differentiating-dementia-with-limited-data-yitong-li-et-al-2024>(5/70 | 57/264) From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data (Yitong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yitong Li, Tom Nuno Wolf, Sebastian Pölsterl, Igor Yakushev, Dennis M. Hedderich, Christian Wachinger. (2024)<br><strong>From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data</strong><br><button class=copy-to-clipboard title="From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Deep Neural Network, Deep Neural Network, Fine-tuning, Self-Distillation, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06253v1.pdf filename=2404.06253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training <b>deep</b> <b>neural</b> <b>networks</b> <b>(DNNs).</b> <b>Self-supervised</b> <b>learning</b> shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) <b>self-supervised</b> <b>pre-training</b> on unlabeled data with Barlow Twins, (ii) <b>self-distillation</b> on task-related data, and (iii) <b>fine-tuning</b> on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at <a href=https://github.com/ai-med/TripletTraining>https://github.com/ai-med/TripletTraining</a>.</p></p class="citation"></blockquote><h3 id=670--58264-omnifusion-technical-report-elizaveta-goncharova-et-al-2024>(6/70 | 58/264) OmniFusion Technical Report (Elizaveta Goncharova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elizaveta Goncharova, Anton Razzhigaev, Matvey Mikhalchuk, Maxim Kurkin, Irina Abdullaeva, Matvey Skripkin, Ivan Oseledets, Denis Dimitrov, Andrey Kuznetsov. (2024)<br><strong>OmniFusion Technical Report</strong><br><button class=copy-to-clipboard title="OmniFusion Technical Report" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 6804, 68T50 (Primary), I-2-7; I-2-10; I-4-9, cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 59<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Mistral, Transformer, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06212v1.pdf filename=2404.06212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Last year, <b>multimodal</b> architectures served up a revolution in AI-based approaches and solutions, extending the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLM).</b> We propose an \textit{OmniFusion} model based on a pretrained <b>LLM</b> and adapters for visual modality. We evaluated and compared several architecture design principles for better text and visual data coupling: MLP and <b>transformer</b> adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their fusing approach, image encoding method (whole image or tiles encoding) and two 7B <b>LLMs</b> (the proprietary one and open-source <b>Mistral).</b> Experiments on 8 visual-language <b>benchmarks</b> show the top score for the best OmniFusion setup in terms of different <b>VQA</b> tasks in comparison with open-source LLaVA-like solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We also propose a variety of situations, where OmniFusion provides highly-detailed answers in different domains: housekeeping, sightseeing, culture, medicine, handwritten and scanned equations recognition, etc. <b>Mistral-based</b> OmniFusion model is an open-source solution with weights, training and inference scripts available at <a href=https://github.com/AIRI-Institute/OmniFusion>https://github.com/AIRI-Institute/OmniFusion</a>.</p></p class="citation"></blockquote><h3 id=770--59264-mambaad-exploring-state-space-models-for-multi-class-unsupervised-anomaly-detection-haoyang-he-et-al-2024>(7/70 | 59/264) MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection (Haoyang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyang He, Yuhu Bai, Jiangning Zhang, Qingdong He, Hongxu Chen, Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, Lei Xie. (2024)<br><strong>MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Convolutional Neural Network, Anomaly Detection, Convolution, Convolutional Neural Network, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06564v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06564v2.pdf filename=2404.06564v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>anomaly</b> <b>detection</b> have seen the efficacy of CNN- and <b>transformer-based</b> approaches. However, <b>CNNs</b> struggle with long-range dependencies, while <b>transformers</b> are burdened by quadratic computational complexity. Mamba-based models, with their superior long-range modeling and linear efficiency, have garnered substantial attention. This study pioneers the application of Mamba to multi-class <b>unsupervised</b> <b>anomaly</b> <b>detection,</b> presenting MambaAD, which consists of a pre-trained encoder and a Mamba decoder featuring (Locality-Enhanced State Space) LSS modules at multi-scales. The proposed LSS module, integrating parallel cascaded (Hybrid State Space) HSS blocks and multi-kernel <b>convolutions</b> operations, effectively captures both long-range and local information. The HSS block, utilizing (Hybrid Scanning) HS encoders, encodes feature maps into five scanning methods and eight directions, thereby strengthening global connections through the (State Space Model) SSM. The use of Hilbert scanning and eight directions significantly improves feature sequence modeling. Comprehensive experiments on six diverse <b>anomaly</b> <b>detection</b> datasets and seven metrics demonstrate state-of-the-art performance, substantiating the method&rsquo;s effectiveness.</p></p class="citation"></blockquote><h3 id=870--60264-morevqa-exploring-modular-reasoning-models-for-video-question-answering-juhong-min-et-al-2024>(8/70 | 60/264) MoReVQA: Exploring Modular Reasoning Models for Video Question Answering (Juhong Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid. (2024)<br><strong>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</strong><br><button class=copy-to-clipboard title="MoReVQA: Exploring Modular Reasoning Models for Video Question Answering" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Grounding, Question Answering, Reasoning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06511v1.pdf filename=2404.06511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the task of video <b>question</b> <b>answering</b> (videoQA) via a decomposed multi-stage, modular <b>reasoning</b> framework. Previous modular methods have shown promise with a single planning stage ungrounded in visual content. However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings. Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a <b>grounding</b> stage, and a final <b>reasoning</b> stage in conjunction with an external memory. All stages are training-free, and performed using <b>few-shot</b> <b>prompting</b> of large models, creating interpretable intermediate outputs at each stage. By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA <b>benchmarks</b> (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning).</p></p class="citation"></blockquote><h3 id=970--61264-can-feedback-enhance-semantic-grounding-in-large-vision-language-models-yuan-hong-liao-et-al-2024>(9/70 | 61/264) Can Feedback Enhance Semantic Grounding in Large Vision-Language Models? (Yuan-Hong Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna. (2024)<br><strong>Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?</strong><br><button class=copy-to-clipboard title="Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Grounding, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06510v1.pdf filename=2404.06510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing semantic <b>grounding</b> abilities in <b>Vision-Language</b> Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore whether VLMs can improve their semantic <b>grounding</b> by &ldquo;receiving&rdquo; feedback, without requiring in-domain data, <b>fine-tuning,</b> or modifications to the network architectures. We systematically analyze this hypothesis using a feedback mechanism composed of a binary signal. We find that if <b>prompted</b> appropriately, VLMs can utilize feedback both in a single step and iteratively, showcasing the potential of feedback as an alternative technique to improve <b>grounding</b> in internet-scale VLMs. Furthermore, VLMs, like <b>LLMs,</b> struggle to self-correct errors out-of-the-box. However, we find that this issue can be mitigated via a binary verification mechanism. Finally, we explore the potential and limitations of amalgamating these findings and applying them iteratively to automatically enhance VLMs&rsquo; <b>grounding</b> performance, showing <b>grounding</b> accuracy consistently improves using automated feedback across all models in all settings investigated. Overall, our iterative framework improves semantic <b>grounding</b> in VLMs by more than 15 accuracy points under noise-free feedback and up to 5 accuracy points under a simple automated binary verification mechanism. The project website is hosted at <a href=https://andrewliao11.github.io/vlms_feedback>https://andrewliao11.github.io/vlms_feedback</a></p></p class="citation"></blockquote><h3 id=1070--62264-exploring-the-potential-of-large-foundation-models-for-open-vocabulary-hoi-detection-ting-lei-et-al-2024>(10/70 | 62/264) Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection (Ting Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ting Lei, Shaofeng Yin, Yang Liu. (2024)<br><strong>Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection</strong><br><button class=copy-to-clipboard title="Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Foundation Model, Zero-shot, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06194v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06194v2.pdf filename=2404.06194v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior <b>zero-shot</b> HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and overlook the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>GPT</b> models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at <a href=https://github.com/ltttpku/CMD-SE-release>https://github.com/ltttpku/CMD-SE-release</a>.</p></p class="citation"></blockquote><h3 id=1170--63264-hfnerf-learning-human-biomechanic-features-with-neural-radiance-fields-arnab-dey-et-al-2024>(11/70 | 63/264) HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields (Arnab Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet. (2024)<br><strong>HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Augmented Reality (AR), Augmented Reality (AR), Foundation Model, Geometry, Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06152v1.pdf filename=2404.06152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including <b>Augmented</b> <b>Reality</b> <b>(AR)/Virtual</b> Reality <b>(VR).</b> HFNeRF leverages 2D pre-trained <b>foundation</b> <b>models</b> toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, <b>geometry,</b> and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.</p></p class="citation"></blockquote><h3 id=1270--64264-greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs-zander-w-blasingame-et-al-2024>(12/70 | 64/264) Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs (Zander W. Blasingame et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zander W. Blasingame, Chen Liu. (2024)<br><strong>Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs</strong><br><button class=copy-to-clipboard title="Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Face Recognition, Autoencoder, Black Box, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06025v1.pdf filename=2404.06025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Morphing attacks are an emerging threat to state-of-the-art <b>Face</b> <b>Recognition</b> (FR) systems, which aim to create a single image that contains the biometric information of multiple identities. Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks. However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a <b>black</b> <b>box,</b> treating it no differently than one would a <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> or Varational <b>AutoEncoder</b> (VAE). We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function. We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset. We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared.</p></p class="citation"></blockquote><h3 id=1370--65264-calibrating-higher-order-statistics-for-few-shot-class-incremental-learning-with-pre-trained-vision-transformers-dipam-goswami-et-al-2024>(13/70 | 65/264) Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers (Dipam Goswami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipam Goswami, Bartłomiej Twardowski, Joost van de Weijer. (2024)<br><strong>Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers</strong><br><button class=copy-to-clipboard title="Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Benchmarking, Few-shot, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06622v1.pdf filename=2404.06622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> class-incremental learning (FSCIL) aims to adapt the model to new classes from very few data (5 samples) without forgetting the previously learned classes. Recent works in many-shot CIL (MSCIL) (using all available training data) exploited pre-trained models to reduce forgetting and achieve better plasticity. In a similar fashion, we use ViT models pre-trained on large-scale datasets for <b>few-shot</b> settings, which face the critical issue of low plasticity. FSCIL methods start with a many-shot first task to learn a very good feature extractor and then move to the <b>few-shot</b> setting from the second task onwards. While the focus of most recent studies is on how to learn the many-shot first task so that the model generalizes to all future <b>few-shot</b> tasks, we explore in this work how to better model the <b>few-shot</b> data using pre-trained models, irrespective of how the first task is trained. Inspired by recent works in MSCIL, we explore how using higher-order feature statistics can influence the classification of <b>few-shot</b> classes. We identify the main challenge of obtaining a good covariance matrix from <b>few-shot</b> data and propose to calibrate the covariance matrix for new classes based on semantic similarity to the many-shot base classes. Using the calibrated feature statistics in combination with existing methods significantly improves <b>few-shot</b> continual classification on several FSCIL <b>benchmarks.</b> Code is available at <a href=https://github.com/dipamgoswami/FSCIL-Calibration>https://github.com/dipamgoswami/FSCIL-Calibration</a>.</p></p class="citation"></blockquote><h3 id=1470--66264-unified-multi-modal-diagnostic-framework-with-reconstruction-pre-training-and-heterogeneity-combat-tuning-yupei-zhang-et-al-2024>(14/70 | 66/264) Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning (Yupei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupei Zhang, Li Pan, Qiushi Yang, Tan Li, Zhen Chen. (2024)<br><strong>Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning</strong><br><button class=copy-to-clipboard title="Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Autoencoder, Fine-tuning, Mixed Reality (MR), Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06057v1.pdf filename=2404.06057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical <b>multi-modal</b> pre-training has revealed promise in computer-aided diagnosis by leveraging large-scale unlabeled datasets. However, existing methods based on masked <b>autoencoders</b> mainly rely on data-level reconstruction tasks, but lack high-level semantic information. Furthermore, two significant heterogeneity challenges hinder the transfer of pre-trained knowledge to downstream tasks, \textit{i.e.}, the distribution heterogeneity between pre-training data and downstream data, and the modality heterogeneity within downstream data. To address these challenges, we propose a Unified Medical <b>Multi-modal</b> Diagnostic (UMD) framework with tailored pre-training and downstream tuning strategies. Specifically, to enhance the representation abilities of vision and language encoders, we propose the Multi-level Reconstruction Pre-training <b>(MR-Pretrain)</b> strategy, including a feature-level and data-level reconstruction, which guides models to capture the semantic information from masked inputs of different modalities. Moreover, to tackle two kinds of heterogeneities during the downstream tuning, we present the heterogeneity-combat downstream tuning strategy, which consists of a Task-oriented Distribution Calibration (TD-Calib) and a Gradient-guided Modality Coordination (GM-Coord). In particular, TD-Calib <b>fine-tunes</b> the pre-trained model regarding the distribution of downstream datasets, and GM-Coord adjusts the gradient weights according to the dynamic optimization status of different modalities. Extensive experiments on five public medical datasets demonstrate the effectiveness of our UMD framework, which remarkably outperforms existing approaches on three kinds of downstream tasks.</p></p class="citation"></blockquote><h3 id=1570--67264-vision2ui-a-real-world-dataset-with-layout-for-code-generation-from-ui-designs-yi-gui-et-al-2024>(15/70 | 67/264) VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs (Yi Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling Dong, Xing Zhou, Wenbin Jiang. (2024)<br><strong>VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs</strong><br><button class=copy-to-clipboard title="VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-SE, cs.CV<br>Keyword Score: 41<br>Keywords: Deep Neural Network, Fine-tuning, Multi-modal, Multi-modal, Code Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06369v1.pdf filename=2404.06369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatically generating UI <b>code</b> <b>from</b> webpage design visions can significantly alleviate the burden of developers, enabling beginner developers or designers to directly generate Web pages from design diagrams. Currently, prior research has accomplished the objective of generating UI <b>code</b> <b>from</b> rudimentary design visions or sketches through designing <b>deep</b> <b>neural</b> <b>networks.</b> Inspired by the groundbreaking advancements achieved by <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), the automatic generation of UI <b>code</b> <b>from</b> high-fidelity design images is now emerging as a viable possibility. Nevertheless, our investigation reveals that existing MLLMs are hampered by the scarcity of authentic, high-quality, and <b>large-scale</b> <b>datasets,</b> <b>leading</b> to unsatisfactory performance in automated UI <b>code</b> <b>generation.</b> To mitigate this gap, we present a novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented with comprehensive layout information, tailored specifically for <b>finetuning</b> MLLMs in UI <b>code</b> <b>generation.</b> Specifically, this dataset is derived through a series of operations, encompassing collecting, cleaning, and filtering of the open-source Common Crawl dataset. In order to uphold its quality, a neural scorer trained on labeled samples is utilized to refine the data, retaining higher-quality instances. Ultimately, this process yields a dataset comprising 2,000 (Much more is coming soon) parallel samples encompassing design visions and UI <b>code.</b> <b>The</b> dataset is available at <a href=https://huggingface.co/datasets/xcodemind/vision2ui>https://huggingface.co/datasets/xcodemind/vision2ui</a>.</p></p class="citation"></blockquote><h3 id=1670--68264-test-time-adaptation-with-salip-a-cascade-of-sam-and-clip-for-zero-shot-medical-image-segmentation-sidra-aleem-et-al-2024>(16/70 | 68/264) Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation (Sidra Aleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier, Kathleen Curran, Noel E. O&rsquo;Connor, Suzanne Little. (2024)<br><strong>Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06362v1.pdf filename=2404.06362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM) and CLIP are remarkable vision <b>foundation</b> <b>models</b> (VFMs). SAM, a <b>prompt</b> driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior <b>prompts</b> tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is <b>prompted</b> by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for <b>prompt</b> engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un <b>prompted</b> SAM. Code and text <b>prompts</b> will be available online.</p></p class="citation"></blockquote><h3 id=1770--69264-learning-embeddings-with-centroid-triplet-loss-for-object-identification-in-robotic-grasping-anas-gouda-et-al-2024>(17/70 | 69/264) Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping (Anas Gouda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Gouda, Max Schwarz, Christopher Reining, Sven Behnke, Alice Kirchheim. (2024)<br><strong>Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping</strong><br><button class=copy-to-clipboard title="Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Fine-tuning, Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06277v1.pdf filename=2404.06277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> are a strong trend in deep learning and computer vision. These models serve as a base for applications as they require minor or no further <b>fine-tuning</b> by developers to integrate into their applications. <b>Foundation</b> <b>models</b> for <b>zero-shot</b> <b>object</b> <b>segmentation</b> such as Segment Anything (SAM) output segmentation masks from images without any further <b>object</b> <b>information.</b> When they are followed in a pipeline by an <b>object</b> <b>identification</b> model, they can perform <b>object</b> <b>detection</b> without training. Here, we focus on training such an <b>object</b> <b>identification</b> model. A crucial practical aspect for an <b>object</b> <b>identification</b> model is to be flexible in input size. As <b>object</b> <b>identification</b> is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers). The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids. CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible. In our experiments, we establish a new state of the art on the ArmBench <b>object</b> <b>identification</b> task, which shows general applicability of our model. We furthermore demonstrate an integrated unseen <b>object</b> <b>detection</b> pipeline on the challenging HOPE dataset, which requires fine-grained detection. There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data.</p></p class="citation"></blockquote><h3 id=1870--70264-label-efficient-3d-object-detection-for-road-side-units-minh-quan-dao-et-al-2024>(18/70 | 70/264) Label-Efficient 3D Object Detection For Road-Side Units (Minh-Quan Dao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh-Quan Dao, Holger Caesar, Julie Stephany Berrio, Mao Shan, Stewart Worrall, Vincent Frémont, Ezio Malis. (2024)<br><strong>Label-Efficient 3D Object Detection For Road-Side Units</strong><br><button class=copy-to-clipboard title="Label-Efficient 3D Object Detection For Road-Side Units" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Fine-tuning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06256v1.pdf filename=2404.06256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Occlusion presents a significant challenge for safety-critical applications such as autonomous driving. Collaborative perception has recently attracted a large research interest thanks to the ability to enhance the perception of autonomous vehicles via deep information fusion with intelligent roadside units (RSU), thus minimizing the impact of occlusion. While significant advancement has been made, the data-hungry nature of these methods creates a major hurdle for their real-world deployment, particularly due to the need for annotated RSU data. Manually annotating the vast amount of RSU data required for training is prohibitively expensive, given the sheer number of intersections and the effort involved in annotating point clouds. We address this challenge by devising a label-efficient <b>object</b> <b>detection</b> method for RSU based on <b>unsupervised</b> <b>object</b> <b>discovery.</b> Our paper introduces two new modules: one for <b>object</b> <b>discovery</b> based on a spatial-temporal aggregation of point clouds, and another for refinement. Furthermore, we demonstrate that <b>fine-tuning</b> on a small portion of annotated data allows our <b>object</b> <b>discovery</b> models to narrow the performance gap with, or even surpass, fully <b>supervised</b> models. Extensive experiments are carried out in simulated and real-world datasets to evaluate our method.</p></p class="citation"></blockquote><h3 id=1970--71264-improving-facial-landmark-detection-accuracy-and-efficiency-with-knowledge-distillation-zong-wei-hong-et-al-2024>(19/70 | 71/264) Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation (Zong-Wei Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zong-Wei Hong, Yu-Chen Lin. (2024)<br><strong>Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Augmented Reality (AR), Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06029v1.pdf filename=2404.06029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The domain of computer vision has experienced significant advancements in facial-landmark detection, becoming increasingly essential across various applications such as <b>augmented</b> <b>reality,</b> facial recognition, and emotion analysis. Unlike <b>object</b> <b>detection</b> or semantic segmentation, which focus on identifying <b>objects</b> <b>and</b> outlining boundaries, faciallandmark detection aims to precisely locate and track critical facial features. However, deploying deep learning-based facial-landmark detection models on embedded systems with limited computational resources poses challenges due to the complexity of facial features, especially in dynamic settings. Additionally, ensuring robustness across diverse ethnicities and expressions presents further obstacles. Existing datasets often lack comprehensive representation of facial nuances, particularly within populations like those in Taiwan. This paper introduces a novel approach to address these challenges through the development of a <b>knowledge</b> <b>distillation</b> method. By transferring <b>knowledge</b> <b>from</b> larger models to smaller ones, we aim to create lightweight yet powerful deep learning models tailored specifically for facial-landmark detection tasks. Our goal is to design models capable of accurately locating facial landmarks under varying conditions, including diverse expressions, orientations, and lighting environments. The ultimate objective is to achieve high accuracy and real-time performance suitable for deployment on embedded systems. This method was successfully implemented and achieved a top 6th place finish out of 165 participants in the IEEE ICME 2024 PAIR competition.</p></p class="citation"></blockquote><h3 id=2070--72264-counterfactual-reasoning-for-multi-label-image-classification-via-patching-based-training-ming-kun-xie-et-al-2024>(20/70 | 72/264) Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training (Ming-Kun Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming-Kun Xie, Jia-Hao Xiao, Pei Peng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang. (2024)<br><strong>Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training</strong><br><button class=copy-to-clipboard title="Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Counter-factual, Counterfactual Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06287v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06287v1.pdf filename=2404.06287v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The key to multi-label image classification (MLC) is to improve model performance by leveraging label correlations. Unfortunately, it has been shown that overemphasizing co-occurrence relationships can cause the overfitting issue of the model, ultimately leading to performance degradation. In this paper, we provide a causal inference framework to show that the correlative features caused by the target object and its co-occurring objects can be regarded as a mediator, which has both positive and negative impacts on model predictions. On the positive side, the mediator enhances the recognition performance of the model by capturing co-occurrence relationships; on the negative side, it has the harmful causal effect that causes the model to make an incorrect prediction for the target object, even when only co-occurring objects are present in an image. To address this problem, we propose a <b>counterfactual</b> <b>reasoning</b> method to measure the total direct effect, achieved by enhancing the direct effect caused only by the target object. Due to the unknown location of the target object, we propose patching-based training and inference to accomplish this goal, which divides an image into multiple patches and identifies the pivot patch that contains the target object. Experimental results on multiple <b>benchmark</b> datasets with diverse configurations validate that the proposed method can achieve state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=2170--73264-lipt-latency-aware-image-processing-transformer-junbo-qiao-et-al-2024>(21/70 | 73/264) LIPT: Latency-aware Image Processing Transformer (Junbo Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbo Qiao, Wei Li, Haizhen Xie, Hanting Chen, Yunshuai Zhou, Zhijun Tu, Jie Hu, Shaohui Lin. (2024)<br><strong>LIPT: Latency-aware Image Processing Transformer</strong><br><button class=copy-to-clipboard title="LIPT: Latency-aware Image Processing Transformer" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06075v1.pdf filename=2404.06075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> is leading a trend in the field of image processing. Despite the great success that existing lightweight image processing <b>transformers</b> have achieved, they are tailored to FLOPs or parameters reduction, rather than practical inference acceleration. In this paper, we present a latency-aware image processing <b>transformer,</b> termed LIPT. We devise the low-latency proportion LIPT block that substitutes memory-intensive operators with the combination of <b>self-attention</b> and <b>convolutions</b> to achieve practical speedup. Specifically, we propose a novel non-volatile sparse masking <b>self-attention</b> (NVSM-SA) that utilizes a pre-computing sparse mask to capture contextual information from a larger window with no extra computation overload. Besides, a high-frequency reparameterization module (HRM) is proposed to make LIPT block reparameterization friendly, which improves the model&rsquo;s detail reconstruction capability. Extensive experiments on multiple image processing tasks (e.g., image super-resolution (SR), JPEG artifact reduction, and image denoising) demonstrate the superiority of LIPT on both latency and PSNR. LIPT achieves real-time GPU inference with state-of-the-art performance on multiple image SR <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2270--74264-object-dynamics-modeling-with-hierarchical-point-cloud-based-representations-chanho-kim-et-al-2024>(22/70 | 74/264) Object Dynamics Modeling with Hierarchical Point Cloud-based Representations (Chanho Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanho Kim, Li Fuxin. (2024)<br><strong>Object Dynamics Modeling with Hierarchical Point Cloud-based Representations</strong><br><button class=copy-to-clipboard title="Object Dynamics Modeling with Hierarchical Point Cloud-based Representations" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Convolution, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06044v1.pdf filename=2404.06044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling object dynamics with a neural network is an important problem with numerous applications. Most recent work has been based on <b>graph</b> <b>neural</b> <b>networks.</b> However, physics happens in 3D space, where geometric information potentially plays an important role in modeling physical phenomena. In this work, we propose a novel U-net architecture based on continuous point <b>convolution</b> which naturally embeds information from 3D coordinates and allows for multi-scale feature representations with established downsampling and upsampling procedures. Bottleneck layers in the downsampled point clouds lead to better long-range interaction modeling. Besides, the flexibility of point <b>convolutions</b> allows our approach to generalize to sparsely sampled points from mesh vertices and dynamically generate features on important interaction points on mesh faces. Experimental results demonstrate that our approach significantly improves the state-of-the-art, especially in scenarios that require accurate gravity or collision <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=2370--75264-smartcontrol-enhancing-controlnet-for-handling-rough-visual-conditions-xiaoyu-liu-et-al-2024>(23/70 | 75/264) SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions (Xiaoyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Liu, Yuxiang Wei, Ming Liu, Xianhui Lin, Peiran Ren, Xuansong Xie, Wangmeng Zuo. (2024)<br><strong>SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions</strong><br><button class=copy-to-clipboard title="SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06451v1.pdf filename=2404.06451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human visual imagination usually begins with analogies or rough sketches. For example, given an image with a girl playing guitar before a building, one may analogously imagine how it seems like if Iron Man playing guitar before Pyramid in Egypt. Nonetheless, visual condition may not be precisely aligned with the imaginary result indicated by text <b>prompt,</b> and existing layout-controllable <b>text-to-image</b> (T2I) generation models is prone to producing degraded generated results with obvious artifacts. To address this issue, we present a novel T2I generation method dubbed SmartControl, which is designed to modify the rough visual conditions for adapting to text <b>prompt.</b> The key idea of our SmartControl is to relax the visual condition on the areas that are conflicted with text <b>prompts.</b> In specific, a Control Scale Predictor (CSP) is designed to identify the conflict regions and predict the local control scales, while a dataset with text <b>prompts</b> and rough visual conditions is constructed for training CSP. It is worth noting that, even with a limited number (e.g., 1,000~2,000) of training samples, our SmartControl can generalize well to unseen objects. Extensive experiments on four typical visual condition types clearly show the efficacy of our SmartControl against state-of-the-arts. Source code, pre-trained models, and datasets are available at <a href=https://github.com/liuxiaoyu1104/SmartControl>https://github.com/liuxiaoyu1104/SmartControl</a>.</p></p class="citation"></blockquote><h3 id=2470--76264-robust-feature-knowledge-distillation-for-enhanced-performance-of-lightweight-crack-segmentation-models-zhaohui-chen-et-al-2024>(24/70 | 76/264) Robust feature knowledge distillation for enhanced performance of lightweight crack segmentation models (Zhaohui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaohui Chen, Elyas Asadi Shamsabadi, Sheng Jiang, Luming Shen, Daniel Dias-da-Costa. (2024)<br><strong>Robust feature knowledge distillation for enhanced performance of lightweight crack segmentation models</strong><br><button class=copy-to-clipboard title="Robust feature knowledge distillation for enhanced performance of lightweight crack segmentation models" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06258v1.pdf filename=2404.06258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision-based crack detection faces deployment challenges due to the size of robust models and edge device limitations. These can be addressed with lightweight models trained with <b>knowledge</b> <b>distillation</b> <b>(KD).</b> However, state-of-the-art (SOTA) <b>KD</b> methods compromise anti-noise robustness. This paper develops Robust Feature <b>Knowledge</b> <b>Distillation</b> (RFKD), a framework to improve robustness while retaining the precision of light models for crack segmentation. RFKD distils <b>knowledge</b> <b>from</b> a teacher model&rsquo;s logit layers and intermediate feature maps while leveraging mixed clean and noisy images to transfer robust patterns to the student model, improving its precision, generalisation, and anti-noise performance. To validate the proposed RFKD, a lightweight crack segmentation model, PoolingCrack Tiny (PCT), with only 0.5 M parameters, is also designed and used as the student to run the framework. The results show a significant enhancement in noisy images, with RFKD reaching a 62% enhanced mean Dice score (mDS) compared to SOTA <b>KD</b> methods.</p></p class="citation"></blockquote><h3 id=2570--77264-diffharmony-latent-diffusion-model-meets-image-harmonization-pengfei-zhou-et-al-2024>(25/70 | 77/264) DiffHarmony: Latent Diffusion Model Meets Image Harmonization (Pengfei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Zhou, Fangxiang Feng, Xiaojie Wang. (2024)<br><strong>DiffHarmony: Latent Diffusion Model Meets Image Harmonization</strong><br><button class=copy-to-clipboard title="DiffHarmony: Latent Diffusion Model Meets Image Harmonization" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Autoencoder, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06139v1.pdf filename=2404.06139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. <b>Diffusion</b> <b>models</b> have recently promoted the rapid development of image-to-image translation tasks . However, training <b>diffusion</b> <b>models</b> from scratch is computationally intensive. <b>Fine-tuning</b> pre-trained latent <b>diffusion</b> <b>models</b> entails dealing with the reconstruction error induced by the image compression <b>autoencoder,</b> making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent <b>diffusion</b> <b>model</b> to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at <a href=https://github.com/nicecv/DiffHarmony>https://github.com/nicecv/DiffHarmony</a> .</p></p class="citation"></blockquote><h3 id=2670--78264-mansformer-efficient-transformer-of-mixed-attention-for-image-deblurring-and-beyond-pin-hung-kuo-et-al-2024>(26/70 | 78/264) Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond (Pin-Hung Kuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pin-Hung Kuo, Jinshan Pan, Shao-Yi Chien, Ming-Hsuan Yang. (2024)<br><strong>Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond</strong><br><button class=copy-to-clipboard title="Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06135v1.pdf filename=2404.06135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> has made an enormous success in natural language processing and high-level vision over the past few years. However, the complexity of <b>self-attention</b> is quadratic to the image size, which makes it infeasible for high-resolution vision tasks. In this paper, we propose the Mansformer, a <b>Transformer</b> of mixed attention that combines multiple <b>self-attentions,</b> gate, and multi-layer perceptions (MLPs), to explore and employ more possibilities of <b>self-attention.</b> Taking efficiency into account, we design four kinds of <b>self-attention,</b> whose complexities are all linear. By elaborate adjustment of the tensor shapes and dimensions for the dot product, we split the typical <b>self-attention</b> of quadratic complexity into four operations of linear complexity. To adaptively merge these different kinds of <b>self-attention,</b> we take advantage of an architecture similar to Squeeze-and-Excitation Networks. Furthermore, we make it to merge the two-staged <b>Transformer</b> design into one stage by the proposed <b>gated-dconv</b> MLP. Image deblurring is our main target, while extensive quantitative and qualitative evaluations show that this method performs favorably against the state-of-the-art methods far more than simply deblurring. The source codes and trained models will be made available to the public.</p></p class="citation"></blockquote><h3 id=2770--79264-incremental-joint-learning-of-depth-pose-and-implicit-scene-representation-on-monocular-camera-in-large-scale-scenes-tianchen-deng-et-al-2024>(27/70 | 79/264) Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes (Tianchen Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianchen Deng, Nailin Wang, Chongdi Wang, Shenghai Yuan, Jingchuan Wang, Danwei Wang, Weidong Chen. (2024)<br><strong>Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes</strong><br><button class=copy-to-clipboard title="Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06050v1.pdf filename=2404.06050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dense scene reconstruction for photo-realistic view synthesis has various applications, such as VR/AR, autonomous vehicles. However, most existing methods have difficulties in large-scale scenes due to three core challenges: \textit{(a) inaccurate depth input.} Accurate depth input is impossible to get in real-world large-scale scenes. \textit{(b) inaccurate pose estimation.} Most existing approaches rely on accurate pre-estimated camera poses. \textit{(c) insufficient scene representation capability.} A single global radiance field lacks the capacity to effectively scale to large-scale scenes. To this end, we propose an incremental joint learning framework, which can achieve accurate depth, pose estimation, and large-scale scene reconstruction. A <b>vision</b> <b>transformer-based</b> network is adopted as the backbone to enhance performance in scale information estimation. For pose estimation, a feature-metric bundle adjustment (FBA) method is designed for accurate and robust camera tracking in large-scale scenes. In terms of implicit scene representation, we propose an incremental scene representation method to construct the entire large-scale scene as multiple local radiance fields to enhance the scalability of 3D scene representation. Extended experiments have been conducted to demonstrate the effectiveness and accuracy of our method in depth estimation, pose estimation, and large-scale scene reconstruction.</p></p class="citation"></blockquote><h3 id=2870--80264-little-strokes-fell-great-oaks-boosting-the-hierarchical-features-for-multi-exposure-image-fusion-pan-mu-et-al-2024>(28/70 | 80/264) Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for Multi-exposure Image Fusion (Pan Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pan Mu, Zhiying Du, Jinyuan Liu, Cong Bai. (2024)<br><strong>Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for Multi-exposure Image Fusion</strong><br><button class=copy-to-clipboard title="Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for Multi-exposure Image Fusion" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06033v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06033v2.pdf filename=2404.06033v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep learning networks have made remarkable strides in the domain of multi-exposure image fusion. Nonetheless, prevailing approaches often involve directly feeding over-exposed and under-exposed images into the network, which leads to the under-utilization of inherent information present in the source images. Additionally, <b>unsupervised</b> techniques predominantly employ rudimentary weighted summation for color channel processing, culminating in an overall desaturated final image tone. To partially mitigate these issues, this study proposes a gamma correction module specifically designed to fully leverage latent information embedded within source images. Furthermore, a modified <b>transformer</b> block, embracing with <b>self-attention</b> mechanisms, is introduced to optimize the fusion process. Ultimately, a novel color enhancement algorithm is presented to augment image saturation while preserving intricate details. The source code is available at <a href=https://github.com/ZhiyingDu/BHFMEF>https://github.com/ZhiyingDu/BHFMEF</a>.</p></p class="citation"></blockquote><h3 id=2970--81264-concept-attention-whitening-for-interpretable-skin-lesion-diagnosis-junlin-hou-et-al-2024>(29/70 | 81/264) Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis (Junlin Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Hou, Jilan Xu, Hao Chen. (2024)<br><strong>Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis</strong><br><button class=copy-to-clipboard title="Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Black Box, Convolutional Neural Network, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05997v1.pdf filename=2404.05997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>black-box</b> <b>nature</b> of deep learning models has raised concerns about their interpretability for successful deployment in real-world clinical applications. To address the concerns, eXplainable Artificial Intelligence (XAI) aims to provide clear and understandable explanations of the decision-making process. In the medical domain, concepts such as attributes of lesions or abnormalities serve as key evidence for deriving diagnostic results. However, existing concept-based models mainly depend on concepts that appear independently and require fine-grained concept annotations such as bounding boxes. A medical image usually contains multiple concepts and the fine-grained concept annotations are difficult to acquire. In this paper, we propose a novel Concept-Attention Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is comprised of a disease diagnosis branch and a concept alignment branch. In the former branch, we train the <b>CNN</b> with a CAW layer inserted to perform skin lesion diagnosis. The CAW layer decorrelates features and aligns image features to conceptual meanings via an orthogonal matrix. In the latter branch, we calculate the orthogonal matrix under the guidance of the concept attention mask. We particularly introduce a <b>weakly-supervised</b> concept mask generator that only leverages coarse concept labels for filtering local regions that are relevant to certain concepts, improving the optimization of the orthogonal matrix. Extensive experiments on two public skin lesion diagnosis datasets demonstrated that CAW not only enhanced interpretability but also maintained a state-of-the-art diagnostic performance.</p></p class="citation"></blockquote><h3 id=3070--82264-tackling-structural-hallucination-in-image-translation-with-local-diffusion-seunghoi-kim-et-al-2024>(30/70 | 82/264) Tackling Structural Hallucination in Image Translation with Local Diffusion (Seunghoi Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J. Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander. (2024)<br><strong>Tackling Structural Hallucination in Image Translation with Local Diffusion</strong><br><button class=copy-to-clipboard title="Tackling Structural Hallucination in Image Translation with Local Diffusion" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Out-of-distribution, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05980v1.pdf filename=2404.05980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>diffusion</b> <b>models</b> have advanced conditioned <b>image</b> <b>generation,</b> yet they struggle with reconstructing <b>out-of-distribution</b> (OOD) <b>images,</b> <b>such</b> as unseen tumors in medical <b>images,</b> <b>causing</b> <code>image hallucination'' and risking misdiagnosis. We hypothesize such hallucinations result from local OOD regions in the conditional &lt;b>images.&lt;/b> &lt;b>We&lt;/b> verify that partitioning the OOD region and conducting separate &lt;b>image&lt;/b> &lt;b>generations&lt;/b> alleviates hallucinations in several applications. From this, we propose a training-free &lt;b>diffusion&lt;/b> &lt;b>framework&lt;/b> that reduces hallucination with multiple Local &lt;b>Diffusion&lt;/b> &lt;b>processes.&lt;/b> Our approach involves OOD estimation followed by two modules: a </code>branching&rsquo;&rsquo; module generates locally both within and outside OOD regions, and a ``fusion&rsquo;&rsquo; module integrates these predictions into one. Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural <b>image</b> <b>datasets,</b> respectively. It also demonstrates compatibility with various pre-trained <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3170--83264-easytrack-efficient-and-compact-one-stream-3d-point-clouds-tracker-baojie-fan-et-al-2024>(31/70 | 83/264) EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker (Baojie Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baojie Fan, Wuyang Zhou, Kai Wang, Shijun Zhou, Fengyu Xu, Jiandong Tian. (2024)<br><strong>EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker</strong><br><button class=copy-to-clipboard title="EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05960v1.pdf filename=2404.05960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most of 3D single object trackers (SOT) in point clouds follow the two-stream multi-stage 3D Siamese or motion tracking paradigms, which process the template and search area point clouds with two parallel branches, built on <b>supervised</b> point cloud backbones. In this work, beyond typical 3D Siamese or motion tracking, we propose a neat and compact one-stream <b>transformer</b> 3D SOT paradigm from the novel perspective, termed as \textbf{EasyTrack}, which consists of three special designs: 1) A 3D point clouds tracking feature pre-training module is developed to exploit the masked autoencoding for learning 3D point clouds tracking representations. 2) A unified 3D tracking feature learning and fusion network is proposed to simultaneously learns target-aware 3D features, and extensively captures mutual correlation through the flexible <b>self-attention</b> mechanism. 3) A target location network in the dense bird&rsquo;s eye view (BEV) feature space is constructed for target classification and regression. Moreover, we develop an enhanced version named EasyTrack++, which designs the center points interaction (CPI) strategy to reduce the ambiguous targets caused by the noise point cloud background information. The proposed EasyTrack and EasyTrack++ set a new state-of-the-art performance ($\textbf{18%}$, $\textbf{40%}$ and $\textbf{3%}$ success gains) in KITTI, NuScenes, and Waymo while runing at \textbf{52.6fps} with few parameters (\textbf{1.3M}). The code will be available at <a href=https://github.com/KnightApple427/Easytrack>https://github.com/KnightApple427/Easytrack</a>.</p></p class="citation"></blockquote><h3 id=3270--84264-audio-visual-generalized-zero-shot-learning-using-pre-trained-large-multi-modal-models-david-kurzendörfer-et-al-2024>(32/70 | 84/264) Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models (David Kurzendörfer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Kurzendörfer, Otniel-Bogdan Mercea, A. Sophia Koepke, Zeynep Akata. (2024)<br><strong>Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models</strong><br><button class=copy-to-clipboard title="Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06309v1.pdf filename=2404.06309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-visual <b>zero-shot</b> <b>learning</b> methods commonly build on features extracted from pre-trained models, e.g. video or audio classification models. However, existing <b>benchmarks</b> predate the popularization of large <b>multi-modal</b> models, such as CLIP and CLAP. In this work, we explore such large pre-trained models to obtain features, i.e. CLIP for visual features, and CLAP for audio features. Furthermore, the CLIP and CLAP text encoders provide class label embeddings which are combined to boost the performance of the system. We propose a simple yet effective model that only relies on feed-forward neural networks, exploiting the strong generalization capabilities of the new audio, visual and textual features. Our framework achieves state-of-the-art performance on VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL with our new features. Code and data available at: <a href=https://github.com/dkurzend/ClipClap-GZSL>https://github.com/dkurzend/ClipClap-GZSL</a>.</p></p class="citation"></blockquote><h3 id=3370--85264-magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion-fan-yang-et-al-2024>(33/70 | 85/264) Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion (Fan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin. (2024)<br><strong>Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</strong><br><button class=copy-to-clipboard title="Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Fine-tuning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06429v1.pdf filename=2404.06429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Benefiting from the rapid development of 2D <b>diffusion</b> <b>models,</b> 3D content creation has made significant progress recently. One promising solution involves the <b>fine-tuning</b> of pre-trained 2D <b>diffusion</b> <b>models</b> to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned <b>diffusion</b> <b>model</b> that significantly refines coarse generative results through a brief period of SDS optimization ($\sim15$min). Compared to the previous text or single image based <b>diffusion</b> <b>models,</b> Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both <b>geometry</b> and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: <a href=https://magic-research.github.io/magic-boost/>https://magic-research.github.io/magic-boost/</a>)</p></p class="citation"></blockquote><h3 id=3470--86264-zest-zero-shot-material-transfer-from-a-single-image-ta-ying-cheng-et-al-2024>(34/70 | 86/264) ZeST: Zero-Shot Material Transfer from a Single Image (Ta-Ying Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani. (2024)<br><strong>ZeST: Zero-Shot Material Transfer from a Single Image</strong><br><button class=copy-to-clipboard title="ZeST: Zero-Shot Material Transfer from a Single Image" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06425v1.pdf filename=2404.06425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose ZeST, a method for <b>zero-shot</b> material transfer to an object in the input image given a material exemplar image. ZeST leverages existing <b>diffusion</b> <b>adapters</b> to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting <b>diffusion</b> <b>model</b> on the object in the input image using depth estimates as <b>geometry</b> cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a <b>zero-shot</b> approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: <a href=https://ttchengab.github.io/zest>https://ttchengab.github.io/zest</a></p></p class="citation"></blockquote><h3 id=3570--87264-automated-national-urban-map-extraction-hasan-nasrallah-et-al-2024>(35/70 | 87/264) Automated National Urban Map Extraction (Hasan Nasrallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hasan Nasrallah, Abed Ellatif Samhat, Cristiano Nattero, Ali J. Ghandour. (2024)<br><strong>Automated National Urban Map Extraction</strong><br><button class=copy-to-clipboard title="Automated National Urban Map Extraction" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06202v1.pdf filename=2404.06202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing countries usually lack the proper governance means to generate and regularly update a national rooftop map. Using traditional photogrammetry and surveying methods to produce a building map at the federal level is costly and time consuming. Using earth observation and deep learning methods, we can bridge this gap and propose an automated pipeline to fetch such national urban maps. This paper aims to exploit the power of fully <b>convolutional</b> <b>neural</b> <b>networks</b> for multi-class buildings&rsquo; instance segmentation to leverage high object-wise accuracy results. Buildings&rsquo; instance segmentation from sub-meter high-resolution satellite images can be achieved with relatively high pixel-wise metric scores. We detail all engineering steps to replicate this work and ensure highly accurate results in dense and slum areas witnessed in regions that lack proper urban planning in the Global South. We applied a case study of the proposed pipeline to Lebanon and successfully produced the first comprehensive national building footprint map with approximately 1 Million units with an 84% accuracy. The proposed architecture relies on advanced augmentation techniques to overcome dataset scarcity, which is often the case in developing countries.</p></p class="citation"></blockquote><h3 id=3670--88264-matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences-axel-barroso-laguna-et-al-2024>(36/70 | 88/264) Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences (Axel Barroso-Laguna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Axel Barroso-Laguna, Sowmya Munukutla, Victor Adrian Prisacariu, Eric Brachmann. (2024)<br><strong>Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences</strong><br><button class=copy-to-clipboard title="Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Augmented Reality (AR), Benchmarking, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06337v1.pdf filename=2404.06337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given two images, we can estimate the relative camera pose between them by establishing image-to-image correspondences. Usually, correspondences are 2D-to-2D and the pose we estimate is defined only up to scale. Some applications, aiming at instant <b>augmented</b> <b>reality</b> anywhere, require scale-metric pose estimates, and hence, they rely on external depth estimators to recover the scale. We present MicKey, a keypoint matching pipeline that is able to predict metric correspondences in 3D camera space. By learning to match 3D coordinates across images, we are able to infer the metric relative pose without depth measurements. Depth measurements are also not required for training, nor are scene reconstructions or image overlap information. MicKey is <b>supervised</b> only by pairs of images and their relative poses. MicKey achieves state-of-the-art performance on the Map-Free Relocalisation <b>benchmark</b> while requiring less supervision than competing approaches.</p></p class="citation"></blockquote><h3 id=3770--89264-geosynth-contextually-aware-high-resolution-satellite-image-synthesis-srikumar-sastry-et-al-2024>(37/70 | 89/264) GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis (Srikumar Sastry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Srikumar Sastry, Subash Khanal, Aayush Dhakal, Nathan Jacobs. (2024)<br><strong>GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis</strong><br><button class=copy-to-clipboard title="GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06637v1.pdf filename=2404.06637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GeoSynth, a model for synthesizing satellite images with global style and image-driven layout control. The global style control is via textual <b>prompts</b> or geographic location. These enable the specification of scene semantics or regional appearance respectively, and can be used together. We train our model on a large dataset of paired satellite imagery, with automatically generated captions, and OpenStreetMap data. We evaluate various combinations of control inputs, including different types of layout controls. Results demonstrate that our model can generate diverse, high-quality images and exhibits excellent <b>zero-shot</b> generalization. The code and model checkpoints are available at <a href=https://github.com/mvrl/GeoSynth>https://github.com/mvrl/GeoSynth</a>.</p></p class="citation"></blockquote><h3 id=3870--90264-spatially-optimized-compact-deep-metric-learning-model-for-similarity-search-md-farhadul-islam-et-al-2024>(38/70 | 90/264) Spatially Optimized Compact Deep Metric Learning Model for Similarity Search (Md. Farhadul Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md. Farhadul Islam, Md. Tanzim Reza, Meem Arafat Manab, Mohammad Rakibul Hasan Mahin, Sarah Zabeen, Jannatun Noor. (2024)<br><strong>Spatially Optimized Compact Deep Metric Learning Model for Similarity Search</strong><br><button class=copy-to-clipboard title="Spatially Optimized Compact Deep Metric Learning Model for Similarity Search" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68, I-4-7; I-2-6; I-2-10, cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: MNIST, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06593v1.pdf filename=2404.06593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatial optimization is often overlooked in many computer vision tasks. Filters should be able to recognize the features of an object regardless of where it is in the image. Similarity search is a crucial task where spatial features decide an important output. The capacity of <b>convolution</b> to capture visual patterns across various locations is limited. In contrast to <b>convolution,</b> the involution kernel is dynamically created at each pixel based on the pixel value and parameters that have been learned. This study demonstrates that utilizing a single layer of involution feature extractor alongside a compact <b>convolution</b> model significantly enhances the performance of similarity search. Additionally, we improve predictions by using the GELU activation function rather than the ReLU. The negligible amount of weight parameters in involution with a compact model with better performance makes the model very useful in real-world implementations. Our proposed model is below 1 megabyte in size. We have experimented with our proposed methodology and other models on CIFAR-10, FashionMNIST, and <b>MNIST</b> datasets. Our proposed method outperforms across all three datasets.</p></p class="citation"></blockquote><h3 id=3970--91264-questmaps-queryable-semantic-topological-maps-for-3d-scene-understanding-yash-mehan-et-al-2024>(39/70 | 91/264) QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding (Yash Mehan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Mehan, Kumaraditya Gupta, Rohit Jayanti, Anirudh Govil, Sourav Garg, Madhava Krishna. (2024)<br><strong>QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding</strong><br><button class=copy-to-clipboard title="QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06442v1.pdf filename=2404.06442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the structural organisation of 3D indoor scenes in terms of rooms is often accomplished via floorplan extraction. Robotic tasks such as planning and navigation require a semantic understanding of the scene as well. This is typically achieved via object-level semantic segmentation. However, such methods struggle to segment out topological regions like &ldquo;kitchen&rdquo; in the scene. In this work, we introduce a two-step pipeline. First, we extract a topological map, i.e., floorplan of the indoor scene using a novel multi-channel occupancy representation. Then, we generate CLIP-aligned features and semantic labels for every room instance based on the objects it contains using a <b>self-attention</b> <b>transformer.</b> Our language-topology alignment supports natural language querying, e.g., a &ldquo;place to cook&rdquo; locates the &ldquo;kitchen&rdquo;. We outperform the current state-of-the-art on room segmentation by ~20% and room classification by ~12%. Our detailed qualitative analysis and ablation studies provide insights into the problem of joint structural and semantic 3D scene understanding.</p></p class="citation"></blockquote><h3 id=4070--92264-lrr-language-driven-resamplable-continuous-representation-against-adversarial-tracking-attacks-jianlang-chen-et-al-2024>(40/70 | 92/264) LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks (Jianlang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianlang Chen, Xuhong Ren, Qing Guo, Felix Juefei-Xu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao. (2024)<br><strong>LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks</strong><br><button class=copy-to-clipboard title="LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Attack, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06247v1.pdf filename=2404.06247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with <b>adversarial</b> <b>perturbations</b> in the incoming frames. This can lead to significant robustness and <b>security</b> issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and <b>adversarial</b> <b>data,</b> we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA <b>adversarial</b> <b>tracking</b> attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under <b>adversarial</b> <b>attacks</b> with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.</p></p class="citation"></blockquote><h3 id=4170--93264-yolc-you-only-look-clusters-for-tiny-object-detection-in-aerial-images-chenguang-liu-et-al-2024>(41/70 | 93/264) YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images (Chenguang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenguang Liu, Guangshuai Gao, Ziyue Huang, Zhenghui Hu, Qingjie Liu, Yunhong Wang. (2024)<br><strong>YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images</strong><br><button class=copy-to-clipboard title="YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06180v1.pdf filename=2404.06180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting <b>objects</b> <b>from</b> aerial images poses significant challenges due to the following factors: 1) Aerial images typically have very large sizes, generally with millions or even hundreds of millions of pixels, while computational resources are limited. 2) Small <b>object</b> <b>size</b> leads to insufficient information for effective detection. 3) Non-uniform <b>object</b> <b>distribution</b> leads to computational resource wastage. To address these issues, we propose YOLC (You Only Look Clusters), an efficient and effective framework that builds on an anchor-free <b>object</b> <b>detector,</b> CenterNet. To overcome the challenges posed by large-scale images and non-uniform <b>object</b> <b>distribution,</b> we introduce a Local Scale Module (LSM) that adaptively searches cluster regions for zooming in for accurate detection. Additionally, we modify the regression loss using Gaussian Wasserstein distance (GWD) to obtain high-quality bounding boxes. Deformable <b>convolution</b> and refinement methods are employed in the detection head to enhance the detection of small <b>objects.</b> <b>We</b> perform extensive experiments on two aerial image datasets, including Visdrone2019 and UAVDT, to demonstrate the effectiveness and superiority of our proposed approach.</p></p class="citation"></blockquote><h3 id=4270--94264-dreamview-injecting-view-specific-text-guidance-into-text-to-3d-generation-junkai-yan-et-al-2024>(42/70 | 94/264) DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation (Junkai Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junkai Yan, Yipeng Gao, Qize Yang, Xihan Wei, Xuansong Xie, Ancong Wu, Wei-Shi Zheng. (2024)<br><strong>DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation</strong><br><button class=copy-to-clipboard title="DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06119v1.pdf filename=2404.06119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-3D generation, which synthesizes 3D assets according to an overall text description, has significantly progressed. However, a challenge arises when the specific appearances need customizing at designated viewpoints but referring solely to the overall description for generating 3D objects. For instance, ambiguity easily occurs when producing a T-shirt with distinct patterns on its front and back using a single overall text guidance. In this work, we propose DreamView, a <b>text-to-image</b> approach enabling multi-view customization while maintaining overall consistency by adaptively injecting the view-specific and overall text guidance through a collaborative text guidance injection module, which can also be lifted to 3D generation via score <b>distillation</b> sampling. DreamView is trained with large-scale rendered multi-view images and their corresponding view-specific texts to learn to balance the separate content manipulation in each view and the global consistency of the overall object, resulting in a dual achievement of customization and consistency. Consequently, DreamView empowers artists to design 3D objects creatively, fostering the creation of more innovative and diverse 3D assets. Code and model will be released at <a href=https://github.com/iSEE-Laboratory/DreamView>https://github.com/iSEE-Laboratory/DreamView</a>.</p></p class="citation"></blockquote><h3 id=4370--95264-band-attention-modulated-retnet-for-face-forgery-detection-zhida-zhang-et-al-2024>(43/70 | 95/264) Band-Attention Modulated RetNet for Face Forgery Detection (Zhida Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhida Zhang, Jie Cao, Wenkui Yang, Qihang Fan, Kai Zhou, Ran He. (2024)<br><strong>Band-Attention Modulated RetNet for Face Forgery Detection</strong><br><button class=copy-to-clipboard title="Band-Attention Modulated RetNet for Face Forgery Detection" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06022v1.pdf filename=2404.06022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>transformer</b> networks are extensively utilized in face forgery detection due to their scalability across large datasets.Despite their success, <b>transformers</b> face challenges in balancing the capture of global context, which is crucial for unveiling forgery clues, with computational complexity.To mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a lightweight network designed to efficiently process extensive visual contexts while avoiding catastrophic forgetting.Our approach empowers the target token to perceive global information by assigning differential attention levels to tokens at varying distances. We implement <b>self-attention</b> along both spatial axes, thereby maintaining spatial priors and easing the computational burden.Moreover, we present the adaptive frequency Band-Attention Modulation mechanism, which treats the entire Discrete Cosine Transform spectrogram as a series of frequency bands with learnable weights.Together, BAR-Net achieves favorable performance on several face forgery datasets, outperforming current state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=4470--96264-training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation-luca-barsellotti-et-al-2024>(44/70 | 96/264) Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation (Luca Barsellotti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Barsellotti, Roberto Amoroso, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara. (2024)<br><strong>Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation</strong><br><button class=copy-to-clipboard title="Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Diffusion Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06542v1.pdf filename=2404.06542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form. Previous works have trained over large amounts of image-caption pairs to enforce pixel-level <b>multimodal</b> alignments. However, captions provide global information about the semantics of a given image but lack direct localization of individual concepts. Further, training on large-scale datasets inevitably brings significant computational costs. In this paper, we propose FreeDA, a training-free <b>diffusion-augmented</b> <b>method</b> for open-vocabulary semantic segmentation, which leverages the ability of <b>diffusion</b> <b>models</b> to visually localize generated concepts and local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an offline stage in which textual-visual reference embeddings are collected, starting from a large set of captions and leveraging visual and semantic contexts. At test time, these are queried to support the visual matching process, which is carried out by jointly considering class-agnostic regions and global semantic similarities. Extensive analyses demonstrate that FreeDA achieves state-of-the-art performance on five datasets, surpassing previous methods by more than 7.0 average points in terms of mIoU and without requiring any training.</p></p class="citation"></blockquote><h3 id=4570--97264-reconstructing-hand-held-objects-in-3d-jane-wu-et-al-2024>(45/70 | 97/264) Reconstructing Hand-Held Objects in 3D (Jane Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jane Wu, Georgios Pavlakos, Georgia Gkioxari, Jitendra Malik. (2024)<br><strong>Reconstructing Hand-Held Objects in 3D</strong><br><button class=copy-to-clipboard title="Reconstructing Hand-Held Objects in 3D" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06507v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06507v2.pdf filename=2404.06507v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Objects manipulated by the hand (i.e., manipulanda) are particularly challenging to reconstruct from in-the-wild RGB images or videos. Not only does the hand occlude much of the object, but also the object is often only visible in a small number of image pixels. At the same time, two strong anchors emerge in this setting: (1) estimated 3D hands help disambiguate the location and scale of the object, and (2) the set of manipulanda is small relative to all possible objects. With these insights in mind, we present a scalable paradigm for handheld object reconstruction that builds on recent breakthroughs in large language/vision models and 3D object datasets. Our model, MCC-Hand-Object (MCC-HO), jointly reconstructs hand and object <b>geometry</b> given a single RGB image and inferred 3D hand as inputs. Subsequently, we use <b>GPT-4(V)</b> to retrieve a 3D object model that matches the object in the image and rigidly align the model to the network-inferred <b>geometry;</b> we call this alignment Retrieval-Augmented Reconstruction (RAR). Experiments demonstrate that MCC-HO achieves state-of-the-art performance on lab and Internet datasets, and we show how RAR can be used to automatically obtain 3D labels for in-the-wild images of hand-object interactions.</p></p class="citation"></blockquote><h3 id=4670--98264-flamefinder-illuminating-obscured-fire-through-smoke-with-attentive-deep-metric-learning-hossein-rajoli-et-al-2024>(46/70 | 98/264) FlameFinder: Illuminating Obscured Fire through Smoke with Attentive Deep Metric Learning (Hossein Rajoli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hossein Rajoli, Sahand Khoshdel, Fatemeh Afghah, Xiaolong Ma. (2024)<br><strong>FlameFinder: Illuminating Obscured Fire through Smoke with Attentive Deep Metric Learning</strong><br><button class=copy-to-clipboard title="FlameFinder: Illuminating Obscured Fire through Smoke with Attentive Deep Metric Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06653v1.pdf filename=2404.06653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>FlameFinder is a deep metric learning (DML) framework designed to accurately detect flames, even when obscured by smoke, using thermal images from firefighter drones during wildfire monitoring. Traditional RGB cameras struggle in such conditions, but thermal cameras can capture smoke-obscured flame features. However, they lack absolute thermal reference points, leading to false positives.To address this issue, FlameFinder utilizes paired thermal-RGB images for training. By learning latent flame features from smoke-free samples, the model becomes less biased towards relative thermal gradients. In testing, it identifies flames in smoky patches by analyzing their equivalent thermal-domain distribution. This method improves performance using both <b>supervised</b> and distance-based <b>clustering</b> metrics.The framework incorporates a flame segmentation method and a DML-aided detection framework. This includes utilizing center loss (CL), triplet center loss (TCL), and triplet cosine center loss (TCCL) to identify optimal cluster representatives for classification. However, the dominance of center loss over the other losses leads to the model missing features sensitive to them. To address this limitation, an attention mechanism is proposed. This mechanism allows for non-uniform feature contribution, amplifying the critical role of cosine and triplet loss in the DML framework. Additionally, it improves interpretability, class discrimination, and decreases intra-class variance. As a result, the proposed model surpasses the baseline by 4.4% in the FLAME2 dataset and 7% in the FLAME3 dataset for unobscured flame detection accuracy. Moreover, it demonstrates enhanced class separation in obscured scenarios compared to VGG19, ResNet18, and three backbone models tailored for flame detection.</p></p class="citation"></blockquote><h3 id=4770--99264-revising-densification-in-gaussian-splatting-samuel-rota-bulò-et-al-2024>(47/70 | 99/264) Revising Densification in Gaussian Splatting (Samuel Rota Bulò et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder. (2024)<br><strong>Revising Densification in Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Revising Densification in Gaussian Splatting" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06109v1.pdf filename=2404.06109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the limitations of Adaptive Density Control (ADC) in 3D Gaussian Splatting (3DGS), a scene representation method achieving high-quality, photorealistic results for novel view synthesis. ADC has been introduced for automatic 3D point primitive management, controlling densification and <b>pruning,</b> however, with certain limitations in the densification logic. Our main contribution is a more principled, pixel-error driven formulation for density control in 3DGS, leveraging an auxiliary, per-pixel error function as the criterion for densification. We further introduce a mechanism to control the total number of primitives generated per scene and correct a bias in the current opacity handling strategy of ADC during cloning operations. Our approach leads to consistent quality improvements across a variety of <b>benchmark</b> scenes, without sacrificing the method&rsquo;s efficiency.</p></p class="citation"></blockquote><h3 id=4870--100264-unified-entropy-optimization-for-open-set-test-time-adaptation-zhengqing-gao-et-al-2024>(48/70 | 100/264) Unified Entropy Optimization for Open-Set Test-Time Adaptation (Zhengqing Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengqing Gao, Xu-Yao Zhang, Cheng-Lin Liu. (2024)<br><strong>Unified Entropy Optimization for Open-Set Test-Time Adaptation</strong><br><button class=copy-to-clipboard title="Unified Entropy Optimization for Open-Set Test-Time Adaptation" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06065v1.pdf filename=2404.06065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Test-time adaptation (TTA) aims at adapting a model pre-trained on the labeled source domain to the unlabeled target domain. Existing methods usually focus on improving TTA performance under covariate shifts, while neglecting semantic shifts. In this paper, we delve into a realistic open-set TTA setting where the target domain may contain samples from unknown classes. Many state-of-the-art closed-set TTA methods perform poorly when applied to open-set scenarios, which can be attributed to the inaccurate estimation of data distribution and model confidence. To address these issues, we propose a simple but effective framework called unified entropy optimization (UniEnt), which is capable of simultaneously adapting to covariate-shifted in-distribution (csID) data and detecting covariate-shifted <b>out-of-distribution</b> (csOOD) data. Specifically, UniEnt first mines pseudo-csID and pseudo-csOOD samples from test data, followed by entropy minimization on the pseudo-csID data and entropy maximization on the pseudo-csOOD data. Furthermore, we introduce UniEnt+ to alleviate the noise caused by hard data partition leveraging sample-level confidence. Extensive experiments on CIFAR <b>benchmarks</b> and Tiny-ImageNet-C show the superiority of our framework. The code is available at <a href=https://github.com/gaozhengqing/UniEnt>https://github.com/gaozhengqing/UniEnt</a></p></p class="citation"></blockquote><h3 id=4970--101264-the-impact-of-print-and-scan-in-heterogeneous-morph-evaluation-scenarios-richard-e-neddo-et-al-2024>(49/70 | 101/264) The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios (Richard E. Neddo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard E. Neddo, Zander W. Blasingame, Chen Liu. (2024)<br><strong>The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios</strong><br><button class=copy-to-clipboard title="The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06559v1.pdf filename=2404.06559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>morphing</b> attacks present an emerging threat to the <b>face</b> <b>recognition</b> system. On top of that, printing and scanning the morphed images could obscure the artifacts generated during the morphing process, which makes morphed image detection even harder. In this work, we investigate the impact that printing and scanning has on morphing attacks through a series of heterogeneous tests. Our experiments show that we can increase the possibility of a false match by up to 5.64% for DiM and 16.00% for StyleGAN2 when providing an image that has been printed and scanned, regardless it is morphed or bona fide, to a <b>Face</b> <b>Recognition</b> (FR) system. Likewise, using Frechet Inception Distance (FID) metric, strictly print-scanned morph attacks performed on average 9.185% stronger than non-print-scanned digital morphs.</p></p class="citation"></blockquote><h3 id=5070--102264-flying-with-photons-rendering-novel-views-of-propagating-light-anagh-malik-et-al-2024>(50/70 | 102/264) Flying with Photons: Rendering Novel Views of Propagating Light (Anagh Malik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anagh Malik, Noah Juravsky, Ryan Po, Gordon Wetzstein, Kiriakos N. Kutulakos, David B. Lindell. (2024)<br><strong>Flying with Photons: Rendering Novel Views of Propagating Light</strong><br><button class=copy-to-clipboard title="Flying with Photons: Rendering Novel Views of Propagating Light" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06493v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06493v2.pdf filename=2404.06493v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an imaging and neural rendering technique that seeks to synthesize videos of light propagating through a scene from novel, moving camera viewpoints. Our approach relies on a new ultrafast imaging setup to capture a first-of-its kind, multi-viewpoint video dataset with picosecond-level temporal resolution. Combined with this dataset, we introduce an efficient neural volume rendering framework based on the transient field. This field is defined as a mapping from a 3D point and 2D direction to a high-dimensional, <b>discrete-time</b> <b>signal</b> that represents time-varying radiance at ultrafast timescales. Rendering with transient fields naturally accounts for effects due to the finite speed of light, including viewpoint-dependent appearance changes caused by light propagation delays to the camera. We render a range of complex effects, including scattering, specular reflection, refraction, and diffraction. Additionally, we demonstrate removing viewpoint-dependent propagation delays using a time warping procedure, rendering of relativistic effects, and video synthesis of direct and global components of light transport.</p></p class="citation"></blockquote><h3 id=5170--103264-learning-state-invariant-representations-of-objects-from-image-collections-with-state-pose-and-viewpoint-changes-rohan-sarkar-et-al-2024>(51/70 | 103/264) Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes (Rohan Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Sarkar, Avinash Kak. (2024)<br><strong>Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes</strong><br><button class=copy-to-clipboard title="Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06470v1.pdf filename=2404.06470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We add one more invariance - state invariance - to the more commonly used other invariances for learning object representations for recognition and retrieval. By state invariance, we mean robust with respect to changes in the structural form of the object, such as when an umbrella is folded, or when an item of clothing is tossed on the floor. Since humans generally have no difficulty in recognizing objects despite such state changes, we are naturally faced with the question of whether it is possible to devise a neural architecture with similar abilities. To that end, we present a novel dataset, ObjectsWithStateChange, that captures state and pose variations in the object images recorded from arbitrary viewpoints. We believe that this dataset will facilitate research in fine-grained object recognition and retrieval of objects that are capable of state changes. The goal of such research would be to train models capable of generating object embeddings that remain invariant to state changes while also staying invariant to transformations induced by changes in viewpoint, pose, illumination, etc. To demonstrate the usefulness of the ObjectsWithStateChange dataset, we also propose a <b>curriculum</b> <b>learning</b> strategy that uses the similarity relationships in the learned embedding space after each epoch to guide the training process. The model learns discriminative features by comparing visually similar objects within and across different categories, encouraging it to differentiate between objects that may be challenging to distinguish due to changes in their state. We believe that this strategy enhances the model&rsquo;s ability to capture discriminative features for fine-grained tasks that may involve objects with state changes, leading to performance improvements on object-level tasks not only on our new dataset, but also on two other challenging multi-view datasets such as ModelNet40 and ObjectPI.</p></p class="citation"></blockquote><h3 id=5270--104264-daf-bevseg-distortion-aware-fisheye-camera-based-birds-eye-view-segmentation-with-occlusion-reasoning-senthil-yogamani-et-al-2024>(52/70 | 104/264) DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird&rsquo;s Eye View Segmentation with Occlusion Reasoning (Senthil Yogamani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Senthil Yogamani, David Unger, Venkatraman Narayanan, Varun Ravi Kumar. (2024)<br><strong>DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird&rsquo;s Eye View Segmentation with Occlusion Reasoning</strong><br><button class=copy-to-clipboard title="DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird's Eye View Segmentation with Occlusion Reasoning" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06352v1.pdf filename=2404.06352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation is an effective way to perform scene understanding. Recently, segmentation in 3D Bird&rsquo;s Eye View (BEV) space has become popular as its directly used by drive policy. However, there is limited work on BEV segmentation for surround-view fisheye cameras, commonly used in commercial vehicles. As this task has no real-world public dataset and existing synthetic datasets do not handle amodal regions due to occlusion, we create a synthetic dataset using the Cognata simulator comprising diverse road types, weather, and lighting conditions. We generalize the BEV segmentation to work with any camera model; this is useful for mixing diverse cameras. We implement a baseline by applying cylindrical rectification on the fisheye images and using a standard LSS-based BEV segmentation model. We demonstrate that we can achieve better performance without undistortion, which has the adverse effects of increased runtime due to pre-processing, reduced field-of-view, and resampling artifacts. Further, we introduce a distortion-aware learnable BEV pooling strategy that is more effective for the fisheye cameras. We extend the model with an occlusion <b>reasoning</b> module, which is critical for estimating in BEV space. Qualitative performance of DaF-BEVSeg is showcased in the video at <a href=https://streamable.com/ge4v51>https://streamable.com/ge4v51</a>.</p></p class="citation"></blockquote><h3 id=5370--105264-spatial-temporal-multi-level-association-for-video-object-segmentation-deshui-miao-et-al-2024>(53/70 | 105/264) Spatial-Temporal Multi-level Association for Video Object Segmentation (Deshui Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deshui Miao, Xin Li, Zhenyu He, Huchuan Lu, Ming-Hsuan Yang. (2024)<br><strong>Spatial-Temporal Multi-level Association for Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Spatial-Temporal Multi-level Association for Video Object Segmentation" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06265v1.pdf filename=2404.06265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing semi-supervised video object segmentation methods either focus on temporal feature matching or spatial-temporal feature modeling. However, they do not address the issues of sufficient target interaction and efficient parallel processing simultaneously, thereby constraining the learning of dynamic, target-aware features. To tackle these limitations, this paper proposes a spatial-temporal multi-level association framework, which jointly associates reference frame, test frame, and object features to achieve sufficient interaction and parallel target ID association with a spatial-temporal memory bank for efficient video object segmentation. Specifically, we construct a spatial-temporal multi-level feature association module to learn better target-aware features, which formulates feature extraction and interaction as the efficient operations of object <b>self-attention,</b> reference object enhancement, and test reference correlation. In addition, we propose a spatial-temporal memory to assist feature association and temporal ID assignment and correlation. We evaluate the proposed method by conducting extensive experiments on numerous video object segmentation datasets, including DAVIS 2016/2017 val, DAVIS 2017 test-dev, and YouTube-VOS 2018/2019 val. The favorable performance against the state-of-the-art methods demonstrates the effectiveness of our approach. All source code and trained models will be made publicly available.</p></p class="citation"></blockquote><h3 id=5470--106264-hyperparameter-free-medical-image-synthesis-for-sharing-data-and-improving-site-specific-segmentation-alexander-chebykin-et-al-2024>(54/70 | 106/264) Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation (Alexander Chebykin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Chebykin, Peter A. N. Bosman, Tanja Alderliesten. (2024)<br><strong>Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation</strong><br><button class=copy-to-clipboard title="Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06240v1.pdf filename=2404.06240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sharing synthetic medical images is a promising alternative to sharing real images that can improve patient privacy and data <b>security.</b> To get good results, existing methods for medical image synthesis must be manually adjusted when they are applied to unseen data. To remove this manual burden, we introduce a Hyperparameter-Free distributed learning method for automatic medical image Synthesis, Sharing, and Segmentation called HyFree-S3. For three diverse segmentation settings (pelvic MRIs, lung X-rays, polyp photos), the use of HyFree-S3 results in improved performance over training only with site-specific data (in the majority of cases). The hyperparameter-free nature of the method should make data synthesis and sharing easier, potentially leading to an increase in the quantity of available data and consequently the quality of the models trained that may ultimately be applied in the clinic. Our code is available at <a href=https://github.com/AwesomeLemon/HyFree-S3>https://github.com/AwesomeLemon/HyFree-S3</a></p></p class="citation"></blockquote><h3 id=5570--107264-automatic-defect-detection-in-sewer-network-using-deep-learning-based-object-detector-bach-ha-et-al-2024>(55/70 | 107/264) Automatic Defect Detection in Sewer Network Using Deep Learning Based Object Detector (Bach Ha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bach Ha, Birgit Schalter, Laura White, Joachim Koehler. (2024)<br><strong>Automatic Defect Detection in Sewer Network Using Deep Learning Based Object Detector</strong><br><button class=copy-to-clipboard title="Automatic Defect Detection in Sewer Network Using Deep Learning Based Object Detector" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06219v1.pdf filename=2404.06219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Maintaining sewer systems in large cities is important, but also time and effort consuming, because visual inspections are currently done manually. To reduce the amount of aforementioned manual work, defects within sewer pipes should be located and classified automatically. In the past, multiple works have attempted solving this problem using classical image processing, machine learning, or a combination of those. However, each provided solution only focus on detecting a limited set of defect/structure types, such as fissure, root, and/or connection. Furthermore, due to the use of hand-crafted features and small training datasets, generalization is also problematic. In order to overcome these deficits, a sizable dataset with 14.7 km of various sewer pipes were annotated by sewer maintenance experts in the scope of this work. On top of that, an <b>object</b> <b>detector</b> (EfficientDet-D0) was trained for automatic defect detection. From the result of several expermients, peculiar natures of defects in the context of <b>object</b> <b>detection,</b> which greatly effect annotation and training process, are found and discussed. At the end, the final detector was able to detect 83% of defects in the test set; out of the missing 17%, only 0.77% are very severe defects. This work provides an example of applying deep learning-based <b>object</b> <b>detection</b> into an important but quiet engineering field. It also gives some practical pointers on how to annotate peculiar <b>&ldquo;object&rdquo;,</b> <b>such</b> as defects.</p></p class="citation"></blockquote><h3 id=5670--108264-unified-physical-digital-attack-detection-challenge-haocheng-yuan-et-al-2024>(56/70 | 108/264) Unified Physical-Digital Attack Detection Challenge (Haocheng Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haocheng Yuan, Ajian Liu, Junze Zheng, Jun Wan, Jiankang Deng, Sergio Escalera, Hugo Jair Escalante, Isabelle Guyon, Zhen Lei. (2024)<br><strong>Unified Physical-Digital Attack Detection Challenge</strong><br><button class=copy-to-clipboard title="Unified Physical-Digital Attack Detection Challenge" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06211v1.pdf filename=2404.06211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>Anti-Spoofing</b> (FAS) is crucial to safeguard <b>Face</b> <b>Recognition</b> (FR) Systems. In real-world scenarios, FRs are confronted with both physical and digital attacks. However, existing algorithms often address only one type of attack at a time, which poses significant limitations in real-world scenarios where FR systems <b>face</b> <b>hybrid</b> physical-digital threats. To facilitate the research of Unified Attack Detection (UAD) algorithms, a large-scale UniAttackData dataset has been collected. UniAttackData is the largest public dataset for Unified Attack Detection, with a total of 28,706 videos, where each unique identity encompasses all advanced attack types. Based on this dataset, we organized a Unified Physical-Digital <b>Face</b> <b>Attack</b> Detection Challenge to boost the research in Unified Attack Detections. It attracted 136 teams for the development phase, with 13 qualifying for the final round. The results re-verified by the organizing team were used for the final ranking. This paper comprehensively reviews the challenge, detailing the dataset introduction, protocol definition, evaluation criteria, and a summary of published results. Finally, we focus on the detailed analysis of the highest-performing algorithms and offer potential directions for unified physical-digital attack detection inspired by this competition. Challenge Website: <a href=https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024>https://sites.google.com/view/face-anti-spoofing-challenge/welcome/challengecvpr2024</a>.</p></p class="citation"></blockquote><h3 id=5770--109264-enhanced-radar-perception-via-multi-task-learning-towards-refined-data-for-sensor-fusion-applications-huawei-sun-et-al-2024>(57/70 | 109/264) Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications (Huawei Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huawei Sun, Hao Feng, Gianfranco Mauro, Julius Ott, Georg Stettinger, Lorenzo Servadei, Robert Wille. (2024)<br><strong>Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications</strong><br><button class=copy-to-clipboard title="Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV, eess-IV, eess-SP<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06165v1.pdf filename=2404.06165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radar and camera fusion yields robustness in perception tasks by leveraging the strength of both sensors. The typical extracted radar point cloud is 2D without height information due to insufficient antennas along the elevation axis, which challenges the network performance. This work introduces a learning-based approach to infer the height of radar points associated with 3D <b>objects.</b> <b>A</b> novel robust regression loss is introduced to address the sparse target challenge. In addition, a multi-task training strategy is employed, emphasizing important features. The average radar absolute height error decreases from 1.69 to 0.25 meters compared to the state-of-the-art height extension method. The estimated target height values are used to preprocess and enrich radar data for downstream perception tasks. Integrating this refined radar information further enhances the performance of existing radar camera fusion models for <b>object</b> <b>detection</b> and depth estimation tasks.</p></p class="citation"></blockquote><h3 id=5870--110264-gaussian-pancakes-geometrically-regularized-3d-gaussian-splatting-for-realistic-endoscopic-reconstruction-sierra-bonilla-et-al-2024>(58/70 | 110/264) Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction (Sierra Bonilla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sierra Bonilla, Shuai Zhang, Dimitrios Psychogyios, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano. (2024)<br><strong>Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction</strong><br><button class=copy-to-clipboard title="Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06128v1.pdf filename=2404.06128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Within colorectal cancer diagnostics, conventional colonoscopy techniques face critical limitations, including a limited field of view and a lack of depth information, which can impede the detection of precancerous lesions. Current methods struggle to provide comprehensive and accurate 3D reconstructions of the colonic surface which can help minimize the missing regions and reinspection for pre-cancerous polyps. Addressing this, we introduce &lsquo;Gaussian Pancakes&rsquo;, a method that leverages 3D Gaussian Splatting (3D GS) combined with a <b>Recurrent</b> <b>Neural</b> <b>Network-based</b> Simultaneous Localization and Mapping (RNNSLAM) system. By introducing geometric and depth regularization into the 3D GS framework, our approach ensures more accurate alignment of Gaussians with the colon surface, resulting in smoother 3D reconstructions with novel viewing of detailed textures and structures. Evaluations across three diverse datasets show that Gaussian Pancakes enhances novel view synthesis quality, surpassing current leading methods with a 18% boost in PSNR and a 16% improvement in SSIM. It also delivers over 100X faster rendering and more than 10X shorter training times, making it a practical tool for real-time applications. Hence, this holds promise for achieving clinical translation for better detection and diagnosis of colorectal cancer.</p></p class="citation"></blockquote><h3 id=5970--111264-hash3d-training-free-acceleration-for-3d-generation-xingyi-yang-et-al-2024>(59/70 | 111/264) Hash3D: Training-free Acceleration for 3D Generation (Xingyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyi Yang, Xinchao Wang. (2024)<br><strong>Hash3D: Training-free Acceleration for 3D Generation</strong><br><button class=copy-to-clipboard title="Hash3D: Training-free Acceleration for 3D Generation" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06091v1.pdf filename=2404.06091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D <b>diffusion</b> <b>models.</b> Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and <b>diffusion</b> <b>time-steps</b> in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the <b>diffusion</b> <b>model&rsquo;s</b> inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D&rsquo;s versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D&rsquo;s integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at <a href=https://adamdad.github.io/hash3D/>https://adamdad.github.io/hash3D/</a>.</p></p class="citation"></blockquote><h3 id=6070--112264-diffusion-based-point-cloud-super-resolution-for-mmwave-radar-data-kai-luan-et-al-2024>(60/70 | 112/264) Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data (Kai Luan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Luan, Chenghao Shi, Neng Wang, Yuwei Cheng, Huimin Lu, Xieyuanli Chen. (2024)<br><strong>Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data</strong><br><button class=copy-to-clipboard title="Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06012v1.pdf filename=2404.06012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the <b>diffusion</b> <b>model</b> defined by mean-reverting stochastic differential equations(SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.</p></p class="citation"></blockquote><h3 id=6170--113264-storyimager-a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion-ming-tao-et-al-2024>(61/70 | 113/264) StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion (Ming Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu. (2024)<br><strong>StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion</strong><br><button class=copy-to-clipboard title="StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05979v1.pdf filename=2404.05979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Story visualization aims to generate a series of realistic and coherent images based on a storyline. Current models adopt a frame-by-frame architecture by transforming the pre-trained <b>text-to-image</b> model into an auto-regressive manner. Although these models have shown notable progress, there are still three flaws. 1) The unidirectional generation of auto-regressive manner restricts the usability in many scenarios. 2) The additional introduced story history encoders bring an extremely high computational cost. 3) The story visualization and continuation models are trained and inferred independently, which is not user-friendly. To these ends, we propose a bidirectional, unified, and efficient framework, namely StoryImager. The StoryImager enhances the storyboard generative ability inherited from the pre-trained <b>text-to-image</b> model for a bidirectional generation. Specifically, we introduce a Target Frame Masking Strategy to extend and unify different story image generation tasks. Furthermore, we propose a Frame-Story Cross Attention Module that decomposes the cross attention for local fidelity and global coherence. Moreover, we design a Contextual Feature Extractor to extract contextual information from the whole storyline. The extensive experimental results demonstrate the excellent performance of our StoryImager. The code is available at <a href=https://github.com/tobran/StoryImager>https://github.com/tobran/StoryImager</a>.</p></p class="citation"></blockquote><h3 id=6270--114264-prompt-driven-universal-model-for-view-agnostic-echocardiography-analysis-sekeun-kim-et-al-2024>(62/70 | 114/264) Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis (Sekeun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sekeun Kim, Hui Ren, Peng Guo, Abder-Rahman Ali, Patrick Zhang, Kyungsang Kim, Xiang Li, Quanzheng Li. (2024)<br><strong>Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis</strong><br><button class=copy-to-clipboard title="Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05916v1.pdf filename=2404.05916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Echocardiography segmentation for cardiac analysis is time-consuming and resource-intensive due to the variability in image quality and the necessity to process scans from various standard views. While current automated segmentation methods in echocardiography show promising performance, they are trained on specific scan views to analyze corresponding data. However, this solution has a limitation as the number of required models increases with the number of standard views. To address this, in this paper, we present a <b>prompt-driven</b> universal method for view-agnostic echocardiography analysis. Considering the domain shift between standard views, we first introduce a method called <b>prompt</b> matching, aimed at learning <b>prompts</b> specific to different views by matching <b>prompts</b> and querying input embeddings using a pre-trained vision model. Then, we utilized a pre-trained medical language model to align textual information with pixel data for accurate segmentation. Extensive experiments on three standard views showed that our approach significantly outperforms the state-of-the-art universal methods and achieves comparable or even better performances over the segmentation model trained and tested on same views.</p></p class="citation"></blockquote><h3 id=6370--115264-roadbev-road-surface-reconstruction-in-birds-eye-view-tong-zhao-et-al-2024>(63/70 | 115/264) RoadBEV: Road Surface Reconstruction in Bird&rsquo;s Eye View (Tong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Zhao, Lei Yang, Yichen Xie, Mingyu Ding, Masayoshi Tomizuka, Yintao Wei. (2024)<br><strong>RoadBEV: Road Surface Reconstruction in Bird&rsquo;s Eye View</strong><br><button class=copy-to-clipboard title="RoadBEV: Road Surface Reconstruction in Bird's Eye View" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06605v1.pdf filename=2404.06605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Road surface conditions, especially <b>geometry</b> profiles, enormously affect driving performance of autonomous vehicles. Vision-based online road reconstruction promisingly captures road information in advance. Existing solutions like monocular depth estimation and stereo matching suffer from modest performance. The recent technique of Bird&rsquo;s-Eye-View (BEV) perception provides immense potential to more reliable and accurate reconstruction. This paper uniformly proposes two simple yet effective models for road elevation reconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate road elevation with monocular and stereo images, respectively. The former directly fits elevation values based on voxel features queried from image view, while the latter efficiently recognizes road elevation patterns based on BEV volume representing discrepancy between left and right voxel features. Insightful analyses reveal their consistence and difference with perspective view. Experiments on real-world dataset verify the models&rsquo; effectiveness and superiority. Elevation errors of RoadBEV-mono and RoadBEV-stereo achieve 1.83cm and 0.56cm, respectively. The estimation performance improves by 50% in BEV based on monocular image. Our models are promising for practical applications, providing valuable references for vision-based BEV perception in autonomous driving. The code is released at <a href=https://github.com/ztsrxh/RoadBEV>https://github.com/ztsrxh/RoadBEV</a>.</p></p class="citation"></blockquote><h3 id=6470--116264-pure-turning-polysemantic-neurons-into-pure-features-by-identifying-relevant-circuits-maximilian-dreyer-et-al-2024>(64/70 | 116/264) PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits (Maximilian Dreyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek, Sebastian Lapuschkin. (2024)<br><strong>PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits</strong><br><button class=copy-to-clipboard title="PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06453v1.pdf filename=2404.06453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of mechanistic interpretability aims to study the role of individual neurons in <b>Deep</b> <b>Neural</b> <b>Networks.</b> Single neurons, however, have the capability to act polysemantically and encode for multiple (unrelated) features, which renders their interpretation difficult. We present a method for disentangling polysemanticity of any <b>Deep</b> <b>Neural</b> <b>Network</b> by decomposing a polysemantic neuron into multiple monosemantic &ldquo;virtual&rdquo; neurons. This is achieved by identifying the relevant sub-graph (&ldquo;circuit&rdquo;) for each &ldquo;pure&rdquo; feature. We demonstrate how our approach allows us to find and disentangle various polysemantic units of ResNet models trained on ImageNet. While evaluating feature visualizations using CLIP, our method effectively disentangles representations, improving upon methods based on neuron activations. Our code is available at <a href=https://github.com/maxdreyer/PURE>https://github.com/maxdreyer/PURE</a>.</p></p class="citation"></blockquote><h3 id=6570--117264-seasonal-fire-prediction-using-spatio-temporal-deep-neural-networks-dimitrios-michail-et-al-2024>(65/70 | 117/264) Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks (Dimitrios Michail et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Michail, Lefki-Ioanna Panagiotou, Charalampos Davalas, Ioannis Prapas, Spyros Kondylatos, Nikolaos Ioannis Bountos, Ioannis Papoutsis. (2024)<br><strong>Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06437v1.pdf filename=2404.06437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With climate change expected to exacerbate fire weather conditions, the accurate anticipation of wildfires on a global scale becomes increasingly crucial for disaster mitigation. In this study, we utilize SeasFire, a comprehensive global wildfire dataset with climate, vegetation, oceanic indices, and human-related variables, to enable seasonal wildfire forecasting with machine learning. For the predictive analysis, we train <b>deep</b> <b>learning</b> <b>models</b> with different architectures that capture the spatio-temporal context leading to wildfires. Our investigation focuses on assessing the effectiveness of these models in predicting the presence of burned areas at varying forecasting time horizons globally, extending up to six months into the future, and on how different spatial or/and temporal context affects the performance of the models. Our findings demonstrate the great potential of <b>deep</b> <b>learning</b> <b>models</b> in seasonal fire forecasting; longer input time-series leads to more robust predictions across varying forecasting horizons, while integrating spatial information to capture wildfire spatio-temporal dynamics boosts performance. Finally, our results hint that in order to enhance performance at longer forecasting horizons, a larger receptive field spatially needs to be considered.</p></p class="citation"></blockquote><h3 id=6670--118264-robust-confidence-intervals-in-stereo-matching-using-possibility-theory-roman-malinowski-et-al-2024>(66/70 | 118/264) Robust Confidence Intervals in Stereo Matching using Possibility Theory (Roman Malinowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roman Malinowski, Emmanuelle Sarrazin, Loïc Dumas, Emmanuel Dubois, Sébastien Destercke. (2024)<br><strong>Robust Confidence Intervals in Stereo Matching using Possibility Theory</strong><br><button class=copy-to-clipboard title="Robust Confidence Intervals in Stereo Matching using Possibility Theory" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-10; I-5-1, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06273v1.pdf filename=2404.06273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method for estimating disparity confidence intervals in stereo matching problems. Confidence intervals provide complementary information to usual confidence measures. To the best of our knowledge, this is the first method creating disparity confidence intervals based on the cost volume. This method relies on possibility distributions to interpret the epistemic uncertainty of the cost volume. Our method has the benefit of having a white-box nature, differing in this respect from current state-of-the-art <b>deep</b> <b>neural</b> <b>networks</b> approaches. The accuracy and size of confidence intervals are validated using the Middlebury stereo datasets as well as a dataset of satellite images. This contribution is freely available on GitHub.</p></p class="citation"></blockquote><h3 id=6770--119264-3d-geometry-aware-deformable-gaussian-splatting-for-dynamic-view-synthesis-zhicheng-lu-et-al-2024>(67/70 | 119/264) 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis (Zhicheng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, Yuchao Dai. (2024)<br><strong>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis</strong><br><button class=copy-to-clipboard title="3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06270v1.pdf filename=2404.06270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a 3D <b>geometry-aware</b> deformable Gaussian Splatting method for dynamic view synthesis. Existing neural radiance fields (NeRF) based solutions learn the deformation in an implicit manner, which cannot incorporate 3D scene <b>geometry.</b> Therefore, the learned deformation is not necessarily geometrically coherent, which results in unsatisfactory dynamic view synthesis and 3D dynamic reconstruction. Recently, 3D Gaussian Splatting provides a new representation of the 3D scene, building upon which the 3D <b>geometry</b> could be exploited in learning the complex 3D deformation. Specifically, the scenes are represented as a collection of 3D Gaussian, where each 3D Gaussian is optimized to move and rotate over time to model the deformation. To enforce the 3D scene <b>geometry</b> constraint during deformation, we explicitly extract 3D <b>geometry</b> features and integrate them in learning the 3D deformation. In this way, our solution achieves 3D <b>geometry-aware</b> deformation modeling, which enables improved dynamic view synthesis and 3D dynamic reconstruction. Extensive experimental results on both synthetic and real datasets prove the superiority of our solution, which achieves new state-of-the-art performance. The project is available at <a href=https://npucvr.github.io/GaGS/>https://npucvr.github.io/GaGS/</a></p></p class="citation"></blockquote><h3 id=6870--120264-ghnerf-learning-generalizable-human-features-with-efficient-neural-radiance-fields-arnab-dey-et-al-2024>(68/70 | 120/264) GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields (Arnab Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I. Comport, Srinath Sridhar, Jean Martinet. (2024)<br><strong>GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields</strong><br><button class=copy-to-clipboard title="GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06246v1.pdf filename=2404.06246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising results in 3D scene representations, including 3D human representations. However, these representations often lack crucial information on the underlying human pose and structure, which is crucial for AR/VR applications and games. In this paper, we introduce a novel approach, termed GHNeRF, designed to address these limitations by learning 2D/3D joint locations of human subjects with NeRF representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract essential human features from 2D images, which are then incorporated into the NeRF framework in order to encode human biomechanic features. This allows our network to simultaneously learn biomechanic features, such as joint locations, along with human <b>geometry</b> and texture. To assess the effectiveness of our method, we conduct a comprehensive comparison with state-of-the-art human NeRF techniques and joint estimation algorithms. Our results show that GHNeRF can achieve state-of-the-art results in near real-time.</p></p class="citation"></blockquote><h3 id=6970--121264-rolling-shutter-correction-with-intermediate-distortion-flow-estimation-mingdeng-cao-et-al-2024>(69/70 | 121/264) Rolling Shutter Correction with Intermediate Distortion Flow Estimation (Mingdeng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingdeng Cao, Sidi Yang, Yujiu Yang, Yinqiang Zheng. (2024)<br><strong>Rolling Shutter Correction with Intermediate Distortion Flow Estimation</strong><br><button class=copy-to-clipboard title="Rolling Shutter Correction with Intermediate Distortion Flow Estimation" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06350v1.pdf filename=2404.06350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes to correct the rolling shutter (RS) distorted images by estimating the distortion flow from the global shutter (GS) to RS directly. Existing methods usually perform correction using the undistortion flow from the RS to GS. They initially predict the flow from consecutive RS frames, subsequently rescaling it as the displacement fields from the RS frame to the underlying GS image using time-dependent scaling factors. Following this, RS-aware forward warping is employed to convert the RS image into its GS counterpart. Nevertheless, this strategy is prone to two shortcomings. First, the undistortion flow estimation is rendered inaccurate by merely linear scaling the flow, due to the complex non-linear motion nature. Second, RS-aware forward warping often results in unavoidable artifacts. To address these limitations, we introduce a new framework that directly estimates the distortion flow and rectifies the RS image with the backward warping operation. More specifically, we first propose a global correlation-based flow attention mechanism to estimate the initial distortion flow and GS feature jointly, which are then refined by the following coarse-to-fine decoder layers. Additionally, a multi-distortion flow prediction strategy is integrated to mitigate the issue of inaccurate flow estimation further. Experimental results validate the effectiveness of the proposed method, which outperforms state-of-the-art approaches on various <b>benchmarks</b> while maintaining high efficiency. The project is available at \url{https://github.com/ljzycmd/DFRSC}.</p></p class="citation"></blockquote><h3 id=7070--122264-colormnet-a-memory-based-deep-spatial-temporal-feature-propagation-network-for-video-colorization-yixin-yang-et-al-2024>(70/70 | 122/264) ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization (Yixin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Yang, Jiangxin Dong, Jinhui Tang, Jinshan Pan. (2024)<br><strong>ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization</strong><br><button class=copy-to-clipboard title="ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06251v1.pdf filename=2404.06251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to effectively explore spatial-temporal features is important for video colorization. Instead of stacking multiple frames along the temporal dimension or recurrently propagating estimated features that will accumulate errors or cannot explore information from far-apart frames, we develop a memory-based feature propagation module that can establish reliable connections with features from far-apart frames and alleviate the influence of inaccurately estimated features. To extract better features from each frame for the above-mentioned feature propagation, we explore the features from large-pretrained visual models to guide the feature estimation of each frame so that the estimated features can model complex scenarios. In addition, we note that adjacent frames usually contain similar contents. To explore this property for better spatial and temporal feature utilization, we develop a local attention module to aggregate the features from adjacent frames in a spatial-temporal neighborhood. We formulate our memory-based feature propagation module, large-pretrained visual model guided feature estimation module, and local attention module into an end-to-end trainable network (named ColorMNet) and show that it performs favorably against state-of-the-art methods on both the <b>benchmark</b> datasets and real-world scenarios. The source code and pre-trained models will be available at \url{https://github.com/yyang181/colormnet}.</p></p class="citation"></blockquote><h2 id=csro-12>cs.RO (12)</h2><h3 id=112--123264-large-language-models-to-the-rescue-deadlock-resolution-in-multi-robot-systems-kunal-garg-et-al-2024>(1/12 | 123/264) Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems (Kunal Garg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kunal Garg, Jacob Arkin, Songyuan Zhang, Nicholas Roy, Chuchu Fan. (2024)<br><strong>Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems</strong><br><button class=copy-to-clipboard title="Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CL, cs-RO, cs.RO, math-OC<br>Keyword Score: 73<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Mixed Reality (MR), In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06413v1.pdf filename=2404.06413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> this paper explores the possibility of using <b>LLMs</b> for deadlock resolution. We propose a hierarchical control framework where an <b>LLM</b> resolves deadlocks by assigning a leader and direction for the leader to move along. A <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> based low-level distributed control policy executes the assigned plan. We systematically study various <b>prompting</b> techniques to improve <b>LLM&rsquo;s</b> performance in resolving deadlocks. In particular, as part of <b>prompt</b> engineering, we provide <b>in-context</b> examples for <b>LLMs.</b> We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that <b>LLM-based</b> high-level planners are effective in resolving deadlocks in <b>MRS.</b></p></p class="citation"></blockquote><h3 id=212--124264-3d-branch-point-cloud-completion-for-robotic-pruning-in-apple-orchards-tian-qiu-et-al-2024>(2/12 | 124/264) 3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards (Tian Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Qiu, Alan Zoubi, Nikolai Spine, Lailiang Cheng, Yu Jiang. (2024)<br><strong>3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards</strong><br><button class=copy-to-clipboard title="3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Deep Neural Network, Geometry, Pruning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05953v1.pdf filename=2404.05953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic branch <b>pruning</b> is a significantly growing research area to cope with the shortage of labor force in the context of agriculture. One fundamental requirement in robotic <b>pruning</b> is the perception of detailed <b>geometry</b> and topology of branches. However, the point clouds obtained in agricultural settings often exhibit incompleteness due to several constraints, thereby restricting the accuracy of downstream robotic <b>pruning.</b> In this work, we addressed the issue of point cloud quality through a <b>simulation-based</b> <b>deep</b> <b>neural</b> <b>network,</b> leveraging a Real-to-Simulation (Real2Sim) data generation pipeline that not only eliminates the need for manual parameterization but also guarantees the realism of simulated data. The <b>simulation-based</b> neural network was applied to jointly perform point cloud completion and skeletonization on real-world partial branches, without additional real-world training. The Sim2Real qualitative completion and skeletonization results showed the model&rsquo;s remarkable capability for <b>geometry</b> reconstruction and topology prediction. Additionally, we quantitatively evaluated the Sim2Real performance by comparing branch-level trait characterization errors using raw incomplete data and complete data. The Mean Absolute Error (MAE) reduced by 75% and 8% for branch diameter and branch angle estimation, respectively, using the best complete data, which indicates the effectiveness of the Real2Sim data in a <b>zero-shot</b> generalization setting. The characterization improvements contributed to the precision and efficacy of robotic branch <b>pruning.</b></p></p class="citation"></blockquote><h3 id=312--125264-genchip-generating-robot-policy-code-for-high-precision-and-contact-rich-manipulation-tasks-kaylee-burns-et-al-2024>(3/12 | 125/264) GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks (Kaylee Burns et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaylee Burns, Ajinkya Jain, Keegan Go, Fei Xia, Michael Stark, Stefan Schaal, Karol Hausman. (2024)<br><strong>GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks</strong><br><button class=copy-to-clipboard title="GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-2-9, cs-AI, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06645v1.pdf filename=2404.06645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been successful at generating robot policy code, but so far these results have been limited to high-level tasks that do not require precise movement. It is an open question how well such approaches work for tasks that require <b>reasoning</b> over contact forces and working within tight success tolerances. We find that, with the right action space, <b>LLMs</b> are capable of successfully generating policies for a variety of contact-rich and high-precision manipulation tasks, even under noisy conditions, such as perceptual errors or grasping inaccuracies. Specifically, we reparameterize the action space to include compliance with constraints on the interaction forces and stiffnesses involved in reaching a target pose. We validate this approach on subtasks derived from the Functional Manipulation <b>Benchmark</b> (FMB) and NIST Task Board <b>Benchmarks.</b> Exposing this action space alongside methods for estimating object poses improves policy generation with an <b>LLM</b> by greater than 3x and 4x when compared to non-compliant action spaces</p></p class="citation"></blockquote><h3 id=412--126264-counting-objects-in-a-robotic-hand-francis-tsow-et-al-2024>(4/12 | 126/264) Counting Objects in a Robotic Hand (Francis Tsow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francis Tsow, Tianze Chen, Yu Sun. (2024)<br><strong>Counting Objects in a Robotic Hand</strong><br><button class=copy-to-clipboard title="Counting Objects in a Robotic Hand" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06631v1.pdf filename=2404.06631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A robot performing multi-object grasping needs to sense the number of objects in the hand after grasping. The count plays an important role in determining the robot&rsquo;s next move and the outcome and efficiency of the whole pick-place process. This paper presents a data-driven <b>contrastive</b> <b>learning-based</b> counting classifier with a modified loss function as a simple and effective approach for object counting despite significant occlusion challenges caused by robotic fingers and objects. The model was validated against other models with three different common shapes (spheres, cylinders, and cubes) in <b>simulation</b> and in a real setup. The proposed <b>contrastive</b> <b>learning-based</b> counting approach achieved above 96% accuracy for all three objects in the real setup.</p></p class="citation"></blockquote><h3 id=512--127264-ai-mole-autonomous-iterative-motion-learning-for-unknown-nonlinear-dynamics-with-extensive-experimental-validation-michael-meindl-et-al-2024>(5/12 | 127/264) AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation (Michael Meindl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Meindl, Simon Bachhuber, Thomas Seel. (2024)<br><strong>AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation</strong><br><button class=copy-to-clipboard title="AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06179v1.pdf filename=2404.06179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes Autonomous Iterative Motion Learning (AI-MOLE), a method that enables systems with unknown, nonlinear dynamics to autonomously learn to solve reference tracking tasks. The method iteratively applies an input trajectory to the unknown dynamics, trains a <b>Gaussian</b> <b>process</b> model based on the experimental data, and utilizes the model to update the input trajectory until desired tracking performance is achieved. Unlike existing approaches, the proposed method determines necessary parameters automatically, i.e., AI-MOLE works plug-and-play and without manual parameter tuning. Furthermore, AI-MOLE only requires input/output information, but can also exploit available state information to accelerate learning. While other approaches are typically only validated in <b>simulation</b> or on a single real-world testbed using manually tuned parameters, we present the unprecedented result of validating the proposed method on three different real-world robots and a total of nine different reference tracking tasks without requiring any a priori model information or manual parameter tuning. Over all systems and tasks, AI-MOLE rapidly learns to track the references without requiring any manual parameter tuning at all, even if only input/output information is available.</p></p class="citation"></blockquote><h3 id=612--128264-robot-safe-planning-in-dynamic-environments-based-on-model-predictive-control-using-control-barrier-function-zetao-lu-et-al-2024>(6/12 | 128/264) Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function (Zetao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zetao Lu, Kaijun Feng, Jun Xu, Haoyao Chen, Yunjiang Lou. (2024)<br><strong>Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function</strong><br><button class=copy-to-clipboard title="Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05952v1.pdf filename=2404.05952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implementing obstacle avoidance in dynamic environments is a challenging problem for robots. Model predictive control (MPC) is a popular strategy for dealing with this type of problem, and recent work mainly uses control barrier function (CBF) as hard constraints to ensure that the system state remains in the safe set. However, in crowded scenarios, effective solutions may not be obtained due to infeasibility problems, resulting in degraded controller performance. We propose a new MPC framework that integrates CBF to tackle the issue of obstacle avoidance in dynamic environments, in which the infeasibility problem induced by hard constraints operating over the whole prediction horizon is solved by softening the constraints and introducing exact penalty, <b>prompting</b> the robot to actively seek out new paths. At the same time, generalized CBF is extended as a single-step safety constraint of the controller to enhance the safety of the robot during navigation. The efficacy of the proposed method is first shown through <b>simulation</b> experiments, in which a double-integrator system and a unicycle system are employed, and the proposed method outperforms other controllers in terms of safety, feasibility, and navigation efficiency. Furthermore, real-world experiment on an MR1000 robot is implemented to demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=712--129264-morpheus-a-multimodal-one-armed-robot-assisted-peeling-system-with-human-users-in-the-loop-ruolin-ye-et-al-2024>(7/12 | 129/264) MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop (Ruolin Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruolin Ye, Yifei Hu, Yuhan, Bian, Luke Kulm, Tapomayukh Bhattacharjee. (2024)<br><strong>MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop</strong><br><button class=copy-to-clipboard title="MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06570v1.pdf filename=2404.06570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Meal preparation is an important instrumental activity of daily living~(IADL). While existing research has explored robotic assistance in meal preparation tasks such as cutting and cooking, the crucial task of peeling has received less attention. Robot-assisted peeling, conventionally a bimanual task, is challenging to deploy in the homes of care recipients using two wheelchair-mounted robot arms due to ergonomic and transferring challenges. This paper introduces a robot-assisted peeling system utilizing a single robotic arm and an assistive cutting board, inspired by the way individuals with one functional hand prepare meals. Our system incorporates a <b>multimodal</b> active perception module to determine whether an area on the food is peeled, a <b>human-in-the-loop</b> long-horizon planner to perform task planning while catering to a user&rsquo;s preference for peeling coverage, and a compliant controller to peel the food items. We demonstrate the system on 12 food items representing the extremes of different shapes, sizes, skin thickness, surface textures, skin vs flesh colors, and deformability.</p></p class="citation"></blockquote><h3 id=812--130264-learning-strategies-for-successful-crowd-navigation-rajshree-daulatabad-et-al-2024>(8/12 | 130/264) Learning Strategies For Successful Crowd Navigation (Rajshree Daulatabad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajshree Daulatabad, Serena Nath. (2024)<br><strong>Learning Strategies For Successful Crowd Navigation</strong><br><button class=copy-to-clipboard title="Learning Strategies For Successful Crowd Navigation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 15<br>Keywords: Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06561v1.pdf filename=2404.06561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Teaching autonomous mobile robots to successfully navigate human crowds is a challenging task. Not only does it require planning, but it requires maintaining social norms which may differ from one context to another. Here we focus on crowd navigation, using a neural network to learn specific strategies in-situ with a robot. This allows us to take into account human behavior and reactions toward a real robot as well as learn strategies that are specific to various scenarios in that context. A <b>CNN</b> takes a top-down image of the scene as input and outputs the next action for the robot to take in terms of speed and angle. Here we present the method, experimental results, and quantitatively evaluate our approach.</p></p class="citation"></blockquote><h3 id=912--131264-deep-reinforcement-learning-based-approach-for-a-single-vehicle-persistent-surveillance-problem-with-fuel-constraints-hritik-bana-et-al-2024>(9/12 | 131/264) Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints (Hritik Bana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hritik Bana, Manav Mishra, Saswata Sarkar, Sujeevraja Sanjeevi, Sujit PB, Kaarthik Sundar. (2024)<br><strong>Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06423v1.pdf filename=2404.06423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a deep <b>reinforcement</b> <b>learning-based</b> approach to tackle a persistent surveillance mission requiring a single unmanned aerial vehicle initially stationed at a depot with fuel or time-of-flight constraints to repeatedly visit a set of targets with equal priority. Owing to the vehicle&rsquo;s fuel or time-of-flight constraints, the vehicle must be regularly refueled, or its battery must be recharged at the depot. The objective of the problem is to determine an optimal sequence of visits to the targets that minimizes the maximum time elapsed between successive visits to any target while ensuring that the vehicle never runs out of fuel or charge. We present a deep <b>reinforcement</b> <b>learning</b> algorithm to solve this problem and present the results of numerical experiments that corroborate the effectiveness of this approach in comparison with common-sense greedy heuristics.</p></p class="citation"></blockquote><h3 id=1012--132264-statistical-modelling-of-driving-scenarios-in-road-traffic-using-fleet-data-of-production-vehicles-christian-reichenbächer-et-al-2024>(10/12 | 132/264) Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles (Christian Reichenbächer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Reichenbächer, Jochen Hipp, Oliver Bringmann. (2024)<br><strong>Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles</strong><br><button class=copy-to-clipboard title="Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06288v1.pdf filename=2404.06288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring the safety of road vehicles at an acceptable level requires the absence of any unreasonable risk arising from all potential hazards linked to the intended au-tomated driving function and its implementation. The assurance that there are no unreasonable risks <b>stemming</b> from hazardous behaviours associated to functional insufficiencies is denoted as safety of intended functionality (SOTIF), a concept outlined in the ISO 21448 standard. In this context, the acquisition of real driving data is considered essential for the verification and validation. For this purpose, we are currently developing a method with which data collect-ed representatively from production vehicles can be modelled into a knowledge-based system in the future. A system that represents the probabilities of occur-rence of concrete driving scenarios over the statistical population of road traffic and makes them usable. The method includes the qualitative and quantitative ab-straction of the drives recorded by the sensors in the vehicles, the possibility of subsequent wireless transmission of the abstracted data from the vehicles and the derivation of the distributions and correlations of scenario parameters. This paper provides a summary of the research project and outlines its central idea. To this end, among other things, the needs for statistical information and da-ta from road traffic are elaborated from ISO 21448, the current state of research is addressed, and methodical aspects are discussed.</p></p class="citation"></blockquote><h3 id=1112--133264-adaptable-recovery-behaviors-in-robotics-a-behavior-trees-and-motion-generatorsbtmg-approach-for-failure-management-faseeh-ahmad-et-al-2024>(11/12 | 133/264) Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management (Faseeh Ahmad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faseeh Ahmad, Matthias Mayr, Sulthan Suresh-Fazeela, Volker Kreuger. (2024)<br><strong>Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management</strong><br><button class=copy-to-clipboard title="Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06129v1.pdf filename=2404.06129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In dynamic operational environments, particularly in collaborative robotics, the inevitability of failures necessitates robust and adaptable recovery strategies. Traditional automated recovery strategies, while effective for predefined scenarios, often lack the flexibility required for on-the-fly task management and adaptation to expected failures. Addressing this gap, we propose a novel approach that models recovery behaviors as adaptable robotic skills, leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy representation. This approach distinguishes itself by employing reinforcement learning~(RL) to dynamically refine recovery behavior parameters, enabling a tailored response to a wide array of failure scenarios with minimal <b>human</b> <b>intervention.</b> We assess our methodology through a series of progressively challenging scenarios within a peg-in-a-hole task, demonstrating the approach&rsquo;s effectiveness in enhancing operational efficiency and task success rates in collaborative robotics settings. We validate our approach using a dual-arm KUKA robot.</p></p class="citation"></blockquote><h3 id=1212--134264-body-design-and-gait-generation-of-chair-type-asymmetrical-tripedal-low-rigidity-robot-shintaro-inoue-et-al-2024>(12/12 | 134/264) Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot (Shintaro Inoue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shintaro Inoue, Kento Kawaharazuka, Kei Okada, Masayuki Inaba. (2024)<br><strong>Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot</strong><br><button class=copy-to-clipboard title="Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05932v1.pdf filename=2404.05932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, a chair-type asymmetric tripedal low-rigidity robot was designed based on the three-legged chair character in the movie &ldquo;Suzume&rdquo; and its gait was generated. Its body structure consists of three legs that are asymmetric to the body, so it cannot be easily balanced. In addition, the actuator is a servo motor that can only feed-forward rotational angle commands and the sensor can only sense the robot&rsquo;s posture quaternion. In such an asymmetric and imperfect body structure, we analyzed how gait is generated in walking and stand-up motions by generating gaits with two different methods: a method using linear completion to connect the postures necessary for the gait discovered through trial and error using the actual robot, and a method using the gait generated by <b>reinforcement</b> <b>learning</b> in the simulator and reflecting it to the actual robot. Both methods were able to generate gait that realized walking and stand-up motions, and interesting gait patterns were observed, which differed depending on the method, and were confirmed on the actual robot. Our code and demonstration videos are available here: <a href=https://github.com/shin0805/Chair-TypeAsymmetricalTripedalRobot.git>https://github.com/shin0805/Chair-TypeAsymmetricalTripedalRobot.git</a></p></p class="citation"></blockquote><h2 id=cond-matstat-mech-1>cond-mat.stat-mech (1)</h2><h3 id=11--135264-message-passing-variational-autoregressive-network-for-solving-intractable-ising-models-qunlong-ma-et-al-2024>(1/1 | 135/264) Message Passing Variational Autoregressive Network for Solving Intractable Ising Models (Qunlong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qunlong Ma, Zhi Ma, Jinlong Xu, Hairui Zhang, Ming Gao. (2024)<br><strong>Message Passing Variational Autoregressive Network for Solving Intractable Ising Models</strong><br><button class=copy-to-clipboard title="Message Passing Variational Autoregressive Network for Solving Intractable Ising Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.stat-mech<br>Categories: cond-mat-dis-nn, cond-mat-stat-mech, cond-mat.stat-mech, cs-LG<br>Keyword Score: 73<br>Keywords: Convolutional Neural Network, Message-Passing, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Deep Neural Network, Unsupervised Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06225v1.pdf filename=2404.06225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many <b>deep</b> <b>neural</b> <b>networks</b> have been used to solve Ising models, including autoregressive neural networks, <b>convolutional</b> <b>neural</b> <b>networks,</b> <b>recurrent</b> <b>neural</b> <b>networks,</b> and <b>graph</b> <b>neural</b> <b>networks.</b> Learning a probability distribution of energy configuration or finding the ground states of a disordered, fully connected Ising model is essential for statistical mechanics and NP-hard problems. Despite tremendous efforts, a neural network architecture with the ability to high-accurately solve these fully connected and extremely intractable problems on larger systems is still lacking. Here we propose a variational autoregressive architecture with a message passing mechanism, which can effectively utilize the interactions between spin variables. The new network trained under an annealing framework outperforms existing methods in solving several prototypical Ising spin Hamiltonians, especially for larger spin systems at low temperatures. The advantages also come from the great mitigation of mode collapse during the training process of <b>deep</b> <b>neural</b> <b>networks.</b> Considering these extremely difficult problems to be solved, our method extends the current computational limits of <b>unsupervised</b> neural networks to solve combinatorial optimization problems.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--136264-multimodal-road-network-generation-based-on-large-language-model-jiajing-chen-et-al-2024>(1/4 | 136/264) Multimodal Road Network Generation Based on Large Language Model (Jiajing Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajing Chen, Weihang Xu, Haiming Cao, Zihuan Xu, Yu Zhang, Zhao Zhang, Siyao Zhang. (2024)<br><strong>Multimodal Road Network Generation Based on Large Language Model</strong><br><button class=copy-to-clipboard title="Multimodal Road Network Generation Based on Large Language Model" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator, ChatGPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06227v1.pdf filename=2404.06227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing popularity of <b>ChatGPT,</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated their capabilities in communication and <b>reasoning,</b> promising for transportation sector intelligentization. However, they still face challenges in domain-specific knowledge. This paper aims to leverage <b>LLMs&rsquo;</b> <b>reasoning</b> and recognition abilities to replace traditional user interfaces and create an &ldquo;intelligent operating system&rdquo; for transportation <b>simulation</b> software, exploring their potential with transportation modeling and <b>simulation.</b> We introduce Network Generation AI (NGAI), integrating <b>LLMs</b> with road network modeling plugins, validated through experiments for accuracy and robustness. NGAI&rsquo;s effective use has reduced modeling costs, revolutionized transportation <b>simulations,</b> optimized user steps, and proposed a novel approach for <b>LLM</b> integration in the transportation field.</p></p class="citation"></blockquote><h3 id=24--137264-apprentices-to-research-assistants-advancing-research-with-large-language-models-m-namvarpour-et-al-2024>(2/4 | 137/264) Apprentices to Research Assistants: Advancing Research with Large Language Models (M. Namvarpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Namvarpour, A. Razi. (2024)<br><strong>Apprentices to Research Assistants: Advancing Research with Large Language Models</strong><br><button class=copy-to-clipboard title="Apprentices to Research Assistants: Advancing Research with Large Language Models" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: I-2; H-5; H-3; K-4; I-7, cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06404v1.pdf filename=2404.06404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While <b>LLMs</b> offer benefits like cost-effectiveness and efficiency, challenges such as <b>prompt</b> tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing <b>LLMs</b> for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as <b>prompt</b> optimization techniques and leveraging human expertise. This study aligns with the <b>&lsquo;LLMs</b> as Research Tools&rsquo; workshop&rsquo;s focus on integrating <b>LLMs</b> into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.</p></p class="citation"></blockquote><h3 id=34--138264-eve-enabling-anyone-to-train-robot-using-augmented-reality-jun-wang-et-al-2024>(3/4 | 138/264) EVE: Enabling Anyone to Train Robot using Augmented Reality (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Chun-Cheng Chang, Jiafei Duan, Dieter Fox, Ranjay Krishna. (2024)<br><strong>EVE: Enabling Anyone to Train Robot using Augmented Reality</strong><br><button class=copy-to-clipboard title="EVE: Enabling Anyone to Train Robot using Augmented Reality" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 20<br>Keywords: Augmented Reality (AR), Augmented Reality (AR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06089v1.pdf filename=2404.06089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task typically requires physical robots and expensive demonstration data from trained human annotators. Consequently, only those with access to physical robots produce demonstrations to train robots. To mitigate this issue, we introduce EVE, an iOS app that enables everyday users to train robots using intuitive <b>augmented</b> <b>reality</b> visualizations without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study ($N=14$, $D=30$) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching-physically moving a real robot-in completion time, usability, motion intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude by enumerating limitations and design considerations for future <b>AR-based</b> demonstration collection systems for robotics.</p></p class="citation"></blockquote><h3 id=44--139264-missing-pieces-how-framing-uncertainty-impacts-longitudinal-trust-in-ai-decision-aids----a-gig-driver-case-study-rex-chen-et-al-2024>(4/4 | 139/264) Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids &ndash; A Gig Driver Case Study (Rex Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rex Chen, Ruiyi Wang, Norman Sadeh, Fei Fang. (2024)<br><strong>Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids &ndash; A Gig Driver Case Study</strong><br><button class=copy-to-clipboard title="Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids -- A Gig Driver Case Study" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06432v1.pdf filename=2404.06432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision aids based on artificial intelligence (AI) are becoming increasingly common. When such systems are deployed in environments with inherent uncertainty, following AI-recommended decisions may lead to a wide range of outcomes. In this work, we investigate how the framing of uncertainty in outcomes impacts users&rsquo; longitudinal trust in AI decision aids, which is crucial to ensuring that these systems achieve their intended purposes. More specifically, we use gig driving as a representative domain to address the question: how does exposing uncertainty at different levels of granularity affect the evolution of users&rsquo; trust and their willingness to rely on recommended decisions? We report on a longitudinal mixed-methods study $(n = 51)$ where we measured the trust of gig drivers as they interacted with an AI-based schedule <b>recommendation</b> tool. Statistically significant quantitative results indicate that participants&rsquo; trust in and willingness to rely on the tool for planning depended on the perceived accuracy of the tool&rsquo;s estimates; that providing ranged estimates improved trust; and that increasing prediction granularity and using hedging language improved willingness to rely on the tool even when trust was low. Additionally, we report on interviews with participants which revealed a diversity of experiences with the tool, suggesting that AI systems must build trust by going beyond general designs to calibrate the expectations of individual users.</p></p class="citation"></blockquote><h2 id=csni-4>cs.NI (4)</h2><h3 id=14--140264-ddpg-e2e-a-novel-policy-gradient-approach-for-end-to-end-communication-systems-bolun-zhang-et-al-2024>(1/4 | 140/264) DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems (Bolun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bolun Zhang, Nguyen Van Huynh, Dinh Thai Hoang, Diep N. Nguyen, Quoc-Viet Pham. (2024)<br><strong>DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems</strong><br><button class=copy-to-clipboard title="DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 65<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Deep Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06257v1.pdf filename=2404.06257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The End-to-end (E2E) learning-based approach has great potential to reshape the existing communication systems by replacing the transceivers with <b>deep</b> <b>neural</b> <b>networks.</b> To this end, the E2E learning approach needs to assume the availability of prior channel information to mathematically formulate a differentiable channel layer for the backpropagation (BP) of the error gradients, thereby jointly optimizing the transmitter and the receiver. However, accurate and instantaneous channel state information is hardly obtained in practical wireless communication scenarios. Moreover, the existing E2E learning-based solutions exhibit limited performance in data transmissions with large block lengths. In this article, these practical issues are addressed by our proposed <b>deep</b> <b>deterministic</b> <b>policy</b> gradient-based E2E communication system. In particular, the proposed solution utilizes a reward feedback mechanism to train both the transmitter and the receiver, which alleviates the information loss of error gradients during BP. In addition, a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)-based</b> architecture is developed to mitigate the curse of dimensionality problem when transmitting messages with large block lengths. Extensive <b>simulations</b> then demonstrate that our proposed solution can not only jointly train the transmitter and the receiver simultaneously without requiring the prior channel knowledge but also can obtain significant performance improvement on block error rate compared to state-of-the-art solutions.</p></p class="citation"></blockquote><h3 id=24--141264-dynamic-d2d-assisted-federated-learning-over-o-ran-performance-analysis-mac-scheduler-and-asymmetric-user-selection-payam-abdisarabshali-et-al-2024>(2/4 | 141/264) Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection (Payam Abdisarabshali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Payam Abdisarabshali, Kwang Taik Kim, Michael Langberg, Weifeng Su, Seyyedali Hosseinalipour. (2024)<br><strong>Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection</strong><br><button class=copy-to-clipboard title="Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-LG, cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Discrete Time, Discrete Time, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06324v1.pdf filename=2404.06324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing studies on <b>federated</b> <b>learning</b> (FL) are mostly focused on system orchestration for static snapshots of the network and making static control decisions (e.g., spectrum allocation). However, real-world wireless networks are susceptible to temporal variations of wireless channel capacity and users&rsquo; datasets. In this paper, we incorporate multi-granular system dynamics (MSDs) into FL, including (M1) dynamic wireless channel capacity, captured by a set of <b>discrete-time</b> <b>events,</b> called $\mathscr{D}$-Events, and (M2) dynamic datasets of users. The latter is characterized by (M2-a) modeling the dynamics of user&rsquo;s dataset size via an ordinary differential equation and (M2-b) introducing dynamic model drift}, formulated via a partial differential inequality} drawing concrete analytical connections between the dynamics of users&rsquo; datasets and FL accuracy. We then conduct FL orchestration under MSDs by introducing dynamic cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique features of open radio access network (O-RAN). DCLM proposes (i) a hierarchical device-to-device (D2D)-assisted model training, (ii) dynamic control decisions through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection. We provide extensive theoretical analysis to study the convergence of DCLM. We then optimize the degrees of freedom (e.g., user selection and spectrum allocation) in DCLM through a highly non-convex optimization problem. We develop a systematic approach to obtain the solution for this problem, opening the door to solving a broad variety of network-aware FL optimization problems. We show the efficiency of DCLM via numerical <b>simulations</b> and provide a series of future directions.</p></p class="citation"></blockquote><h3 id=34--142264-streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai-wanting-yang-et-al-2024>(3/4 | 142/264) Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI (Wanting Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanting Yang, Zehui Xiong, Tony Q. S. Quek, Xuemin Shen. (2024)<br><strong>Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI</strong><br><button class=copy-to-clipboard title="Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 13<br>Keywords: Generative AI, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06182v1.pdf filename=2404.06182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of 6G, featuring compelling visions of digital twins and metaverses, Extended Reality (XR) has emerged as a vital conduit connecting the digital and physical realms, garnering widespread interest. Ensuring a fully immersive wireless XR experience stands as a paramount technical necessity, demanding the liberation of XR from the confines of wired connections. In this paper, we first introduce the technologies applied in the wireless XR domain, delve into their benefits and limitations, and highlight the ongoing challenges. We then propose a novel deployment framework for a broad XR pipeline, termed &ldquo;GeSa-XRF&rdquo;, inspired by the core philosophy of Semantic Communication (SemCom) which shifts the concern from &ldquo;how&rdquo; to transmit to &ldquo;what&rdquo; to transmit. Particularly, the framework comprises three stages: data collection, data analysis, and data delivery. In each stage, we integrate semantic awareness to achieve streamlined transmission and employ <b>Generative</b> <b>Artificial</b> Intelligence (GAI) to achieve collaborative refinements. For the data collection of <b>multi-modal</b> data with differentiated data volumes and heterogeneous latency requirements, we propose a novel SemCom paradigm based on <b>multi-modal</b> fusion and separation and a GAI-based robust superposition scheme. To perform a comprehensive data analysis, we employ multi-task learning to perform the prediction of field of view and personalized attention and discuss the possible preprocessing approaches assisted by GAI. Lastly, for the data delivery stage, we present a semantic-aware multicast-based delivery strategy aimed at reducing pixel level redundant transmissions and introduce the GAI collaborative refinement approach. The performance gain of the proposed GeSa-XRF is preliminarily demonstrated through a case study.</p></p class="citation"></blockquote><h3 id=44--143264-deterministic-and-probabilistic-p4-enabled-lightweight-in-band-network-telemetry-konstantinos-papadopoulos-et-al-2024>(4/4 | 143/264) Deterministic and Probabilistic P4-Enabled Lightweight In-Band Network Telemetry (Konstantinos Papadopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantinos Papadopoulos, Panagiotis Papadimitriou, Chrysa Papagianni. (2024)<br><strong>Deterministic and Probabilistic P4-Enabled Lightweight In-Band Network Telemetry</strong><br><button class=copy-to-clipboard title="Deterministic and Probabilistic P4-Enabled Lightweight In-Band Network Telemetry" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: C-2-3 Network Operations, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06582v1.pdf filename=2404.06582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In-band network telemetry (INT), empowered by programmable dataplanes such as P4, comprises a viable approach to network monitoring and telemetry analysis. However, P4-INT as well as other existing frameworks for INT yield a substantial transmission overhead, which grows linearly with the number of hops and the number of telemetry values. To address this issue, we present a deterministic and a probabilistic technique for lightweight INT, termed as DLINT and PLINT,respectively. In particular, DLINT exercises per-flow aggregation by spreading the telemetry values across the packets of a flow. DLINT relies on switch coordination through the use of per-flow telemetry states, maintained within P4 switches. Furthermore, DLINT utilizes <b>Bloom</b> Filters (BF) in order to compress the state lookup tables within P4 switches. On the other hand, PLINT employs a probabilistic approach based on reservoir sampling. PLINT essentially empowers every INT node to insert telemetry values with equal probability within each packet. Our evaluation results corroborate that both proposed techniques alleviate the transmission overhead of P4-INT, while maintaining a high degree of monitoring accuracy. In addition, we perform a comparative evaluation between DLINT and PLINT. DLINT is more effective in conveying path traces to the telemetry server, whereas PLINT detects more promptly path updates exploiting its more efficient INT header space utilization</p></p class="citation"></blockquote><h2 id=cslg-39>cs.LG (39)</h2><h3 id=139--144264-automated-federated-pipeline-for-parameter-efficient-fine-tuning-of-large-language-models-zihan-fang-et-al-2024>(1/39 | 144/264) Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models (Zihan Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Fang, Zheng Lin, Zhe Chen, Xianhao Chen, Yue Gao, Yuguang Fang. (2024)<br><strong>Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models</strong><br><button class=copy-to-clipboard title="Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Federated Learning, Fine-tuning, Fine-tuning, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06448v1.pdf filename=2404.06448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, for many downstream tasks, it is necessary to <b>fine-tune</b> <b>LLMs</b> using private data. While <b>federated</b> <b>learning</b> offers a promising privacy-preserving solution to <b>LLM</b> <b>fine-tuning,</b> the substantial size of an <b>LLM,</b> combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to <b>LLM</b> <b>fine-tuning.</b> To tackle these problems, we design and implement an automated <b>federated</b> <b>pipeline,</b> named FedPipe, to <b>fine-tune</b> <b>LLMs</b> with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be <b>fine-tuned</b> based on their contributions to the <b>LLM</b> training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to <b>fine-tune</b> the whole <b>LLM.</b> Finally, it appropriately <b>quantizes</b> the parameters of <b>LLM</b> to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=239--145264-fair-graph-neural-network-with-supervised-contrastive-regularization-mahdi-tavassoli-kejani-et-al-2024>(2/39 | 145/264) Fair Graph Neural Network with Supervised Contrastive Regularization (Mahdi Tavassoli Kejani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Tavassoli Kejani, Fadi Dornaika, Jean-Michel Loubes. (2024)<br><strong>Fair Graph Neural Network with Supervised Contrastive Regularization</strong><br><button class=copy-to-clipboard title="Fair Graph Neural Network with Supervised Contrastive Regularization" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Counter-factual, Fairness, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06090v1.pdf filename=2404.06090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have made significant advancements, particularly in tasks such as <b>node</b> <b>classification,</b> link prediction, and <b>graph</b> <b>representation.</b> <b>However,</b> challenges arise from biases that can be hidden not only in the <b>node</b> <b>attributes</b> but also in the connections between entities. Therefore, ensuring <b>fairness</b> in <b>graph</b> <b>neural</b> <b>network</b> learning has become a critical problem. To address this issue, we propose a novel model for training <b>fairness-aware</b> <b>GNN,</b> which enhances the <b>Counterfactual</b> Augmented Fair <b>Graph</b> <b>Neural</b> <b>Network</b> Framework (CAF). Our approach integrates <b>Supervised</b> Contrastive Loss and Environmental Loss to enhance both accuracy and <b>fairness.</b> Experimental validation on three real datasets demonstrates the superiority of our proposed model over CAF and several other existing <b>graph-based</b> <b>learning</b> <b>methods.</b></p></p class="citation"></blockquote><h3 id=339--146264-aggressive-or-imperceptible-or-both-network-pruning-assisted-hybrid-byzantines-in-federated-learning-emre-ozfatura-et-al-2024>(3/39 | 146/264) Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning (Emre Ozfatura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emre Ozfatura, Kerem Ozfatura, Alptekin Kupcu, Deniz Gunduz. (2024)<br><strong>Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning</strong><br><button class=copy-to-clipboard title="Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Federated Learning, Outlier Detection, Pruning, Simulation, Simulator, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06230v1.pdf filename=2404.06230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) has been introduced to enable a large number of clients, possibly mobile devices, to collaborate on generating a generalized machine learning model thanks to utilizing a larger number of local samples without sharing to offer certain privacy to collaborating clients. However, due to the participation of a large number of clients, it is often difficult to profile and verify each client, which leads to a <b>security</b> threat that malicious participants may hamper the accuracy of the trained model by conveying poisoned models during the training. Hence, the aggregation framework at the parameter server also needs to minimize the detrimental effects of these malicious clients. A plethora of attack and defence strategies have been analyzed in the literature. However, often the Byzantine problem is analyzed solely from the <b>outlier</b> <b>detection</b> perspective, being oblivious to the topology of neural networks (NNs). In the scope of this work, we argue that by extracting certain side information specific to the NN topology, one can design stronger attacks. Hence, inspired by the sparse neural networks, we introduce a hybrid sparse Byzantine attack that is composed of two parts: one exhibiting a sparse nature and attacking only certain NN locations with higher sensitivity, and the other being more silent but accumulating over time, where each ideally targets a different type of defence mechanism, and together they form a strong but imperceptible attack. Finally, we show through extensive <b>simulations</b> that the proposed hybrid Byzantine attack is effective against 8 different defence methods.</p></p class="citation"></blockquote><h3 id=439--147264-elephants-never-forget-memorization-and-learning-of-tabular-data-in-large-language-models-sebastian-bordt-et-al-2024>(4/39 | 147/264) Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models (Sebastian Bordt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana. (2024)<br><strong>Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models</strong><br><button class=copy-to-clipboard title="Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06209v1.pdf filename=2404.06209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many have shown how <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that <b>LLMs</b> have memorized many popular tabular datasets verbatim. We then compare the <b>few-shot</b> <b>learning</b> performance of <b>LLMs</b> on datasets that were seen during training to the performance on datasets released after training. We find that <b>LLMs</b> perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, <b>LLMs</b> show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the <b>in-context</b> statistical learning abilities of <b>LLMs.</b> Without <b>fine-tuning,</b> we find them to be limited. This suggests that much of the <b>few-shot</b> <b>performance</b> on novel datasets is due to the <b>LLM&rsquo;s</b> world knowledge. Overall, our results highlight the importance of testing whether an <b>LLM</b> has seen an evaluation dataset during pre-training. We make the exposure tests we developed available as the tabmemcheck Python package at <a href=https://github.com/interpretml/LLM-Tabular-Memorization-Checker>https://github.com/interpretml/LLM-Tabular-Memorization-Checker</a></p></p class="citation"></blockquote><h3 id=539--148264-sccdcg-efficient-deep-structural-clustering-for-single-cell-rna-seq-via-deep-cut-informed-graph-embedding-ping-xu-et-al-2024>(5/39 | 148/264) scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding (Ping Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ping Xu, Zhiyuan Ning, Meng Xiao, Guihai Feng, Xin Li, Yuanchun Zhou, Pengfei Wang. (2024)<br><strong>scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding</strong><br><button class=copy-to-clipboard title="scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-GN<br>Keyword Score: 56<br>Keywords: Graph, Graph Embedding, Graph Neural Network, Autoencoder, Clustering, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06167v1.pdf filename=2404.06167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements. Despite its potential, traditional <b>clustering</b> methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies. Existing strategies, including <b>graph</b> <b>neural</b> <b>networks,</b> face challenges in handling the inefficiency due to scRNA-seq data&rsquo;s intrinsic high-dimension and high-sparsity. Addressing these limitations, we introduce scCDCG (single-cell RNA-seq <b>Clustering</b> via Deep Cut-informed <b>Graph),</b> <b>a</b> <b>novel</b> framework designed for efficient and accurate <b>clustering</b> of scRNA-seq data that simultaneously utilizes intercellular high-order structural information. scCDCG comprises three main components: (i) A <b>graph</b> <b>embedding</b> <b>module</b> utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior <b>graph</b> <b>neural</b> <b>network</b> methods. (ii) A <b>self-supervised</b> <b>learning</b> module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity. (iii) An <b>autoencoder-based</b> feature learning module that simplifies model complexity through effective dimension reduction and feature extraction. Our extensive experiments on 6 datasets demonstrate scCDCG&rsquo;s superior performance and efficiency compared to 7 established models, underscoring scCDCG&rsquo;s potential as a transformative tool in scRNA-seq data analysis. Our code is available at: <a href=https://github.com/XPgogogo/scCDCG>https://github.com/XPgogogo/scCDCG</a>.</p></p class="citation"></blockquote><h3 id=639--149264-generative-pre-trained-transformer-for-symbolic-regression-base-in-context-reinforcement-learning-yanjie-li-et-al-2024>(6/39 | 149/264) Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning (Yanjie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng. (2024)<br><strong>Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Reinforcement Learning, GPT, Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06330v1.pdf filename=2404.06330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or <b>reinforcement</b> <b>learning</b> algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this <b>GPT</b> can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \textbf{FormulaGPT}, which trains a <b>GPT</b> using massive sparse reward learning histories of <b>reinforcement</b> <b>learning-based</b> SR algorithms as training data. After training, the SR algorithm based on <b>reinforcement</b> <b>learning</b> is <b>distilled</b> into a <b>Transformer.</b> When new test data comes, FormulaGPT can directly generate a <b>&ldquo;reinforcement</b> <b>learning</b> process&rdquo; and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.</p></p class="citation"></blockquote><h3 id=739--150264-does-transformer-interpretability-transfer-to-rnns-gonçalo-paulo-et-al-2024>(7/39 | 150/264) Does Transformer Interpretability Transfer to RNNs? (Gonçalo Paulo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gonçalo Paulo, Thomas Marshall, Nora Belrose. (2024)<br><strong>Does Transformer Interpretability Transfer to RNNs?</strong><br><button class=copy-to-clipboard title="Does Transformer Interpretability Transfer to RNNs?" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Recurrent Neural Network, Recurrent Neural Network, Transformer, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05971v1.pdf filename=2404.05971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>recurrent</b> <b>neural</b> <b>network</b> architectures, such as Mamba and RWKV, have enabled <b>RNNs</b> to match or exceed the performance of equal-size <b>transformers</b> in terms of language modeling <b>perplexity</b> and downstream evaluations, suggesting that future systems may be built on completely new architectures. In this paper, we examine if selected interpretability methods originally designed for <b>transformer</b> language models will transfer to these up-and-coming <b>recurrent</b> <b>architectures.</b> <b>Specifically,</b> we focus on steering model outputs via contrastive activation addition, on eliciting latent predictions via the tuned lens, and eliciting latent knowledge from models <b>fine-tuned</b> to produce false outputs under certain conditions. Our results show that most of these techniques are effective when applied to <b>RNNs,</b> and we show that it is possible to improve some of them by taking advantage of <b>RNNs&rsquo;</b> compressed state.</p></p class="citation"></blockquote><h3 id=839--151264-causalbench-a-comprehensive-benchmark-for-causal-learning-capability-of-large-language-models-yu-zhou-et-al-2024>(8/39 | 151/264) CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models (Yu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhou, Xingyu Wu, Beicheng Huang, Jibin Wu, Liang Feng, Kay Chen Tan. (2024)<br><strong>CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models</strong><br><button class=copy-to-clipboard title="CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Counter-factual, Chain-of-thought, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06349v1.pdf filename=2404.06349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causality reveals fundamental principles behind data distributions in real-world scenarios, and the capability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to understand causality directly impacts their efficacy across explaining outputs, adapting to new evidence, and generating <b>counterfactuals.</b> With the proliferation of <b>LLMs,</b> the evaluation of this capacity is increasingly garnering attention. However, the absence of a comprehensive <b>benchmark</b> has rendered existing evaluation studies being straightforward, undiversified, and homogeneous. To address these challenges, this paper proposes a comprehensive <b>benchmark,</b> namely CausalBench, to evaluate the causality understanding capabilities of <b>LLMs.</b> Originating from the causal research community, CausalBench encompasses three causal learning-related tasks, which facilitate a convenient comparison of <b>LLMs&rsquo;</b> performance with classic causal learning algorithms. Meanwhile, causal networks of varying scales and densities are integrated in CausalBench, to explore the upper limits of <b>LLMs&rsquo;</b> capabilities across task scenarios of varying difficulty. Notably, background knowledge and structured data are also incorporated into CausalBench to thoroughly unlock the underlying potential of <b>LLMs</b> for long-text comprehension and prior information utilization. Based on CausalBench, this paper evaluates nineteen leading <b>LLMs</b> and unveils insightful conclusions in diverse aspects. Firstly, we present the strengths and weaknesses of <b>LLMs</b> and quantitatively explore the upper limits of their capabilities across various scenarios. Meanwhile, we further discern the adaptability and abilities of <b>LLMs</b> to specific structural networks and complex chain of thought structures. Moreover, this paper quantitatively presents the differences across diverse information sources and uncovers the gap between <b>LLMs&rsquo;</b> capabilities in causal understanding within textual contexts and numerical domains.</p></p class="citation"></blockquote><h3 id=939--152264-aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts-shaona-ghosh-et-al-2024>(9/39 | 152/264) AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts (Shaona Ghosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien. (2024)<br><strong>AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts</strong><br><button class=copy-to-clipboard title="AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Benchmarking, Generative AI, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05993v1.pdf filename=2404.05993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>generative</b> <b>AI</b> become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and <b>benchmarks</b> that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help <b>benchmark</b> <b>LLM</b> models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple <b>LLM-based</b> safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art <b>LLM-based</b> safety models and general purpose <b>LLMs,</b> but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the <b>LLM</b> alignment phase does not negatively impact the performance of the aligned models on <b>MT</b> Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of <b>LLM</b> content safety experts in deployment</p></p class="citation"></blockquote><h3 id=1039--153264-policy-guided-diffusion-matthew-thomas-jackson-et-al-2024>(10/39 | 153/264) Policy-Guided Diffusion (Matthew Thomas Jackson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, Jakob Foerster. (2024)<br><strong>Policy-Guided Diffusion</strong><br><button class=copy-to-clipboard title="Policy-Guided Diffusion" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Distribution Shift, Distribution Shift, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06356v1.pdf filename=2404.06356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many real-world settings, agents must learn from an <b>offline</b> <b>dataset</b> <b>gathered</b> by some prior behavior policy. Such a setting naturally leads to <b>distribution</b> <b>shift</b> between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided <b>diffusion.</b> <b>Our</b> method uses <b>diffusion</b> <b>models</b> to generate entire trajectories under the behavior <b>distribution,</b> <b>applying</b> guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided <b>diffusion</b> <b>models</b> a regularized form of the target <b>distribution</b> <b>that</b> balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an <b>offline</b> <b>world</b> <b>model</b> baseline. Using synthetic experience from policy-guided <b>diffusion</b> <b>as</b> a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard <b>offline</b> <b>reinforcement</b> <b>learning</b> algorithms and environments. Our approach provides an effective alternative to autoregressive <b>offline</b> <b>world</b> <b>models,</b> opening the door to the controllable generation of synthetic training data.</p></p class="citation"></blockquote><h3 id=1139--154264-differential-privacy-for-anomaly-detection-analyzing-the-trade-off-between-privacy-and-explainability-fatima-ezzeddine-et-al-2024>(11/39 | 154/264) Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability (Fatima Ezzeddine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano. (2024)<br><strong>Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability</strong><br><button class=copy-to-clipboard title="Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Explainable AI, Outlier Detection, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06144v1.pdf filename=2404.06144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> (AD), also referred to as <b>outlier</b> <b>detection,</b> is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data. Such a process finds wide application in various fields, such as finance and healthcare. While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount. The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties. In this work, we exploit the trade-off of applying <b>Explainable</b> <b>AI</b> (XAI) through SHapley Additive exPlanations (SHAP) and <b>differential</b> <b>privacy</b> (DP). We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability. Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model. We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm.</p></p class="citation"></blockquote><h3 id=1239--155264-variational-stochastic-gradient-descent-for-deep-neural-networks-haotian-chen-et-al-2024>(12/39 | 155/264) Variational Stochastic Gradient Descent for Deep Neural Networks (Haotian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Chen, Anna Kuzina, Babak Esmaeili, Jakub M Tomczak. (2024)<br><strong>Variational Stochastic Gradient Descent for Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Variational Stochastic Gradient Descent for Deep Neural Networks" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 35<br>Keywords: Deep Neural Network, Probabilistic Model, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06549v1.pdf filename=2404.06549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing <b>deep</b> <b>neural</b> <b>networks</b> is one of the main tasks in successful <b>deep</b> <b>learning.</b> <b>Current</b> state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a <b>probabilistic</b> <b>framework</b> for better estimation of gradients and modeling uncertainties. Here, we propose to combine both approaches, resulting in the Variational <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> (VSGD) optimizer. We model gradient updates as a <b>probabilistic</b> <b>model</b> and utilize <b>stochastic</b> <b>variational</b> <b>inference</b> (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four <b>deep</b> <b>neural</b> <b>network</b> architectures, where we show that VSGD outperforms Adam and <b>SGD.</b></p></p class="citation"></blockquote><h3 id=1339--156264-pfl-research-simulation-framework-for-accelerating-research-in-private-federated-learning-filip-granqvist-et-al-2024>(13/39 | 156/264) pfl-research: simulation framework for accelerating research in Private Federated Learning (Filip Granqvist et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Granqvist, Congzheng Song, Áine Cahill, Rogier van Dalen, Martin Pelikan, Yi Sheng Chan, Xiaojun Feng, Natarajan Krishnaswami, Vojta Jina, Mona Chitnis. (2024)<br><strong>pfl-research: simulation framework for accelerating research in Private Federated Learning</strong><br><button class=copy-to-clipboard title="pfl-research: simulation framework for accelerating research in Private Federated Learning" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06430v1.pdf filename=2404.06430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a <b>simulation</b> environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce pfl-research, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that pfl-research is 7-72$\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of <b>benchmarks</b> that evaluates an algorithm&rsquo;s overall performance on a diverse set of realistic scenarios. The code is available on GitHub at <a href=https://github.com/apple/pfl-research>https://github.com/apple/pfl-research</a>.</p></p class="citation"></blockquote><h3 id=1439--157264-the-impact-of-data-set-similarity-and-diversity-on-transfer-learning-success-in-time-series-forecasting-claudia-ehrig-et-al-2024>(14/39 | 157/264) The impact of data set similarity and diversity on transfer learning success in time series forecasting (Claudia Ehrig et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Claudia Ehrig, Catherine Cleophas, Germain Forestier. (2024)<br><strong>The impact of data set similarity and diversity on transfer learning success in time series forecasting</strong><br><button class=copy-to-clipboard title="The impact of data set similarity and diversity on transfer learning success in time series forecasting" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Transfer Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06198v1.pdf filename=2404.06198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models, pre-trained on a similar or diverse source data set, have become pivotal in enhancing the efficiency and accuracy of time series forecasting on target data sets by leveraging <b>transfer</b> <b>learning.</b> While <b>benchmarks</b> validate the performance of model generalization on various target data sets, there is no structured research providing similarity and diversity measures explaining which characteristics of source and target data lead to <b>transfer</b> <b>learning</b> success. Our study pioneers in systematically evaluating the impact of source-target similarity and source diversity on <b>zero-shot</b> and <b>fine-tuned</b> forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We investigate these dynamics using pre-trained neural networks across five public source datasets, applied in forecasting five target data sets, including real-world wholesales data. We identify two feature-based similarity and diversity measures showing: Source-target similarity enhances forecasting accuracy and reduces bias, while source diversity enhances forecasting accuracy and uncertainty estimation and increases the bias.</p></p class="citation"></blockquote><h3 id=1539--158264-fmda-ot-federated-multi-source-domain-adaptation-through-optimal-transport-omar-ghannou-et-al-2024>(15/39 | 158/264) FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport (Omar Ghannou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omar Ghannou, Younès Bennani. (2024)<br><strong>FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport</strong><br><button class=copy-to-clipboard title="FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Fine-tuning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06599v1.pdf filename=2404.06599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-source <b>Domain</b> <b>Adaptation</b> (MDA) aims to adapt models trained on multiple labeled source <b>domains</b> <b>to</b> an unlabeled target <b>domain.</b> <b>In</b> this paper, we introduce our approach as a collaborative MDA framework, which comprises two adaptation phases. Firstly, we conduct <b>domain</b> <b>adaptation</b> for each source individually with the target, utilizing optimal transport. Then, in the second phase, which constitutes the final part of the framework, we design the architecture of centralized <b>federated</b> <b>learning</b> to collaborate the N models representing the N sources. This architecture offers the advantage of using the sources without accessing their data, thus resolving data privacy issues inherent in <b>domain</b> <b>adaptation.</b> Additionally, during this phase, the server guides and <b>fine-tunes</b> the adaptation using a small number of pseudo-labeled samples available in the target <b>domain,</b> <b>referred</b> to as the target validation subset of the dataset.</p></p class="citation"></blockquote><h3 id=1639--159264-diverse-randomized-value-functions-a-provably-pessimistic-approach-for-offline-reinforcement-learning-xudong-yu-et-al-2024>(16/39 | 159/264) Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning (Xudong Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Yu, Chenjia Bai, Hongyi Guo, Changhong Wang, Zhen Wang. (2024)<br><strong>Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Offline Reinforcement Learning, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06188v1.pdf filename=2404.06188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>Reinforcement</b> <b>Learning</b> (RL) faces distributional shift and unreliable value estimation, especially for <b>out-of-distribution</b> (OOD) actions. To address this, existing uncertainty-based methods penalize the value function with uncertainty quantification and demand numerous ensemble networks, posing computational challenges and suboptimal outcomes. In this paper, we introduce a novel strategy employing diverse randomized value functions to estimate the posterior distribution of $Q$-values. It provides robust uncertainty quantification and estimates lower confidence bounds (LCB) of $Q$-values. By applying moderate value penalties for OOD actions, our method fosters a provably pessimistic approach. We also emphasize on diversity within randomized value functions and enhance efficiency by introducing a diversity regularization method, reducing the requisite number of networks. These modules lead to reliable value estimation and efficient policy learning from <b>offline</b> <b>data.</b> <b>Theoretical</b> analysis shows that our method recovers the provably efficient LCB-penalty under linear MDP assumptions. Extensive empirical results also demonstrate that our proposed method significantly outperforms baseline methods in terms of performance and parametric efficiency.</p></p class="citation"></blockquote><h3 id=1739--160264-clip-embed-kd-computationally-efficient-knowledge-distillation-using-embeddings-as-teachers-lakshmi-nair-2024>(17/39 | 160/264) CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers (Lakshmi Nair, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lakshmi Nair. (2024)<br><strong>CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers</strong><br><button class=copy-to-clipboard title="CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06170v1.pdf filename=2404.06170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pre-training (CLIP) has been shown to improve <b>zero-shot</b> generalization capabilities of language and vision models. In this paper, we extend CLIP for efficient <b>knowledge</b> <b>distillation,</b> by utilizing embeddings as teachers. Typical <b>knowledge</b> <b>distillation</b> frameworks require running forward passes through a teacher model, which is often prohibitive in the case of billion or trillion parameter teachers. In these cases, using only the embeddings of the teacher models to guide the <b>distillation</b> can yield significant computational savings. Our preliminary findings show that CLIP-based <b>knowledge</b> <b>distillation</b> with embeddings can outperform full scale <b>knowledge</b> <b>distillation</b> using $9\times$ less memory and $8\times$ less training time. Code available at: <a href=https://github.com/lnairGT/CLIP-Distillation/>https://github.com/lnairGT/CLIP-Distillation/</a></p></p class="citation"></blockquote><h3 id=1839--161264-zero-shot-relational-learning-for-multimodal-knowledge-graphs-rui-cai-et-al-2024>(18/39 | 161/264) Zero-Shot Relational Learning for Multimodal Knowledge Graphs (Rui Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Cai, Shichao Pei, Xiangliang Zhang. (2024)<br><strong>Zero-Shot Relational Learning for Multimodal Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Zero-Shot Relational Learning for Multimodal Knowledge Graphs" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MM, cs.LG<br>Keyword Score: 24<br>Keywords: Graph, Knowledge Graph, Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06220v1.pdf filename=2404.06220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relational learning is an essential task in the domain of <b>knowledge</b> <b>representation,</b> particularly in <b>knowledge</b> <b>graph</b> completion (KGC).While relational learning in traditional single-modal settings has been extensively studied, exploring it within a <b>multimodal</b> KGC context presents distinct challenges and opportunities. One of the major challenges is inference on newly discovered relations without any associated training data. This <b>zero-shot</b> relational learning scenario poses unique requirements for <b>multimodal</b> KGC, i.e., utilizing multimodality to facilitate relational learning. However, existing works fail to support the leverage of <b>multimodal</b> information and leave the problem unexplored. In this paper, we propose a novel end-to-end framework, consisting of three components, i.e., <b>multimodal</b> learner, structure consolidator, and relation embedding generator, to integrate diverse <b>multimodal</b> information and <b>knowledge</b> <b>graph</b> structures to facilitate the <b>zero-shot</b> relational learning. Evaluation results on two <b>multimodal</b> <b>knowledge</b> <b>graphs</b> demonstrate the superior performance of our proposed method.</p></p class="citation"></blockquote><h3 id=1939--162264-dynamic-deep-learning-based-super-resolution-for-the-shallow-water-equations-maximilian-witte-et-al-2024>(19/39 | 162/264) Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations (Maximilian Witte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Witte, Fabricio Rodrigues Lapolli, Philip Freese, Sebastian Götschel, Daniel Ruprecht, Peter Korn, Christopher Kadow. (2024)<br><strong>Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations</strong><br><button class=copy-to-clipboard title="Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65M99, 68T07, 86-08, 35-11, cs-LG, cs.LG, physics-comp-ph, physics-flu-dyn<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06400v1.pdf filename=2404.06400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using the nonlinear shallow water equations as <b>benchmark,</b> we demonstrate that a <b>simulation</b> with the ICON-O ocean model with a 20km resolution that is frequently corrected by a U-net-type neural network can achieve discretization errors of a <b>simulation</b> with 10km resolution. The network, originally developed for image-based super-resolution in post-processing, is trained to compute the difference between solutions on both meshes and is used to correct the coarse mesh every 12h. Our setup is the Galewsky test case, modeling transition of a barotropic instability into turbulent flow. We show that the ML-corrected coarse resolution run correctly maintains a balance flow and captures the transition to turbulence in line with the higher resolution <b>simulation.</b> After 8 day of <b>simulation,</b> the $L_2$-error of the corrected run is similar to a <b>simulation</b> run on the finer mesh. While mass is conserved in the corrected runs, we observe some spurious generation of kinetic energy.</p></p class="citation"></blockquote><h3 id=2039--163264-pgtnet-a-process-graph-transformer-network-for-remaining-time-prediction-of-business-process-instances-keyvan-amiri-elyasi-et-al-2024>(20/39 | 163/264) PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances (Keyvan Amiri Elyasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyvan Amiri Elyasi, Han van der Aa, Heiner Stuckenschmidt. (2024)<br><strong>PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances</strong><br><button class=copy-to-clipboard title="PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Transformer, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06267v1.pdf filename=2404.06267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PGTNet, an approach that transforms event logs into <b>graph</b> datasets and leverages <b>graph-oriented</b> data for training Process <b>Graph</b> <b>Transformer</b> Networks to predict the remaining time of business process instances. PGTNet consistently outperforms state-of-the-art deep learning approaches across a diverse range of 20 publicly available real-world event logs. Notably, our approach is most promising for highly complex processes, where existing deep learning approaches encounter difficulties <b>stemming</b> from their limited ability to learn control-flow relationships among process activities and capture long-range dependencies. PGTNet addresses these challenges, while also being able to consider multiple process perspectives during the learning process.</p></p class="citation"></blockquote><h3 id=2139--164264-hyperparameter-selection-in-continual-learning-thomas-l-lee-et-al-2024>(21/39 | 164/264) Hyperparameter Selection in Continual Learning (Thomas L. Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas L. Lee, Sigrid Passano Hellan, Linus Ericsson, Elliot J. Crowley, Amos Storkey. (2024)<br><strong>Hyperparameter Selection in Continual Learning</strong><br><button class=copy-to-clipboard title="Hyperparameter Selection in Continual Learning" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Continual Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06466v1.pdf filename=2404.06466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>continual</b> <b>learning</b> (CL) &ndash; where a learner trains on a stream of data &ndash; standard hyperparameter optimisation (HPO) cannot be applied, as a learner does not have access to all of the data at the same time. This has <b>prompted</b> the development of CL-specific HPO frameworks. The most popular way to tune hyperparameters in CL is to repeatedly train over the whole data stream with different hyperparameter settings. However, this end-of-training HPO is unrealistic as in practice a learner can only see the stream once. Hence, there is an open question: what HPO framework should a practitioner use for a CL problem in reality? This paper answers this question by evaluating several realistic HPO frameworks. We find that all the HPO frameworks considered, including end-of-training HPO, perform similarly. We therefore advocate using the realistic and most computationally efficient method: fitting the hyperparameters on the first task and then fixing them throughout training.</p></p class="citation"></blockquote><h3 id=2239--165264-scrdit-generating-single-cell-rna-seq-data-by-diffusion-transformers-and-accelerating-sampling-shengze-dong-et-al-2024>(22/39 | 165/264) scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling (Shengze Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengze Dong, Zhuorui Cui, Ding Liu, Jinzhi Lei. (2024)<br><strong>scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling</strong><br><button class=copy-to-clipboard title="scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-GN<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06153v1.pdf filename=2404.06153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample. While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties. Results: Our study introduces a generative approach termed scRNA-seq Diffusion <b>Transformer</b> (scRDiT). This method generates virtual scRNA-seq data by leveraging a real dataset. The method is a neural network constructed based on Denoising Diffusion <b>Probabilistic</b> <b>Models</b> (DDPMs) and Diffusion <b>Transformers</b> (DiTs). This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples. This scheme allows us to learn data features from actual scRNA-seq samples during model training. Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance. Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples. Availability and implementation: <a href=https://github.com/DongShengze/scRDiT>https://github.com/DongShengze/scRDiT</a></p></p class="citation"></blockquote><h3 id=2339--166264-graph-reinforcement-learning-for-combinatorial-optimization-a-survey-and-unifying-perspective-victor-alexandru-darvariu-et-al-2024>(23/39 | 166/264) Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective (Victor-Alexandru Darvariu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi. (2024)<br><strong>Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective</strong><br><button class=copy-to-clipboard title="Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06492v1.pdf filename=2404.06492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> are a natural representation for systems based on relations between connected entities. Combinatorial optimization problems, which arise when considering an objective function related to a process of interest on discrete structures, are often challenging due to the rapid growth of the solution space. The trial-and-error paradigm of <b>Reinforcement</b> <b>Learning</b> has recently emerged as a promising alternative to traditional methods, such as exact algorithms and (meta)heuristics, for discovering better decision-making strategies in a variety of disciplines including chemistry, computer science, and statistics. Despite the fact that they arose in markedly different fields, these techniques share significant commonalities. Therefore, we set out to synthesize this work in a unifying perspective that we term <b>Graph</b> <b>Reinforcement</b> <b>Learning,</b> interpreting it as a constructive decision-making method for <b>graph</b> problems. After covering the relevant technical background, we review works along the dividing line of whether the goal is to optimize <b>graph</b> structure given a process of interest, or to optimize the outcome of the process itself under fixed <b>graph</b> structure. Finally, we discuss the common challenges facing the field and open research questions. In contrast with other surveys, the present work focuses on non-canonical <b>graph</b> problems for which performant algorithms are typically not known and <b>Reinforcement</b> <b>Learning</b> is able to provide efficient and effective solutions.</p></p class="citation"></blockquote><h3 id=2439--167264-efficient-multi-task-reinforcement-learning-via-task-specific-action-correction-jinyuan-feng-et-al-2024>(24/39 | 167/264) Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction (Jinyuan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyuan Feng, Min Chen, Zhiqiang Pu, Tenghai Qiu, Jianqiang Yi. (2024)<br><strong>Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction</strong><br><button class=copy-to-clipboard title="Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05950v1.pdf filename=2404.05950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-task <b>reinforcement</b> <b>learning</b> (MTRL) demonstrate potential for enhancing the generalization of a robot, enabling it to perform multiple tasks concurrently. However, the performance of MTRL may still be susceptible to conflicts between tasks and negative interference. To facilitate efficient MTRL, we propose Task-Specific Action Correction (TSAC), a general and complementary approach designed for simultaneous learning of multiple tasks. TSAC decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). To alleviate conflicts resulting from excessive focus on specific tasks&rsquo; details in SP, ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective and achieve generalization across tasks. Additional rewards transform the original problem into a multi-objective MTRL problem. Furthermore, to convert the multi-objective MTRL into a single-objective formulation, TSAC assigns a virtual expected budget to the sparse rewards and employs Lagrangian method to transform a constrained single-objective optimization into an unconstrained one. Experimental evaluations conducted on Meta-World&rsquo;s MT10 and MT50 <b>benchmarks</b> demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in both sample efficiency and effective action execution.</p></p class="citation"></blockquote><h3 id=2539--168264-federated-learning-model-for-predicting-major-postoperative-complications-yonggi-park-et-al-2024>(25/39 | 168/264) Federated learning model for predicting major postoperative complications (Yonggi Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonggi Park, Yuanfang Ren, Benjamin Shickel, Ziyuan Guan, Ayush Patela, Yingbo Ma, Zhenhong Hu, Tyler J. Loftus, Parisa Rashidi, Tezcan Ozrazgat-Baslanti, Azra Bihorac. (2024)<br><strong>Federated learning model for predicting major postoperative complications</strong><br><button class=copy-to-clipboard title="Federated learning model for predicting major postoperative complications" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06641v1.pdf filename=2404.06641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: The accurate prediction of postoperative complication risk using Electronic Health Records (EHR) and artificial intelligence shows great potential. Training a robust artificial intelligence model typically requires large-scale and diverse datasets. In reality, collecting medical data often encounters challenges surrounding privacy protection. Methods: This retrospective cohort study includes adult patients who were admitted to UFH Gainesville (GNV) (n = 79,850) and Jacksonville (JAX) (n = 28,636) for any type of inpatient surgical procedure. Using perioperative and intraoperative features, we developed <b>federated</b> <b>learning</b> models to predict nine major postoperative complications (i.e., prolonged intensive care unit stay and mechanical ventilation). We compared <b>federated</b> <b>learning</b> models with local learning models trained on a single site and central learning models trained on pooled dataset from two centers. Results: Our <b>federated</b> <b>learning</b> models achieved the area under the receiver operating characteristics curve (AUROC) values ranged from 0.81 for wound complications to 0.92 for prolonged ICU stay at UFH GNV center. At UFH JAX center, these values ranged from 0.73-0.74 for wound complications to 0.92-0.93 for hospital mortality. <b>Federated</b> <b>learning</b> models achieved comparable AUROC performance to central learning models, except for prolonged ICU stay, where the performance of <b>federated</b> <b>learning</b> models was slightly higher than central learning models at UFH GNV center, but slightly lower at UFH JAX center. In addition, our <b>federated</b> <b>learning</b> model obtained comparable performance to the best local learning model at each center, demonstrating strong generalizability. Conclusion: <b>Federated</b> <b>learning</b> is shown to be a useful tool to train robust and generalizable models from large scale data across multiple institutions where data protection barriers are high.</p></p class="citation"></blockquote><h3 id=2639--169264-simultaneous-linear-connectivity-of-neural-networks-modulo-permutation-ekansh-sharma-et-al-2024>(26/39 | 169/264) Simultaneous linear connectivity of neural networks modulo permutation (Ekansh Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick, Gintare Karolina Dziugaite. (2024)<br><strong>Simultaneous linear connectivity of neural networks modulo permutation</strong><br><button class=copy-to-clipboard title="Simultaneous linear connectivity of neural networks modulo permutation" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06498v1.pdf filename=2404.06498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks&rsquo; loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports &ldquo;weak linear connectivity&rdquo;-that for each pair of networks belonging to a set of <b>SGD</b> solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim &ldquo;strong linear connectivity&rdquo;-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.</p></p class="citation"></blockquote><h3 id=2739--170264-exploring-neural-network-landscapes-star-shaped-and-geodesic-connectivity-zhanran-lin-et-al-2024>(27/39 | 170/264) Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity (Zhanran Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanran Lin, Puheng Li, Lei Wu. (2024)<br><strong>Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity</strong><br><button class=copy-to-clipboard title="Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06391v1.pdf filename=2404.06391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the most intriguing findings in the structure of neural network landscape is the phenomenon of mode connectivity: For two typical global minima, there exists a path connecting them without barrier. This concept of mode connectivity has played a crucial role in understanding important phenomena in deep learning. In this paper, we conduct a fine-grained analysis of this connectivity phenomenon. First, we demonstrate that in the overparameterized case, the connecting path can be as simple as a two-piece linear path, and the path length can be nearly equal to the Euclidean distance. This finding suggests that the landscape should be nearly convex in a certain sense. Second, we uncover a surprising star-shaped connectivity: For a finite number of typical minima, there exists a center on minima manifold that connects all of them simultaneously via linear paths. These results are provably valid for linear networks and two-layer ReLU networks under a teacher-student setup, and are empirically supported by models trained on <b>MNIST</b> and CIFAR-10.</p></p class="citation"></blockquote><h3 id=2839--171264-high-noise-scheduling-is-a-must-mahmut-s-gokmen-et-al-2024>(28/39 | 171/264) High Noise Scheduling is a Must (Mahmut S. Gokmen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahmut S. Gokmen, Cody Bumgardner, Jie Zhang, Ge Wang, Jin Chen. (2024)<br><strong>High Noise Scheduling is a Must</strong><br><button class=copy-to-clipboard title="High Noise Scheduling is a Must" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06353v1.pdf filename=2404.06353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques. Current advancements move one step forward consistency training techniques and eliminates the limitation of <b>distillation</b> training. Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum. In this study, it is investigated the balance between high and low noise levels in noise distribution and offered polynomial noise distribution to maintain the stability. This proposed polynomial noise distribution is also supported with a predefined Karras noises to prevent unique noise levels arises with Karras noise generation algorithm. Furthermore, by elimination of learned noisy steps with a curriculum based on sinusoidal function increase the performance of the model in denoising. To make a fair comparison with the latest released consistency model training techniques, experiments are conducted with same hyper-parameters except curriculum and noise distribution. The models utilized during experiments are determined with low depth to prove the robustness of our proposed technique. The results show that the polynomial noise distribution outperforms the model trained with log-normal noise distribution, yielding a 33.54 FID score after 100,000 training steps with constant discretization steps. Additionally, the implementation of a sinusoidal-based curriculum enhances denoising performance, resulting in a FID score of 30.48.</p></p class="citation"></blockquote><h3 id=2939--172264-on-adversarial-training-and-the-1-nearest-neighbor-classifier-amir-hagai-et-al-2024>(29/39 | 172/264) On adversarial training and the 1 Nearest Neighbor classifier (Amir Hagai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Hagai, Yair Weiss. (2024)<br><strong>On adversarial training and the 1 Nearest Neighbor classifier</strong><br><button class=copy-to-clipboard title="On adversarial training and the 1 Nearest Neighbor classifier" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06313v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06313v2.pdf filename=2404.06313v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability to fool deep learning classifiers with tiny perturbations of the input has lead to the development of <b>adversarial</b> <b>training</b> in which the loss with respect to <b>adversarial</b> <b>examples</b> is minimized in addition to the training examples. While <b>adversarial</b> <b>training</b> improves the robustness of the learned classifiers, the procedure is computationally expensive, sensitive to hyperparameters and may still leave the classifier vulnerable to other types of small perturbations. In this paper we analyze the <b>adversarial</b> <b>robustness</b> of the 1 Nearest Neighbor (1NN) classifier and compare its performance to <b>adversarial</b> <b>training.</b> We prove that under reasonable assumptions, the 1 NN classifier will be robust to {\em any} small image perturbation of the training images and will give high <b>adversarial</b> <b>accuracy</b> on test images as the number of training examples goes to infinity. In experiments with 45 different binary image classification problems taken from CIFAR10, we find that 1NN outperform TRADES (a powerful <b>adversarial</b> <b>training</b> algorithm) in terms of average <b>adversarial</b> <b>accuracy.</b> In additional experiments with 69 pretrained robust models for CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness to perturbations that are only slightly different from those seen during training. Taken together, our results suggest that modern <b>adversarial</b> <b>training</b> methods still fall short of the robustness of the simple 1NN classifier. our code can be found at <a href=https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier>https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier</a></p></p class="citation"></blockquote><h3 id=3039--173264-algorithms-for-caching-and-mts-with-reduced-number-of-predictions-karim-abdel-sadek-et-al-2024>(30/39 | 173/264) Algorithms for Caching and MTS with reduced number of predictions (Karim Abdel Sadek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karim Abdel Sadek, Marek Elias. (2024)<br><strong>Algorithms for Caching and MTS with reduced number of predictions</strong><br><button class=copy-to-clipboard title="Algorithms for Caching and MTS with reduced number of predictions" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06280v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06280v2.pdf filename=2404.06280v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation &ndash; this motivated Im et al. &lsquo;22 to introduce the study of algorithms which use predictions parsimoniously. We design parsimonious algorithms for caching and <b>MTS</b> with action predictions, proposed by Antoniadis et al. &lsquo;20, focusing on the parameters of consistency (performance with perfect predictions) and smoothness (dependence of their performance on the prediction error). Our algorithm for caching is 1-consistent, robust, and its smoothness deteriorates with the decreasing number of available predictions. We propose an algorithm for general <b>MTS</b> whose consistency and smoothness both scale linearly with the decreasing number of predictions. Without the restriction on the number of available predictions, both algorithms match the earlier guarantees achieved by Antoniadis et al. &lsquo;20.</p></p class="citation"></blockquote><h3 id=3139--174264-feel-good-thompson-sampling-for-contextual-dueling-bandits-xuheng-li-et-al-2024>(31/39 | 174/264) Feel-Good Thompson Sampling for Contextual Dueling Bandits (Xuheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuheng Li, Heyang Zhao, Quanquan Gu. (2024)<br><strong>Feel-Good Thompson Sampling for Contextual Dueling Bandits</strong><br><button class=copy-to-clipboard title="Feel-Good Thompson Sampling for Contextual Dueling Bandits" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06013v1.pdf filename=2404.06013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contextual dueling <b>bandits,</b> where a learner compares two options based on context and receives feedback indicating which was preferred, extends classic dueling <b>bandits</b> by incorporating contextual information for decision-making and preference learning. Several algorithms based on the upper confidence bound (UCB) have been proposed for linear contextual dueling <b>bandits.</b> However, no algorithm based on posterior sampling has been developed in this setting, despite the empirical success observed in traditional contextual <b>bandits.</b> In this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for linear contextual dueling <b>bandits.</b> At the core of our algorithm is a new Feel-Good exploration term specifically tailored for dueling <b>bandits.</b> This term leverages the independence of the two selected arms, thereby avoiding a cross term in the analysis. We show that our algorithm achieves nearly minimax-optimal regret, i.e., $\tilde{\mathcal{O}}(d\sqrt T)$, where $d$ is the model dimension and $T$ is the time horizon. Finally, we evaluate our algorithm on synthetic data and observe that FGTS.CDB outperforms existing algorithms by a large margin.</p></p class="citation"></blockquote><h3 id=3239--175264-a-cyber-manufacturing-iot-system-for-adaptive-machine-learning-model-deployment-by-interactive-causality-enabled-self-labeling-yutian-ren-et-al-2024>(32/39 | 175/264) A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling (Yutian Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutian Ren, Yuqi He, Xuyin Zhang, Aaron Yen, G. P. Li. (2024)<br><strong>A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling</strong><br><button class=copy-to-clipboard title="A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY, stat-ME<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05976v1.pdf filename=2404.05976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications. To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence. Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data <b>distribution</b> <b>shifts.</b> The unique features of the self-labeling method require a novel software system to support dynamism at various levels. This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service. The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously. AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers. A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance.</p></p class="citation"></blockquote><h3 id=3339--176264-adagossip-adaptive-consensus-step-size-for-decentralized-deep-learning-with-communication-compression-sai-aparna-aketi-et-al-2024>(33/39 | 176/264) AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression (Sai Aparna Aketi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Aparna Aketi, Abolfazl Hashemi, Kaushik Roy. (2024)<br><strong>AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression</strong><br><button class=copy-to-clipboard title="AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05919v1.pdf filename=2404.05919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server. However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups. To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature. Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training. In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents. We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion <b>MNIST,</b> Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance ($0-2%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression.</p></p class="citation"></blockquote><h3 id=3439--177264-deep-reinforcement-learning-for-personalized-diagnostic-decision-pathways-using-electronic-health-records-a-comparative-study-on-anemia-and-systemic-lupus-erythematosus-lillian-muyama-et-al-2024>(34/39 | 177/264) Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus (Lillian Muyama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lillian Muyama, Antoine Neuraz, Adrien Coulet. (2024)<br><strong>Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05913v1.pdf filename=2404.05913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Clinical diagnosis is typically reached by following a series of steps recommended by guidelines authored by colleges of experts. Accordingly, guidelines play a crucial role in rationalizing clinical decisions but suffer from limitations as they are built to cover the majority of the population and fail at covering patients with uncommon conditions. Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and practices. Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep <b>Reinforcement</b> <b>Learning</b> (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs). We apply DRL on synthetic, but realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow the schema of a decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score. We particularly evaluate the robustness of our approaches to noisy and missing data since these frequently occur in EHRs. Results: In both use cases, and in the presence of imperfect data, our best DRL algorithms exhibit competitive performance when compared to the traditional classifiers, with the added advantage that they enable the progressive generation of a pathway to the suggested diagnosis which can both guide and explain the decision-making process. Conclusion: DRL offers the opportunity to learn personalized decision pathways to diagnosis. We illustrate with our two use cases their advantages: they generate step-by-step pathways that are self-explanatory; and their correctness is competitive when compared to state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=3539--178264-bayesian-survival-analysis-by-approximate-inference-of-neural-networks-christian-marius-lillelund-et-al-2024>(35/39 | 178/264) Bayesian Survival Analysis by Approximate Inference of Neural Networks (Christian Marius Lillelund et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Marius Lillelund, Martin Magris, Christian Fischer Pedersen. (2024)<br><strong>Bayesian Survival Analysis by Approximate Inference of Neural Networks</strong><br><button class=copy-to-clipboard title="Bayesian Survival Analysis by Approximate Inference of Neural Networks" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 8<br>Keywords: Benchmarking, Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06421v1.pdf filename=2404.06421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting future events always comes with uncertainty, but traditional non-Bayesian methods cannot distinguish certain from uncertain predictions or explain the confidence in their predictions. In survival analysis, Bayesian methods applied to state-of-the-art solutions in the healthcare and biomedical field are still novel, and their implications have not been fully evaluated. In this paper, we study the benefits of modeling uncertainty in <b>deep</b> <b>neural</b> <b>networks</b> for survival analysis with a focus on prediction and calibration performance. For this, we present a Bayesian <b>deep</b> <b>learning</b> <b>framework</b> that consists of three Bayesian network architectures, which we train by optimizing the Cox partial likelihood and combining input-dependent aleatoric uncertainty with model-specific epistemic uncertainty. This enables us to provide uncertainty estimates as credible intervals when predicting the survival curve or as a probability density function over the predicted median survival times. For our empirical analyses, we evaluated our proposed method on four <b>benchmark</b> datasets and found that our method demonstrates prediction performance comparable to the state-of-the-art based on the concordance index and outperforms all other Cox-based approaches in terms of the mean absolute error. Our work explicitly compares the extent to which different Bayesian approximation techniques differ from each other and improves the prediction over traditional non-Bayesian alternatives.</p></p class="citation"></blockquote><h3 id=3639--179264-unifying-low-dimensional-observations-in-deep-learning-through-the-deep-linear-unconstrained-feature-model-connall-garrod-et-al-2024>(36/39 | 179/264) Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model (Connall Garrod et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connall Garrod, Jonathan P. Keating. (2024)<br><strong>Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model</strong><br><button class=copy-to-clipboard title="Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06106v1.pdf filename=2404.06106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>deep</b> <b>neural</b> <b>networks</b> have achieved high performance across various tasks. Recently, researchers have noted occurrences of low-dimensional structure in the weights, Hessian&rsquo;s, gradients, and feature vectors of these networks, spanning different datasets and architectures when trained to convergence. In this analysis, we theoretically demonstrate these observations arising, and show how they can be unified within a generalized unconstrained feature model that can be considered analytically. Specifically, we consider a previously described structure called Neural Collapse, and its multi-layer counterpart, <b>Deep</b> <b>Neural</b> <b>Collapse,</b> which emerges when the network approaches global optima. This phenomenon explains the other observed low-dimensional behaviours on a layer-wise level, such as the bulk and outlier structure seen in Hessian spectra, and the alignment of gradient descent with the outlier eigenspace of the Hessian. Empirical results in both the <b>deep</b> <b>linear</b> <b>unconstrained</b> feature model and its non-linear equivalent support these predicted observations.</p></p class="citation"></blockquote><h3 id=3739--180264-go4align-group-optimization-for-multi-task-alignment-jiayi-shen-et-al-2024>(37/39 | 180/264) GO4Align: Group Optimization for Multi-Task Alignment (Jiayi Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Shen, Cheems Wang, Zehao Xiao, Nanne Van Noord, Marcel Worring. (2024)<br><strong>GO4Align: Group Optimization for Multi-Task Alignment</strong><br><button class=copy-to-clipboard title="GO4Align: Group Optimization for Multi-Task Alignment" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06486v1.pdf filename=2404.06486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes \textit{GO4Align}, a multi-task optimization approach that tackles task imbalance by explicitly aligning the optimization across tasks. To achieve this, we design an adaptive group risk minimization strategy, compromising two crucial techniques in implementation: (i) dynamical group assignment, which clusters similar tasks based on task interactions; (ii) risk-guided group indicators, which exploit consistent task correlations with risk information from previous iterations. Comprehensive experimental results on diverse typical <b>benchmarks</b> demonstrate our method&rsquo;s performance superiority with even lower computational costs.</p></p class="citation"></blockquote><h3 id=3839--181264-online-learning-of-decision-trees-with-thompson-sampling-ayman-chaouki-et-al-2024>(38/39 | 181/264) Online Learning of Decision Trees with Thompson Sampling (Ayman Chaouki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayman Chaouki, Jesse Read, Albert Bifet. (2024)<br><strong>Online Learning of Decision Trees with Thompson Sampling</strong><br><button class=copy-to-clipboard title="Online Learning of Decision Trees with Thompson Sampling" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06403v1.pdf filename=2404.06403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision Trees are prominent prediction models for interpretable Machine Learning. They have been thoroughly researched, mostly in the batch setting with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3 and CART. Unfortunately, these methods are of heuristic nature, they rely on greedy splits offering no guarantees of global optimality and often leading to unnecessarily complex and hard-to-interpret Decision Trees. Recent breakthroughs addressed this suboptimality issue in the batch setting, but no such work has considered the online setting with data arriving in a stream. To this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling Decision Trees (TSDT), able to produce optimal Decision Trees in an online setting. We analyse our algorithm and prove its almost sure convergence to the optimal tree. Furthermore, we conduct extensive experiments to validate our findings empirically. The proposed TSDT outperforms existing algorithms on several <b>benchmarks,</b> all while presenting the practical advantage of being tailored to the online setting.</p></p class="citation"></blockquote><h3 id=3939--182264-a-lightweight-measure-of-classification-difficulty-from-application-dataset-characteristics-bryan-bo-cao-et-al-2024>(39/39 | 182/264) A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics (Bryan Bo Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bryan Bo Cao, Abhinav Sharma, Lawrence O&rsquo;Gorman, Michael Coss, Shubham Jain. (2024)<br><strong>A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics</strong><br><button class=copy-to-clipboard title="A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65D19, cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05981v1.pdf filename=2404.05981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite accuracy and computation <b>benchmarks</b> being widely available to help choose among neural network models, these are usually trained on datasets with many classes, and do not give a precise idea of performance for applications of few (&lt; 10) classes. The conventional procedure to predict performance is to train and test repeatedly on the different models and dataset variations of interest. However, this is computationally expensive. We propose an efficient classification difficulty measure that is calculated from the number of classes and intra- and inter-class similarity metrics of the dataset. After a single stage of training and testing per model family, relative performance for different datasets and models of the same family can be predicted by comparing difficulty measures - without further training and testing. We show how this measure can help a practitioner select a computationally efficient model for a small dataset 6 to 29x faster than through repeated training and testing. We give an example of use of the measure for an industrial application in which options are identified to select a model 42% smaller than the baseline YOLOv5-nano model, and if class merging from 3 to 2 classes meets requirements, 85% smaller.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--183264-aisaq-all-in-storage-anns-with-product-quantization-for-dram-free-information-retrieval-kento-tatsuno-et-al-2024>(1/4 | 183/264) AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval (Kento Tatsuno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, Jun Deguchi. (2024)<br><strong>AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval</strong><br><button class=copy-to-clipboard title="AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-DS, cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Quantization, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06004v1.pdf filename=2404.06004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In approximate nearest neighbor search (ANNS) methods based on approximate proximity <b>graphs,</b> DiskANN achieves good recall-speed balance for large-scale datasets using both of RAM and storage. Despite it claims to save memory usage by loading compressed vectors by product <b>quantization</b> (PQ), its memory usage increases in proportion to the scale of datasets. In this paper, we propose All-in-Storage ANNS with Product <b>Quantization</b> (AiSAQ), which offloads the compressed vectors to storage. Our method achieves $\sim$10 MB memory usage in query search even with billion-scale datasets with minor performance degradation. AiSAQ also reduces the index load time before query search, which enables the index switch between muitiple billion-scale datasets and significantly enhances the flexibility of <b>retrieval-augmented</b> <b>generation</b> <b>(RAG).</b> This method is applicable to all <b>graph-based</b> ANNS algorithms and can be combined with higher-spec ANNS methods in the future.</p></p class="citation"></blockquote><h3 id=24--184264-dre-generating-recommendation-explanations-by-aligning-large-language-models-at-data-level-shen-gao-et-al-2024>(2/4 | 184/264) DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level (Shen Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Gao, Yifan Wang, Jiabao Fang, Lisi Chen, Peng Han, Shuo Shang. (2024)<br><strong>DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level</strong><br><button class=copy-to-clipboard title="DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 38<br>Keywords: Benchmarking, Black Box, Knowledge Distillation, Recommendation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06311v1.pdf filename=2404.06311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems play a crucial role in various domains, suggesting items based on user behavior.However, the lack of transparency in presenting <b>recommendations</b> can lead to user confusion. In this paper, we introduce Data-level <b>Recommendation</b> Explanation (DRE), a non-intrusive explanation framework for <b>black-box</b> <b>recommendation</b> models.Different from existing methods, DRE does not require any intermediary representations of the <b>recommendation</b> model or latent alignment training, mitigating potential performance issues.We propose a data-level alignment method, leveraging <b>large</b> <b>language</b> <b>models</b> to reason relationships between user data and recommended items.Additionally, we address the challenge of enriching the details of the explanation by introducing target-aware user preference <b>distillation,</b> utilizing item reviews. Experimental results on <b>benchmark</b> datasets demonstrate the effectiveness of the DRE in providing accurate and user-centric explanations, enhancing user engagement with recommended item.</p></p class="citation"></blockquote><h3 id=34--185264-end-to-end-training-of-multimodal-model-and-ranking-model-xiuqi-deng-et-al-2024>(3/4 | 185/264) End-to-end training of Multimodal Model and ranking Model (Xiuqi Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiuqi Deng, Lu Xu, Xiyao Li, Jinkai Yu, Erpeng Xue, Zhongyuan Wang, Di Zhang, Zhaojie Liu, Guorui Zhou, Yang Song, Na Mou, Shen Jiang, Han Li. (2024)<br><strong>End-to-end training of Multimodal Model and ranking Model</strong><br><button class=copy-to-clipboard title="End-to-end training of Multimodal Model and ranking Model" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Recommendation, Recommender System, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06078v1.pdf filename=2404.06078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional <b>recommender</b> <b>systems</b> heavily rely on ID features, which often encounter challenges related to cold-start and generalization. Modeling pre-extracted content features can mitigate these issues, but is still a suboptimal solution due to the discrepancies between training tasks and model parameters. End-to-end training presents a promising solution for these problems, yet most of the existing works mainly focus on retrieval models, leaving the <b>multimodal</b> techniques under-utilized. In this paper, we propose an industrial <b>multimodal</b> <b>recommendation</b> framework named EM3: End-to-end training of <b>Multimodal</b> Model and ranking Model, which sufficiently utilizes <b>multimodal</b> information and allows personalized ranking tasks to directly train the core modules in the <b>multimodal</b> model to obtain more task-oriented content features, without overburdening resource consumption. First, we propose Fusion-Q-Former, which consists of <b>transformers</b> and a set of trainable queries, to fuse different modalities and generate fixed-length and robust <b>multimodal</b> embeddings. Second, in our sequential modeling for user content interest, we utilize Low-Rank Adaptation technique to alleviate the conflict between huge resource consumption and long sequence length. Third, we propose a novel Content-ID-Contrastive learning task to complement the advantages of content and ID by aligning them with each other, obtaining more task-oriented content embeddings and more generalized ID embeddings. In experiments, we implement EM3 on different ranking models in two scenario, achieving significant improvements in both offline evaluation and online A/B test, verifying the generalizability of our method. Ablation studies and visualization are also performed. Furthermore, we also conduct experiments on two public datasets to show that our proposed method outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=44--186264-wasserstein-dependent-graph-attention-network-for-collaborative-filtering-with-uncertainty-haoxuan-li-et-al-2024>(4/4 | 186/264) Wasserstein Dependent Graph Attention Network for Collaborative Filtering with Uncertainty (Haoxuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxuan Li, Yuanxin Ouyang, Zhuang Liu, Wenge Rong, Zhang Xiong. (2024)<br><strong>Wasserstein Dependent Graph Attention Network for Collaborative Filtering with Uncertainty</strong><br><button class=copy-to-clipboard title="Wasserstein Dependent Graph Attention Network for Collaborative Filtering with Uncertainty" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-IT, cs.IR, math-IT<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Mutual Information, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05962v1.pdf filename=2404.05962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative filtering (CF) is an essential technique in <b>recommender</b> <b>systems</b> that provides personalized <b>recommendations</b> by only leveraging user-item interactions. However, most CF methods represent users and items as fixed points in the latent space, lacking the ability to capture uncertainty. In this paper, we propose a novel approach, called the Wasserstein dependent <b>Graph</b> ATtention network (W-GAT), for collaborative filtering with uncertainty. We utilize <b>graph</b> attention network and Wasserstein distance to address the limitations of LightGCN and Kullback-Leibler divergence (KL) divergence to learn Gaussian embedding for each user and item. Additionally, our method incorporates Wasserstein-dependent <b>mutual</b> <b>information</b> further to increase the similarity between positive pairs and to tackle the challenges induced by KL divergence. Experimental results on three <b>benchmark</b> datasets show the superiority of W-GAT compared to several representative baselines. Extensive experimental analysis validates the effectiveness of W-GAT in capturing uncertainty by modeling the range of user preferences and categories associated with items.</p></p class="citation"></blockquote><h2 id=csdb-4>cs.DB (4)</h2><h3 id=14--187264-dimensionality-reduction-in-sentence-transformer-vector-databases-with-fast-fourier-transform-vitaly-bulgakov-et-al-2024>(1/4 | 187/264) Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform (Vitaly Bulgakov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vitaly Bulgakov, Alec Segal. (2024)<br><strong>Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform</strong><br><button class=copy-to-clipboard title="Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-CL, cs-DB, cs-LG, cs.DB<br>Keyword Score: 50<br>Keywords: Recommendation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06278v1.pdf filename=2404.06278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> models and image processing, this FFT-based approach promises to improve data <b>retrieval</b> <b>processes</b> <b>and</b> enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and <b>recommendation</b> systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.</p></p class="citation"></blockquote><h3 id=24--188264-automatic-configuration-tuning-on-cloud-database-a-survey-limeng-zhang-et-al-2024>(2/4 | 188/264) Automatic Configuration Tuning on Cloud Database: A Survey (Limeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Limeng Zhang, M. Ali Babar. (2024)<br><strong>Automatic Configuration Tuning on Cloud Database: A Survey</strong><br><button class=copy-to-clipboard title="Automatic Configuration Tuning on Cloud Database: A Survey" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 33<br>Keywords: Benchmarking, Pruning, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06043v1.pdf filename=2404.06043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Faced with the challenges of big data, modern cloud database management systems are designed to efficiently store, organize, and retrieve data, supporting optimal performance, scalability, and reliability for complex data processing and analysis. However, achieving good performance in modern databases is non-trivial as they are notorious for having dozens of configurable knobs, such as hardware setup, software setup, database physical and logical design, etc., that control runtime behaviors and impact database performance. To find the optimal configuration for achieving optimal performance, extensive research has been conducted on automatic parameter tuning in DBMS. This paper provides a comprehensive survey of predominant configuration tuning techniques, including Bayesian optimization-based solutions, Neural network-based solutions, <b>Reinforcement</b> <b>learning-based</b> solutions, and Search-based solutions. Moreover, it investigates the fundamental aspects of parameter tuning pipeline, including tuning objective, workload characterization, feature <b>pruning,</b> knowledge from experience, configuration <b>recommendation,</b> and experimental settings. We highlight technique comparisons in each component, corresponding solutions, and introduce the experimental setting for performance evaluation. Finally, we conclude this paper and present future research opportunities. This paper aims to assist future researchers and practitioners in gaining a better understanding of automatic parameter tuning in cloud databases by providing state-of-the-art existing solutions, research directions, and evaluation <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=34--189264-pm4pyllm-a-comprehensive-module-for-implementing-pm-on-llms-alessandro-berti-2024>(3/4 | 189/264) PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs (Alessandro Berti, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Berti. (2024)<br><strong>PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs</strong><br><button class=copy-to-clipboard title="PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06035v1.pdf filename=2404.06035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>pm4py is a process mining library for Python implementing several process mining (PM) artifacts and algorithms. It also offers methods to integrate PM with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> This paper examines how the current paradigms of PM on <b>LLM</b> are implemented in pm4py, identifying challenges such as privacy, hallucinations, and the context window limit.</p></p class="citation"></blockquote><h3 id=44--190264-balanced-partitioning-for-optimizing-big-graph-computation-complexities-and-approximation-algorithms-baoling-ning-et-al-2024>(4/4 | 190/264) Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms (Baoling Ning et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baoling Ning, Jianzhong Li. (2024)<br><strong>Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms</strong><br><button class=copy-to-clipboard title="Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs-DS, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05949v1.pdf filename=2404.05949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> partitioning is a key fundamental problem in the area of big <b>graph</b> computation. Previous works do not consider the practical requirements when optimizing the big data analysis in real applications. In this paper, motivated by optimizing the big data computing applications, two typical problems of <b>graph</b> partitioning are studied. The first problem is to optimize the performance of specific workloads by <b>graph</b> partitioning, which lacks of algorithms with performance guarantees. The second problem is to optimize the computation of motifs by <b>graph</b> partitioning, which has not been focused by previous works. First, the formal definitions of the above two problems are introduced, and the semidefinite programming representations are also designed based on the analysis of the properties of the two problems. For the motif based partitioning problem, it is proved to be NP-complete even for the special case of $k=2$ and the motif is a triangle, and its inapproximability is also shown by proving that there are no efficient algorithms with finite approximation ratio. Finally, using the semidefinite programming and sophisticated rounding techniques, the bi-criteria $O(\sqrt{\log n\log k})$-approximation algorithms with polynomial time cost are designed and analyzed for them.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--191264-using-few-shot-learning-to-classify-primary-lung-cancer-and-other-malignancy-with-lung-metastasis-in-cytological-imaging-via-endobronchial-ultrasound-procedures-ching-kai-lin-et-al-2024>(1/3 | 191/264) Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures (Ching-Kai Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ching-Kai Lin, Di-Chun Wei, Yun-Chien Cheng. (2024)<br><strong>Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures</strong><br><button class=copy-to-clipboard title="Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06080v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06080v2.pdf filename=2404.06080v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to establish a computer-aided diagnosis system for endobronchial ultrasound (EBUS) surgery to assist physicians in the preliminary diagnosis of metastatic cancer. This involves arranging immediate examinations for other sites of metastatic cancer after EBUS surgery, eliminating the need to wait for reports, thereby shortening the waiting time by more than half and enabling patients to detect other cancers earlier, allowing for early planning and implementation of treatment plans. Unlike previous studies on cell image classification, which have abundant datasets for training, this study must also be able to make effective classifications despite the limited amount of case data for lung metastatic cancer. In the realm of small data set classification methods, <b>Few-shot</b> <b>learning</b> (FSL) has become mainstream in recent years. Through its ability to train on small datasets and its strong generalization capabilities, FSL shows potential in this task of lung metastatic cell image classification. This study will adopt the approach of <b>Few-shot</b> <b>learning,</b> referencing existing proposed models, and designing a model architecture for classifying lung metastases cell images. Batch Spectral Regularization (BSR) will be incorporated as a loss update parameter, and the <b>Finetune</b> method of PMF will be modified. In terms of test results, the addition of BSR and the modified <b>Finetune</b> method further increases the accuracy by 8.89% to 65.60%, outperforming other FSL methods. This study confirms that FSL is superior to <b>supervised</b> and <b>transfer</b> <b>learning</b> in classifying metastatic cancer and demonstrates that using BSR as a loss function and modifying <b>Finetune</b> can enhance the model&rsquo;s capabilities.</p></p class="citation"></blockquote><h3 id=23--192264-fortifying-fully-convolutional-generative-adversarial-networks-for-image-super-resolution-using-divergence-measures-arkaprabha-basu-et-al-2024>(2/3 | 192/264) Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures (Arkaprabha Basu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arkaprabha Basu, Kushal Bose, Sankha Subhra Mullick, Anish Chakrabarty, Swagatam Das. (2024)<br><strong>Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures</strong><br><button class=copy-to-clipboard title="Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06294v1.pdf filename=2404.06294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Super-Resolution (SR) is a time-hallowed image processing problem that aims to improve the quality of a Low-Resolution (LR) sample up to the standard of its High-Resolution (HR) counterpart. We aim to address this by introducing Super-Resolution Generator (SuRGe), a fully-convolutional <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)-based</b> architecture for SR. We show that distinct <b>convolutional</b> features obtained at increasing depths of a <b>GAN</b> generator can be optimally combined by a set of learnable convex weights to improve the quality of generated SR samples. In the process, we employ the Jensen-Shannon and the Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of distributions to further aid the generator of SuRGe to better exploit the available information in an attempt to improve SR. Moreover, we train the discriminator of SuRGe with the Wasserstein loss with gradient penalty, to primarily prevent mode collapse. The proposed SuRGe, as an end-to-end <b>GAN</b> workflow tailor-made for super-resolution, offers improved performance while maintaining low inference time. The efficacy of SuRGe is substantiated by its superior performance compared to 18 state-of-the-art contenders on 10 <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=33--193264-latup-net-a-lightweight-3d-attention-u-net-with-parallel-convolutions-for-brain-tumor-segmentation-ebtihal-j-alwadee-et-al-2024>(3/3 | 193/264) LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation (Ebtihal J. Alwadee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ebtihal J. Alwadee, Xianfang Sun, Yipeng Qin, Frank C. Langbein. (2024)<br><strong>LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation</strong><br><button class=copy-to-clipboard title="LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05911v1.pdf filename=2404.05911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI) scans is crucial for <b>prompt</b> and effective treatment. However, this process faces the challenge of precise delineation due to the tumors&rsquo; complex heterogeneity. Moreover, energy sustainability targets and resource limitations, especially in developing countries, require efficient and accessible medical imaging solutions. The proposed architecture, a Lightweight 3D ATtention U-Net with Parallel <b>convolutions,</b> LATUP-Net, addresses these issues. It is specifically designed to reduce computational requirements significantly while maintaining high segmentation performance. By incorporating parallel <b>convolutions,</b> it enhances feature representation by capturing multi-scale information. It further integrates an attention mechanism to refine segmentation through selective feature recalibration. LATUP-Net achieves promising segmentation performance: the average Dice scores for the whole tumor, tumor core, and enhancing tumor on the BraTS2020 dataset are 88.41%, 83.82%, and 73.67%, and on the BraTS2021 dataset, they are 90.29%, 89.54%, and 83.92%, respectively. Hausdorff distance metrics further indicate its improved ability to delineate tumor boundaries. With its significantly reduced computational demand using only 3.07 M parameters, about 59 times fewer than other state-of-the-art models, and running on a single V100 GPU, LATUP-Net stands out as a promising solution for real-world clinical applications, particularly in settings with limited resources. Investigations into the model&rsquo;s interpretability, utilizing gradient-weighted class activation mapping and confusion matrices, reveal that while attention mechanisms enhance the segmentation of small regions, their impact is nuanced. Achieving the most accurate tumor delineation requires carefully balancing local and global features.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--194264-the-x-lance-technical-report-for-interspeech-2024-speech-processing-using-discrete-speech-unit-challenge-yiwei-guo-et-al-2024>(1/2 | 194/264) The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge (Yiwei Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Guo, Chenrun Wang, Yifan Yang, Hankun Wang, Ziyang Ma, Chenpeng Du, Shuai Wang, Hanzheng Li, Shuai Fan, Hui Zhang, Xie Chen, Kai Yu. (2024)<br><strong>The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge</strong><br><button class=copy-to-clipboard title="The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, eess-AS, eess.AS<br>Keyword Score: 50<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06079v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06079v2.pdf filename=2404.06079v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discrete <b>speech</b> <b>tokens</b> have been more and more popular in multiple <b>speech</b> <b>processing</b> fields, including <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR),</b> <b>text-to-speech</b> <b>(TTS)</b> and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the <b>TTS</b> (acoustic + vocoder), SVS, and <b>ASR</b> tracks in the Interspeech 2024 <b>Speech</b> <b>Processing</b> Using Discrete <b>Speech</b> <b>Unit</b> Challenge. Notably, we achieved 1st rank on the leaderboard in the <b>TTS</b> track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.</p></p class="citation"></blockquote><h3 id=22--195264-masked-modeling-duo-towards-a-universal-audio-pre-training-framework-daisuke-niizumi-et-al-2024>(2/2 | 195/264) Masked Modeling Duo: Towards a Universal Audio Pre-training Framework (Daisuke Niizumi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, Kunio Kashino. (2024)<br><strong>Masked Modeling Duo: Towards a Universal Audio Pre-training Framework</strong><br><button class=copy-to-clipboard title="Masked Modeling Duo: Towards a Universal Audio Pre-training Framework" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: 68T07, cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06095v1.pdf filename=2404.06095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) using masked prediction has made great strides in general-purpose audio representation. This study proposes Masked Modeling Duo (M2D), an improved masked prediction SSL, which learns by predicting representations of masked input signals that serve as training signals. Unlike conventional methods, M2D obtains a training signal by encoding only the masked part, encouraging the two networks in M2D to model the input. While M2D improves general-purpose audio representations, a specialized representation is essential for real-world applications, such as in industrial and medical domains. The often confidential and proprietary data in such domains is typically limited in size and has a different distribution from that in pre-training datasets. Therefore, we propose M2D for X (M2D-X), which extends M2D to enable the pre-training of specialized representations for an application X. M2D-X learns from M2D and an additional task and inputs background noise. We make the additional task configurable to serve diverse applications, while the background noise helps learn on small data and forms a denoising task that makes representation robust. With these design choices, M2D-X should learn a representation specialized to serve various application needs. Our experiments confirmed that the representations for general-purpose audio, specialized for the highly competitive AudioSet and speech domain, and a small-data medical task achieve top-level performance, demonstrating the potential of using our models as a universal audio pre-training framework. Our code is available online for future studies at <a href=https://github.com/nttcslab/m2d>https://github.com/nttcslab/m2d</a></p></p class="citation"></blockquote><h2 id=csse-8>cs.SE (8)</h2><h3 id=18--196264-on-evaluating-the-efficiency-of-source-code-generated-by-llms-changan-niu-et-al-2024>(1/8 | 196/264) On Evaluating the Efficiency of Source Code Generated by LLMs (Changan Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, Vincent Ng. (2024)<br><strong>On Evaluating the Efficiency of Source Code Generated by LLMs</strong><br><button class=copy-to-clipboard title="On Evaluating the Efficiency of Source Code Generated by LLMs" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06041v1.pdf filename=2404.06041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen the remarkable capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for <b>code</b> <b>generation.</b> Different from existing work that evaluate the correctness of the <b>code</b> <b>generated</b> by <b>LLMs,</b> we propose to further evaluate its efficiency. More efficient <b>code</b> <b>can</b> lead to higher performance and execution efficiency of programs and software completed by <b>LLM-assisted</b> programming. First, we evaluate the efficiency of the <b>code</b> <b>generated</b> by <b>LLMs</b> on two <b>benchmarks,</b> HumanEval and MBPP. Then, we choose a set of programming problems from the online judge platform LeetCode to conduct a more difficult evaluation. Finally, we explore several <b>prompts</b> that would enable <b>LLMs</b> to generate more efficient code.</p></p class="citation"></blockquote><h3 id=28--197264-model-generation-from-requirements-with-llms-an-exploratory-study-alessio-ferrari-et-al-2024>(2/8 | 197/264) Model Generation from Requirements with LLMs: an Exploratory Study (Alessio Ferrari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessio Ferrari, Sallam Abualhaija, Chetan Arora. (2024)<br><strong>Model Generation from Requirements with LLMs: an Exploratory Study</strong><br><button class=copy-to-clipboard title="Model Generation from Requirements with LLMs: an Exploratory Study" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2; K-6-3; D-2-1; D-3-1; D-2-2; D-2-10; D-2-2; I-2; I-2-7, cs-CL, cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06371v1.pdf filename=2404.06371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complementing natural language (NL) requirements with graphical models can improve stakeholders&rsquo; communication and provide directions for system design. However, creating models from requirements involves manual effort. The advent of generative <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> <b>ChatGPT</b> being a notable example, offers promising avenues for automated assistance in model generation. This paper investigates the capability of <b>ChatGPT</b> to generate a specific type of model, i.e., UML sequence diagrams, from NL requirements. We conduct a qualitative study in which we examine the sequence diagrams generated by <b>ChatGPT</b> for 28 requirements documents of various types and from different domains. Observations from the analysis of the generated diagrams have systematically been captured through evaluation logs, and categorized through thematic analysis. Our results indicate that, although the models generally conform to the standard and exhibit a reasonable level of understandability, their completeness and correctness with respect to the specified requirements often present challenges. This issue is particularly pronounced in the presence of requirements smells, such as ambiguity and inconsistency. The insights derived from this study can influence the practical utilization of <b>LLMs</b> in the RE process, and open the door to novel RE-specific <b>prompting</b> strategies targeting effective model generation.</p></p class="citation"></blockquote><h3 id=38--198264-open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning-zhihao-lin-et-al-2024>(3/8 | 198/264) Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning (Zhihao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Lin, Wei Ma, Tao Lin, Yaowen Zheng, Jingquan Ge, Jun Wang, Jacques Klein, Tegawende Bissyande, Yang Liu, Li Li. (2024)<br><strong>Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning</strong><br><button class=copy-to-clipboard title="Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Federated Learning, Large Language Model, Large Language Model, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06201v1.pdf filename=2404.06201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organisations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on <b>federated</b> <b>learning</b> (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and <b>security.</b> Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on FL, our research examines the effect of code data heterogeneity on FL performance.</p></p class="citation"></blockquote><h3 id=48--199264-perplexed-understanding-when-large-language-models-are-confused-nathan-cooper-et-al-2024>(4/8 | 199/264) Perplexed: Understanding When Large Language Models are Confused (Nathan Cooper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathan Cooper, Torsten Scholak. (2024)<br><strong>Perplexed: Understanding When Large Language Models are Confused</strong><br><button class=copy-to-clipboard title="Perplexed: Understanding When Large Language Models are Confused" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06634v1.pdf filename=2404.06634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become dominant in the Natural Language Processing (NLP) field causing a huge surge in progress in a short amount of time. However, their limitations are still a mystery and have primarily been explored through tailored datasets to analyze a specific human-level skill such as negation, name resolution, etc. In this paper, we introduce perplexed, a library for exploring where a particular language model is perplexed. To show the flexibility and types of insights that can be gained by perplexed, we conducted a case study focused on <b>LLMs</b> for <b>code</b> <b>generation</b> using an additional tool we built to help with the analysis of <b>code</b> <b>models</b> called codetokenizer. Specifically, we explore success and failure cases at the token level of <b>code</b> <b>LLMs</b> under different scenarios pertaining to the type of coding structure the model is predicting, e.g., a variable name or operator, and how predicting of internal verses external method invocations impact performance. From this analysis, we found that our studied <b>code</b> <b>LLMs</b> had their worst performance on coding structures where the <b>code</b> <b>was</b> not syntactically correct. Additionally, we found the models to generally perform worse at predicting internal method invocations than external ones. We have open sourced both of these tools to allow the research community to better understand <b>LLMs</b> in general and <b>LLMs</b> for <b>code</b> <b>generation.</b></p></p class="citation"></blockquote><h3 id=58--200264-a-rag-method-for-source-code-inquiry-tailored-to-long-context-llms-toshihiro-kamiya-2024>(5/8 | 200/264) A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs (Toshihiro Kamiya, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toshihiro Kamiya. (2024)<br><strong>A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs</strong><br><button class=copy-to-clipboard title="A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: 68-04, D-2-3; D-2-5, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06082v1.pdf filename=2404.06082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the context length limitation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has been mitigated, it still hinders their application to software development tasks. This study proposes a method incorporating execution traces into <b>RAG</b> for inquiries about source code. Small-scale experiments confirm a tendency for the method to contribute to improving <b>LLM</b> response quality.</p></p class="citation"></blockquote><h3 id=68--201264-public-private-funding-models-in-open-source-software-development-a-case-study-on-scikit-learn-cailean-osborne-2024>(6/8 | 201/264) Public-private funding models in open source software development: A case study on scikit-learn (Cailean Osborne, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cailean Osborne. (2024)<br><strong>Public-private funding models in open source software development: A case study on scikit-learn</strong><br><button class=copy-to-clipboard title="Public-private funding models in open source software development: A case study on scikit-learn" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: K-4-1, cs-AI, cs-CY, cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Recommendation, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06484v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06484v2.pdf filename=2404.06484v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Governments are increasingly allocating funding for open source software (OSS) development to address concerns related to software <b>security,</b> digital sovereignty, and national competitiveness in science and innovation, amongst others. While announcements of governmental funding are generally well-received by OSS developers, we still have a limited understanding of OSS developers evaluate the relative benefits and drawbacks of such funding compared to other types of funding. This paper explores this question through a case study on scikit-learn, a Python library for machine learning, whose funding model combines research grants, commercial sponsorship, community donations, and a 32 million euro grant from the France&rsquo;s artificial intelligence strategy. Through 25 interviews with scikit-learn&rsquo;s maintainers and funders, this study makes two key contributions to research and practice. First, the study illustrates how the maintainers have weaved public and private funding into their project to ensure the continued provision of scikit-learn as a digital public good, as well as the importance of diversified funding and governance protocols for funding to safeguard the community ethos of the project. Second, it offers practical <b>recommendations</b> to various stakeholders. For OSS developer communities, it illustrates the benefits of a diversified funding model in balancing the merits and drawbacks of different funding sources. For companies, it serves as a reminder that sponsoring developers or OSS projects can significantly support OSS maintainers, who often struggle with limited resources and towering workloads. For governments, it emphasises the importance of funding the maintenance of existing OSS in addition to or exclusively funding the development of new OSS libraries or features. The paper concludes with suggestions for future research directions.</p></p class="citation"></blockquote><h3 id=78--202264-assessing-the-understandability-and-acceptance-of-attack-defense-trees-for-modelling-security-requirements-giovanna-broccia-et-al-2024>(7/8 | 202/264) Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements (Giovanna Broccia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanna Broccia, Maurice H. ter Beek, Alberto Lluch Lafuente, Paola Spoletini, Alessio Ferrari. (2024)<br><strong>Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements</strong><br><button class=copy-to-clipboard title="Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-1, cs-CR, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06386v1.pdf filename=2404.06386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context and Motivation Attack-Defense Trees (ADTs) are a graphical notation used to model and assess <b>security</b> requirements. ADTs are widely popular, as they can facilitate communication between different stakeholders involved in system <b>security</b> evaluation, and they are formal enough to be verified, e.g., with model checkers. Question/Problem While the quality of this notation has been primarily assessed quantitatively, its understandability has never been evaluated despite being mentioned as a key factor for its success. Principal idea/Results In this paper, we conduct an experiment with 25 human subjects to assess the understandability and user acceptance of the ADT notation. The study focuses on performance-based variables and perception-based variables, with the aim of evaluating the relationship between these measures and how they might impact the practical use of the notation. The results confirm a good level of understandability of ADTs. Participants consider them useful, and they show intention to use them. Contribution This is the first study empirically supporting the understandability of ADTs, thereby contributing to the theory of <b>security</b> requirements engineering.</p></p class="citation"></blockquote><h3 id=88--203264-on-test-sequence-generation-using-multi-objective-particle-swarm-optimization-zain-iqbal-et-al-2024>(8/8 | 203/264) On Test Sequence Generation using Multi-Objective Particle Swarm Optimization (Zain Iqbal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zain Iqbal, Kashif Zafar, Aden Iqbal, Ayesha Khan. (2024)<br><strong>On Test Sequence Generation using Multi-Objective Particle Swarm Optimization</strong><br><button class=copy-to-clipboard title="On Test Sequence Generation using Multi-Objective Particle Swarm Optimization" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06568v1.pdf filename=2404.06568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software testing is an important and essential part of the software development life cycle and accounts for almost one-third of system development costs. In the software industry, testing costs can account for about 35% to 40% of the total cost of a software project. Therefore, providing efficient ways to test software is critical to reduce cost, time, and effort. <b>Black-box</b> <b>testing</b> and White-box testing are two essential components of software testing. <b>Black-box</b> <b>testing</b> focuses on the software&rsquo;s functionality, while White-box testing examines its internal structure. These tests contribute significantly to ensuring program coverage, which remains one of the main goals of the software testing paradigm. One of the main problems in this area is the identification of appropriate paths for program coverage, which are referred to as test sequences. Creating an automated and effective test sequence is a challenging task in the software testing process. In the proposed methodology, the challenge of &ldquo;test sequence generation&rdquo; is considered a multi-objective optimization problem that includes the Oracle cost and the path, both of which are optimized in a symmetrical manner to achieve optimal software testing. Multi-Objective Particle Swarm Optimization (MOPSO) is used to represent the test sequences with the highest priority and the lowest Oracle cost as optimal. The performance of the implemented approach is compared with the Multi-Objective Firefly Algorithm (MOFA) for generating test sequences. The MOPSO-based solution outperforms the MOFA-based approach and simultaneously provides the optimal solution for both objectives.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--204264-mupt-a-generative-symbolic-music-pretrained-transformer-xingwei-qu-et-al-2024>(1/2 | 204/264) MuPT: A Generative Symbolic Music Pretrained Transformer (Xingwei Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu Tan, Stephen W. Huang, Wenhu Chen, Jie Fu, Ge Zhang. (2024)<br><strong>MuPT: A Generative Symbolic Music Pretrained Transformer</strong><br><button class=copy-to-clipboard title="MuPT: A Generative Symbolic Music Pretrained Transformer" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Transformer, Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06393v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06393v2.pdf filename=2404.06393v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that <b>LLMs</b> are inherently more compatible with ABC Notation, which aligns more closely with their design and strengths, thereby enhancing the model&rsquo;s performance in musical composition. To address the challenges associated with misaligned measures from different tracks during generation, we propose the development of a Synchronized Multi-Track ABC Notation (SMT-ABC Notation), which aims to preserve coherence across multiple musical tracks. Our contributions include a series of models capable of handling up to 8192 tokens, covering 90% of the symbolic music data in our training set. Furthermore, we explore the implications of the Symbolic Music <b>Scaling</b> <b>Law</b> (SMS Law) on model performance. The results indicate a promising direction for future research in music generation, offering extensive resources for community-led research through our open-source contributions.</p></p class="citation"></blockquote><h3 id=22--205264-exploring-diverse-sounds-identifying-outliers-in-a-music-corpus-le-cai-et-al-2024>(2/2 | 205/264) Exploring Diverse Sounds: Identifying Outliers in a Music Corpus (Le Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Cai, Sam Ferguson, Gengfa Fang, Hani Alshamrani. (2024)<br><strong>Exploring Diverse Sounds: Identifying Outliers in a Music Corpus</strong><br><button class=copy-to-clipboard title="Exploring Diverse Sounds: Identifying Outliers in a Music Corpus" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-IR, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06103v1.pdf filename=2404.06103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing research on music <b>recommendation</b> systems primarily focuses on recommending similar music, thereby often neglecting diverse and distinctive musical recordings. Musical outliers can provide valuable insights due to the inherent diversity of music itself. In this paper, we explore music outliers, investigating their potential usefulness for music discovery and <b>recommendation</b> systems. We argue that not all outliers should be treated as noise, as they can offer interesting perspectives and contribute to a richer understanding of an artist&rsquo;s work. We introduce the concept of &lsquo;Genuine&rsquo; music outliers and provide a definition for them. These genuine outliers can reveal unique aspects of an artist&rsquo;s repertoire and hold the potential to enhance music discovery by exposing listeners to novel and diverse musical experiences.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--206264-map-optical-properties-to-subwavelength-structures-directly-via-a-diffusion-model-shijie-rao-et-al-2024>(1/1 | 206/264) Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model (Shijie Rao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Rao, Kaiyu Cui, Yidong Huang, Jiawei Yang, Yali Li, Shengjin Wang, Xue Feng, Fang Liu, Wei Zhang. (2024)<br><strong>Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model</strong><br><button class=copy-to-clipboard title="Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-AI, physics-optics, physics.optics<br>Keyword Score: 40<br>Keywords: Diffusion Model, Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05959v1.pdf filename=2404.05959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subwavelength photonic structures and metamaterials provide revolutionary approaches for controlling light. The inverse design methods proposed for these subwavelength structures are vital to the development of new photonic devices. However, most of the existing inverse design methods cannot realize direct mapping from optical properties to photonic structures but instead rely on forward <b>simulation</b> methods to perform iterative optimization. In this work, we exploit the powerful generative abilities of artificial intelligence (AI) and propose a practical inverse design method based on latent <b>diffusion</b> <b>models.</b> Our method maps directly the optical properties to structures without the requirement of forward <b>simulation</b> and iterative optimization. Here, the given optical properties can work as <b>&ldquo;prompts&rdquo;</b> and guide the constructed model to correctly &ldquo;draw&rdquo; the required photonic structures. Experiments show that our direct mapping-based inverse design method can generate subwavelength photonic structures at high fidelity while following the given optical properties. This may change the method used for optical design and greatly accelerate the research on new photonic devices.</p></p class="citation"></blockquote><h2 id=csai-8>cs.AI (8)</h2><h3 id=18--207264-building-a-knowledge-graph-to-enrich-chatgpt-responses-in-manufacturing-service-discovery-yunqing-li-et-al-2024>(1/8 | 207/264) Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing Service Discovery (Yunqing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunqing Li, Binil Starly. (2024)<br><strong>Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing Service Discovery</strong><br><button class=copy-to-clipboard title="Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing Service Discovery" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 38<br>Keywords: Graph, Graph Embedding, Knowledge Graph, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06571v1.pdf filename=2404.06571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy. The advent of advanced <b>large</b> <b>language</b> <b>models</b> has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of <b>knowledge</b> <b>domains.</b> However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery. This research explores the potential of leveraging <b>Knowledge</b> <b>Graphs</b> <b>in</b> conjunction with <b>ChatGPT</b> to streamline the process for prospective clients in identifying small manufacturing enterprises. In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service <b>Knowledge</b> <b>Graph</b> <b>from</b> an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America. The <b>Knowledge</b> <b>Graph</b> <b>and</b> the learned <b>graph</b> <b>embedding</b> vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability. The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service <b>Knowledge</b> <b>Network</b> <b>Graph</b> <b>that</b> can potentially interconnect multiple types of <b>Knowledge</b> <b>Graphs</b> <b>that</b> span industry sectors, geopolitical boundaries, and business domains. The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers&rsquo; weblinks, manufacturing services, certifications, and location entity types.</p></p class="citation"></blockquote><h3 id=28--208264-agentquest-a-modular-benchmark-framework-to-measure-progress-and-improve-llm-agents-luca-gioacchini-et-al-2024>(2/8 | 208/264) AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents (Luca Gioacchini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence. (2024)<br><strong>AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents</strong><br><button class=copy-to-clipboard title="AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06411v1.pdf filename=2404.06411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advances made by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have led to the pursuit of <b>LLM</b> agents that can solve intricate, multi-step <b>reasoning</b> tasks. As with any research pursuit, <b>benchmarking</b> and evaluation are key corner stones to efficient and reliable progress. However, existing <b>benchmarks</b> are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest &ndash; a framework where (i) both <b>benchmarks</b> and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track <b>LLM</b> agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under <a href=https://github.com/nec-research/agentquest>https://github.com/nec-research/agentquest</a>.</p></p class="citation"></blockquote><h3 id=38--209264-agentscodriver-large-language-model-empowered-collaborative-driving-with-lifelong-learning-senkang-hu-et-al-2024>(3/8 | 209/264) AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning (Senkang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Senkang Hu, Zhengru Fang, Zihan Fang, Xianhao Chen, Yuguang Fang. (2024)<br><strong>AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning</strong><br><button class=copy-to-clipboard title="AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06345v1.pdf filename=2404.06345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, <b>reasoning</b> engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.</p></p class="citation"></blockquote><h3 id=48--210264-enhancing-decision-analysis-with-a-large-language-model-pydecision-a-comprehensive-library-of-mcda-methods-in-python-valdecy-pereira-et-al-2024>(4/8 | 210/264) Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python (Valdecy Pereira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Valdecy Pereira, Marcio Pereira Basilio, Carlos Henrique Tarjano SantosCarlos Henrique Tarjano Santos. (2024)<br><strong>Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python</strong><br><button class=copy-to-clipboard title="Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06370v1.pdf filename=2404.06370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Multicriteria decision analysis (MCDA) has become increasingly essential for decision-making in complex environments. In response to this need, the pyDecision library, implemented in Python and available at <a href=https://bit.ly/3tLFGtH>https://bit.ly/3tLFGtH</a>, has been developed to provide a comprehensive and accessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond offering a vast range of techniques, the library provides visualization tools for more intuitive results interpretation. In addition to these features, pyDecision has integrated <b>ChatGPT,</b> an advanced <b>Large</b> <b>Language</b> <b>Model,</b> where decision-makers can use <b>ChatGPT</b> to discuss and compare the outcomes of different methods, providing a more interactive and intuitive understanding of the solutions. Findings: <b>Large</b> <b>Language</b> <b>Models</b> are undeniably potent but can sometimes be a double-edged sword. Its answers may be misleading without rigorous verification of its outputs, especially for researchers lacking deep domain expertise. It&rsquo;s imperative to approach its insights with a discerning eye and a solid foundation in the relevant field. Originality: With the integration of MCDA methods and <b>ChatGPT,</b> pyDecision is a significant contribution to the scientific community, as it is an invaluable resource for researchers, practitioners, and decision-makers navigating complex decision-making problems and seeking the most appropriate solutions based on MCDA methods.</p></p class="citation"></blockquote><h3 id=58--211264-wus-method-can-boost-symbolic-ai-to-rival-silver-medalists-and-alphageometry-to-outperform-gold-medalists-at-imo-geometry-shiven-sinha-et-al-2024>(5/8 | 211/264) Wu&rsquo;s Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry (Shiven Sinha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat, Matthias Bethge. (2024)<br><strong>Wu&rsquo;s Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry</strong><br><button class=copy-to-clipboard title="Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CG, cs-CL, cs-LG, cs.AI<br>Keyword Score: 15<br>Keywords: Geometry, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06405v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06405v2.pdf filename=2404.06405v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Proving geometric theorems constitutes a hallmark of visual <b>reasoning</b> combining both intuitive and logical skills. Therefore, automated theorem proving of Olympiad-level <b>geometry</b> problems is considered a notable milestone in human-level automated <b>reasoning.</b> The introduction of AlphaGeometry, a neuro-symbolic model trained with 100 million synthetic samples, marked a major breakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO) problems whereas the reported baseline based on Wu&rsquo;s method solved only ten. In this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry, and find that Wu&rsquo;s method is surprisingly strong. Wu&rsquo;s method alone can solve 15 problems, and some of them are not solved by any of the other methods. This leads to two key findings: (i) Combining Wu&rsquo;s method with the classic synthetic methods of deductive databases and angle, ratio, and distance chasing solves 21 out of 30 methods by just using a CPU-only laptop with a time limit of 5 minutes per problem. Essentially, this classic method solves just 4 problems less than AlphaGeometry and establishes the first fully symbolic baseline strong enough to rival the performance of an IMO silver medalist. (ii) Wu&rsquo;s method even solves 2 of the 5 problems that AlphaGeometry failed to solve. Thus, by combining AlphaGeometry with Wu&rsquo;s method we set a new state-of-the-art for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the first AI method which outperforms an IMO gold medalist.</p></p class="citation"></blockquote><h3 id=68--212264-autonomous-evaluation-and-refinement-of-digital-agents-jiayi-pan-et-al-2024>(6/8 | 212/264) Autonomous Evaluation and Refinement of Digital Agents (Jiayi Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, Alane Suhr. (2024)<br><strong>Autonomous Evaluation and Refinement of Digital Agents</strong><br><button class=copy-to-clipboard title="Autonomous Evaluation and Refinement of Digital Agents" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06474v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06474v2.pdf filename=2404.06474v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that domain-general automatic evaluators can significantly improve the performance of agents for web navigation and device control. We experiment with multiple evaluation models that trade off between inference cost, modularity of design, and accuracy. We validate the performance of these models in several popular <b>benchmarks</b> for digital agents, finding between 74.4 and 92.9% agreement with oracle evaluation metrics. Finally, we use these evaluators to improve the performance of existing agents via <b>fine-tuning</b> and inference-time guidance. Without any additional supervision, we improve state-of-the-art performance by 29% on the popular <b>benchmark</b> WebArena, and achieve a 75% relative improvement in a challenging domain transfer scenario.</p></p class="citation"></blockquote><h3 id=78--213264-automatically-learning-htn-methods-from-landmarks-ruoxi-li-et-al-2024>(7/8 | 213/264) Automatically Learning HTN Methods from Landmarks (Ruoxi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruoxi Li, Dana Nau, Mark Roberts, Morgan Fine-Morris. (2024)<br><strong>Automatically Learning HTN Methods from Landmarks</strong><br><button class=copy-to-clipboard title="Automatically Learning HTN Methods from Landmarks" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06325v1.pdf filename=2404.06325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical Task Network (HTN) planning usually requires a domain engineer to provide manual input about how to decompose a planning problem. Even HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer to annotate the tasks with information about what to learn. We introduce CURRICULAMA, an HTN method learning algorithm that completely automates the learning process. It uses landmark analysis to compose annotated tasks and leverages <b>curriculum</b> <b>learning</b> to order the learning of methods from simpler to more complex. This eliminates the need for manual input, resolving a core issue with HTN-MAKER. We prove CURRICULAMA&rsquo;s soundness, and show experimentally that it has a substantially similar convergence rate in learning a complete set of methods to HTN-MAKER.</p></p class="citation"></blockquote><h3 id=88--214264-goat-bench-a-benchmark-for-multi-modal-lifelong-navigation-mukul-khanna-et-al-2024>(8/8 | 214/264) GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation (Mukul Khanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi. (2024)<br><strong>GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation</strong><br><button class=copy-to-clipboard title="GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06609v1.pdf filename=2404.06609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a <b>benchmark</b> for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We <b>benchmark</b> monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.</p></p class="citation"></blockquote><h2 id=csne-8>cs.NE (8)</h2><h3 id=18--215264-evolving-loss-functions-for-specific-image-augmentation-techniques-brandon-morgan-et-al-2024>(1/8 | 215/264) Evolving Loss Functions for Specific Image Augmentation Techniques (Brandon Morgan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brandon Morgan, Dean Hougen. (2024)<br><strong>Evolving Loss Functions for Specific Image Augmentation Techniques</strong><br><button class=copy-to-clipboard title="Evolving Loss Functions for Specific Image Augmentation Techniques" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 35<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06633v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06633v1.pdf filename=2404.06633v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work in Neural Loss Function Search (NLFS) has shown a lack of correlation between smaller surrogate functions and large <b>convolutional</b> <b>neural</b> <b>networks</b> with massive regularization. We expand upon this research by revealing another disparity that exists, correlation between different types of image augmentation techniques. We show that different loss functions can perform well on certain image augmentation techniques, while performing poorly on others. We exploit this disparity by performing an evolutionary search on five types of image augmentation techniques in the hopes of finding image augmentation specific loss functions. The best loss functions from each evolution were then taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each of the five image augmentation techniques. The best from that were then taken and evaluated by <b>fine-tuning</b> EfficientNetV2Small on the CARS, Oxford-Flowers, and Caltech datasets across each of the five image augmentation techniques. Multiple loss functions were found that outperformed cross-entropy across multiple experiments. In the end, we found a single loss function, which we called the inverse bessel logarithm loss, that was able to outperform cross-entropy across the majority of experiments.</p></p class="citation"></blockquote><h3 id=28--216264-exploring-the-true-potential-evaluating-the-black-box-optimization-capability-of-large-language-models-beichen-huang-et-al-2024>(2/8 | 216/264) Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models (Beichen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beichen Huang, Xingyu Wu, Yu Zhou, Jibin Wu, Liang Feng, Ran Cheng, Kay Chen Tan. (2024)<br><strong>Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models</strong><br><button class=copy-to-clipboard title="Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06290v1.pdf filename=2404.06290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained widespread popularity and demonstrated exceptional performance not only in natural language processing (NLP) tasks but also in non-linguistic domains. Their potential as artificial general intelligence extends beyond NLP, showcasing promising capabilities in diverse optimization scenarios. Despite this rising trend, whether the integration of <b>LLMs</b> into these <b>black-box</b> <b>optimization</b> problems is genuinely beneficial remains unexplored. This paper endeavors to tackle this issue by offering deeper insights into the potential of <b>LLMs</b> in optimization tasks through a comprehensive investigation. Our approach involves a comprehensive evaluation, covering both discrete and continuous optimization problems, aiming to assess the efficacy and distinctive characteristics that <b>LLMs</b> bring to the realm of optimization. Our findings reveal both the limitations and advantages of <b>LLMs</b> in optimization. On one hand, despite consuming the significant power required to run the model, <b>LLMs</b> exhibit subpar performance and lack desirable properties in pure numerical tasks, primarily due to a mismatch between the problem domain and their processing capabilities. On the other hand, although <b>LLMs</b> may not be ideal for traditional numerical optimization, their potential in broader optimization contexts remains promising. <b>LLMs</b> exhibit the ability to solve problems in non-numerical domains and can leverage heuristics from the <b>prompt</b> to enhance their performance. To the best of our knowledge, this work presents the first systematic evaluation of <b>LLMs</b> for numerical optimization, offering a progressive, wide-coverage, and behavioral analysis. Our findings pave the way for a deeper understanding of <b>LLMs&rsquo;</b> role in optimization and guide future application in diverse scenarios for <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=38--217264-emergent-braitenberg-style-behaviours-for-navigating-the-vizdoom-my-way-home-labyrinth-caleidgh-bayer-et-al-2024>(3/8 | 217/264) Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way Home&rsquo; Labyrinth (Caleidgh Bayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caleidgh Bayer, Robert J. Smith, Malcolm I. Heywood. (2024)<br>**Emergent Braitenberg-style Behaviours for Navigating the ViZDoom <code>My Way Home' Labyrinth** &lt;br/> &lt;button class="copy-to-clipboard" title="Emergent Braitenberg-style Behaviours for Navigating the ViZDoom </code>My Way Home&rsquo; Labyrinth" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 23<br>Keywords: Graph, Convolution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06529v1.pdf filename=2404.06529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The navigation of complex labyrinths with tens of rooms under visual partially observable state is typically addressed using recurrent deep <b>reinforcement</b> <b>learning</b> architectures. In this work, we show that navigation can be achieved through the emergent evolution of a simple Braitentberg-style heuristic that structures the interaction between agent and labyrinth, i.e. complex behaviour from simple heuristics. To do so, the approach of tangled program <b>graphs</b> is assumed in which programs cooperatively coevolve to develop a modular indexing scheme that only employs 0.8% of the state space. We attribute this simplicity to several biases implicit in the representation, such as the use of pixel indexing as opposed to deploying a <b>convolutional</b> kernel or image processing operators.</p></p class="citation"></blockquote><h3 id=48--218264-an-enhanced-grey-wolf-optimizer-with-elite-inheritance-and-balance-search-mechanisms-jianhua-jiang-et-al-2024>(4/8 | 218/264) An Enhanced Grey Wolf Optimizer with Elite Inheritance and Balance Search Mechanisms (Jianhua Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhua Jiang, Ziying Zhao, Weihua Li, Keqin Li. (2024)<br><strong>An Enhanced Grey Wolf Optimizer with Elite Inheritance and Balance Search Mechanisms</strong><br><button class=copy-to-clipboard title="An Enhanced Grey Wolf Optimizer with Elite Inheritance and Balance Search Mechanisms" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06524v1.pdf filename=2404.06524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Grey Wolf Optimizer (GWO) is recognized as a novel meta-heuristic algorithm inspired by the social leadership hierarchy and hunting mechanism of grey wolves. It is well-known for its simple parameter setting, fast convergence speed, and strong optimization capability. In the original GWO, there are two significant design flaws in its fundamental optimization mechanisms. Problem (1): the algorithm fails to inherit from elite positions from the last iteration when generating the next positions of the wolf population, potentially leading to suboptimal solutions. Problem (2): the positions of the population are updated based on the central position of the three leading wolves (alpha, beta, delta), without a balanced mechanism between local and global search. To tackle these problems, an enhanced Grey Wolf Optimizer with Elite Inheritance Mechanism and Balance Search Mechanism, named as EBGWO, is proposed to improve the effectiveness of the position updating and the quality of the convergence solutions. The IEEE CEC 2014 <b>benchmark</b> functions suite and a series of <b>simulation</b> tests are employed to evaluate the performance of the proposed algorithm. The <b>simulation</b> tests involve a comparative study between EBGWO, three GWO variants, GWO and two well-known meta-heuristic algorithms. The experimental results demonstrate that the proposed EBGWO algorithm outperforms other meta-heuristic algorithms in both accuracy and convergence speed. Three engineering optimization problems are adopted to prove its capability in processing real-world problems. The results indicate that the proposed EBGWO outperforms several popular algorithms.</p></p class="citation"></blockquote><h3 id=58--219264-evolving-collective-behavior-in-self-organizing-particle-systems-devendra-parkar-et-al-2024>(5/8 | 219/264) Evolving Collective Behavior in Self-Organizing Particle Systems (Devendra Parkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Devendra Parkar, Kirtus G. Leyba, Raylene A. Faerber, Joshua J. Daymude. (2024)<br><strong>Evolving Collective Behavior in Self-Organizing Particle Systems</strong><br><button class=copy-to-clipboard title="Evolving Collective Behavior in Self-Organizing Particle Systems" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05915v1.pdf filename=2404.05915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local interactions drive emergent collective behavior, which pervades biological and social complex systems. But uncovering the interactions that produce a desired behavior remains a core challenge. In this paper, we present EvoSOPS, an evolutionary framework that searches landscapes of stochastic distributed algorithms for those that achieve a mathematically specified target behavior. These algorithms govern self-organizing particle systems (SOPS) comprising individuals with no persistent memory and strictly local sensing and movement. For aggregation, phototaxing, and separation behaviors, EvoSOPS discovers algorithms that achieve 4.2-15.3% higher fitness than those from the existing &ldquo;stochastic approach to SOPS&rdquo; based on mathematical theory from statistical physics. EvoSOPS is also flexibly applied to new behaviors such as object coating where the stochastic approach would require bespoke, extensive analysis. Finally, we <b>distill</b> insights from the diverse, best-fitness genomes produced for aggregation across repeated EvoSOPS runs to demonstrate how EvoSOPS can bootstrap future theoretical investigations into SOPS algorithms for new behaviors.</p></p class="citation"></blockquote><h3 id=68--220264-temporal-true-and-surrogate-fitness-landscape-analysis-for-expensive-bi-objective-optimisation-c-j-rodriguez-et-al-2024>(6/8 | 220/264) Temporal True and Surrogate Fitness Landscape Analysis for Expensive Bi-Objective Optimisation (C. J. Rodriguez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>C. J. Rodriguez, S. L. Thomson, T. Alderliesten, P. A. N. Bosman. (2024)<br><strong>Temporal True and Surrogate Fitness Landscape Analysis for Expensive Bi-Objective Optimisation</strong><br><button class=copy-to-clipboard title="Temporal True and Surrogate Fitness Landscape Analysis for Expensive Bi-Objective Optimisation" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06557v1.pdf filename=2404.06557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many real-world problems have expensive-to-compute fitness functions and are multi-objective in nature. Surrogate-assisted evolutionary algorithms are often used to tackle such problems. Despite this, literature about analysing the fitness landscapes induced by surrogate models is limited, and even non-existent for multi-objective problems. This study addresses this critical gap by comparing landscapes of the true fitness function with those of surrogate models for multi-objective functions. Moreover, it does so temporally by examining landscape features at different points in time during optimisation, in the vicinity of the population at that point in time. We consider the BBOB bi-objective <b>benchmark</b> functions in our experiments. The results of the fitness landscape analysis reveals significant differences between true and surrogate features at different time points during optimisation. Despite these differences, the true and surrogate landscape features still show high correlations between each other. Furthermore, this study identifies which landscape features are related to search and demonstrates that both surrogate and true landscape features are capable of predicting algorithm performance. These findings indicate that temporal analysis of the landscape features may help to facilitate the design of surrogate switching approaches to improve performance in multi-objective optimisation.</p></p class="citation"></blockquote><h3 id=78--221264-synaptogen-a-cross-domain-generative-device-model-for-large-scale-neuromorphic-circuit-design-tyler-hennen-et-al-2024>(7/8 | 221/264) Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design (Tyler Hennen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tyler Hennen, Leon Brackmann, Tobias Ziegler, Sebastian Siegel, Stephan Menzel, Rainer Waser, Dirk J. Wouters, Daniel Bedau. (2024)<br><strong>Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design</strong><br><button class=copy-to-clipboard title="Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cond-mat-mtrl-sci, cs-NE, cs.NE, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06344v1.pdf filename=2404.06344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a fast generative modeling approach for resistive memories that reproduces the complex statistical properties of real-world devices. To enable efficient modeling of analog circuits, the model is implemented in Verilog-A. By training on extensive measurement data of integrated 1T1R arrays (6,000 cycles of 512 devices), an autoregressive stochastic process accurately accounts for the cross-correlations between the switching parameters, while non-linear transformations ensure agreement with both cycle-to-cycle (C2C) and device-to-device (D2D) variability. <b>Benchmarks</b> show that this statistically comprehensive model achieves read/write throughputs exceeding those of even highly simplified and deterministic compact models.</p></p class="citation"></blockquote><h3 id=88--222264-using-3-objective-evolutionary-algorithms-for-the-dynamic-chance-constrained-knapsack-problem-ishara-hewa-pathiranage-et-al-2024>(8/8 | 222/264) Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem (Ishara Hewa Pathiranage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishara Hewa Pathiranage, Frank Neumann, Denis Antipov, Aneta Neumann. (2024)<br><strong>Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem</strong><br><button class=copy-to-clipboard title="Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06014v1.pdf filename=2404.06014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world optimization problems often involve stochastic and dynamic components. Evolutionary algorithms are particularly effective in these scenarios, as they can easily adapt to uncertain and changing environments but often uncertainty and dynamic changes are studied in isolation. In this paper, we explore the use of 3-objective evolutionary algorithms for the chance constrained knapsack problem with dynamic constraints. In our setting, the weights of the items are stochastic and the knapsack&rsquo;s capacity changes over time. We introduce a 3-objective formulation that is able to deal with the stochastic and dynamic components at the same time and is independent of the confidence level required for the constraint. This new approach is then compared to the 2-objective formulation which is limited to a single confidence level. We evaluate the approach using two different multi-objective evolutionary algorithms (MOEAs), namely the global simple evolutionary multi-objective optimizer (GSEMO) and the multi-objective evolutionary algorithm based on decomposition (MOEA/D), across various <b>benchmark</b> scenarios. Our analysis highlights the advantages of the 3-objective formulation over the 2-objective formulation in addressing the dynamic chance constrained knapsack problem.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=11--223264-sam-i-am-semantic-boosting-for-zero-shot-atomic-scale-electron-micrograph-segmentation-waqwoya-abebe-et-al-2024>(1/1 | 223/264) SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation (Waqwoya Abebe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Waqwoya Abebe, Jan Strube, Luanzheng Guo, Nathan R. Tallent, Oceane Bel, Steven Spurgeon, Christina Doty, Ali Jannesari. (2024)<br><strong>SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation</strong><br><button class=copy-to-clipboard title="SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06638v1.pdf filename=2404.06638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image segmentation is a critical enabler for tasks ranging from medical diagnostics to autonomous driving. However, the correct segmentation semantics - where are boundaries located? what segments are logically similar? - change depending on the domain, such that state-of-the-art <b>foundation</b> <b>models</b> can generate meaningless and incorrect results. Moreover, in certain domains, <b>fine-tuning</b> and retraining techniques are infeasible: obtaining labels is costly and time-consuming; domain images (micrographs) can be exponentially diverse; and data sharing (for third-party retraining) is restricted. To enable rapid adaptation of the best segmentation technology, we propose the concept of semantic boosting: given a <b>zero-shot</b> <b>foundation</b> <b>model,</b> guide its segmentation and adjust results to match domain expectations. We apply semantic boosting to the Segment Anything Model (SAM) to obtain microstructure segmentation for transmission electron microscopy. Our booster, SAM-I-Am, extracts geometric and textural features of various intermediate masks to perform mask removal and mask merging operations. We demonstrate a <b>zero-shot</b> performance increase of (absolute) +21.35%, +12.6%, +5.27% in mean IoU, and a -9.91%, -18.42%, -4.06% drop in mean false positive masks across images of three difficulty classes over vanilla SAM (ViT-L).</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=17--224264-xflex-hydro-demonstrators-grid-services-assessment-and-ancillary-services-matrix-elaboration-christophe-nicolet-et-al-2024>(1/7 | 224/264) XFLEX HYDRO demonstrators grid services assessment and Ancillary Services Matrix elaboration (Christophe Nicolet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophe Nicolet, Matthieu Dreyer, Christian Landry, Sébastien Alligné, Antoine Béguin, Yves Vaillant, Stefan Tobler, Goekhan Sari, Grégory Païs, Matteo Bianciotto, Steve Sawyer, Richard Taylor, Manuel Vaz Castro, Maria Helena Vasconcelos, Carlos Moreira. (2024)<br><strong>XFLEX HYDRO demonstrators grid services assessment and Ancillary Services Matrix elaboration</strong><br><button class=copy-to-clipboard title="XFLEX HYDRO demonstrators grid services assessment and Ancillary Services Matrix elaboration" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06275v1.pdf filename=2404.06275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the methodology and key results which enabled to establish the so-called Ancillary Service Matrix (ASM) presenting the ability to deliver the different ancillary services of each of the 6 demonstrators of the XFLEX HYDRO research project combined with the applicable technologies studied in this analysis. These technologies include i) the variable speed technology with Doubly Fed Induction Machine (DFIM) or Full Size Frequency Converters (FSFC), ii) the Smart Power Plant Supervisor (SPPS) enabling to extend the operating range of the hydraulic units in turbine mode based on a better knowledge of the hydro unit wear and tear and associated costs over the full unit operating range, iii) the hydraulic short circuit (HSC) operation leading to simultaneous operation of pump and turbines of Pumped Storage Power Plants (PSPP) and iv) the Hydro-Battery-Hybrid (HBH) applied at Run-of-River demonstrator. The demonstrators considered for this study includes 4 pumped storage power plants, 1 conventional hydro storage plant and 1 run-of-the-river plant. For each demonstrator a 1D <b>simulation</b> model was developed and validated and was further enhanced to include the model of control system enabling to address the various ancillary services. The systematic 1D numerical <b>simulation</b> of ancillary service contribution of each demonstrator and related technologies enabled to quantify the magnitude of active power response to contribute to the different grid services. The results have been scored between 0 and 5 for each ancillary service, allowing to populate the Ancillary Service Matrix which is summarizing the results in a graphical and synthetic way. The analysis of the score of the Ancillary Services Matrix enables the reader to draw several key conclusions about the benefits unlocked by the implementation of these technologies which are <b>summarized</b> in the paper.</p></p class="citation"></blockquote><h3 id=27--225264-traffic-signal-control-and-speed-offset-coordination-using-q-learning-for-arterial-road-networks-tianchen-yuan-et-al-2024>(2/7 | 225/264) Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks (Tianchen Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianchen Yuan, Petros A. Ioannou. (2024)<br><strong>Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks</strong><br><button class=copy-to-clipboard title="Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06382v1.pdf filename=2404.06382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Arterial traffic interacts with freeway traffic, yet the two are controlled independently. Arterial traffic signals do not take into account freeway traffic and how ramps control ingress traffic and have no control over egress traffic from the freeway. This often results in long queues in either direction that block ramps and spill over to arterial streets or freeway lanes. In this paper, we propose an adaptive arterial traffic control strategy that combines traffic signal control (TSC) and dynamic speed offset (DSO) coordination using a Q-learning algorithm for a traffic network that involves a freeway segment and adjacent arterial streets. The TSC agent computes the signal cycle length and split based on observed intersection demands and adjacent freeway off-ramp queues. The DSO agent computes the relative offset and the recommended speeds of both ways between consecutive intersections based on their physical distance, intersection queues, and signal cycles. We evaluate the performance of the proposed arterial traffic control strategy using microscopic traffic <b>simulations</b> of an arterial corridor with seven intersections near the I-710 freeway. The proposed QL-based control significantly outperforms a fixed-time control and MAXBAND in terms of the travel time and the number of stops under low or moderate demands. In high-demand scenarios, the travel-time benefit provided by the QL-based control is reduced as it mitigates off-ramp and intersection queues, which is a necessary trade-off in our perspective. In addition, mutual benefit is obtained by implementing freeway and arterial traffic control simultaneously.</p></p class="citation"></blockquote><h3 id=37--226264-inertia-emulation-contribution-of-frades-2-variable-speed-pump-turbine-to-power-network-stability-christophe-nicolet-et-al-2024>(3/7 | 226/264) Inertia emulation contribution of Frades 2 variable speed pump-turbine to power network stability (Christophe Nicolet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophe Nicolet, Antoine Béguin, Matthieu Dreyer, Sébastien Alligné, Alexander Jung, Diogo Cordeiro, Carlos Moreira. (2024)<br><strong>Inertia emulation contribution of Frades 2 variable speed pump-turbine to power network stability</strong><br><button class=copy-to-clipboard title="Inertia emulation contribution of Frades 2 variable speed pump-turbine to power network stability" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06299v1.pdf filename=2404.06299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is addressing the quantification and the comparison of pumped storage power plants, PSPP, contribution to synchronous inertia and synthetic inertia when fixed speed and variable speed motor-generators technologies are considered, respectively. Therefore, a grid stability study was conducted by means of 1D SIMSEN <b>simulation</b> for the 2 x 395 MW PSPP Frades 2 in Portugal with both fixed speed and variable speed technologies in case of operation connected to an infinite power network or to an islanded 4.4 GW synchronous power network.</p></p class="citation"></blockquote><h3 id=47--227264-a-large-scale-simulation-method-for-neuromorphic-circuits-amir-shahhosseini-et-al-2024>(4/7 | 227/264) A Large-Scale Simulation Method for Neuromorphic Circuits (Amir Shahhosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Shahhosseini, Thomas Chaffey, Rodolphe Sepulchre. (2024)<br><strong>A Large-Scale Simulation Method for Neuromorphic Circuits</strong><br><button class=copy-to-clipboard title="A Large-Scale Simulation Method for Neuromorphic Circuits" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06255v1.pdf filename=2404.06255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Splitting algorithms are well-established in convex optimization and are designed to solve large-scale problems. Using such algorithms to simulate the behavior of nonlinear circuit networks provides scalable methods for the <b>simulation</b> and design of neuromorphic systems. For circuits made of linear capacitors and inductors with nonlinear resistive elements, we propose a splitting that breaks the network into its LTI lossless component and its static resistive component. This splitting has both physical and algorithmic advantages and allows for separate calculations in the time domain and in the frequency domain. To demonstrate the scalability of this approach, a network made from one hundred neurons modeled by the well-known FitzHugh-Nagumo circuit with all-to-all diffusive coupling is simulated.</p></p class="citation"></blockquote><h3 id=57--228264-learning-locally-interacting-discrete-dynamical-systems-towards-data-efficient-and-scalable-prediction-beomseok-kang-et-al-2024>(5/7 | 228/264) Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction (Beomseok Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beomseok Kang, Harshit Kumar, Minah Lee, Biswadeep Chakraborty, Saibal Mukhopadhyay. (2024)<br><strong>Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction</strong><br><button class=copy-to-clipboard title="Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Augmented Reality (AR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06460v1.pdf filename=2404.06460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Locally interacting dynamical systems, such as epidemic spread, rumor propagation through crowd, and forest fire, exhibit complex global dynamics originated from local, relatively simple, and often stochastic interactions between dynamic elements. Their temporal evolution is often driven by transitions between a finite number of discrete states. Despite significant advancements in predictive modeling through deep learning, such interactions among many elements have rarely explored as a specific domain for predictive modeling. We present Attentive Recurrent Neural Cellular Automata <b>(AR-NCA),</b> to effectively discover unknown local state transition rules by associating the temporal information between neighboring cells in a permutation-invariant manner. <b>AR-NCA</b> exhibits the superior generalizability across various system configurations (i.e., spatial distribution of states), data efficiency and robustness in extremely data-limited scenarios even in the presence of stochastic interactions, and scalability through spatial dimension-independent prediction.</p></p class="citation"></blockquote><h3 id=67--229264-a-data-driven-approach-to-uio-based-fault-diagnosis-giulio-fattore-et-al-2024>(6/7 | 229/264) A data-driven approach to UIO-based fault diagnosis (Giulio Fattore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulio Fattore, Maria Elena Valcher. (2024)<br><strong>A data-driven approach to UIO-based fault diagnosis</strong><br><button class=copy-to-clipboard title="A data-driven approach to UIO-based fault diagnosis" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06158v1.pdf filename=2404.06158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we propose a data-driven approach to the design of a residual generator, based on a dead-beat unknown-input observer, for linear time-invariant <b>discrete-time</b> <b>state-space</b> models, whose state equation is affected both by disturbances and by actuator faults. We first review the modelbased conditions for the existence of such a residual generator, and then prove that under suitable assumptions on the collected historical data, we are both able to determine if the problem is solvable and to identify the matrices of a possible residual generator. We propose an algorithm that, based only on the collected data (and not on the system description), is able to perform both tasks. An illustrating example and some remarks on limitations and possible extensions of the current results conclude the paper.</p></p class="citation"></blockquote><h3 id=77--230264-learning-model-predictive-control-parameters-via-bayesian-optimization-for-battery-fast-charging-sebastian-hirt-et-al-2024>(7/7 | 230/264) Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging (Sebastian Hirt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Hirt, Andreas Höhl, Joachim Schaeffer, Johannes Pohlodek, Richard D. Braatz, Rolf Findeisen. (2024)<br><strong>Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging</strong><br><button class=copy-to-clipboard title="Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06125v1.pdf filename=2404.06125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tuning parameters in model predictive control (MPC) presents significant challenges, particularly when there is a notable discrepancy between the controller&rsquo;s predictions and the actual behavior of the closed-loop plant. This mismatch may stem from factors like substantial model-plant differences, limited prediction horizons that do not cover the entire time of interest, or unforeseen system disturbances. Such mismatches can jeopardize both performance and safety, including constraint satisfaction. Traditional methods address this issue by modifying the finite horizon cost function to better reflect the overall operational cost, learning parts of the prediction model from data, or implementing robust MPC strategies, which might be either computationally intensive or overly cautious. As an alternative, directly optimizing or learning the controller parameters to enhance closed-loop performance has been proposed. We apply Bayesian optimization for efficient learning of unknown model parameters and parameterized constraint backoff terms, aiming to improve closed-loop performance of battery fast charging. This approach establishes a hierarchical control framework where Bayesian optimization directly <b>fine-tunes</b> closed-loop behavior towards a global and long-term objective, while MPC handles lower-level, short-term control tasks. For lithium-ion battery fast charging, we show that the learning approach not only ensures safe operation but also maximizes closed-loop performance. This includes maintaining the battery&rsquo;s operation below its maximum terminal voltage and reducing charging times, all achieved using a standard nominal MPC model with a short horizon and notable initial model-plant mismatch.</p></p class="citation"></blockquote><h2 id=physicsdata-an-1>physics.data-an (1)</h2><h3 id=11--231264-a-feature-based-information-theoretic-approach-for-detecting-interpretable-long-timescale-pairwise-interactions-from-time-series-aria-nguyen-et-al-2024>(1/1 | 231/264) A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series (Aria Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aria Nguyen, Oscar McMullin, Joseph T. Lizier, Ben D. Fulcher. (2024)<br><strong>A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series</strong><br><button class=copy-to-clipboard title="A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.data-an<br>Categories: cs-IT, math-IT, physics-data-an, physics.data-an, stat-ME<br>Keyword Score: 30<br>Keywords: Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05929v1.pdf filename=2404.05929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantifying relationships between components of a complex system is critical to understanding the rich network of interactions that characterize the behavior of the system. Traditional methods for detecting pairwise dependence of time series, such as Pearson correlation, Granger causality, and <b>mutual</b> <b>information,</b> are computed directly in the space of measured time-series values. But for systems in which interactions are mediated by statistical properties of the time series (`time-series features&rsquo;) over longer timescales, this approach can fail to capture the underlying dependence from limited and noisy time-series data, and can be challenging to interpret. Addressing these issues, here we introduce an information-theoretic method for detecting dependence between time series mediated by time-series features that provides interpretable insights into the nature of the interactions. Our method extracts a candidate set of time-series features from sliding windows of the source time series and assesses their role in mediating a relationship to values of the target process. Across <b>simulations</b> of three different generative processes, we demonstrate that our feature-based approach can outperform a traditional inference approach based on raw time-series values, especially in challenging scenarios characterized by short time-series lengths, high noise levels, and long interaction timescales. Our work introduces a new tool for inferring and interpreting feature-mediated interactions from time-series data, contributing to the broader landscape of quantitative analysis in complex systems research, with potential applications in various domains including but not limited to neuroscience, finance, climate science, and engineering.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--232264-from-protoscience-to-epistemic-monoculture-how-benchmarking-set-the-stage-for-the-deep-learning-revolution-bernard-j-koch-et-al-2024>(1/2 | 232/264) From Protoscience to Epistemic Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution (Bernard J. Koch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernard J. Koch, David Peterson. (2024)<br><strong>From Protoscience to Epistemic Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution</strong><br><button class=copy-to-clipboard title="From Protoscience to Epistemic Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-LG, cs.CY<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Generative AI, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06647v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06647v2.pdf filename=2404.06647v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past decade, AI research has focused heavily on building ever-larger deep learning models. This approach has simultaneously unlocked incredible achievements in science and technology, and hindered AI from overcoming long-standing limitations with respect to explainability, ethical harms, and environmental efficiency. Drawing on qualitative interviews and computational analyses, our three-part history of AI research traces the creation of this &ldquo;epistemic monoculture&rdquo; back to a radical reconceptualization of scientific progress that began in the late 1980s. In the first era of AI research (1950s-late 1980s), researchers and patrons approached AI as a &ldquo;basic&rdquo; science that would advance through autonomous exploration and organic assessments of progress (e.g., peer-review, theoretical consensus). The failure of this approach led to a retrenchment of funding in the 1980s. Amid this &ldquo;AI Winter,&rdquo; an intervention by the U.S. government reoriented the field towards measurable progress on tasks of military and commercial interest. A new evaluation system called <b>&ldquo;benchmarking&rdquo;</b> provided an objective way to quantify progress on tasks by focusing exclusively on increasing predictive accuracy on example datasets. <b>Distilling</b> science down to verifiable metrics clarified the roles of scientists, allowed the field to rapidly integrate talent, and provided clear signals of significance and progress. But history has also revealed a tradeoff to this streamlined approach to science: the consolidation around external interests and inherent conservatism of <b>benchmarking</b> has disincentivized exploration beyond scaling monoculture. In the discussion, we explain how AI&rsquo;s monoculture offers a compelling challenge to the belief that basic, exploration-driven research is needed for scientific progress. Implications for the spread of AI monoculture to other sciences in the era of <b>generative</b> <b>AI</b> are also discussed.</p></p class="citation"></blockquote><h3 id=22--233264-automatic-authorities-power-and-ai-seth-lazar-2024>(2/2 | 233/264) Automatic Authorities: Power and AI (Seth Lazar, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seth Lazar. (2024)<br><strong>Automatic Authorities: Power and AI</strong><br><button class=copy-to-clipboard title="Automatic Authorities: Power and AI" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05990v1.pdf filename=2404.05990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As rapid advances in Artificial Intelligence and the rise of some of history&rsquo;s most potent corporations meet the diminished neoliberal state, people are increasingly subject to power exercised by means of automated systems. Machine learning and related computational technologies now underpin vital government services. They connect consumers and producers in new algorithmic markets. They determine how we find out about everything from how to vote to where to get vaccinated, and whose speech is amplified, reduced, or restricted. And a new wave of products based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> will further transform our economic and political lives. Automatic Authorities are automated computational systems used to exercise power over us by determining what we may know, what we may have, and what our options will be. In response to their rise, scholars working on the societal impacts of AI and related technologies have advocated shifting attention from how to make AI systems beneficial or fair towards a critical analysis of these new power relations. But power is everywhere, and is not necessarily bad. On what basis should we object to new or intensified power relations, and what can be done to justify them? This paper introduces the philosophical materials with which to formulate these questions, and offers preliminary answers. It starts by pinning down the concept of power, focusing on the ability that some agents have to shape others&rsquo; lives. It then explores how AI enables and intensifies the exercise of power so understood, and sketches three problems with power and three ways to solve those problems. It emphasises, in particular, that justifying power requires more than satisfying substantive justificatory criteria; standards of proper authority and procedural legitimacy must also be met. We need to know not only what power may be used for, but how it may be used, and by whom.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--234264-commute-with-community-enhancing-shared-travel-through-social-networks-tian-siyuan-et-al-2024>(1/2 | 234/264) Commute with Community: Enhancing Shared Travel through Social Networks (Tian Siyuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Siyuan, Dai Renjie, Wang Junhao, He Zhengxiao. (2024)<br><strong>Commute with Community: Enhancing Shared Travel through Social Networks</strong><br><button class=copy-to-clipboard title="Commute with Community: Enhancing Shared Travel through Social Networks" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05987v1.pdf filename=2404.05987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Shared mobility redefines urban transportation, offering economic and environmental benefits by reducing pollution and urban congestion. However, in the post-pandemic era, the shared mobility sector is grappling with a crisis of trust, particularly concerning passenger hesistancy towards shared transportation options. To address these problems, in this paper we take social network into consideration and propose a novel carpooling matching framework based on <b>graph</b> <b>neural</b> <b>network</b> and <b>reinforcement</b> <b>learning,increasing</b> the carpooling rate to 48% and reducing the average delay time to 6.1 minutes and average detour distance to 2.8km. Furthermore, we introduce an innovative metric, termed &rsquo;tolerance&rsquo; for mobility scheduling models to effectively quantify users&rsquo; sensitivity to social distancing. We conduct a sensitivity analysis to demonstrate that our model offers a viable approach to amplify the benefits, delivering resilient strategies for the advancement and proliferation of shared mobility incentives.</p></p class="citation"></blockquote><h3 id=22--235264-echo-chambers-in-the-age-of-algorithms-an-audit-of-twitters-friend-recommender-system-kayla-duskin-et-al-2024>(2/2 | 235/264) Echo Chambers in the Age of Algorithms: An Audit of Twitter&rsquo;s Friend Recommender System (Kayla Duskin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kayla Duskin, Joseph S. Schafer, Jevin D. West, Emma S. Spiro. (2024)<br><strong>Echo Chambers in the Age of Algorithms: An Audit of Twitter&rsquo;s Friend Recommender System</strong><br><button class=copy-to-clipboard title="Echo Chambers in the Age of Algorithms: An Audit of Twitter's Friend Recommender System" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06422v1.pdf filename=2404.06422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The presence of political misinformation and ideological echo chambers on social media platforms is concerning given the important role that these sites play in the public&rsquo;s exposure to news and current events. Algorithmic systems employed on these platforms are presumed to play a role in these phenomena, but little is known about their mechanisms and effects. In this work, we conduct an algorithmic audit of Twitter&rsquo;s Who-To-Follow friend <b>recommendation</b> system, the first empirical audit that investigates the impact of this algorithm in-situ. We create automated Twitter accounts that initially follow left and right affiliated U.S. politicians during the 2022 U.S. midterm elections and then grow their information networks using the platform&rsquo;s <b>recommender</b> <b>system.</b> We pair the experiment with an observational study of Twitter users who already follow the same politicians. Broadly, we find that while following the <b>recommendation</b> algorithm leads accounts into dense and reciprocal neighborhoods that structurally resemble echo chambers, the <b>recommender</b> <b>also</b> results in less political homogeneity of a user&rsquo;s network compared to accounts growing their networks through social endorsement. Furthermore, accounts that exclusively followed users recommended by the algorithm had fewer opportunities to encounter content centered on false or misleading election narratives compared to choosing friends based on social endorsement.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--236264-qiskit-torch-module-fast-prototyping-of-quantum-neural-networks-nico-meyer-et-al-2024>(1/2 | 236/264) Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks (Nico Meyer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nico Meyer, Christian Ufrecht, Maniraman Periyasamy, Axel Plinge, Christopher Mutschler, Daniel D. Scherer, Andreas Maier. (2024)<br><strong>Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks</strong><br><button class=copy-to-clipboard title="Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, cs-SE, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06314v1.pdf filename=2404.06314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum computer <b>simulation</b> software is an integral tool for the research efforts in the quantum computing community. An important aspect is the efficiency of respective frameworks, especially for training variational quantum algorithms. Focusing on the widely used Qiskit software environment, we develop the qiskit-torch-module. It improves runtime performance by two orders of magnitude over comparable libraries, while facilitating low-overhead integration with existing codebases. Moreover, the framework provides advanced tools for integrating quantum neural networks with PyTorch. The pipeline is tailored for single-machine compute systems, which constitute a widely employed setup in day-to-day research efforts.</p></p class="citation"></blockquote><h3 id=22--237264-quantum-state-generation-with-structure-preserving-diffusion-model-yuchen-zhu-et-al-2024>(2/2 | 237/264) Quantum State Generation with Structure-Preserving Diffusion Model (Yuchen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Zhu, Tianrong Chen, Evangelos A. Theodorou, Xie Chen, Molei Tao. (2024)<br><strong>Quantum State Generation with Structure-Preserving Diffusion Model</strong><br><button class=copy-to-clipboard title="Quantum State Generation with Structure-Preserving Diffusion Model" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph, stat-ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06336v1.pdf filename=2404.06336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article considers the generative modeling of the states of quantum systems, and an approach based on denoising <b>diffusion</b> <b>model</b> is proposed. The key contribution is an algorithmic innovation that respects the physical nature of quantum states. More precisely, the commonly used density matrix representation of mixed-state has to be complex-valued Hermitian, positive semi-definite, and trace one. Generic <b>diffusion</b> <b>models,</b> or other generative methods, may not be able to generate data that strictly satisfy these structural constraints, even if all training data do. To develop a machine learning algorithm that has physics hard-wired in, we leverage the recent development of Mirror <b>Diffusion</b> <b>Model</b> and design a previously unconsidered mirror map, to enable strict structure-preserving generation. Both unconditional generation and conditional generation via classifier-free guidance are experimentally demonstrated efficacious, the latter even enabling the design of new quantum states when generated on unseen labels.</p></p class="citation"></blockquote><h2 id=mathdg-1>math.DG (1)</h2><h3 id=11--238264-a-singular-riemannian-geometry-approach-to-deep-neural-networks-iii-piecewise-differentiable-layers-and-random-walks-on-n-dimensional-classes-alessandro-benfenati-et-al-2024>(1/1 | 238/264) A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes (Alessandro Benfenati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Benfenati, Alessio Marta. (2024)<br><strong>A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes</strong><br><button class=copy-to-clipboard title="A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.DG<br>Categories: cs-LG, math-DG, math.DG<br>Keyword Score: 20<br>Keywords: Convolution, Deep Neural Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06104v1.pdf filename=2404.06104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks are playing a crucial role in everyday life, with the most modern generative models able to achieve impressive results. Nonetheless, their functioning is still not very clear, and several strategies have been adopted to study how and why these model reach their outputs. A common approach is to consider the data in an Euclidean settings: recent years has witnessed instead a shift from this paradigm, moving thus to more general framework, namely Riemannian <b>Geometry.</b> Two recent works introduced a geometric framework to study neural networks making use of singular Riemannian metrics. In this paper we extend these results to <b>convolutional,</b> residual and recursive neural networks, studying also the case of non-differentiable activation functions, such as ReLU. We illustrate our findings with some numerical experiments on classification of images and thermodynamic problems.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--239264-collaborative-edge-ai-inference-over-cloud-ran-pengfei-zhang-et-al-2024>(1/2 | 239/264) Collaborative Edge AI Inference over Cloud-RAN (Pengfei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Zhang, Dingzhu Wen, Guangxu Zhu, Qimei Chen, Kaifeng Han, Yuanming Shi. (2024)<br><strong>Collaborative Edge AI Inference over Cloud-RAN</strong><br><button class=copy-to-clipboard title="Collaborative Edge AI Inference over Cloud-RAN" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-AI, cs-IT, cs-LG, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06007v1.pdf filename=2404.06007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a cloud radio access network (Cloud-RAN) based collaborative edge AI inference architecture is proposed. Specifically, geographically distributed devices capture real-time noise-corrupted sensory data samples and extract the noisy local feature vectors, which are then aggregated at each remote radio head (RRH) to suppress sensing noise. To realize efficient uplink feature aggregation, we allow each RRH receives local feature vectors from all devices over the same resource blocks simultaneously by leveraging an over-the-air computation (AirComp) technique. Thereafter, these aggregated feature vectors are <b>quantized</b> and transmitted to a central processor (CP) for further aggregation and downstream inference tasks. Our aim in this work is to maximize the inference accuracy via a surrogate accuracy metric called discriminant gain, which measures the discernibility of different classes in the feature space. The key challenges lie on simultaneously suppressing the coupled sensing noise, AirComp distortion caused by hostile wireless channels, and the <b>quantization</b> error resulting from the limited capacity of fronthaul links. To address these challenges, this work proposes a joint transmit precoding, receive beamforming, and <b>quantization</b> error control scheme to enhance the inference accuracy. Extensive numerical experiments demonstrate the effectiveness and superiority of our proposed optimization algorithm compared to various baselines.</p></p class="citation"></blockquote><h3 id=22--240264-distributed-massive-mimo-system-with-dynamic-clustering-in-leo-satellite-networks-khaled-humadi-et-al-2024>(2/2 | 240/264) Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks (Khaled Humadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaled Humadi, Gunes Karabulut Kurt, Halim Yanikomeroglu. (2024)<br><strong>Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks</strong><br><button class=copy-to-clipboard title="Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06024v1.pdf filename=2404.06024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed massive multiple-input multiple output (mMIMO) system for low earth orbit (LEO) satellite networks is introduced as a promising technique to provide broadband connectivity. Nevertheless, several challenges persist in implementing distributed mMIMO systems for LEO satellite networks. These challenges include providing scalable massive access implementation as the system complexity increases with network size. Another challenging issue is the asynchronous arrival of signals at the user terminals due to the different propagation delays among distributed antennas in space, which destroys the coherent transmission, and consequently degrades the system performance. In this paper, we propose a scalable distributed mMIMO system for LEO satellite networks based on dynamic user-centric <b>clustering.</b> Aiming to obtain scalable implementation, new algorithms for initial cooperative access, cluster selection, and cluster handover are provided. In addition, phase shift-aware precoding is implemented to compensate for the propagation delay phase shifts. The performance of the proposed user-centric distributed mMIMO is compared with two baseline configurations: the non-cooperative transmission systems, where each user connects to only a single satellite, and the full-cooperative distributed mMIMO systems, where all satellites contribute serving each user. The numerical results show the potential of the proposed distributed mMIMO system to enhance system spectral efficiency when compared to noncooperative transmission systems. Additionally, it demonstrates the ability to minimize the serving cluster size for each user, thereby reducing the overall system complexity in comparison to the full-cooperative distributed mMIMO systems.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--241264-milgrams-experiment-in-the-knowledge-space-individual-navigation-strategies-manran-zhu-et-al-2024>(1/1 | 241/264) Milgram&rsquo;s experiment in the knowledge space: Individual navigation strategies (Manran Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manran Zhu, János Kertész. (2024)<br><strong>Milgram&rsquo;s experiment in the knowledge space: Individual navigation strategies</strong><br><button class=copy-to-clipboard title="Milgram's experiment in the knowledge space: Individual navigation strategies" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-IR, physics-soc-ph, physics.soc-ph<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06591v1.pdf filename=2404.06591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data deluge characteristic for our times has led to information overload, posing a significant challenge to effectively finding our way through the digital landscape. Addressing this issue requires an in-depth understanding of how we navigate through the abundance of information. Previous research has discovered multiple patterns in how individuals navigate in the geographic, social, and information spaces, yet individual differences in strategies for navigation in the knowledge space has remained largely unexplored. To bridge the gap, we conducted an online experiment where participants played a navigation game on Wikipedia and completed questionnaires about their personal information. Utilizing a <b>graph</b> <b>embedding</b> trained on the English Wikipedia, our study identified distinctive strategies that participants adopt: when the target is a famous person, participants typically use the geographical and occupational information of the target to navigate, reminiscent of hub-driven and proximity-driven approaches, respectively. We discovered that many participants playing the same game exhibit a &ldquo;wisdom of the crowd&rdquo; effect: The set of strategies provide a good estimate for the information landscape around the target indicating that the individual differences complement each other.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--242264-the-central-spanning-tree-problem-enrique-fita-sanmartín-et-al-2024>(1/1 | 242/264) The Central Spanning Tree Problem (Enrique Fita Sanmartín et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrique Fita Sanmartín, Christoph Schnörr, Fred A. Hamprecht. (2024)<br><strong>The Central Spanning Tree Problem</strong><br><button class=copy-to-clipboard title="The Central Spanning Tree Problem" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-CV, cs-DM, cs-DS, cs.DM, math-CO, math-OC<br>Keyword Score: 13<br>Keywords: Graph, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06447v1.pdf filename=2404.06447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spanning trees are an important primitive in many data analysis tasks, when a data set needs to be <b>summarized</b> in terms of its &ldquo;skeleton&rdquo;, or when a tree-shaped <b>graph</b> over all observations is required for downstream processing. Popular definitions of spanning trees include the minimum spanning tree and the optimum distance spanning tree, a.k.a. the minimum routing cost tree. When searching for the shortest spanning tree but admitting additional branching points, even shorter spanning trees can be realized: Steiner trees. Unfortunately, both minimum spanning and Steiner trees are not robust with respect to noise in the observations; that is, small perturbations of the original data set often lead to drastic changes in the associated spanning trees. In response, we make two contributions when the data lies in a Euclidean space: on the theoretical side, we introduce a new optimization problem, the &ldquo;(branched) central spanning tree&rdquo;, which subsumes all previously mentioned definitions as special cases. On the practical side, we show empirically that the (branched) central spanning tree is more robust to noise in the data, and as such is better suited to <b>summarize</b> a data set in terms of its skeleton. We also propose a heuristic to address the NP-hard optimization problem, and illustrate its use on single cell RNA expression data from biology and 3D point clouds of plants.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=13--243264-oracle-net-for-nonlinear-compressed-sensing-in-electrical-impedance-tomography-reconstruction-problems-damiana-lazzaro-et-al-2024>(1/3 | 243/264) Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems (Damiana Lazzaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Damiana Lazzaro, Serena Morigi, Luca Ratti. (2024)<br><strong>Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems</strong><br><button class=copy-to-clipboard title="Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65K10, 65N20, 68T07, 49K20, cs-NA, math-NA, math.NA<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06342v1.pdf filename=2404.06342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse recovery principles play an important role in solving many nonlinear ill-posed inverse problems. We investigate a variational framework with support Oracle for compressed sensing sparse reconstructions, where the available measurements are nonlinear and possibly corrupted by noise. A <b>graph</b> <b>neural</b> <b>network,</b> named Oracle-Net, is proposed to predict the support from the nonlinear measurements and is integrated into a regularized recovery model to enforce sparsity. The derived nonsmooth optimization problem is then efficiently solved through a constrained proximal gradient method. Error bounds on the approximate solution of the proposed Oracle-based optimization are provided in the context of the ill-posed Electrical Impedance Tomography problem. Numerical solutions of the EIT nonlinear inverse reconstruction problem confirm the potential of the proposed method which improves the reconstruction quality from undersampled measurements, under sparsity assumptions.</p></p class="citation"></blockquote><h3 id=23--244264-low-rank-generalized-alternating-direction-implicit-iteration-method-for-solving-matrix-equations-juan-zhang-et-al-2024>(2/3 | 244/264) Low-rank generalized alternating direction implicit iteration method for solving matrix equations (Juan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Zhang, Wenlu Xun. (2024)<br><strong>Low-rank generalized alternating direction implicit iteration method for solving matrix equations</strong><br><button class=copy-to-clipboard title="Low-rank generalized alternating direction implicit iteration method for solving matrix equations" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06034v1.pdf filename=2404.06034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an effective low-rank generalized alternating direction implicit iteration (R-GADI) method for solving large-scale sparse and stable Lyapunov matrix equations and <b>continuous-time</b> <b>algebraic</b> Riccati matrix equations. The method is based on generalized alternating direction implicit iteration (GADI), which exploits the low-rank property of matrices and utilizes the Cholesky factorization approach for solving. The advantage of the new algorithm lies in its direct and efficient low-rank formulation, which is a variant of the Cholesky decomposition in the Lyapunov GADI method, saving storage space and making it computationally effective. When solving the <b>continuous-time</b> <b>algebraic</b> Riccati matrix equation, the Riccati equation is first simplified to a Lyapunov equation using the Newton method, and then the R-GADI method is employed for computation. Additionally, we analyze the convergence of the R-GADI method and prove its consistency with the convergence of the GADI method. Finally, the effectiveness of the new algorithm is demonstrated through corresponding numerical experiments.</p></p class="citation"></blockquote><h3 id=33--245264-optimization-methods-for-solving-matrix-equations-juan-zhang-et-al-2024>(3/3 | 245/264) Optimization methods for solving matrix equations (Juan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Zhang, Xiao Luo. (2024)<br><strong>Optimization methods for solving matrix equations</strong><br><button class=copy-to-clipboard title="Optimization methods for solving matrix equations" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: Augmented Reality (AR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06030v1.pdf filename=2404.06030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we focus on using optimization methods to solve matrix equations by transforming the problem of solving the Sylvester matrix equation or continuous algebraic Riccati equation into an optimization problem. Initially, we use a constrained convex optimization method (CCOM) to solve the Sylvester matrix equation with $\ell_{2,1}$-norm, where we provide a convergence analysis and numerical examples of CCOM; however, the results show that the algorithm is not efficient. To address this issue, we employ classical quasi-Newton methods such as DFP and BFGS algorithms to solve the Sylvester matrix equation and present the convergence and numerical results of the algorithm. Additionally, we compare these algorithms with the CG algorithm and <b>AR</b> algorithm, and our results demonstrate that the presented algorithms are effective. Furthermore, we propose a unified framework of the alternating direction multiplier method (ADMM) for directly solving the continuous algebraic Riccati equation (CARE), and we provide the convergence and numerical results of ADMM. Our experimental results indicate that ADMM is an effective optimization algorithm for solving CARE. Finally, to improve the effectiveness of the optimization method for solving Riccati equation, we propose the Newton-ADMM algorithm framework, where the outer iteration of this method is the classical Newton method, and the inner iteration involves using ADMM to solve Lyapunov matrix equations inexactly. We also provide the convergence and numerical results of this algorithm, which our results demonstrate are more efficient than ADMM for solving CARE.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--246264-notnets-accelerating-microservices-by-bypassing-the-network-peter-alvaro-et-al-2024>(1/5 | 246/264) NotNets: Accelerating Microservices by Bypassing the Network (Peter Alvaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Alvaro, Matthew Adiletta, Adrian Cockroft, Frank Hady, Ramesh Illikkal, Esteban Ramos, James Tsai, Robert Soulé. (2024)<br><strong>NotNets: Accelerating Microservices by Bypassing the Network</strong><br><button class=copy-to-clipboard title="NotNets: Accelerating Microservices by Bypassing the Network" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06581v1.pdf filename=2404.06581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote procedure calls are the workhorse of distributed systems. However, as software engineering trends, such as micro-services and serverless computing, push applications towards ever finer-grained decompositions, the overhead of RPC-based communication is becoming too great to bear. In this paper, we argue that point solutions that attempt to optimize one aspect of RPC logic are unlikely to mitigate these ballooning communication costs. Rather, we need a dramatic reappraisal of how we provide communication. Towards this end, we propose to emulate <b>message-passing</b> RPCs by sharing message payloads and metadata on CXL 3.0-backed far memory. We provide initial evidence of feasibility and analyze the expected benefits.</p></p class="citation"></blockquote><h3 id=25--247264-communication-efficient-large-scale-distributed-deep-learning-a-comprehensive-survey-feng-liang-et-al-2024>(2/5 | 247/264) Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey (Feng Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Liang, Zhen Zhang, Haifeng Lu, Victor C. M. Leung, Yanyi Guo, Xiping Hu. (2024)<br><strong>Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06114v1.pdf filename=2404.06114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on <b>large-scale</b> <b>distributed</b> <b>deep</b> learning. In contrast to traditional distributed deep learning, the <b>large-scale</b> <b>scenario</b> <b>poses</b> new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a <b>large</b> <b>scale.</b> <b>This</b> article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in <b>large-scale</b> <b>distributed</b> <b>deep</b> learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of <b>large-scale</b> <b>distributed</b> <b>training.</b> Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a <b>large-scale</b> <b>and</b> <b>heterogeneous</b> setting. Finally, we conduct a case study on the distributed training of <b>large</b> <b>language</b> <b>models</b> at a <b>large</b> <b>scale</b> <b>to</b> illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of <b>large-scale</b> <b>distributed</b> <b>deep</b> learning and to reveal promising future research directions toward communication-efficient solutions in this scope.</p></p class="citation"></blockquote><h3 id=35--248264-a-systematic-literature-survey-of-sparse-matrix-vector-multiplication-jianhua-gao-et-al-2024>(3/5 | 248/264) A Systematic Literature Survey of Sparse Matrix-Vector Multiplication (Jianhua Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhua Gao, Bingjie Liu, Weixing Ji, Hua Huang. (2024)<br><strong>A Systematic Literature Survey of Sparse Matrix-Vector Multiplication</strong><br><button class=copy-to-clipboard title="A Systematic Literature Survey of Sparse Matrix-Vector Multiplication" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: 68-02, 68W10, 65F50, A-1; D-1-3; G-1-3, cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06047v1.pdf filename=2404.06047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse matrix-vector multiplication (SpMV) is a crucial computing kernel with widespread applications in iterative algorithms. Over the past decades, research on SpMV optimization has made remarkable strides, giving rise to various optimization contributions. However, the comprehensive and systematic literature survey that introduces, analyzes, discusses, and <b>summarizes</b> the advancements of SpMV in recent years is currently lacking. Aiming to fill this gap, this paper compares existing techniques and analyzes their strengths and weaknesses. We begin by highlighting two representative applications of SpMV, then conduct an in-depth overview of the important techniques that optimize SpMV on modern architectures, which we specifically classify as classic, auto-tuning, machine learning, and mixed-precision-based optimization. We also elaborate on the hardware-based architectures, including CPU, GPU, FPGA, processing in Memory, heterogeneous, and distributed platforms. We present a comprehensive experimental evaluation that compares the performance of state-of-the-art SpMV implementations. Based on our findings, we identify several challenges and point out future research directions. This survey is intended to provide researchers with a comprehensive understanding of SpMV optimization on modern architectures and provide guidance for future work.</p></p class="citation"></blockquote><h3 id=45--249264-a-comprehensive-benchmarking-analysis-of-fault-recovery-in-stream-processing-frameworks-adriano-vogel-et-al-2024>(4/5 | 249/264) A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks (Adriano Vogel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adriano Vogel, Sören Henning, Esteban Perez-Wohlfeil, Otmar Ertl, Rick Rabiser. (2024)<br><strong>A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks</strong><br><button class=copy-to-clipboard title="A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-SE, cs.DC<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06203v1.pdf filename=2404.06203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, several software systems rely on stream processing architectures to deliver scalable performance and handle large volumes of data in near real time. Stream processing frameworks facilitate scalable computing by distributing the application&rsquo;s execution across multiple machines. Despite performance being extensively studied, the measurement of fault tolerance-a key and most appealing feature offered by stream processing frameworks-has still not been measured properly with updated and comprehensive testbeds. Moreover, the impact that fault recovery can have on performance is mostly ignored. This paper provides a comprehensive analysis of fault recovery performance, stability, and recovery time in a cloud-native environment with modern open-source frameworks, namely Flink, Kafka Streams, and Spark Structured Streaming. Our <b>benchmarking</b> analysis is inspired by chaos engineering to inject failures. Generally, our results indicate that much has changed compared to previous studies on fault recovery in distributed stream processing. In particular, the results indicate that Flink can be the fastest and stablest under failures. Moreover, Kafka Streams shows performance instabilities after failures, which is due to its current repartitioning strategy that can be suboptimal in terms of load balancing. Spark Structured Streaming shows suitable fault recovery performance and stability, but with higher event latency. Our study intends to (i) help industry practitioners in choosing the most suitable stream processing framework for efficient and reliable executions of data-intensive applications; (ii) support researchers in applying and extending our research method as well as our <b>benchmark;</b> (iii) identify, prevent, and assist in solving potential issues in production deployments.</p></p class="citation"></blockquote><h3 id=55--250264-a-survey-of-distributed-graph-algorithms-on-massive-graphs-lingkai-meng-et-al-2024>(5/5 | 250/264) A Survey of Distributed Graph Algorithms on Massive Graphs (Lingkai Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng, Xue Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin, Jingren Zhou. (2024)<br><strong>A Survey of Distributed Graph Algorithms on Massive Graphs</strong><br><button class=copy-to-clipboard title="A Survey of Distributed Graph Algorithms on Massive Graphs" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06037v1.pdf filename=2404.06037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed processing of large-scale <b>graph</b> data has many practical applications and has been widely studied. In recent years, a lot of distributed <b>graph</b> processing frameworks and algorithms have been proposed. While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments. Applying <b>graph</b> tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth. In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed <b>graph</b> algorithms. We first conduct a systematic analysis of the inherent challenges in distributed <b>graph</b> processing, followed by presenting an overview of existing general solutions. Subsequently, we survey the challenges highlighted in recent distributed <b>graph</b> processing papers and the strategies adopted to address them. Finally, we discuss the current research trends and identify potential future opportunities.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--251264-geodirdock-guiding-docking-along-geodesic-paths-raúl-miñán-et-al-2024>(1/1 | 251/264) GeoDirDock: Guiding Docking Along Geodesic Paths (Raúl Miñán et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raúl Miñán, Javier Gallardo, Álvaro Ciudad, Alexis Molina. (2024)<br><strong>GeoDirDock: Guiding Docking Along Geodesic Paths</strong><br><button class=copy-to-clipboard title="GeoDirDock: Guiding Docking Along Geodesic Paths" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06481v1.pdf filename=2404.06481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces GeoDirDock (GDD), a novel approach to molecular docking that enhances the accuracy and physical plausibility of ligand docking predictions. GDD guides the denoising process of a <b>diffusion</b> <b>model</b> along geodesic paths within multiple spaces representing translational, rotational, and torsional degrees of freedom. Our method leverages expert knowledge to direct the generative modeling process, specifically targeting desired protein-ligand interaction regions. We demonstrate that GDD significantly outperforms existing blind docking methods in terms of RMSD accuracy and physicochemical pose realism. Our results indicate that incorporating domain expertise into the <b>diffusion</b> <b>process</b> leads to more biologically relevant docking predictions. Additionally, we explore the potential of GDD for lead optimization in drug discovery through angle transfer in maximal common substructure (MCS) docking, showcasing its capability to predict ligand orientations for chemically similar compounds accurately.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--252264-mechanised-hypersafety-proofs-about-structured-data-extended-version-vladimir-gladshtein-et-al-2024>(1/1 | 252/264) Mechanised Hypersafety Proofs about Structured Data: Extended Version (Vladimir Gladshtein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Gladshtein, Qiyuan Zhao, Willow Ahrens, Saman Amarasinghe, Ilya Sergey. (2024)<br><strong>Mechanised Hypersafety Proofs about Structured Data: Extended Version</strong><br><button class=copy-to-clipboard title="Mechanised Hypersafety Proofs about Structured Data: Extended Version" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-LO, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06477v1.pdf filename=2404.06477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Arrays are a fundamental abstraction to represent collections of data. It is often possible to exploit structural properties of the data stored in an array (e.g., repetition or sparsity) to develop a specialised representation optimised for space efficiency. Formally <b>reasoning</b> about correctness of manipulations with such structured data is challenging, as they are often composed of multiple loops with non-trivial invariants. In this work, we observe that specifications for structured data manipulations can be phrased as hypersafety properties, i.e., predicates that relate traces of $k$ programs. To turn this observation into an effective verification methodology, we developed the Logic for Graceful Tensor Manipulation (LGTM), a new Hoare-style relational separation logic for specifying and verifying computations over structured data. The key enabling idea of LGTM is that of parametrised hypersafety specifications that allow the number $k$ of the program components to depend on the program variables. We implemented LGTM as a foundational embedding into Coq, mechanising its rules, meta-theory, and the proof of soundness. Furthermore, we developed a library of domain-specific tactics that automate computer-aided hypersafety <b>reasoning,</b> resulting in pleasantly short proof scripts that enjoy a high degree of reuse. We argue for the effectiveness of relational <b>reasoning</b> about structured data in LGTM by specifying and mechanically proving correctness of 13 case studies including computations on compressed arrays and efficient operations over multiple kinds of sparse tensors.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--253264-the-power-in-communication-power-regularization-of-communication-for-autonomy-in-cooperative-multi-agent-reinforcement-learning-nancirose-piazza-et-al-2024>(1/1 | 253/264) The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning (Nancirose Piazza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nancirose Piazza, Vahid Behzadan, Stefan Sarkadi. (2024)<br><strong>The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06387v1.pdf filename=2404.06387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Communication plays a vital role for coordination in Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) systems. However, misaligned agents can exploit other agents&rsquo; trust and delegated power to the communication medium. In this paper, we propose power regularization as a method to limit the adverse effects of communication by misaligned agents, specifically communication which impairs the performance of cooperative agents. Power is a measure of the influence one agent&rsquo;s actions have over another agent&rsquo;s policy. By introducing power regularization, we aim to allow designers to control or reduce agents&rsquo; dependency on communication when appropriate, and make them more resilient to performance deterioration due to misuses of communication. We investigate several environments in which power regularization can be a valuable capability for learning different policies that reduce the effect of power dynamics between agents during communication.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--254264-towards-practical-meshlet-compression-bastian-kuth-et-al-2024>(1/1 | 254/264) Towards Practical Meshlet Compression (Bastian Kuth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bastian Kuth, Max Oberberger, Felix Kawala, Sander Reitter, Sebastian Michel, Matthäus Chajdas, Quirin Meyer. (2024)<br><strong>Towards Practical Meshlet Compression</strong><br><button class=copy-to-clipboard title="Towards Practical Meshlet Compression" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-3-5, cs-GR, cs.GR<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06359v1.pdf filename=2404.06359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a codec specifically designed for meshlet compression, optimized for rapid data-parallel GPU decompression within a mesh shader. Our compression strategy orders triangles in optimal generalized triangle strips (GTSs), which we generate by formulating the creation as a mixed integer linear program (MILP). Our method achieves index buffer compression rates of 16:1 compared to the vertex pipeline and crack-free vertex attribute <b>quantization</b> based on user preference. The 15.5 million triangles of our teaser image decompress and render in 0.59 ms on an AMD Radeon RX 7900 XTX.</p></p class="citation"></blockquote><h2 id=cslo-2>cs.LO (2)</h2><h3 id=12--255264-a-semantic-proof-of-generalised-cut-elimination-for-deep-inference-robert-atkey-et-al-2024>(1/2 | 255/264) A Semantic Proof of Generalised Cut Elimination for Deep Inference (Robert Atkey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Atkey, Wen Kokke. (2024)<br><strong>A Semantic Proof of Generalised Cut Elimination for Deep Inference</strong><br><button class=copy-to-clipboard title="A Semantic Proof of Generalised Cut Elimination for Deep Inference" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06233v1.pdf filename=2404.06233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiplicative-Additive System Virtual (MAV) is a logic that extends Multiplicative-Additive Linear Logic with a self-dual non-commutative operator expressing the concept of &ldquo;before&rdquo; or &ldquo;sequencing&rdquo;. MAV is also an extenson of the the logic Basic System Virtual (BV) with additives. Formulas in BV have an appealing reading as processes with parallel and sequential composition. MAV adds internal and external choice operators. BV and MAV are also closely related to Concurrent sKleene Algebras. Proof systems for MAV and BV are Deep Inference systems, which allow inference rules to be applied anywhere inside a structure. As with any proof system, a key question is whether proofs in MAV can be reduced to a normal form, removing detours and the introduction of structures not present in the original goal. In Sequent Calcluli systems, this property is referred to as Cut Elimination. Deep Inference systems have an analogous Cut rule and other rules that are not present in normalised proofs. Cut Elimination for Deep Inference systems has the same metatheoretic benefits as for Sequent Calculi systems, including consistency and decidability. Proofs of Cut Elimination for BV, MAV, and other Deep Inference systems present in the literature have relied on intrincate syntactic <b>reasoning</b> and complex termination measures. We present a concise semantic proof that all MAV proofs can be reduced to a normal form avoiding the Cut rule and other &ldquo;non analytic&rdquo; rules. We also develop soundness and completeness proofs of MAV (and BV) with respect to a class of models. We have mechanised all our proofs in the Agda proof assistant, which provides both assurance of their correctness as well as yielding an executable normalisation procedure.</p></p class="citation"></blockquote><h3 id=22--256264-syndicate-synergistic-synthesis-of-ranking-function-and-invariants-for-termination-analysis-yasmin-sarita-et-al-2024>(2/2 | 256/264) Syndicate: Synergistic Synthesis of Ranking Function and Invariants for Termination Analysis (Yasmin Sarita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasmin Sarita, Avaljot Singh, Shaurya Gomber, Gagandeep Singh, Mahesh Vishwanathan. (2024)<br><strong>Syndicate: Synergistic Synthesis of Ranking Function and Invariants for Termination Analysis</strong><br><button class=copy-to-clipboard title="Syndicate: Synergistic Synthesis of Ranking Function and Invariants for Termination Analysis" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05951v1.pdf filename=2404.05951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several techniques have been developed to prove the termination of programs. Finding ranking functions is one of the common approaches to do so. A ranking function must be bounded and must reduce at every iteration for all the reachable program states. Since the set of reachable states is often unknown, invariants serve as an over-approximation. Further, in the case of nested loops, the initial set of program states for the nested loop can be determined by the invariant of the outer loop. So, invariants play an important role in proving the validity of a ranking function in the absence of the exact reachable states. However, in the existing techniques, either the invariants are synthesized independently, or combined with ranking function synthesis into a single query, both of which are inefficient. We observe that a guided search for invariants and ranking functions can have benefits in terms of the number of programs that can be proved to terminate and the time needed to identify a proof of termination. So, in this work, we develop Syndicate, a novel framework that synergistically guides the search for both the ranking function and an invariant that together constitute a proof of termination. Owing to our synergistic approach, Syndicate can not only prove the termination of more <b>benchmarks</b> but also achieves a reduction ranging from 17% to 70% in the average runtime as compared to existing state-of-the-art termination analysis tools. We also prove that Syndicate is relatively complete, i.e., if there exists a ranking function and an invariant in their respective templates that can be used to prove the termination of a program, then Syndicate will always find it if there exist complete procedures for the template-specific functions in our framework.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--257264-advancements-in-radiomics-and-artificial-intelligence-for-thyroid-cancer-diagnosis-milad-yousefi-et-al-2024>(1/1 | 257/264) Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis (Milad Yousefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milad Yousefi, Shadi Farabi Maleki, Ali Jafarizadeh, Mahya Ahmadpour Youshanlui, Aida Jafari, Siamak Pedrammehr, Roohallah Alizadehsani, Ryszard Tadeusiewicz, Pawel Plawiak. (2024)<br><strong>Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis</strong><br><button class=copy-to-clipboard title="Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: J-3-2; J-3-3, cs-AI, eess-IV, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07239v1.pdf filename=2404.07239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thyroid cancer is an increasing global health concern that requires advanced diagnostic methods. The application of AI and radiomics to thyroid cancer diagnosis is examined in this review. A review of multiple databases was conducted in compliance with PRISMA guidelines until October 2023. A combination of keywords led to the discovery of an English academic publication on thyroid cancer and related subjects. 267 papers were returned from the original search after 109 duplicates were removed. Relevant studies were selected according to predetermined criteria after 124 articles were eliminated based on an examination of their abstract and title. After the comprehensive analysis, an additional six studies were excluded. Among the 28 included studies, radiomics analysis, which incorporates ultrasound (US) images, demonstrated its effectiveness in diagnosing thyroid cancer. Various results were noted, some of the studies presenting new strategies that outperformed the status quo. The literature has emphasized various challenges faced by AI models, including interpretability issues, dataset constraints, and operator dependence. The synthesized findings of the 28 included studies mentioned the need for standardization efforts and prospective multicenter studies to address these concerns. Furthermore, approaches to overcome these obstacles were identified, such as advances in <b>explainable</b> <b>AI</b> technology and personalized medicine techniques. The review focuses on how AI and radiomics could transform the diagnosis and treatment of thyroid cancer. Despite challenges, future research on multidisciplinary cooperation, clinical applicability validation, and algorithm improvement holds the potential to improve patient outcomes and diagnostic precision in the treatment of thyroid cancer.</p></p class="citation"></blockquote><h2 id=mathct-1>math.CT (1)</h2><h3 id=11--258264-monoidal-context-theory-mario-román-2024>(1/1 | 258/264) Monoidal Context Theory (Mario Román, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mario Román. (2024)<br><strong>Monoidal Context Theory</strong><br><button class=copy-to-clipboard title="Monoidal Context Theory" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CT<br>Categories: 18M99, cs-LO, math-CT, math.CT<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06192v1.pdf filename=2404.06192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We universally characterize the produoidal category of monoidal lenses over a monoidal category. In the same way that each category induces a cofree promonoidal category of spliced arrows, each monoidal category induces a cofree produoidal category of monoidal spliced arrows; monoidal lenses are the free normalization of the cofree produoidal category of monoidal spliced arrows. We apply the characterization of symmetric monoidal lenses to the analysis of multi-party <b>message-passing</b> protocols. We introduce a minimalistic axiomatization of message passing &ndash; message theories &ndash; and we construct combinatorially the free message theory over a set. Symmetric monoidal lenses are the derivations of the free message theory over a symmetric monoidal category.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--259264-the-turnpike-property-for-high-dimensional-interacting-agent-systems-in-discrete-time-martin-gugat-et-al-2024>(1/1 | 259/264) The turnpike property for high-dimensional interacting agent systems in discrete time (Martin Gugat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Gugat, Michael Herty, Jiehong Liu, Chiara Segala. (2024)<br><strong>The turnpike property for high-dimensional interacting agent systems in discrete time</strong><br><button class=copy-to-clipboard title="The turnpike property for high-dimensional interacting agent systems in discrete time" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-NA, math-NA, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06134v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06134v2.pdf filename=2404.06134v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the interior turnpike phenomenon for <b>discrete-time</b> <b>multi-agent</b> optimal control problems. While for continuous systems the turnpike property has been established, we focus here on first-order discretizations of such systems. It is shown that the resulting time-discrete system inherits the turnpike property with estimates of the same type as in the continuous case. In particular, we prove that the <b>discrete</b> <b>time</b> optimal control problem is strictly dissipative and the cheap control assumption holds.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--260264-deep-learning-method-for-computing-committor-functions-with-adaptive-sampling-bo-lin-et-al-2024>(1/1 | 260/264) Deep Learning Method for Computing Committor Functions with Adaptive Sampling (Bo Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Lin, Weiqing Ren. (2024)<br><strong>Deep Learning Method for Computing Committor Functions with Adaptive Sampling</strong><br><button class=copy-to-clipboard title="Deep Learning Method for Computing Committor Functions with Adaptive Sampling" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-LG, cs-NA, math-NA, physics-comp-ph, physics.comp-ph<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06206v1.pdf filename=2404.06206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The committor function is a central object for quantifying the transitions between metastable states of dynamical systems. Recently, a number of computational methods based on <b>deep</b> <b>neural</b> <b>networks</b> have been developed for computing the high-dimensional committor function. The success of the methods relies on sampling adequate data for the transition, which still is a challenging task for complex systems at low temperatures. In this work, we propose a <b>deep</b> <b>learning</b> <b>method</b> with two novel adaptive sampling schemes (I and II). In the two schemes, the data are generated actively with a modified potential where the bias potential is constructed from the learned committor function. We theoretically demonstrate the advantages of the sampling schemes and show that the data in sampling scheme II are uniformly distributed along the transition tube. This makes a promising method for studying the transition of complex systems. The efficiency of the method is illustrated in high-dimensional systems including the alanine dipeptide and a solvated dimer system.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--261264-extremal-minimal-bipartite-matching-covered-graphs-amit-kumar-mallik-et-al-2024>(1/1 | 261/264) Extremal minimal bipartite matching covered graphs (Amit Kumar Mallik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Kumar Mallik, Ajit A. Diwan, Nishad Kothari. (2024)<br><strong>Extremal minimal bipartite matching covered graphs</strong><br><button class=copy-to-clipboard title="Extremal minimal bipartite matching covered graphs" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06445v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06445v2.pdf filename=2404.06445v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A connected <b>graph,</b> on four or more vertices, is matching covered if every edge is present in some perfect matching. An ear decomposition theorem (similar to the one for $2$-connected <b>graphs)</b> exists for bipartite matching covered <b>graphs</b> due to Hetyei. From the results and proofs of Lov'asz and Plummer, that rely on Hetyei&rsquo;s theorem, one may deduce that any minimal bipartite matching covered <b>graph</b> has at least $2(m-n+2)$ vertices of degree two (where minimal means that deleting any edge results in a <b>graph</b> that is not matching covered); such a <b>graph</b> is said to be extremal if it attains the stated lower bound. In this paper, we provide a complete characterization of the class of extremal minimal bipartite matching covered <b>graphs.</b> In particular, we prove that every such <b>graph</b> $G$ is obtained from two copies of a tree devoid of degree two vertices, say $T$ and $T&rsquo;$, by adding edges &ndash; each of which joins a leaf of $T$ with the corresponding leaf of $T&rsquo;$. Apart from the aforementioned bound, there are four other bounds that appear in, or may be deduced from, the work of Lov'asz and Plummer. Each of these bounds leads to a notion of extremality. In this paper, we obtain a complete characterization of all of these extremal classes and also establish relationships between them. Two of our characterizations are in the same spirit as the one stated above. For the remaining two extremal classes, we reduce each of them to one of the already characterized extremal classes using standard matching theoretic operations.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--262264-characterizations-of-sparsifiability-for-affine-csps-and-symmetric-csps-sanjeev-khanna-et-al-2024>(1/3 | 262/264) Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs (Sanjeev Khanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjeev Khanna, Aaron L. Putterman, Madhu Sudan. (2024)<br><strong>Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs</strong><br><button class=copy-to-clipboard title="Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06327v1.pdf filename=2404.06327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CSP sparsification, introduced by Kogan and Krauthgamer (ITCS 2015), considers the following question: when can an instance of a constraint satisfaction problem be sparsified (by retaining a weighted subset of the constraints) while still roughly capturing the weight of constraints satisfied by {\em every} assignment. CSP sparsification generalizes and abstracts other commonly studied problems including <b>graph</b> cut-sparsification, hypergraph cut-sparsification and hypergraph XOR-sparsification. A central question here is to understand what properties of a constraint predicate $P:\Sigma^r \to {0,1}$ (where variables are assigned values in $\Sigma$) allow for nearly linear-size sparsifiers (in the number of variables). In this work (1) we significantly extend the class of CSPs for which nearly linear-size, and other non-trivial, sparsifications exist and give classifications in some broad settings and (2) give a polynomial-time algorithm to extract this sparsification. Our results captured in item (1) completely classify all symmetric Boolean predicates $P$ (i.e., on the Boolean domain $\Sigma = {0,1}$) that allow nearly-linear-size sparsifications. Symmetric Boolean CSPs already capture all the special classes of sparisifcation listed above including hypergraph cut-sparsification and variants. Our study of symmetric CSPs reveals an inherent, previously undetected, number-theoretic phenomenon that determines near-linear size sparsifiability. We also completely classify the set of Boolean predicates $P$ that allow non-trivial ($o(n^r)$-size) sparsifications, thus answering an open question from the work of Kogan and Krauthgamer.</p></p class="citation"></blockquote><h3 id=23--263264-fully-dynamic-matching-and-ordered-ruzsa-szemerédi-graphs-soheil-behnezhad-et-al-2024>(2/3 | 263/264) Fully Dynamic Matching and Ordered Ruzsa-Szemerédi Graphs (Soheil Behnezhad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soheil Behnezhad, Alma Ghafari. (2024)<br><strong>Fully Dynamic Matching and Ordered Ruzsa-Szemerédi Graphs</strong><br><button class=copy-to-clipboard title="Fully Dynamic Matching and Ordered Ruzsa-Szemerédi Graphs" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.06069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.06069v1.pdf filename=2404.06069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the fully dynamic maximum matching problem. In this problem, the goal is to efficiently maintain an approximate maximum matching of a <b>graph</b> that is subject to edge insertions and deletions. Our focus is particularly on algorithms that maintain the edges of a $(1-\epsilon)$-approximate maximum matching for an arbitrarily small constant $\epsilon > 0$. Until recently, the fastest known algorithm for this problem required $\Theta(n)$ time per update where $n$ is the number of vertices. This bound was slightly improved to $n/(\log^* n)^{\Omega(1)}$ by Assadi, Behnezhad, Khanna, and Li [STOC'23] and very recently to $n/2^{\Omega(\sqrt{\log n})}$ by Liu [ArXiv'24]. Whether this can be improved to $n^{1-\Omega(1)}$ remains a major open problem. In this paper, we present a new algorithm that maintains a $(1-\epsilon)$-approximate maximum matching. The update-time of our algorithm is parametrized based on the density of a certain class of <b>graphs</b> that we call Ordered Ruzsa-Szemer'edi (ORS) <b>graphs,</b> a generalization of the well-known Ruzsa-Szemer'edi <b>graphs.</b> While determining the density of ORS (or RS) remains a hard problem in combinatorics, we prove that if the existing constructions of ORS <b>graphs</b> are optimal, then our algorithm runs in $n^{1/2+O(\epsilon)}$ time for any fixed $\epsilon > 0$ which would be significantly faster than existing near-linear in $n$ time algorithms. Our second main contribution is a better upper bound on density of both ORS and RS <b>graphs</b> with linear size matchings. The previous best upper bound was due to a proof of the triangle-removal lemma from more than a decade ago due to Fox [Annals of Mathematics &lsquo;11].</p></p class="citation"></blockquote><h3 id=33--264264-polynomial-time-derivation-of-optimal-k-tree-topology-from-markov-networks-fereshteh-r-dastjerdi-et-al-2024>(3/3 | 264/264) Polynomial-time derivation of optimal k-tree topology from Markov networks (Fereshteh R. Dastjerdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fereshteh R. Dastjerdi, Liming Cai. (2024)<br><strong>Polynomial-time derivation of optimal k-tree topology from Markov networks</strong><br><button class=copy-to-clipboard title="Polynomial-time derivation of optimal k-tree topology from Markov networks" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05991v1.pdf filename=2404.05991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Characterization of joint probability distribution for large networks of random variables remains a challenging task in data science. Probabilistic <b>graph</b> approximation with simple topologies has practically been resorted to; typically the tree topology makes joint probability computation much simpler and can be effective for statistical inference on insufficient data. However, to characterize network components where multiple variables cooperate closely to influence others, model topologies beyond a tree are needed, which unfortunately are infeasible to acquire. In particular, our previous work has related optimal approximation of Markov networks of tree-width k >=2 closely to the <b>graph-theoretic</b> problem of finding maximum spanning k-tree (MSkT), which is a provably intractable task. This paper investigates optimal approximation of Markov networks with k-tree topology that retains some designated underlying subgraph. Such a subgraph may encode certain background information that arises in scientific applications, for example, about a known significant pathway in gene networks or the indispensable backbone connectivity in the residue interaction <b>graphs</b> for a biomolecule 3D structure. In particular, it is proved that the \beta-retaining MSkT problem, for a number of classes \beta of <b>graphs,</b> admit O(n^{k+1})-time algorithms for every fixed k>= 1. These \beta-retaining MSkT algorithms offer efficient solutions for approximation of Markov networks with k-tree topology in the situation where certain persistent information needs to be retained.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.10</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.12</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscr-10>cs.CR (10)</a><ul><li><a href=#110--1264-sandwich-attack-multi-language-mixture-adaptive-attack-on-llms-bibek-upadhayay-et-al-2024>(1/10 | 1/264) Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs (Bibek Upadhayay et al., 2024)</a></li><li><a href=#210--2264-flex-flexible-federated-learning-framework-francisco-herrera-et-al-2024>(2/10 | 2/264) FLEX: FLEXible Federated Learning Framework (Francisco Herrera et al., 2024)</a></li><li><a href=#310--3264-boosting-digital-safeguards-blending-cryptography-and-steganography-anamitra-maiti-et-al-2024>(3/10 | 3/264) Boosting Digital Safeguards: Blending Cryptography and Steganography (Anamitra Maiti et al., 2024)</a></li><li><a href=#410--4264-deep-learning-based-out-of-distribution-source-code-data-identification-how-far-we-have-gone-van-nguyen-et-al-2024>(4/10 | 4/264) Deep Learning-Based Out-of-distribution Source Code Data Identification: How Far We Have Gone? (Van Nguyen et al., 2024)</a></li><li><a href=#510--5264-current-affairs-a-measurement-study-of-deployment-and-security-trends-in-ev-charging-infrastructure-marcell-szakály-et-al-2024>(5/10 | 5/264) Current Affairs: A Measurement Study of Deployment and Security Trends in EV Charging Infrastructure (Marcell Szakály et al., 2024)</a></li><li><a href=#610--6264-software-based-security-framework-for-edge-and-mobile-iot-josé-cecílio-et-al-2024>(6/10 | 6/264) Software-based Security Framework for Edge and Mobile IoT (José Cecílio et al., 2024)</a></li><li><a href=#710--7264-towards-robust-domain-generation-algorithm-classification-arthur-drichel-et-al-2024>(7/10 | 7/264) Towards Robust Domain Generation Algorithm Classification (Arthur Drichel et al., 2024)</a></li><li><a href=#810--8264-privacy-preserving-scanpath-comparison-for-pervasive-eye-tracking-suleyman-ozdel-et-al-2024>(8/10 | 8/264) Privacy-preserving Scanpath Comparison for Pervasive Eye Tracking (Suleyman Ozdel et al., 2024)</a></li><li><a href=#910--9264-s-box-security-analysis-of-nist-lightweight-cryptography-candidates-a-critical-empirical-study-mahnoor-naseer-et-al-2024>(9/10 | 9/264) S-box Security Analysis of NIST Lightweight Cryptography Candidates: A Critical Empirical Study (Mahnoor Naseer et al., 2024)</a></li><li><a href=#1010--10264-is-your-ai-truly-yours-leveraging-blockchain-for-copyrights-provenance-and-lineage-yilin-sai-et-al-2024>(10/10 | 10/264) Is Your AI Truly Yours? Leveraging Blockchain for Copyrights, Provenance, and Lineage (Yilin Sai et al., 2024)</a></li></ul></li><li><a href=#cscl-42>cs.CL (42)</a><ul><li><a href=#142--11264-characterizing-multimodal-long-form-summarization-a-case-study-on-financial-reports-tianyu-cao-et-al-2024>(1/42 | 11/264) Characterizing Multimodal Long-form Summarization: A Case Study on Financial Reports (Tianyu Cao et al., 2024)</a></li><li><a href=#242--12264-llms-reading-comprehension-is-affected-by-parametric-knowledge-and-struggles-with-hypothetical-statements-victoria-basmov-et-al-2024>(2/42 | 12/264) LLMs&rsquo; Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements (Victoria Basmov et al., 2024)</a></li><li><a href=#342--13264-rar-b-reasoning-as-retrieval-benchmark-chenghao-xiao-et-al-2024>(3/42 | 13/264) RAR-b: Reasoning as Retrieval Benchmark (Chenghao Xiao et al., 2024)</a></li><li><a href=#442--14264-llm2vec-large-language-models-are-secretly-powerful-text-encoders-parishad-behnamghader-et-al-2024>(4/42 | 14/264) LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders (Parishad BehnamGhader et al., 2024)</a></li><li><a href=#542--15264-privacy-preserving-prompt-engineering-a-survey-kennedy-edemacu-et-al-2024>(5/42 | 15/264) Privacy Preserving Prompt Engineering: A Survey (Kennedy Edemacu et al., 2024)</a></li><li><a href=#642--16264-text-based-reasoning-about-vector-graphics-zhenhailong-wang-et-al-2024>(6/42 | 16/264) Text-Based Reasoning About Vector Graphics (Zhenhailong Wang et al., 2024)</a></li><li><a href=#742--17264-visualwebbench-how-far-have-multimodal-llms-evolved-in-web-page-understanding-and-grounding-junpeng-liu-et-al-2024>(7/42 | 17/264) VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding? (Junpeng Liu et al., 2024)</a></li><li><a href=#842--18264-optimization-methods-for-personalizing-large-language-models-through-retrieval-augmentation-alireza-salemi-et-al-2024>(8/42 | 18/264) Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation (Alireza Salemi et al., 2024)</a></li><li><a href=#942--19264-less-is-more-for-improving-automatic-evaluation-of-factual-consistency-tong-wang-et-al-2024>(9/42 | 19/264) Less is More for Improving Automatic Evaluation of Factual Consistency (Tong Wang et al., 2024)</a></li><li><a href=#1042--20264-all-in-one-an-empirical-study-of-gpt-for-few-shot-aspect-based-sentiment-anlaysis-baoxing-jiang-2024>(10/42 | 20/264) All in One: An Empirical Study of GPT for Few-Shot Aspect-Based Sentiment Anlaysis (Baoxing Jiang, 2024)</a></li><li><a href=#1142--21264-latent-distance-guided-alignment-training-for-large-language-models-haotian-luo-et-al-2024>(11/42 | 21/264) Latent Distance Guided Alignment Training for Large Language Models (Haotian Luo et al., 2024)</a></li><li><a href=#1242--22264-cendol-open-instruction-tuned-generative-large-language-models-for-indonesian-languages-samuel-cahyawijaya-et-al-2024>(12/42 | 22/264) Cendol: Open Instruction-tuned Generative Large Language Models for Indonesian Languages (Samuel Cahyawijaya et al., 2024)</a></li><li><a href=#1342--23264-not-understanding-latin-poetic-style-with-deep-learning-ben-nagy-2024>(13/42 | 23/264) (Not) Understanding Latin Poetic Style with Deep Learning (Ben Nagy, 2024)</a></li><li><a href=#1442--24264-khayyam-challenge-persianmmlu-is-your-llm-truly-wise-to-the-persian-language-omid-ghahroodi-et-al-2024>(14/42 | 24/264) Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language? (Omid Ghahroodi et al., 2024)</a></li><li><a href=#1542--25264-ada-leval-evaluating-long-context-llms-with-length-adaptable-benchmarks-chonghua-wang-et-al-2024>(15/42 | 25/264) Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks (Chonghua Wang et al., 2024)</a></li><li><a href=#1642--26264-take-a-look-at-it-rethinking-how-to-evaluate-language-model-jailbreak-hongyu-cai-et-al-2024>(16/42 | 26/264) Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak (Hongyu Cai et al., 2024)</a></li><li><a href=#1742--27264-clinlinker-medical-entity-linking-of-clinical-concept-mentions-in-spanish-fernando-gallego-et-al-2024>(17/42 | 27/264) ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish (Fernando Gallego et al., 2024)</a></li><li><a href=#1842--28264-minicpm-unveiling-the-potential-of-small-language-models-with-scalable-training-strategies-shengding-hu-et-al-2024>(18/42 | 28/264) MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies (Shengding Hu et al., 2024)</a></li><li><a href=#1942--29264-low-cost-generation-and-evaluation-of-dictionary-example-sentences-bill-cai-et-al-2024>(19/42 | 29/264) Low-Cost Generation and Evaluation of Dictionary Example Sentences (Bill Cai et al., 2024)</a></li><li><a href=#2042--30264-vi-ood-a-unified-representation-learning-framework-for-textual-out-of-distribution-detection-li-ming-zhan-et-al-2024>(20/42 | 30/264) VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection (Li-Ming Zhan et al., 2024)</a></li><li><a href=#2142--31264-identifying-shopping-intent-in-product-qa-for-proactive-recommendations-besnik-fetahu-et-al-2024>(21/42 | 31/264) Identifying Shopping Intent in Product QA for Proactive Recommendations (Besnik Fetahu et al., 2024)</a></li><li><a href=#2242--32264-freeeval-a-modular-framework-for-trustworthy-and-efficient-evaluation-of-large-language-models-zhuohao-yu-et-al-2024>(22/42 | 32/264) FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models (Zhuohao Yu et al., 2024)</a></li><li><a href=#2342--33264-event-enhanced-retrieval-in-real-time-search-yanan-zhang-et-al-2024>(23/42 | 33/264) Event-enhanced Retrieval in Real-time Search (Yanan Zhang et al., 2024)</a></li><li><a href=#2442--34264-interplay-of-machine-translation-diacritics-and-diacritization-wei-rui-chen-et-al-2024>(24/42 | 34/264) Interplay of Machine Translation, Diacritics, and Diacritization (Wei-Rui Chen et al., 2024)</a></li><li><a href=#2542--35264-what-is-your-favorite-gender-mlm-gender-bias-evaluation-in-multilingual-masked-language-models-jeongrok-yu-et-al-2024>(25/42 | 35/264) What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models (Jeongrok Yu et al., 2024)</a></li><li><a href=#2642--36264-comparing-two-model-designs-for-clinical-note-generation-is-an-llm-a-useful-evaluator-of-consistency-nathan-brake-et-al-2024>(26/42 | 36/264) Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency? (Nathan Brake et al., 2024)</a></li><li><a href=#2742--37264-pitfalls-of-conversational-llms-on-news-debiasing-ipek-baris-schlicht-et-al-2024>(27/42 | 37/264) Pitfalls of Conversational LLMs on News Debiasing (Ipek Baris Schlicht et al., 2024)</a></li><li><a href=#2842--38264-clue-instruct-text-based-clue-generation-for-educational-crossword-puzzles-andrea-zugarini-et-al-2024>(28/42 | 38/264) Clue-Instruct: Text-Based Clue Generation for Educational Crossword Puzzles (Andrea Zugarini et al., 2024)</a></li><li><a href=#2942--39264-smurfcat-at-semeval-2024-task-6-leveraging-synthetic-data-for-hallucination-detection-elisei-rykov-et-al-2024>(29/42 | 39/264) SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection (Elisei Rykov et al., 2024)</a></li><li><a href=#3042--40264-making-old-kurdish-publications-processable-by-augmenting-available-optical-character-recognition-engines-blnd-yaseen-et-al-2024>(30/42 | 40/264) Making Old Kurdish Publications Processable by Augmenting Available Optical Character Recognition Engines (Blnd Yaseen et al., 2024)</a></li><li><a href=#3142--41264-call-for-papers-the-2nd-babylm-challenge-sample-efficient-pretraining-on-a-developmentally-plausible-corpus-leshem-choshen-et-al-2024>(31/42 | 41/264) [Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus (Leshem Choshen et al., 2024)</a></li><li><a href=#3242--42264-ruler-whats-the-real-context-size-of-your-long-context-language-models-cheng-ping-hsieh-et-al-2024>(32/42 | 42/264) RULER: What&rsquo;s the Real Context Size of Your Long-Context Language Models? (Cheng-Ping Hsieh et al., 2024)</a></li><li><a href=#3342--43264-event-extraction-in-basque-typologically-motivated-cross-lingual-transfer-learning-analysis-mikel-zubillaga-et-al-2024>(33/42 | 43/264) Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis (Mikel Zubillaga et al., 2024)</a></li><li><a href=#3442--44264-surveyagent-a-conversational-system-for-personalized-and-efficient-research-survey-xintao-wang-et-al-2024>(34/42 | 44/264) SurveyAgent: A Conversational System for Personalized and Efficient Research Survey (Xintao Wang et al., 2024)</a></li><li><a href=#3542--45264-nemo-dataset-of-emotional-speech-in-polish-iwona-christop-2024>(35/42 | 45/264) nEMO: Dataset of Emotional Speech in Polish (Iwona Christop, 2024)</a></li><li><a href=#3642--46264-thoughtsculpt-reasoning-with-intermediate-revision-and-search-yizhou-chi-et-al-2024>(36/42 | 46/264) THOUGHTSCULPT: Reasoning with Intermediate Revision and Search (Yizhou Chi et al., 2024)</a></li><li><a href=#3742--47264-exploring-the-necessity-of-visual-modality-in-multimodal-machine-translation-using-authentic-datasets-zi-long-et-al-2024>(37/42 | 47/264) Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets (Zi Long et al., 2024)</a></li><li><a href=#3842--48264-fairpair-a-robust-evaluation-of-biases-in-language-models-through-paired-perturbations-jane-dwivedi-yu-et-al-2024>(38/42 | 48/264) FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations (Jane Dwivedi-Yu et al., 2024)</a></li><li><a href=#3942--49264-generalizable-sarcasm-detection-is-just-around-the-corner-of-course-hyewon-jang-et-al-2024>(39/42 | 49/264) Generalizable Sarcasm Detection Is Just Around The Corner, Of Course! (Hyewon Jang et al., 2024)</a></li><li><a href=#4042--50264-finding-fake-reviews-in-e-commerce-platforms-by-using-hybrid-algorithms-mathivanan-periasamy-et-al-2024>(40/42 | 50/264) Finding fake reviews in e-commerce platforms by using hybrid algorithms (Mathivanan Periasamy et al., 2024)</a></li><li><a href=#4142--51264-detection-of-fields-of-applications-in-biomedical-abstracts-with-the-support-of-argumentation-elements-mariana-neves-2024>(41/42 | 51/264) Detection of fields of applications in biomedical abstracts with the support of argumentation elements (Mariana Neves, 2024)</a></li><li><a href=#4242--52264-leveraging-interesting-facts-to-enhance-user-engagement-with-conversational-interfaces-nikhita-vedula-et-al-2024>(42/42 | 52/264) Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces (Nikhita Vedula et al., 2024)</a></li></ul></li><li><a href=#cscv-70>cs.CV (70)</a><ul><li><a href=#170--53264-playing-to-vision-foundation-models-strengths-in-stereo-matching-chuang-wei-liu-et-al-2024>(1/70 | 53/264) Playing to Vision Foundation Model&rsquo;s Strengths in Stereo Matching (Chuang-Wei Liu et al., 2024)</a></li><li><a href=#270--54264-actnetformer-transformer-resnet-hybrid-method-for-semi-supervised-action-recognition-in-videos-sharana-dharshikgan-suresh-dass-et-al-2024>(2/70 | 54/264) ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos (Sharana Dharshikgan Suresh Dass et al., 2024)</a></li><li><a href=#370--55264-anchor-based-robust-finetuning-of-vision-language-models-jinwei-han-et-al-2024>(3/70 | 55/264) Anchor-based Robust Finetuning of Vision-Language Models (Jinwei Han et al., 2024)</a></li><li><a href=#470--56264-internlm-xcomposer2-4khd-a-pioneering-large-vision-language-model-handling-resolutions-from-336-pixels-to-4k-hd-xiaoyi-dong-et-al-2024>(4/70 | 56/264) InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD (Xiaoyi Dong et al., 2024)</a></li><li><a href=#570--57264-from-barlow-twins-to-triplet-training-differentiating-dementia-with-limited-data-yitong-li-et-al-2024>(5/70 | 57/264) From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data (Yitong Li et al., 2024)</a></li><li><a href=#670--58264-omnifusion-technical-report-elizaveta-goncharova-et-al-2024>(6/70 | 58/264) OmniFusion Technical Report (Elizaveta Goncharova et al., 2024)</a></li><li><a href=#770--59264-mambaad-exploring-state-space-models-for-multi-class-unsupervised-anomaly-detection-haoyang-he-et-al-2024>(7/70 | 59/264) MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection (Haoyang He et al., 2024)</a></li><li><a href=#870--60264-morevqa-exploring-modular-reasoning-models-for-video-question-answering-juhong-min-et-al-2024>(8/70 | 60/264) MoReVQA: Exploring Modular Reasoning Models for Video Question Answering (Juhong Min et al., 2024)</a></li><li><a href=#970--61264-can-feedback-enhance-semantic-grounding-in-large-vision-language-models-yuan-hong-liao-et-al-2024>(9/70 | 61/264) Can Feedback Enhance Semantic Grounding in Large Vision-Language Models? (Yuan-Hong Liao et al., 2024)</a></li><li><a href=#1070--62264-exploring-the-potential-of-large-foundation-models-for-open-vocabulary-hoi-detection-ting-lei-et-al-2024>(10/70 | 62/264) Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection (Ting Lei et al., 2024)</a></li><li><a href=#1170--63264-hfnerf-learning-human-biomechanic-features-with-neural-radiance-fields-arnab-dey-et-al-2024>(11/70 | 63/264) HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields (Arnab Dey et al., 2024)</a></li><li><a href=#1270--64264-greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs-zander-w-blasingame-et-al-2024>(12/70 | 64/264) Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs (Zander W. Blasingame et al., 2024)</a></li><li><a href=#1370--65264-calibrating-higher-order-statistics-for-few-shot-class-incremental-learning-with-pre-trained-vision-transformers-dipam-goswami-et-al-2024>(13/70 | 65/264) Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers (Dipam Goswami et al., 2024)</a></li><li><a href=#1470--66264-unified-multi-modal-diagnostic-framework-with-reconstruction-pre-training-and-heterogeneity-combat-tuning-yupei-zhang-et-al-2024>(14/70 | 66/264) Unified Multi-modal Diagnostic Framework with Reconstruction Pre-training and Heterogeneity-combat Tuning (Yupei Zhang et al., 2024)</a></li><li><a href=#1570--67264-vision2ui-a-real-world-dataset-with-layout-for-code-generation-from-ui-designs-yi-gui-et-al-2024>(15/70 | 67/264) VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs (Yi Gui et al., 2024)</a></li><li><a href=#1670--68264-test-time-adaptation-with-salip-a-cascade-of-sam-and-clip-for-zero-shot-medical-image-segmentation-sidra-aleem-et-al-2024>(16/70 | 68/264) Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation (Sidra Aleem et al., 2024)</a></li><li><a href=#1770--69264-learning-embeddings-with-centroid-triplet-loss-for-object-identification-in-robotic-grasping-anas-gouda-et-al-2024>(17/70 | 69/264) Learning Embeddings with Centroid Triplet Loss for Object Identification in Robotic Grasping (Anas Gouda et al., 2024)</a></li><li><a href=#1870--70264-label-efficient-3d-object-detection-for-road-side-units-minh-quan-dao-et-al-2024>(18/70 | 70/264) Label-Efficient 3D Object Detection For Road-Side Units (Minh-Quan Dao et al., 2024)</a></li><li><a href=#1970--71264-improving-facial-landmark-detection-accuracy-and-efficiency-with-knowledge-distillation-zong-wei-hong-et-al-2024>(19/70 | 71/264) Improving Facial Landmark Detection Accuracy and Efficiency with Knowledge Distillation (Zong-Wei Hong et al., 2024)</a></li><li><a href=#2070--72264-counterfactual-reasoning-for-multi-label-image-classification-via-patching-based-training-ming-kun-xie-et-al-2024>(20/70 | 72/264) Counterfactual Reasoning for Multi-Label Image Classification via Patching-Based Training (Ming-Kun Xie et al., 2024)</a></li><li><a href=#2170--73264-lipt-latency-aware-image-processing-transformer-junbo-qiao-et-al-2024>(21/70 | 73/264) LIPT: Latency-aware Image Processing Transformer (Junbo Qiao et al., 2024)</a></li><li><a href=#2270--74264-object-dynamics-modeling-with-hierarchical-point-cloud-based-representations-chanho-kim-et-al-2024>(22/70 | 74/264) Object Dynamics Modeling with Hierarchical Point Cloud-based Representations (Chanho Kim et al., 2024)</a></li><li><a href=#2370--75264-smartcontrol-enhancing-controlnet-for-handling-rough-visual-conditions-xiaoyu-liu-et-al-2024>(23/70 | 75/264) SmartControl: Enhancing ControlNet for Handling Rough Visual Conditions (Xiaoyu Liu et al., 2024)</a></li><li><a href=#2470--76264-robust-feature-knowledge-distillation-for-enhanced-performance-of-lightweight-crack-segmentation-models-zhaohui-chen-et-al-2024>(24/70 | 76/264) Robust feature knowledge distillation for enhanced performance of lightweight crack segmentation models (Zhaohui Chen et al., 2024)</a></li><li><a href=#2570--77264-diffharmony-latent-diffusion-model-meets-image-harmonization-pengfei-zhou-et-al-2024>(25/70 | 77/264) DiffHarmony: Latent Diffusion Model Meets Image Harmonization (Pengfei Zhou et al., 2024)</a></li><li><a href=#2670--78264-mansformer-efficient-transformer-of-mixed-attention-for-image-deblurring-and-beyond-pin-hung-kuo-et-al-2024>(26/70 | 78/264) Mansformer: Efficient Transformer of Mixed Attention for Image Deblurring and Beyond (Pin-Hung Kuo et al., 2024)</a></li><li><a href=#2770--79264-incremental-joint-learning-of-depth-pose-and-implicit-scene-representation-on-monocular-camera-in-large-scale-scenes-tianchen-deng-et-al-2024>(27/70 | 79/264) Incremental Joint Learning of Depth, Pose and Implicit Scene Representation on Monocular Camera in Large-scale Scenes (Tianchen Deng et al., 2024)</a></li><li><a href=#2870--80264-little-strokes-fell-great-oaks-boosting-the-hierarchical-features-for-multi-exposure-image-fusion-pan-mu-et-al-2024>(28/70 | 80/264) Little Strokes Fell Great Oaks: Boosting the Hierarchical Features for Multi-exposure Image Fusion (Pan Mu et al., 2024)</a></li><li><a href=#2970--81264-concept-attention-whitening-for-interpretable-skin-lesion-diagnosis-junlin-hou-et-al-2024>(29/70 | 81/264) Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis (Junlin Hou et al., 2024)</a></li><li><a href=#3070--82264-tackling-structural-hallucination-in-image-translation-with-local-diffusion-seunghoi-kim-et-al-2024>(30/70 | 82/264) Tackling Structural Hallucination in Image Translation with Local Diffusion (Seunghoi Kim et al., 2024)</a></li><li><a href=#3170--83264-easytrack-efficient-and-compact-one-stream-3d-point-clouds-tracker-baojie-fan-et-al-2024>(31/70 | 83/264) EasyTrack: Efficient and Compact One-stream 3D Point Clouds Tracker (Baojie Fan et al., 2024)</a></li><li><a href=#3270--84264-audio-visual-generalized-zero-shot-learning-using-pre-trained-large-multi-modal-models-david-kurzendörfer-et-al-2024>(32/70 | 84/264) Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models (David Kurzendörfer et al., 2024)</a></li><li><a href=#3370--85264-magic-boost-boost-3d-generation-with-mutli-view-conditioned-diffusion-fan-yang-et-al-2024>(33/70 | 85/264) Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion (Fan Yang et al., 2024)</a></li><li><a href=#3470--86264-zest-zero-shot-material-transfer-from-a-single-image-ta-ying-cheng-et-al-2024>(34/70 | 86/264) ZeST: Zero-Shot Material Transfer from a Single Image (Ta-Ying Cheng et al., 2024)</a></li><li><a href=#3570--87264-automated-national-urban-map-extraction-hasan-nasrallah-et-al-2024>(35/70 | 87/264) Automated National Urban Map Extraction (Hasan Nasrallah et al., 2024)</a></li><li><a href=#3670--88264-matching-2d-images-in-3d-metric-relative-pose-from-metric-correspondences-axel-barroso-laguna-et-al-2024>(36/70 | 88/264) Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences (Axel Barroso-Laguna et al., 2024)</a></li><li><a href=#3770--89264-geosynth-contextually-aware-high-resolution-satellite-image-synthesis-srikumar-sastry-et-al-2024>(37/70 | 89/264) GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis (Srikumar Sastry et al., 2024)</a></li><li><a href=#3870--90264-spatially-optimized-compact-deep-metric-learning-model-for-similarity-search-md-farhadul-islam-et-al-2024>(38/70 | 90/264) Spatially Optimized Compact Deep Metric Learning Model for Similarity Search (Md. Farhadul Islam et al., 2024)</a></li><li><a href=#3970--91264-questmaps-queryable-semantic-topological-maps-for-3d-scene-understanding-yash-mehan-et-al-2024>(39/70 | 91/264) QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding (Yash Mehan et al., 2024)</a></li><li><a href=#4070--92264-lrr-language-driven-resamplable-continuous-representation-against-adversarial-tracking-attacks-jianlang-chen-et-al-2024>(40/70 | 92/264) LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks (Jianlang Chen et al., 2024)</a></li><li><a href=#4170--93264-yolc-you-only-look-clusters-for-tiny-object-detection-in-aerial-images-chenguang-liu-et-al-2024>(41/70 | 93/264) YOLC: You Only Look Clusters for Tiny Object Detection in Aerial Images (Chenguang Liu et al., 2024)</a></li><li><a href=#4270--94264-dreamview-injecting-view-specific-text-guidance-into-text-to-3d-generation-junkai-yan-et-al-2024>(42/70 | 94/264) DreamView: Injecting View-specific Text Guidance into Text-to-3D Generation (Junkai Yan et al., 2024)</a></li><li><a href=#4370--95264-band-attention-modulated-retnet-for-face-forgery-detection-zhida-zhang-et-al-2024>(43/70 | 95/264) Band-Attention Modulated RetNet for Face Forgery Detection (Zhida Zhang et al., 2024)</a></li><li><a href=#4470--96264-training-free-open-vocabulary-segmentation-with-offline-diffusion-augmented-prototype-generation-luca-barsellotti-et-al-2024>(44/70 | 96/264) Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation (Luca Barsellotti et al., 2024)</a></li><li><a href=#4570--97264-reconstructing-hand-held-objects-in-3d-jane-wu-et-al-2024>(45/70 | 97/264) Reconstructing Hand-Held Objects in 3D (Jane Wu et al., 2024)</a></li><li><a href=#4670--98264-flamefinder-illuminating-obscured-fire-through-smoke-with-attentive-deep-metric-learning-hossein-rajoli-et-al-2024>(46/70 | 98/264) FlameFinder: Illuminating Obscured Fire through Smoke with Attentive Deep Metric Learning (Hossein Rajoli et al., 2024)</a></li><li><a href=#4770--99264-revising-densification-in-gaussian-splatting-samuel-rota-bulò-et-al-2024>(47/70 | 99/264) Revising Densification in Gaussian Splatting (Samuel Rota Bulò et al., 2024)</a></li><li><a href=#4870--100264-unified-entropy-optimization-for-open-set-test-time-adaptation-zhengqing-gao-et-al-2024>(48/70 | 100/264) Unified Entropy Optimization for Open-Set Test-Time Adaptation (Zhengqing Gao et al., 2024)</a></li><li><a href=#4970--101264-the-impact-of-print-and-scan-in-heterogeneous-morph-evaluation-scenarios-richard-e-neddo-et-al-2024>(49/70 | 101/264) The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios (Richard E. Neddo et al., 2024)</a></li><li><a href=#5070--102264-flying-with-photons-rendering-novel-views-of-propagating-light-anagh-malik-et-al-2024>(50/70 | 102/264) Flying with Photons: Rendering Novel Views of Propagating Light (Anagh Malik et al., 2024)</a></li><li><a href=#5170--103264-learning-state-invariant-representations-of-objects-from-image-collections-with-state-pose-and-viewpoint-changes-rohan-sarkar-et-al-2024>(51/70 | 103/264) Learning State-Invariant Representations of Objects from Image Collections with State, Pose, and Viewpoint Changes (Rohan Sarkar et al., 2024)</a></li><li><a href=#5270--104264-daf-bevseg-distortion-aware-fisheye-camera-based-birds-eye-view-segmentation-with-occlusion-reasoning-senthil-yogamani-et-al-2024>(52/70 | 104/264) DaF-BEVSeg: Distortion-aware Fisheye Camera based Bird&rsquo;s Eye View Segmentation with Occlusion Reasoning (Senthil Yogamani et al., 2024)</a></li><li><a href=#5370--105264-spatial-temporal-multi-level-association-for-video-object-segmentation-deshui-miao-et-al-2024>(53/70 | 105/264) Spatial-Temporal Multi-level Association for Video Object Segmentation (Deshui Miao et al., 2024)</a></li><li><a href=#5470--106264-hyperparameter-free-medical-image-synthesis-for-sharing-data-and-improving-site-specific-segmentation-alexander-chebykin-et-al-2024>(54/70 | 106/264) Hyperparameter-Free Medical Image Synthesis for Sharing Data and Improving Site-Specific Segmentation (Alexander Chebykin et al., 2024)</a></li><li><a href=#5570--107264-automatic-defect-detection-in-sewer-network-using-deep-learning-based-object-detector-bach-ha-et-al-2024>(55/70 | 107/264) Automatic Defect Detection in Sewer Network Using Deep Learning Based Object Detector (Bach Ha et al., 2024)</a></li><li><a href=#5670--108264-unified-physical-digital-attack-detection-challenge-haocheng-yuan-et-al-2024>(56/70 | 108/264) Unified Physical-Digital Attack Detection Challenge (Haocheng Yuan et al., 2024)</a></li><li><a href=#5770--109264-enhanced-radar-perception-via-multi-task-learning-towards-refined-data-for-sensor-fusion-applications-huawei-sun-et-al-2024>(57/70 | 109/264) Enhanced Radar Perception via Multi-Task Learning: Towards Refined Data for Sensor Fusion Applications (Huawei Sun et al., 2024)</a></li><li><a href=#5870--110264-gaussian-pancakes-geometrically-regularized-3d-gaussian-splatting-for-realistic-endoscopic-reconstruction-sierra-bonilla-et-al-2024>(58/70 | 110/264) Gaussian Pancakes: Geometrically-Regularized 3D Gaussian Splatting for Realistic Endoscopic Reconstruction (Sierra Bonilla et al., 2024)</a></li><li><a href=#5970--111264-hash3d-training-free-acceleration-for-3d-generation-xingyi-yang-et-al-2024>(59/70 | 111/264) Hash3D: Training-free Acceleration for 3D Generation (Xingyi Yang et al., 2024)</a></li><li><a href=#6070--112264-diffusion-based-point-cloud-super-resolution-for-mmwave-radar-data-kai-luan-et-al-2024>(60/70 | 112/264) Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data (Kai Luan et al., 2024)</a></li><li><a href=#6170--113264-storyimager-a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion-ming-tao-et-al-2024>(61/70 | 113/264) StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion (Ming Tao et al., 2024)</a></li><li><a href=#6270--114264-prompt-driven-universal-model-for-view-agnostic-echocardiography-analysis-sekeun-kim-et-al-2024>(62/70 | 114/264) Prompt-driven Universal Model for View-Agnostic Echocardiography Analysis (Sekeun Kim et al., 2024)</a></li><li><a href=#6370--115264-roadbev-road-surface-reconstruction-in-birds-eye-view-tong-zhao-et-al-2024>(63/70 | 115/264) RoadBEV: Road Surface Reconstruction in Bird&rsquo;s Eye View (Tong Zhao et al., 2024)</a></li><li><a href=#6470--116264-pure-turning-polysemantic-neurons-into-pure-features-by-identifying-relevant-circuits-maximilian-dreyer-et-al-2024>(64/70 | 116/264) PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits (Maximilian Dreyer et al., 2024)</a></li><li><a href=#6570--117264-seasonal-fire-prediction-using-spatio-temporal-deep-neural-networks-dimitrios-michail-et-al-2024>(65/70 | 117/264) Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks (Dimitrios Michail et al., 2024)</a></li><li><a href=#6670--118264-robust-confidence-intervals-in-stereo-matching-using-possibility-theory-roman-malinowski-et-al-2024>(66/70 | 118/264) Robust Confidence Intervals in Stereo Matching using Possibility Theory (Roman Malinowski et al., 2024)</a></li><li><a href=#6770--119264-3d-geometry-aware-deformable-gaussian-splatting-for-dynamic-view-synthesis-zhicheng-lu-et-al-2024>(67/70 | 119/264) 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis (Zhicheng Lu et al., 2024)</a></li><li><a href=#6870--120264-ghnerf-learning-generalizable-human-features-with-efficient-neural-radiance-fields-arnab-dey-et-al-2024>(68/70 | 120/264) GHNeRF: Learning Generalizable Human Features with Efficient Neural Radiance Fields (Arnab Dey et al., 2024)</a></li><li><a href=#6970--121264-rolling-shutter-correction-with-intermediate-distortion-flow-estimation-mingdeng-cao-et-al-2024>(69/70 | 121/264) Rolling Shutter Correction with Intermediate Distortion Flow Estimation (Mingdeng Cao et al., 2024)</a></li><li><a href=#7070--122264-colormnet-a-memory-based-deep-spatial-temporal-feature-propagation-network-for-video-colorization-yixin-yang-et-al-2024>(70/70 | 122/264) ColorMNet: A Memory-based Deep Spatial-Temporal Feature Propagation Network for Video Colorization (Yixin Yang et al., 2024)</a></li></ul></li><li><a href=#csro-12>cs.RO (12)</a><ul><li><a href=#112--123264-large-language-models-to-the-rescue-deadlock-resolution-in-multi-robot-systems-kunal-garg-et-al-2024>(1/12 | 123/264) Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems (Kunal Garg et al., 2024)</a></li><li><a href=#212--124264-3d-branch-point-cloud-completion-for-robotic-pruning-in-apple-orchards-tian-qiu-et-al-2024>(2/12 | 124/264) 3D Branch Point Cloud Completion for Robotic Pruning in Apple Orchards (Tian Qiu et al., 2024)</a></li><li><a href=#312--125264-genchip-generating-robot-policy-code-for-high-precision-and-contact-rich-manipulation-tasks-kaylee-burns-et-al-2024>(3/12 | 125/264) GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks (Kaylee Burns et al., 2024)</a></li><li><a href=#412--126264-counting-objects-in-a-robotic-hand-francis-tsow-et-al-2024>(4/12 | 126/264) Counting Objects in a Robotic Hand (Francis Tsow et al., 2024)</a></li><li><a href=#512--127264-ai-mole-autonomous-iterative-motion-learning-for-unknown-nonlinear-dynamics-with-extensive-experimental-validation-michael-meindl-et-al-2024>(5/12 | 127/264) AI-MOLE: Autonomous Iterative Motion Learning for Unknown Nonlinear Dynamics with Extensive Experimental Validation (Michael Meindl et al., 2024)</a></li><li><a href=#612--128264-robot-safe-planning-in-dynamic-environments-based-on-model-predictive-control-using-control-barrier-function-zetao-lu-et-al-2024>(6/12 | 128/264) Robot Safe Planning In Dynamic Environments Based On Model Predictive Control Using Control Barrier Function (Zetao Lu et al., 2024)</a></li><li><a href=#712--129264-morpheus-a-multimodal-one-armed-robot-assisted-peeling-system-with-human-users-in-the-loop-ruolin-ye-et-al-2024>(7/12 | 129/264) MORPHeus: a Multimodal One-armed Robot-assisted Peeling System with Human Users In-the-loop (Ruolin Ye et al., 2024)</a></li><li><a href=#812--130264-learning-strategies-for-successful-crowd-navigation-rajshree-daulatabad-et-al-2024>(8/12 | 130/264) Learning Strategies For Successful Crowd Navigation (Rajshree Daulatabad et al., 2024)</a></li><li><a href=#912--131264-deep-reinforcement-learning-based-approach-for-a-single-vehicle-persistent-surveillance-problem-with-fuel-constraints-hritik-bana-et-al-2024>(9/12 | 131/264) Deep Reinforcement Learning-Based Approach for a Single Vehicle Persistent Surveillance Problem with Fuel Constraints (Hritik Bana et al., 2024)</a></li><li><a href=#1012--132264-statistical-modelling-of-driving-scenarios-in-road-traffic-using-fleet-data-of-production-vehicles-christian-reichenbächer-et-al-2024>(10/12 | 132/264) Statistical Modelling of Driving Scenarios in Road Traffic using Fleet Data of Production Vehicles (Christian Reichenbächer et al., 2024)</a></li><li><a href=#1112--133264-adaptable-recovery-behaviors-in-robotics-a-behavior-trees-and-motion-generatorsbtmg-approach-for-failure-management-faseeh-ahmad-et-al-2024>(11/12 | 133/264) Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion Generators(BTMG) Approach for Failure Management (Faseeh Ahmad et al., 2024)</a></li><li><a href=#1212--134264-body-design-and-gait-generation-of-chair-type-asymmetrical-tripedal-low-rigidity-robot-shintaro-inoue-et-al-2024>(12/12 | 134/264) Body Design and Gait Generation of Chair-Type Asymmetrical Tripedal Low-rigidity Robot (Shintaro Inoue et al., 2024)</a></li></ul></li><li><a href=#cond-matstat-mech-1>cond-mat.stat-mech (1)</a><ul><li><a href=#11--135264-message-passing-variational-autoregressive-network-for-solving-intractable-ising-models-qunlong-ma-et-al-2024>(1/1 | 135/264) Message Passing Variational Autoregressive Network for Solving Intractable Ising Models (Qunlong Ma et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--136264-multimodal-road-network-generation-based-on-large-language-model-jiajing-chen-et-al-2024>(1/4 | 136/264) Multimodal Road Network Generation Based on Large Language Model (Jiajing Chen et al., 2024)</a></li><li><a href=#24--137264-apprentices-to-research-assistants-advancing-research-with-large-language-models-m-namvarpour-et-al-2024>(2/4 | 137/264) Apprentices to Research Assistants: Advancing Research with Large Language Models (M. Namvarpour et al., 2024)</a></li><li><a href=#34--138264-eve-enabling-anyone-to-train-robot-using-augmented-reality-jun-wang-et-al-2024>(3/4 | 138/264) EVE: Enabling Anyone to Train Robot using Augmented Reality (Jun Wang et al., 2024)</a></li><li><a href=#44--139264-missing-pieces-how-framing-uncertainty-impacts-longitudinal-trust-in-ai-decision-aids----a-gig-driver-case-study-rex-chen-et-al-2024>(4/4 | 139/264) Missing Pieces: How Framing Uncertainty Impacts Longitudinal Trust in AI Decision Aids &ndash; A Gig Driver Case Study (Rex Chen et al., 2024)</a></li></ul></li><li><a href=#csni-4>cs.NI (4)</a><ul><li><a href=#14--140264-ddpg-e2e-a-novel-policy-gradient-approach-for-end-to-end-communication-systems-bolun-zhang-et-al-2024>(1/4 | 140/264) DDPG-E2E: A Novel Policy Gradient Approach for End-to-End Communication Systems (Bolun Zhang et al., 2024)</a></li><li><a href=#24--141264-dynamic-d2d-assisted-federated-learning-over-o-ran-performance-analysis-mac-scheduler-and-asymmetric-user-selection-payam-abdisarabshali-et-al-2024>(2/4 | 141/264) Dynamic D2D-Assisted Federated Learning over O-RAN: Performance Analysis, MAC Scheduler, and Asymmetric User Selection (Payam Abdisarabshali et al., 2024)</a></li><li><a href=#34--142264-streamlined-transmission-a-semantic-aware-xr-deployment-framework-enhanced-by-generative-ai-wanting-yang-et-al-2024>(3/4 | 142/264) Streamlined Transmission: A Semantic-Aware XR Deployment Framework Enhanced by Generative AI (Wanting Yang et al., 2024)</a></li><li><a href=#44--143264-deterministic-and-probabilistic-p4-enabled-lightweight-in-band-network-telemetry-konstantinos-papadopoulos-et-al-2024>(4/4 | 143/264) Deterministic and Probabilistic P4-Enabled Lightweight In-Band Network Telemetry (Konstantinos Papadopoulos et al., 2024)</a></li></ul></li><li><a href=#cslg-39>cs.LG (39)</a><ul><li><a href=#139--144264-automated-federated-pipeline-for-parameter-efficient-fine-tuning-of-large-language-models-zihan-fang-et-al-2024>(1/39 | 144/264) Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models (Zihan Fang et al., 2024)</a></li><li><a href=#239--145264-fair-graph-neural-network-with-supervised-contrastive-regularization-mahdi-tavassoli-kejani-et-al-2024>(2/39 | 145/264) Fair Graph Neural Network with Supervised Contrastive Regularization (Mahdi Tavassoli Kejani et al., 2024)</a></li><li><a href=#339--146264-aggressive-or-imperceptible-or-both-network-pruning-assisted-hybrid-byzantines-in-federated-learning-emre-ozfatura-et-al-2024>(3/39 | 146/264) Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid Byzantines in Federated Learning (Emre Ozfatura et al., 2024)</a></li><li><a href=#439--147264-elephants-never-forget-memorization-and-learning-of-tabular-data-in-large-language-models-sebastian-bordt-et-al-2024>(4/39 | 147/264) Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models (Sebastian Bordt et al., 2024)</a></li><li><a href=#539--148264-sccdcg-efficient-deep-structural-clustering-for-single-cell-rna-seq-via-deep-cut-informed-graph-embedding-ping-xu-et-al-2024>(5/39 | 148/264) scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via Deep Cut-informed Graph Embedding (Ping Xu et al., 2024)</a></li><li><a href=#639--149264-generative-pre-trained-transformer-for-symbolic-regression-base-in-context-reinforcement-learning-yanjie-li-et-al-2024>(6/39 | 149/264) Generative Pre-Trained Transformer for Symbolic Regression Base In-Context Reinforcement Learning (Yanjie Li et al., 2024)</a></li><li><a href=#739--150264-does-transformer-interpretability-transfer-to-rnns-gonçalo-paulo-et-al-2024>(7/39 | 150/264) Does Transformer Interpretability Transfer to RNNs? (Gonçalo Paulo et al., 2024)</a></li><li><a href=#839--151264-causalbench-a-comprehensive-benchmark-for-causal-learning-capability-of-large-language-models-yu-zhou-et-al-2024>(8/39 | 151/264) CausalBench: A Comprehensive Benchmark for Causal Learning Capability of Large Language Models (Yu Zhou et al., 2024)</a></li><li><a href=#939--152264-aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts-shaona-ghosh-et-al-2024>(9/39 | 152/264) AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts (Shaona Ghosh et al., 2024)</a></li><li><a href=#1039--153264-policy-guided-diffusion-matthew-thomas-jackson-et-al-2024>(10/39 | 153/264) Policy-Guided Diffusion (Matthew Thomas Jackson et al., 2024)</a></li><li><a href=#1139--154264-differential-privacy-for-anomaly-detection-analyzing-the-trade-off-between-privacy-and-explainability-fatima-ezzeddine-et-al-2024>(11/39 | 154/264) Differential Privacy for Anomaly Detection: Analyzing the Trade-off Between Privacy and Explainability (Fatima Ezzeddine et al., 2024)</a></li><li><a href=#1239--155264-variational-stochastic-gradient-descent-for-deep-neural-networks-haotian-chen-et-al-2024>(12/39 | 155/264) Variational Stochastic Gradient Descent for Deep Neural Networks (Haotian Chen et al., 2024)</a></li><li><a href=#1339--156264-pfl-research-simulation-framework-for-accelerating-research-in-private-federated-learning-filip-granqvist-et-al-2024>(13/39 | 156/264) pfl-research: simulation framework for accelerating research in Private Federated Learning (Filip Granqvist et al., 2024)</a></li><li><a href=#1439--157264-the-impact-of-data-set-similarity-and-diversity-on-transfer-learning-success-in-time-series-forecasting-claudia-ehrig-et-al-2024>(14/39 | 157/264) The impact of data set similarity and diversity on transfer learning success in time series forecasting (Claudia Ehrig et al., 2024)</a></li><li><a href=#1539--158264-fmda-ot-federated-multi-source-domain-adaptation-through-optimal-transport-omar-ghannou-et-al-2024>(15/39 | 158/264) FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal Transport (Omar Ghannou et al., 2024)</a></li><li><a href=#1639--159264-diverse-randomized-value-functions-a-provably-pessimistic-approach-for-offline-reinforcement-learning-xudong-yu-et-al-2024>(16/39 | 159/264) Diverse Randomized Value Functions: A Provably Pessimistic Approach for Offline Reinforcement Learning (Xudong Yu et al., 2024)</a></li><li><a href=#1739--160264-clip-embed-kd-computationally-efficient-knowledge-distillation-using-embeddings-as-teachers-lakshmi-nair-2024>(17/39 | 160/264) CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers (Lakshmi Nair, 2024)</a></li><li><a href=#1839--161264-zero-shot-relational-learning-for-multimodal-knowledge-graphs-rui-cai-et-al-2024>(18/39 | 161/264) Zero-Shot Relational Learning for Multimodal Knowledge Graphs (Rui Cai et al., 2024)</a></li><li><a href=#1939--162264-dynamic-deep-learning-based-super-resolution-for-the-shallow-water-equations-maximilian-witte-et-al-2024>(19/39 | 162/264) Dynamic Deep Learning Based Super-Resolution For The Shallow Water Equations (Maximilian Witte et al., 2024)</a></li><li><a href=#2039--163264-pgtnet-a-process-graph-transformer-network-for-remaining-time-prediction-of-business-process-instances-keyvan-amiri-elyasi-et-al-2024>(20/39 | 163/264) PGTNet: A Process Graph Transformer Network for Remaining Time Prediction of Business Process Instances (Keyvan Amiri Elyasi et al., 2024)</a></li><li><a href=#2139--164264-hyperparameter-selection-in-continual-learning-thomas-l-lee-et-al-2024>(21/39 | 164/264) Hyperparameter Selection in Continual Learning (Thomas L. Lee et al., 2024)</a></li><li><a href=#2239--165264-scrdit-generating-single-cell-rna-seq-data-by-diffusion-transformers-and-accelerating-sampling-shengze-dong-et-al-2024>(22/39 | 165/264) scRDiT: Generating single-cell RNA-seq data by diffusion transformers and accelerating sampling (Shengze Dong et al., 2024)</a></li><li><a href=#2339--166264-graph-reinforcement-learning-for-combinatorial-optimization-a-survey-and-unifying-perspective-victor-alexandru-darvariu-et-al-2024>(23/39 | 166/264) Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective (Victor-Alexandru Darvariu et al., 2024)</a></li><li><a href=#2439--167264-efficient-multi-task-reinforcement-learning-via-task-specific-action-correction-jinyuan-feng-et-al-2024>(24/39 | 167/264) Efficient Multi-Task Reinforcement Learning via Task-Specific Action Correction (Jinyuan Feng et al., 2024)</a></li><li><a href=#2539--168264-federated-learning-model-for-predicting-major-postoperative-complications-yonggi-park-et-al-2024>(25/39 | 168/264) Federated learning model for predicting major postoperative complications (Yonggi Park et al., 2024)</a></li><li><a href=#2639--169264-simultaneous-linear-connectivity-of-neural-networks-modulo-permutation-ekansh-sharma-et-al-2024>(26/39 | 169/264) Simultaneous linear connectivity of neural networks modulo permutation (Ekansh Sharma et al., 2024)</a></li><li><a href=#2739--170264-exploring-neural-network-landscapes-star-shaped-and-geodesic-connectivity-zhanran-lin-et-al-2024>(27/39 | 170/264) Exploring Neural Network Landscapes: Star-Shaped and Geodesic Connectivity (Zhanran Lin et al., 2024)</a></li><li><a href=#2839--171264-high-noise-scheduling-is-a-must-mahmut-s-gokmen-et-al-2024>(28/39 | 171/264) High Noise Scheduling is a Must (Mahmut S. Gokmen et al., 2024)</a></li><li><a href=#2939--172264-on-adversarial-training-and-the-1-nearest-neighbor-classifier-amir-hagai-et-al-2024>(29/39 | 172/264) On adversarial training and the 1 Nearest Neighbor classifier (Amir Hagai et al., 2024)</a></li><li><a href=#3039--173264-algorithms-for-caching-and-mts-with-reduced-number-of-predictions-karim-abdel-sadek-et-al-2024>(30/39 | 173/264) Algorithms for Caching and MTS with reduced number of predictions (Karim Abdel Sadek et al., 2024)</a></li><li><a href=#3139--174264-feel-good-thompson-sampling-for-contextual-dueling-bandits-xuheng-li-et-al-2024>(31/39 | 174/264) Feel-Good Thompson Sampling for Contextual Dueling Bandits (Xuheng Li et al., 2024)</a></li><li><a href=#3239--175264-a-cyber-manufacturing-iot-system-for-adaptive-machine-learning-model-deployment-by-interactive-causality-enabled-self-labeling-yutian-ren-et-al-2024>(32/39 | 175/264) A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling (Yutian Ren et al., 2024)</a></li><li><a href=#3339--176264-adagossip-adaptive-consensus-step-size-for-decentralized-deep-learning-with-communication-compression-sai-aparna-aketi-et-al-2024>(33/39 | 176/264) AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression (Sai Aparna Aketi et al., 2024)</a></li><li><a href=#3439--177264-deep-reinforcement-learning-for-personalized-diagnostic-decision-pathways-using-electronic-health-records-a-comparative-study-on-anemia-and-systemic-lupus-erythematosus-lillian-muyama-et-al-2024>(34/39 | 177/264) Deep Reinforcement Learning for Personalized Diagnostic Decision Pathways Using Electronic Health Records: A Comparative Study on Anemia and Systemic Lupus Erythematosus (Lillian Muyama et al., 2024)</a></li><li><a href=#3539--178264-bayesian-survival-analysis-by-approximate-inference-of-neural-networks-christian-marius-lillelund-et-al-2024>(35/39 | 178/264) Bayesian Survival Analysis by Approximate Inference of Neural Networks (Christian Marius Lillelund et al., 2024)</a></li><li><a href=#3639--179264-unifying-low-dimensional-observations-in-deep-learning-through-the-deep-linear-unconstrained-feature-model-connall-garrod-et-al-2024>(36/39 | 179/264) Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model (Connall Garrod et al., 2024)</a></li><li><a href=#3739--180264-go4align-group-optimization-for-multi-task-alignment-jiayi-shen-et-al-2024>(37/39 | 180/264) GO4Align: Group Optimization for Multi-Task Alignment (Jiayi Shen et al., 2024)</a></li><li><a href=#3839--181264-online-learning-of-decision-trees-with-thompson-sampling-ayman-chaouki-et-al-2024>(38/39 | 181/264) Online Learning of Decision Trees with Thompson Sampling (Ayman Chaouki et al., 2024)</a></li><li><a href=#3939--182264-a-lightweight-measure-of-classification-difficulty-from-application-dataset-characteristics-bryan-bo-cao-et-al-2024>(39/39 | 182/264) A Lightweight Measure of Classification Difficulty from Application Dataset Characteristics (Bryan Bo Cao et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--183264-aisaq-all-in-storage-anns-with-product-quantization-for-dram-free-information-retrieval-kento-tatsuno-et-al-2024>(1/4 | 183/264) AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval (Kento Tatsuno et al., 2024)</a></li><li><a href=#24--184264-dre-generating-recommendation-explanations-by-aligning-large-language-models-at-data-level-shen-gao-et-al-2024>(2/4 | 184/264) DRE: Generating Recommendation Explanations by Aligning Large Language Models at Data-level (Shen Gao et al., 2024)</a></li><li><a href=#34--185264-end-to-end-training-of-multimodal-model-and-ranking-model-xiuqi-deng-et-al-2024>(3/4 | 185/264) End-to-end training of Multimodal Model and ranking Model (Xiuqi Deng et al., 2024)</a></li><li><a href=#44--186264-wasserstein-dependent-graph-attention-network-for-collaborative-filtering-with-uncertainty-haoxuan-li-et-al-2024>(4/4 | 186/264) Wasserstein Dependent Graph Attention Network for Collaborative Filtering with Uncertainty (Haoxuan Li et al., 2024)</a></li></ul></li><li><a href=#csdb-4>cs.DB (4)</a><ul><li><a href=#14--187264-dimensionality-reduction-in-sentence-transformer-vector-databases-with-fast-fourier-transform-vitaly-bulgakov-et-al-2024>(1/4 | 187/264) Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform (Vitaly Bulgakov et al., 2024)</a></li><li><a href=#24--188264-automatic-configuration-tuning-on-cloud-database-a-survey-limeng-zhang-et-al-2024>(2/4 | 188/264) Automatic Configuration Tuning on Cloud Database: A Survey (Limeng Zhang et al., 2024)</a></li><li><a href=#34--189264-pm4pyllm-a-comprehensive-module-for-implementing-pm-on-llms-alessandro-berti-2024>(3/4 | 189/264) PM4Py.LLM: a Comprehensive Module for Implementing PM on LLMs (Alessandro Berti, 2024)</a></li><li><a href=#44--190264-balanced-partitioning-for-optimizing-big-graph-computation-complexities-and-approximation-algorithms-baoling-ning-et-al-2024>(4/4 | 190/264) Balanced Partitioning for Optimizing Big Graph Computation: Complexities and Approximation Algorithms (Baoling Ning et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--191264-using-few-shot-learning-to-classify-primary-lung-cancer-and-other-malignancy-with-lung-metastasis-in-cytological-imaging-via-endobronchial-ultrasound-procedures-ching-kai-lin-et-al-2024>(1/3 | 191/264) Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures (Ching-Kai Lin et al., 2024)</a></li><li><a href=#23--192264-fortifying-fully-convolutional-generative-adversarial-networks-for-image-super-resolution-using-divergence-measures-arkaprabha-basu-et-al-2024>(2/3 | 192/264) Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures (Arkaprabha Basu et al., 2024)</a></li><li><a href=#33--193264-latup-net-a-lightweight-3d-attention-u-net-with-parallel-convolutions-for-brain-tumor-segmentation-ebtihal-j-alwadee-et-al-2024>(3/3 | 193/264) LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation (Ebtihal J. Alwadee et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--194264-the-x-lance-technical-report-for-interspeech-2024-speech-processing-using-discrete-speech-unit-challenge-yiwei-guo-et-al-2024>(1/2 | 194/264) The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge (Yiwei Guo et al., 2024)</a></li><li><a href=#22--195264-masked-modeling-duo-towards-a-universal-audio-pre-training-framework-daisuke-niizumi-et-al-2024>(2/2 | 195/264) Masked Modeling Duo: Towards a Universal Audio Pre-training Framework (Daisuke Niizumi et al., 2024)</a></li></ul></li><li><a href=#csse-8>cs.SE (8)</a><ul><li><a href=#18--196264-on-evaluating-the-efficiency-of-source-code-generated-by-llms-changan-niu-et-al-2024>(1/8 | 196/264) On Evaluating the Efficiency of Source Code Generated by LLMs (Changan Niu et al., 2024)</a></li><li><a href=#28--197264-model-generation-from-requirements-with-llms-an-exploratory-study-alessio-ferrari-et-al-2024>(2/8 | 197/264) Model Generation from Requirements with LLMs: an Exploratory Study (Alessio Ferrari et al., 2024)</a></li><li><a href=#38--198264-open-source-ai-based-se-tools-opportunities-and-challenges-of-collaborative-software-learning-zhihao-lin-et-al-2024>(3/8 | 198/264) Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning (Zhihao Lin et al., 2024)</a></li><li><a href=#48--199264-perplexed-understanding-when-large-language-models-are-confused-nathan-cooper-et-al-2024>(4/8 | 199/264) Perplexed: Understanding When Large Language Models are Confused (Nathan Cooper et al., 2024)</a></li><li><a href=#58--200264-a-rag-method-for-source-code-inquiry-tailored-to-long-context-llms-toshihiro-kamiya-2024>(5/8 | 200/264) A RAG Method for Source Code Inquiry Tailored to Long-Context LLMs (Toshihiro Kamiya, 2024)</a></li><li><a href=#68--201264-public-private-funding-models-in-open-source-software-development-a-case-study-on-scikit-learn-cailean-osborne-2024>(6/8 | 201/264) Public-private funding models in open source software development: A case study on scikit-learn (Cailean Osborne, 2024)</a></li><li><a href=#78--202264-assessing-the-understandability-and-acceptance-of-attack-defense-trees-for-modelling-security-requirements-giovanna-broccia-et-al-2024>(7/8 | 202/264) Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements (Giovanna Broccia et al., 2024)</a></li><li><a href=#88--203264-on-test-sequence-generation-using-multi-objective-particle-swarm-optimization-zain-iqbal-et-al-2024>(8/8 | 203/264) On Test Sequence Generation using Multi-Objective Particle Swarm Optimization (Zain Iqbal et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--204264-mupt-a-generative-symbolic-music-pretrained-transformer-xingwei-qu-et-al-2024>(1/2 | 204/264) MuPT: A Generative Symbolic Music Pretrained Transformer (Xingwei Qu et al., 2024)</a></li><li><a href=#22--205264-exploring-diverse-sounds-identifying-outliers-in-a-music-corpus-le-cai-et-al-2024>(2/2 | 205/264) Exploring Diverse Sounds: Identifying Outliers in a Music Corpus (Le Cai et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--206264-map-optical-properties-to-subwavelength-structures-directly-via-a-diffusion-model-shijie-rao-et-al-2024>(1/1 | 206/264) Map Optical Properties to Subwavelength Structures Directly via a Diffusion Model (Shijie Rao et al., 2024)</a></li></ul></li><li><a href=#csai-8>cs.AI (8)</a><ul><li><a href=#18--207264-building-a-knowledge-graph-to-enrich-chatgpt-responses-in-manufacturing-service-discovery-yunqing-li-et-al-2024>(1/8 | 207/264) Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing Service Discovery (Yunqing Li et al., 2024)</a></li><li><a href=#28--208264-agentquest-a-modular-benchmark-framework-to-measure-progress-and-improve-llm-agents-luca-gioacchini-et-al-2024>(2/8 | 208/264) AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents (Luca Gioacchini et al., 2024)</a></li><li><a href=#38--209264-agentscodriver-large-language-model-empowered-collaborative-driving-with-lifelong-learning-senkang-hu-et-al-2024>(3/8 | 209/264) AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning (Senkang Hu et al., 2024)</a></li><li><a href=#48--210264-enhancing-decision-analysis-with-a-large-language-model-pydecision-a-comprehensive-library-of-mcda-methods-in-python-valdecy-pereira-et-al-2024>(4/8 | 210/264) Enhancing Decision Analysis with a Large Language Model: pyDecision a Comprehensive Library of MCDA Methods in Python (Valdecy Pereira et al., 2024)</a></li><li><a href=#58--211264-wus-method-can-boost-symbolic-ai-to-rival-silver-medalists-and-alphageometry-to-outperform-gold-medalists-at-imo-geometry-shiven-sinha-et-al-2024>(5/8 | 211/264) Wu&rsquo;s Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry (Shiven Sinha et al., 2024)</a></li><li><a href=#68--212264-autonomous-evaluation-and-refinement-of-digital-agents-jiayi-pan-et-al-2024>(6/8 | 212/264) Autonomous Evaluation and Refinement of Digital Agents (Jiayi Pan et al., 2024)</a></li><li><a href=#78--213264-automatically-learning-htn-methods-from-landmarks-ruoxi-li-et-al-2024>(7/8 | 213/264) Automatically Learning HTN Methods from Landmarks (Ruoxi Li et al., 2024)</a></li><li><a href=#88--214264-goat-bench-a-benchmark-for-multi-modal-lifelong-navigation-mukul-khanna-et-al-2024>(8/8 | 214/264) GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation (Mukul Khanna et al., 2024)</a></li></ul></li><li><a href=#csne-8>cs.NE (8)</a><ul><li><a href=#18--215264-evolving-loss-functions-for-specific-image-augmentation-techniques-brandon-morgan-et-al-2024>(1/8 | 215/264) Evolving Loss Functions for Specific Image Augmentation Techniques (Brandon Morgan et al., 2024)</a></li><li><a href=#28--216264-exploring-the-true-potential-evaluating-the-black-box-optimization-capability-of-large-language-models-beichen-huang-et-al-2024>(2/8 | 216/264) Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models (Beichen Huang et al., 2024)</a></li><li><a href=#38--217264-emergent-braitenberg-style-behaviours-for-navigating-the-vizdoom-my-way-home-labyrinth-caleidgh-bayer-et-al-2024>(3/8 | 217/264) Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way Home&rsquo; Labyrinth (Caleidgh Bayer et al., 2024)</a></li><li><a href=#48--218264-an-enhanced-grey-wolf-optimizer-with-elite-inheritance-and-balance-search-mechanisms-jianhua-jiang-et-al-2024>(4/8 | 218/264) An Enhanced Grey Wolf Optimizer with Elite Inheritance and Balance Search Mechanisms (Jianhua Jiang et al., 2024)</a></li><li><a href=#58--219264-evolving-collective-behavior-in-self-organizing-particle-systems-devendra-parkar-et-al-2024>(5/8 | 219/264) Evolving Collective Behavior in Self-Organizing Particle Systems (Devendra Parkar et al., 2024)</a></li><li><a href=#68--220264-temporal-true-and-surrogate-fitness-landscape-analysis-for-expensive-bi-objective-optimisation-c-j-rodriguez-et-al-2024>(6/8 | 220/264) Temporal True and Surrogate Fitness Landscape Analysis for Expensive Bi-Objective Optimisation (C. J. Rodriguez et al., 2024)</a></li><li><a href=#78--221264-synaptogen-a-cross-domain-generative-device-model-for-large-scale-neuromorphic-circuit-design-tyler-hennen-et-al-2024>(7/8 | 221/264) Synaptogen: A cross-domain generative device model for large-scale neuromorphic circuit design (Tyler Hennen et al., 2024)</a></li><li><a href=#88--222264-using-3-objective-evolutionary-algorithms-for-the-dynamic-chance-constrained-knapsack-problem-ishara-hewa-pathiranage-et-al-2024>(8/8 | 222/264) Using 3-Objective Evolutionary Algorithms for the Dynamic Chance Constrained Knapsack Problem (Ishara Hewa Pathiranage et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#11--223264-sam-i-am-semantic-boosting-for-zero-shot-atomic-scale-electron-micrograph-segmentation-waqwoya-abebe-et-al-2024>(1/1 | 223/264) SAM-I-Am: Semantic Boosting for Zero-shot Atomic-Scale Electron Micrograph Segmentation (Waqwoya Abebe et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#17--224264-xflex-hydro-demonstrators-grid-services-assessment-and-ancillary-services-matrix-elaboration-christophe-nicolet-et-al-2024>(1/7 | 224/264) XFLEX HYDRO demonstrators grid services assessment and Ancillary Services Matrix elaboration (Christophe Nicolet et al., 2024)</a></li><li><a href=#27--225264-traffic-signal-control-and-speed-offset-coordination-using-q-learning-for-arterial-road-networks-tianchen-yuan-et-al-2024>(2/7 | 225/264) Traffic Signal Control and Speed Offset Coordination Using Q-Learning for Arterial Road Networks (Tianchen Yuan et al., 2024)</a></li><li><a href=#37--226264-inertia-emulation-contribution-of-frades-2-variable-speed-pump-turbine-to-power-network-stability-christophe-nicolet-et-al-2024>(3/7 | 226/264) Inertia emulation contribution of Frades 2 variable speed pump-turbine to power network stability (Christophe Nicolet et al., 2024)</a></li><li><a href=#47--227264-a-large-scale-simulation-method-for-neuromorphic-circuits-amir-shahhosseini-et-al-2024>(4/7 | 227/264) A Large-Scale Simulation Method for Neuromorphic Circuits (Amir Shahhosseini et al., 2024)</a></li><li><a href=#57--228264-learning-locally-interacting-discrete-dynamical-systems-towards-data-efficient-and-scalable-prediction-beomseok-kang-et-al-2024>(5/7 | 228/264) Learning Locally Interacting Discrete Dynamical Systems: Towards Data-Efficient and Scalable Prediction (Beomseok Kang et al., 2024)</a></li><li><a href=#67--229264-a-data-driven-approach-to-uio-based-fault-diagnosis-giulio-fattore-et-al-2024>(6/7 | 229/264) A data-driven approach to UIO-based fault diagnosis (Giulio Fattore et al., 2024)</a></li><li><a href=#77--230264-learning-model-predictive-control-parameters-via-bayesian-optimization-for-battery-fast-charging-sebastian-hirt-et-al-2024>(7/7 | 230/264) Learning Model Predictive Control Parameters via Bayesian Optimization for Battery Fast Charging (Sebastian Hirt et al., 2024)</a></li></ul></li><li><a href=#physicsdata-an-1>physics.data-an (1)</a><ul><li><a href=#11--231264-a-feature-based-information-theoretic-approach-for-detecting-interpretable-long-timescale-pairwise-interactions-from-time-series-aria-nguyen-et-al-2024>(1/1 | 231/264) A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series (Aria Nguyen et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--232264-from-protoscience-to-epistemic-monoculture-how-benchmarking-set-the-stage-for-the-deep-learning-revolution-bernard-j-koch-et-al-2024>(1/2 | 232/264) From Protoscience to Epistemic Monoculture: How Benchmarking Set the Stage for the Deep Learning Revolution (Bernard J. Koch et al., 2024)</a></li><li><a href=#22--233264-automatic-authorities-power-and-ai-seth-lazar-2024>(2/2 | 233/264) Automatic Authorities: Power and AI (Seth Lazar, 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--234264-commute-with-community-enhancing-shared-travel-through-social-networks-tian-siyuan-et-al-2024>(1/2 | 234/264) Commute with Community: Enhancing Shared Travel through Social Networks (Tian Siyuan et al., 2024)</a></li><li><a href=#22--235264-echo-chambers-in-the-age-of-algorithms-an-audit-of-twitters-friend-recommender-system-kayla-duskin-et-al-2024>(2/2 | 235/264) Echo Chambers in the Age of Algorithms: An Audit of Twitter&rsquo;s Friend Recommender System (Kayla Duskin et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--236264-qiskit-torch-module-fast-prototyping-of-quantum-neural-networks-nico-meyer-et-al-2024>(1/2 | 236/264) Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks (Nico Meyer et al., 2024)</a></li><li><a href=#22--237264-quantum-state-generation-with-structure-preserving-diffusion-model-yuchen-zhu-et-al-2024>(2/2 | 237/264) Quantum State Generation with Structure-Preserving Diffusion Model (Yuchen Zhu et al., 2024)</a></li></ul></li><li><a href=#mathdg-1>math.DG (1)</a><ul><li><a href=#11--238264-a-singular-riemannian-geometry-approach-to-deep-neural-networks-iii-piecewise-differentiable-layers-and-random-walks-on-n-dimensional-classes-alessandro-benfenati-et-al-2024>(1/1 | 238/264) A singular Riemannian Geometry Approach to Deep Neural Networks III. Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes (Alessandro Benfenati et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--239264-collaborative-edge-ai-inference-over-cloud-ran-pengfei-zhang-et-al-2024>(1/2 | 239/264) Collaborative Edge AI Inference over Cloud-RAN (Pengfei Zhang et al., 2024)</a></li><li><a href=#22--240264-distributed-massive-mimo-system-with-dynamic-clustering-in-leo-satellite-networks-khaled-humadi-et-al-2024>(2/2 | 240/264) Distributed Massive MIMO System with Dynamic Clustering in LEO Satellite Networks (Khaled Humadi et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--241264-milgrams-experiment-in-the-knowledge-space-individual-navigation-strategies-manran-zhu-et-al-2024>(1/1 | 241/264) Milgram&rsquo;s experiment in the knowledge space: Individual navigation strategies (Manran Zhu et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--242264-the-central-spanning-tree-problem-enrique-fita-sanmartín-et-al-2024>(1/1 | 242/264) The Central Spanning Tree Problem (Enrique Fita Sanmartín et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#13--243264-oracle-net-for-nonlinear-compressed-sensing-in-electrical-impedance-tomography-reconstruction-problems-damiana-lazzaro-et-al-2024>(1/3 | 243/264) Oracle-Net for nonlinear compressed sensing in Electrical Impedance Tomography reconstruction problems (Damiana Lazzaro et al., 2024)</a></li><li><a href=#23--244264-low-rank-generalized-alternating-direction-implicit-iteration-method-for-solving-matrix-equations-juan-zhang-et-al-2024>(2/3 | 244/264) Low-rank generalized alternating direction implicit iteration method for solving matrix equations (Juan Zhang et al., 2024)</a></li><li><a href=#33--245264-optimization-methods-for-solving-matrix-equations-juan-zhang-et-al-2024>(3/3 | 245/264) Optimization methods for solving matrix equations (Juan Zhang et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--246264-notnets-accelerating-microservices-by-bypassing-the-network-peter-alvaro-et-al-2024>(1/5 | 246/264) NotNets: Accelerating Microservices by Bypassing the Network (Peter Alvaro et al., 2024)</a></li><li><a href=#25--247264-communication-efficient-large-scale-distributed-deep-learning-a-comprehensive-survey-feng-liang-et-al-2024>(2/5 | 247/264) Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey (Feng Liang et al., 2024)</a></li><li><a href=#35--248264-a-systematic-literature-survey-of-sparse-matrix-vector-multiplication-jianhua-gao-et-al-2024>(3/5 | 248/264) A Systematic Literature Survey of Sparse Matrix-Vector Multiplication (Jianhua Gao et al., 2024)</a></li><li><a href=#45--249264-a-comprehensive-benchmarking-analysis-of-fault-recovery-in-stream-processing-frameworks-adriano-vogel-et-al-2024>(4/5 | 249/264) A Comprehensive Benchmarking Analysis of Fault Recovery in Stream Processing Frameworks (Adriano Vogel et al., 2024)</a></li><li><a href=#55--250264-a-survey-of-distributed-graph-algorithms-on-massive-graphs-lingkai-meng-et-al-2024>(5/5 | 250/264) A Survey of Distributed Graph Algorithms on Massive Graphs (Lingkai Meng et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--251264-geodirdock-guiding-docking-along-geodesic-paths-raúl-miñán-et-al-2024>(1/1 | 251/264) GeoDirDock: Guiding Docking Along Geodesic Paths (Raúl Miñán et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--252264-mechanised-hypersafety-proofs-about-structured-data-extended-version-vladimir-gladshtein-et-al-2024>(1/1 | 252/264) Mechanised Hypersafety Proofs about Structured Data: Extended Version (Vladimir Gladshtein et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--253264-the-power-in-communication-power-regularization-of-communication-for-autonomy-in-cooperative-multi-agent-reinforcement-learning-nancirose-piazza-et-al-2024>(1/1 | 253/264) The Power in Communication: Power Regularization of Communication for Autonomy in Cooperative Multi-Agent Reinforcement Learning (Nancirose Piazza et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--254264-towards-practical-meshlet-compression-bastian-kuth-et-al-2024>(1/1 | 254/264) Towards Practical Meshlet Compression (Bastian Kuth et al., 2024)</a></li></ul></li><li><a href=#cslo-2>cs.LO (2)</a><ul><li><a href=#12--255264-a-semantic-proof-of-generalised-cut-elimination-for-deep-inference-robert-atkey-et-al-2024>(1/2 | 255/264) A Semantic Proof of Generalised Cut Elimination for Deep Inference (Robert Atkey et al., 2024)</a></li><li><a href=#22--256264-syndicate-synergistic-synthesis-of-ranking-function-and-invariants-for-termination-analysis-yasmin-sarita-et-al-2024>(2/2 | 256/264) Syndicate: Synergistic Synthesis of Ranking Function and Invariants for Termination Analysis (Yasmin Sarita et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--257264-advancements-in-radiomics-and-artificial-intelligence-for-thyroid-cancer-diagnosis-milad-yousefi-et-al-2024>(1/1 | 257/264) Advancements in Radiomics and Artificial Intelligence for Thyroid Cancer Diagnosis (Milad Yousefi et al., 2024)</a></li></ul></li><li><a href=#mathct-1>math.CT (1)</a><ul><li><a href=#11--258264-monoidal-context-theory-mario-román-2024>(1/1 | 258/264) Monoidal Context Theory (Mario Román, 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--259264-the-turnpike-property-for-high-dimensional-interacting-agent-systems-in-discrete-time-martin-gugat-et-al-2024>(1/1 | 259/264) The turnpike property for high-dimensional interacting agent systems in discrete time (Martin Gugat et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--260264-deep-learning-method-for-computing-committor-functions-with-adaptive-sampling-bo-lin-et-al-2024>(1/1 | 260/264) Deep Learning Method for Computing Committor Functions with Adaptive Sampling (Bo Lin et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--261264-extremal-minimal-bipartite-matching-covered-graphs-amit-kumar-mallik-et-al-2024>(1/1 | 261/264) Extremal minimal bipartite matching covered graphs (Amit Kumar Mallik et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--262264-characterizations-of-sparsifiability-for-affine-csps-and-symmetric-csps-sanjeev-khanna-et-al-2024>(1/3 | 262/264) Characterizations of Sparsifiability for Affine CSPs and Symmetric CSPs (Sanjeev Khanna et al., 2024)</a></li><li><a href=#23--263264-fully-dynamic-matching-and-ordered-ruzsa-szemerédi-graphs-soheil-behnezhad-et-al-2024>(2/3 | 263/264) Fully Dynamic Matching and Ordered Ruzsa-Szemerédi Graphs (Soheil Behnezhad et al., 2024)</a></li><li><a href=#33--264264-polynomial-time-derivation-of-optimal-k-tree-topology-from-markov-networks-fereshteh-r-dastjerdi-et-al-2024>(3/3 | 264/264) Polynomial-time derivation of optimal k-tree topology from Markov networks (Fereshteh R. Dastjerdi et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>