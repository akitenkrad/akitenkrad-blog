<!doctype html><html><head><title>arXiv @ 2024.04.07</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.07"><meta property="og:description" content="Primary Categories cs.AI (4) cs.AR (1) cs.CC (1) cs.CE (1) cs.CG (1) cs.CL (23) cs.CR (9) cs.CV (26) cs.CY (2) cs.DB (1) cs.DS (2) cs.GR (1) cs.HC (2) cs.IR (2) cs.LG (24) cs.NE (1) cs.NI (1) cs.PL (2) cs.RO (10) cs.SE (1) eess.IV (2) eess.SY (4) math.NA (2) math.OC (1) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG cs.RO Active Learning 1 Alpaca 1 Anomaly Detection 1 Benchmarking 8 10 8 Black Box 1 Code Generation 1 Common-sense Reasoning 1 Continual Learning 1 Contrastive Learning 1 Convolution 1 3 1 Convolutional Neural Network 5 3 Data Augmentation 1 2 2 Diffusion Model 3 1 1 Document Classification 1 Explainable AI 1 Fairness 2 Federated Learning 1 Few-shot 1 1 Few-shot Learning 1 Fine-tuning 5 2 3 1 GPT 2 GPT-2 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240407000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-07T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.07"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240407000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Apr 7, 2024</p></div><div class=title><h1>arXiv @ 2024.04.07</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csai-4>cs.AI (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscl-23>cs.CL (23)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscv-26>cs.CV (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cshc-2>cs.HC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csir-2>cs.IR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cslg-24>cs.LG (24)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csro-10>cs.RO (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csse-1>cs.SE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>8</td><td>10</td><td>8</td><td></td></tr><tr><td>Black Box</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>5</td><td>3</td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>3</td><td>1</td><td>1</td></tr><tr><td>Document Classification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>5</td><td>2</td><td>3</td><td>1</td></tr><tr><td>GPT</td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-3</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>2</td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>1</td><td>2</td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>3</td><td>3</td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>22</td><td>1</td><td>2</td><td>4</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Masked Language Model</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td>3</td><td>2</td><td>2</td></tr><tr><td>Natural Language Inference</td><td>6</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Noise-tolerant</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Online Reinforcement Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Open Information Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>5</td><td>4</td><td></td><td>1</td></tr><tr><td>Prompt Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>3</td><td>2</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>5</td><td>1</td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Stance Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>2</td><td>2</td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscv-26>cs.CV (26)</h2><h3 id=126--1125-enhancing-breast-cancer-diagnosis-in-mammography-evaluation-and-integration-of-convolutional-neural-networks-and-explainable-ai-maryam-ahmed-et-al-2024>(1/26 | 1/125) Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI (Maryam Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir. (2024)<br><strong>Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI</strong><br><button class=copy-to-clipboard title="Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 91<br>Keywords: Black Box, Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Explainable AI, Fairness, Fine-tuning, Multi-modal, Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03892v1.pdf filename=2404.03892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study introduces an integrated framework combining <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Explainable</b> <b>Artificial</b> Intelligence (XAI) for the enhanced diagnosis of breast cancer using the CBIS-DDSM dataset. Utilizing a <b>fine-tuned</b> ResNet50 architecture, our investigation not only provides effective differentiation of mammographic images into benign and malignant categories but also addresses the opaque <b>&ldquo;black-box&rdquo;</b> <b>nature</b> of deep learning models by employing XAI methodologies, namely Grad-CAM, LIME, and SHAP, to interpret <b>CNN</b> decision-making processes for healthcare professionals. Our methodology encompasses an elaborate <b>data</b> <b>preprocessing</b> pipeline and advanced <b>data</b> <b>augmentation</b> techniques to counteract dataset limitations, and <b>transfer</b> <b>learning</b> using pre-trained networks, such as VGG-16, DenseNet and ResNet was employed. A focal point of our study is the evaluation of XAI&rsquo;s effectiveness in interpreting model predictions, highlighted by utilising the Hausdorff measure to assess the alignment between AI-generated explanations and expert annotations quantitatively. This approach plays a critical role for XAI in promoting trustworthiness and ethical <b>fairness</b> in AI-assisted diagnostics. The findings from our research illustrate the effective collaboration between <b>CNNs</b> and XAI in advancing diagnostic methods for breast cancer, thereby facilitating a more seamless integration of advanced AI technologies within clinical settings. By enhancing the interpretability of AI-driven decisions, this work lays the groundwork for improved collaboration between AI systems and medical practitioners, ultimately enriching patient care. Furthermore, the implications of our research extend well beyond the current methodologies, advocating for subsequent inquiries into the integration of <b>multimodal</b> <b>data</b> <b>and</b> the refinement of AI explanations to satisfy the needs of clinical practice.</p></p class="citation"></blockquote><h3 id=226--2125-image-text-co-decomposition-for-text-supervised-semantic-segmentation-ji-jia-wu-et-al-2024>(2/26 | 2/125) Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation (Ji-Jia Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Yung-Yu Chuang, Yen-Yu Lin. (2024)<br><strong>Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Image2text, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04231v1.pdf filename=2404.04231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses text-supervised semantic segmentation, aiming to learn a model capable of segmenting arbitrary visual concepts within images by using only <b>image-text</b> pairs without dense annotations. Existing methods have demonstrated that <b>contrastive</b> <b>learning</b> on <b>image-text</b> pairs effectively aligns visual segments with the meanings of texts. We notice that there is a discrepancy between text alignment and semantic segmentation: A text often consists of multiple semantic concepts, whereas semantic segmentation strives to create semantically homogeneous segments. To address this issue, we propose a novel framework, <b>Image-Text</b> Co-Decomposition (CoDe), where the paired image and text are jointly decomposed into a set of image regions and a set of word segments, respectively, and <b>contrastive</b> <b>learning</b> is developed to enforce region-word alignment. To work with a <b>vision-language</b> model, we present a <b>prompt</b> <b>learning</b> mechanism that derives an extra representation to highlight an image segment or a word segment of interest, with which more effective features can be extracted from that segment. Comprehensive experimental results demonstrate that our method performs favorably against existing text-supervised semantic segmentation methods on six <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=326--3125-dynamic-prompt-optimizing-for-text-to-image-generation-wenyi-mo-et-al-2024>(3/26 | 3/125) Dynamic Prompt Optimizing for Text-to-Image Generation (Wenyi Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang. (2024)<br><strong>Dynamic Prompt Optimizing for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Dynamic Prompt Optimizing for Text-to-Image Generation" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Online Reinforcement Learning, Reinforcement Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04095v1.pdf filename=2404.04095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generative models, specifically those based on <b>diffusion</b> <b>models</b> like Imagen and Stable <b>Diffusion,</b> <b>have</b> made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text <b>prompts.</b> Users assign weights or alter the injection time steps of certain words in the text <b>prompts</b> to improve the quality of generated images. However, the success of fine-control <b>prompts</b> depends on the accuracy of the text <b>prompts</b> and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the \textbf{P}rompt \textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the original <b>prompts</b> for image generation, we further employ an <b>online</b> <b>reinforcement</b> <b>learning</b> strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control <b>prompts.</b> The reward function during training encourages the model to consider aesthetic score, semantic consistency, and user preferences. Experimental results demonstrate that our proposed method effectively improves the original <b>prompts,</b> generating visually more appealing images while maintaining semantic alignment. Code is available at <a href=https://github.com/Mowenyii/PAE>https://github.com/Mowenyii/PAE</a>.</p></p class="citation"></blockquote><h3 id=426--4125-learning-correlation-structures-for-vision-transformers-manjin-kim-et-al-2024>(4/26 | 4/125) Learning Correlation Structures for Vision Transformers (Manjin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manjin Kim, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho. (2024)<br><strong>Learning Correlation Structures for Vision Transformers</strong><br><button class=copy-to-clipboard title="Learning Correlation Structures for Vision Transformers" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03924v1.pdf filename=2404.03924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new attention mechanism, dubbed structural <b>self-attention</b> (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via <b>convolution</b> and uses them to dynamically aggregate local contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural <b>vision</b> <b>transformer</b> (StructViT) and evaluate its effectiveness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-1K, Kinetics-400, Something-Something V1 & V2, Diving-48, and FineGym.</p></p class="citation"></blockquote><h3 id=526--5125-sigma-siamese-mamba-network-for-multi-modal-semantic-segmentation-zifu-wan-et-al-2024>(5/26 | 5/125) Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation (Zifu Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zifu Wan, Yuhao Wang, Silong Yong, Pingping Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie. (2024)<br><strong>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Convolutional Neural Network, Multi-modal, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04256v1.pdf filename=2404.04256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> semantic segmentation significantly enhances AI agents&rsquo; perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable segmentation. In this work, we introduce Sigma, a Siamese Mamba network for <b>multi-modal</b> semantic segmentation, utilizing the Selective Structured State Space Model, Mamba. Unlike conventional methods that rely on <b>CNNs,</b> with their limited local receptive fields, or <b>Vision</b> <b>Transformers</b> (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields coverage with linear complexity. By employing a Siamese encoder and innovating a Mamba fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our method, Sigma, is rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in <b>multi-modal</b> perception tasks. Code is available at <a href=https://github.com/zifuwan/Sigma>https://github.com/zifuwan/Sigma</a>.</p></p class="citation"></blockquote><h3 id=626--6125-diffop-net-a-differential-operator-based-fully-convolutional-network-for-unsupervised-deformable-image-registration-jiong-wu-2024>(6/26 | 6/125) DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration (Jiong Wu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiong Wu. (2024)<br><strong>DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration</strong><br><button class=copy-to-clipboard title="DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04244v1.pdf filename=2404.04244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>unsupervised</b> <b>deformable</b> image registration methods usually rely on metrics applied to the gradients of predicted displacement or velocity fields as a regularization term to ensure transformation smoothness, which potentially limits registration accuracy. In this study, we propose a novel approach to enhance <b>unsupervised</b> <b>deformable</b> image registration by introducing a new differential operator into the registration framework. This operator, acting on the velocity field and mapping it to a dual space, ensures the smoothness of the velocity field during optimization, facilitating accurate deformable registration. In addition, to tackle the challenge of capturing large deformations inside image pairs, we introduce a Cross-Coordinate Attention module (CCA) and embed it into a proposed Fully <b>Convolutional</b> <b>Networks</b> (FCNs)-based multi-resolution registration architecture. Evaluation experiments are conducted on two magnetic resonance imaging (MRI) datasets. Compared to various state-of-the-art registration approaches, including a traditional algorithm and three representative <b>unsupervised</b> <b>learning-based</b> methods, our method achieves superior accuracies, maintaining desirable diffeomorphic properties, and exhibiting promising registration speed.</p></p class="citation"></blockquote><h3 id=726--7125-who-evaluates-the-evaluations-objectively-scoring-text-to-image-prompt-coherence-metrics-with-t2iscorescore-ts2-michael-saxon-et-al-2024>(7/26 | 7/125) Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2) (Michael Saxon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang. (2024)<br><strong>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</strong><br><button class=copy-to-clipboard title="Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Graph, Benchmarking, Benchmarking, Text2image, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04251v1.pdf filename=2404.04251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With advances in the quality of <b>text-to-image</b> (T2I) models has come interest in <b>benchmarking</b> their <b>prompt</b> faithfulness-the semantic coherence of generated images to the <b>prompts</b> they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and <b>vision-language</b> models (VLMs). However, these metrics are not rigorously compared and <b>benchmarked,</b> instead presented against few weak baselines by correlation to human Likert scores over a set of easy-to-discriminate images. We introduce T2IScoreScore (TS2), a curated set of semantic error <b>graphs</b> containing a <b>prompt</b> and a set increasingly erroneous images. These allow us to rigorously judge whether a given <b>prompt</b> faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I <b>prompt</b> faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.</p></p class="citation"></blockquote><h3 id=826--8125-identity-decoupling-for-multi-subject-personalization-of-text-to-image-models-sangwon-jang-et-al-2024>(8/26 | 8/125) Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models (Sangwon Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwon Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang. (2024)<br><strong>Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Data Augmentation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04243v1.pdf filename=2404.04243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have shown remarkable success in generating a personalized subject based on a few reference images. However, current methods struggle with handling multiple subjects simultaneously, often resulting in mixed identities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by the Segment Anything Model for both training and inference, as a form of <b>data</b> <b>augmentation</b> for training and initialization for the generation process. Our experiments demonstrate that MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. In human evaluation, MuDI shows twice as many successes for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% compared to the strongest baseline. More results are available at <a href=https://mudi-t2i.github.io/>https://mudi-t2i.github.io/</a>.</p></p class="citation"></blockquote><h3 id=926--9125-physical-property-understanding-from-language-embedded-feature-fields-albert-j-zhai-et-al-2024>(9/26 | 9/125) Physical Property Understanding from Language-Embedded Feature Fields (Albert J. Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, Shenlong Wang. (2024)<br><strong>Physical Property Understanding from Language-Embedded Feature Fields</strong><br><button class=copy-to-clipboard title="Physical Property Understanding from Language-Embedded Feature Fields" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04242v1.pdf filename=2404.04242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can computers perceive the physical properties of objects solely through vision? Research in cognitive science and vision science has shown that humans excel at identifying materials and estimating their physical properties based purely on visual appearance. In this paper, we present a novel approach for dense prediction of the physical properties of objects using a collection of images. Inspired by how humans reason about physics through vision, we leverage <b>large</b> <b>language</b> <b>models</b> to propose candidate materials for each object. We then construct a language-embedded point cloud and estimate the physical properties of each 3D point using a <b>zero-shot</b> kernel regression approach. Our method is accurate, annotation-free, and applicable to any object in the open world. Experiments demonstrate the effectiveness of the proposed approach in various physical property <b>reasoning</b> tasks, such as estimating the mass of common objects, as well as other properties like friction and hardness.</p></p class="citation"></blockquote><h3 id=1026--10125-rasim-a-range-aware-high-fidelity-rgb-d-data-simulation-pipeline-for-real-world-applications-xingyu-liu-et-al-2024>(10/26 | 10/125) RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications (Xingyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Liu, Chenyangguang Zhang, Gu Wang, Ruida Zhang, Xiangyang Ji. (2024)<br><strong>RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications</strong><br><button class=copy-to-clipboard title="RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03962v1.pdf filename=2404.03962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a range-aware RGB-D data <b>simulation</b> pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any <b>finetuning</b> and excel at downstream RGB-D perception tasks.</p></p class="citation"></blockquote><h3 id=1126--11125-concept-weaver-enabling-multi-concept-fusion-in-text-to-image-models-gihyun-kwon-et-al-2024>(11/26 | 11/125) Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models (Gihyun Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, Fabian Caba Heilbron. (2024)<br><strong>Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03913v1.pdf filename=2404.03913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While there has been significant progress in customizing <b>text-to-image</b> generation models, generating images that combine multiple personalized concepts remains challenging. In this work, we introduce Concept Weaver, a method for composing customized <b>text-to-image</b> <b>diffusion</b> <b>models</b> at inference time. Specifically, the method breaks the process into two steps: creating a template image aligned with the semantics of input <b>prompts,</b> and then personalizing the template using a concept fusion strategy. The fusion strategy incorporates the appearance of the target concepts into the template image while retaining its structural details. The results indicate that our method can generate multiple custom concepts with higher identity fidelity compared to alternative approaches. Furthermore, the method is shown to seamlessly handle more than two concepts and closely follow the semantic meaning of the input <b>prompt</b> without blending appearances across different subjects.</p></p class="citation"></blockquote><h3 id=1226--12125-3d-facial-expressions-through-analysis-by-neural-synthesis-george-retsinas-et-al-2024>(12/26 | 12/125) 3D Facial Expressions through Analysis-by-Neural-Synthesis (George Retsinas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Retsinas, Panagiotis P. Filntisis, Radek Danecek, Victoria F. Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos. (2024)<br><strong>3D Facial Expressions through Analysis-by-Neural-Synthesis</strong><br><button class=copy-to-clipboard title="3D Facial Expressions through Analysis-by-Neural-Synthesis" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Reconstruction Loss, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04104v1.pdf filename=2404.04104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While existing methods for 3D face <b>reconstruction</b> <b>from</b> in-the-wild images excel at recovering the overall face shape, they commonly miss subtle, extreme, asymmetric, or rarely observed expressions. We improve upon these methods with SMIRK (Spatial Modeling for Image-based <b>Reconstruction</b> <b>of</b> Kinesics), which faithfully reconstructs expressive 3D faces from images. We identify two key limitations in existing methods: shortcomings in their <b>self-supervised</b> training formulation, and a lack of expression diversity in the training images. For training, most methods employ differentiable rendering to compare a predicted face mesh with the input image, along with a plethora of additional loss functions. This differentiable rendering loss not only has to provide supervision to optimize for 3D face <b>geometry,</b> camera, albedo, and lighting, which is an ill-posed optimization problem, but the domain gap between rendering and input image further hinders the learning process. Instead, SMIRK replaces the differentiable rendering with a neural rendering module that, given the rendered predicted mesh <b>geometry,</b> and sparsely sampled pixels of the input image, generates a face image. As the neural rendering gets color information from sampled image pixels, supervising with neural rendering-based <b>reconstruction</b> <b>loss</b> can focus solely on the <b>geometry.</b> Further, it enables us to generate images of the input identity with varying expressions while training. These are then utilized as input to the <b>reconstruction</b> <b>model</b> and used as supervision with ground truth <b>geometry.</b> This effectively augments the training data and enhances the generalization for diverse expressions. Our qualitative, quantitative and particularly our perceptual evaluations demonstrate that SMIRK achieves the new state-of-the art performance on accurate expression <b>reconstruction.</b> <b>Project</b> webpage: <a href=https://georgeretsi.github.io/smirk/>https://georgeretsi.github.io/smirk/</a>.</p></p class="citation"></blockquote><h3 id=1326--13125-label-propagation-for-zero-shot-classification-with-vision-language-models-vladan-stojnić-et-al-2024>(13/26 | 13/125) Label Propagation for Zero-shot Classification with Vision-Language Models (Vladan Stojnić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladan Stojnić, Yannis Kalantidis, Giorgos Tolias. (2024)<br><strong>Label Propagation for Zero-shot Classification with Vision-Language Models</strong><br><button class=copy-to-clipboard title="Label Propagation for Zero-shot Classification with Vision-Language Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04072v1.pdf filename=2404.04072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs) have demonstrated impressive performance on <b>zero-shot</b> classification, i.e. classification when provided merely with a list of class names. In this paper, we tackle the case of <b>zero-shot</b> classification in the presence of unlabeled data. We leverage the <b>graph</b> structure of the unlabeled data and introduce ZLaP, a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to <b>graphs</b> containing both text and image features and further propose an efficient method for performing inductive inference based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: <a href=https://github.com/vladan-stojnic/ZLaP>https://github.com/vladan-stojnic/ZLaP</a></p></p class="citation"></blockquote><h3 id=1426--14125-neural-symbolic-videoqa-learning-compositional-spatio-temporal-reasoning-for-real-world-video-question-answering-lili-liang-et-al-2024>(14/26 | 14/125) Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering (Lili Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang. (2024)<br><strong>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering</strong><br><button class=copy-to-clipboard title="Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04007v1.pdf filename=2404.04007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional spatio-temporal <b>reasoning</b> poses a significant challenge in the field of video <b>question</b> <b>answering</b> (VideoQA). Existing approaches struggle to establish effective symbolic <b>reasoning</b> structures, which are crucial for answering compositional spatio-temporal <b>questions.</b> <b>To</b> address this challenge, we propose a neural-symbolic framework called Neural-Symbolic VideoQA (NS-VideoQA), specifically designed for real-world VideoQA tasks. The uniqueness and superiority of NS-VideoQA are two-fold: 1) It proposes a Scene Parser Network (SPN) to transform static-dynamic video scenes into Symbolic Representation (SR), structuralizing persons, objects, relations, and action chronologies. 2) A Symbolic <b>Reasoning</b> Machine (SRM) is designed for top-down <b>question</b> <b>decompositions</b> and bottom-up compositional <b>reasonings.</b> Specifically, a polymorphic program executor is constructed for internally consistent <b>reasoning</b> from SR to the final answer. As a result, Our NS-VideoQA not only improves the compositional spatio-temporal <b>reasoning</b> in real-world VideoQA task, but also enables step-by-step error analysis by tracing the intermediate results. Experimental evaluations on the AGQA Decomp <b>benchmark</b> demonstrate the effectiveness of the proposed NS-VideoQA framework. Empirical studies further confirm that NS-VideoQA exhibits internal consistency in answering compositional <b>questions</b> <b>and</b> significantly improves the capability of spatio-temporal and logical inference for VideoQA tasks.</p></p class="citation"></blockquote><h3 id=1526--15125-physics-inspired-synthesized-underwater-image-dataset-reina-kaneko-et-al-2024>(15/26 | 15/125) Physics-Inspired Synthesized Underwater Image Dataset (Reina Kaneko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reina Kaneko, Hiroshi Higashi, Yuichi Tanaka. (2024)<br><strong>Physics-Inspired Synthesized Underwater Image Dataset</strong><br><button class=copy-to-clipboard title="Physics-Inspired Synthesized Underwater Image Dataset" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03998v1.pdf filename=2404.03998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the physics-inspired synthesized underwater image dataset (PHISWID), a dataset tailored for enhancing underwater image processing through physics-inspired image synthesis. Deep learning approaches to underwater image enhancement typically demand extensive datasets, yet acquiring paired clean and degraded underwater ones poses significant challenges. While several underwater image datasets have been proposed using physics-based synthesis, a publicly accessible collection has been lacking. Additionally, most underwater image synthesis approaches do not intend to reproduce atmospheric scenes, resulting in incomplete enhancement. PHISWID addresses this gap by offering a set of paired ground-truth (atmospheric) and synthetically degraded underwater images, showcasing not only color degradation but also the often-neglected effects of marine snow, a composite of organic matter and sand particles that considerably impairs underwater image clarity. The dataset applies these degradations to atmospheric RGB-D images, enhancing the dataset&rsquo;s realism and applicability. PHISWID is particularly valuable for training deep neural networks in a <b>supervised</b> <b>learning</b> setting and for objectively assessing image quality in <b>benchmark</b> analyses. Our results reveal that even a basic U-Net architecture, when trained with PHISWID, substantially outperforms existing methods in underwater image enhancement. We intend to release PHISWID publicly, contributing a significant resource to the advancement of underwater imaging technology.</p></p class="citation"></blockquote><h3 id=1626--16125-no-time-to-train-empowering-non-parametric-networks-for-few-shot-3d-scene-segmentation-xiangyang-zhu-et-al-2024>(16/26 | 16/125) No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation (Xiangyang Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao. (2024)<br><strong>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</strong><br><button class=copy-to-clipboard title="No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04050v1.pdf filename=2404.04050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to <b>few-shot</b> <b>learning.</b> Current 3D <b>few-shot</b> <b>segmentation</b> methods first pre-train models on &lsquo;seen&rsquo; classes, and then evaluate their generalization performance on &lsquo;unseen&rsquo; classes. However, the prior pre-training stage not only introduces excessive time overhead but also incurs a significant domain gap on &lsquo;unseen&rsquo; classes. To tackle these issues, we propose a Non-parametric Network for <b>few-shot</b> <b>3D</b> Segmentation, Seg-NN, and its Parametric variant, Seg-PN. Without training, Seg-NN extracts dense representations by hand-crafted filters and achieves comparable performance to existing parametric models. Due to the elimination of pre-training, Seg-NN can alleviate the domain gap issue and save a substantial amount of time. Based on Seg-NN, Seg-PN only requires training a lightweight QUEry-Support Transferring (QUEST) module, which enhances the interaction between the support set and query set. Experiments suggest that Seg-PN outperforms previous state-of-the-art method by +4.19% and +7.71% mIoU on S3DIS and ScanNet datasets respectively, while reducing training time by -90%, indicating its effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=1726--17125-instructhumans-editing-animated-3d-human-textures-with-instructions-jiayin-zhu-et-al-2024>(17/26 | 17/125) InstructHumans: Editing Animated 3D Human Textures with Instructions (Jiayin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayin Zhu, Linlin Yang, Angela Yao. (2024)<br><strong>InstructHumans: Editing Animated 3D Human Textures with Instructions</strong><br><button class=copy-to-clipboard title="InstructHumans: Editing Animated 3D Human Textures with Instructions" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04037v1.pdf filename=2404.04037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present InstructHumans, a novel framework for instruction-driven 3D human texture editing. Existing text-based editing methods use Score <b>Distillation</b> Sampling (SDS) to <b>distill</b> guidance from generative models. This work shows that naively using such scores is harmful to editing as they destroy consistency with the source avatar. Instead, we propose an alternate SDS for Editing (SDS-E) that selectively incorporates subterms of SDS across diffusion timesteps. We further enhance SDS-E with spatial smoothness regularization and gradient-based viewpoint sampling to achieve high-quality edits with sharp and high-fidelity detailing. InstructHumans significantly outperforms existing 3D editing methods, consistent with the initial avatar while faithful to the textual instructions. Project page: <a href=https://jyzhu.top/instruct-humans>https://jyzhu.top/instruct-humans</a> .</p></p class="citation"></blockquote><h3 id=1826--18125-voltavision-a-transfer-learning-model-for-electronic-component-classification-anas-mohammad-ishfaqul-muktadir-osmani-et-al-2024>(18/26 | 18/125) VoltaVision: A Transfer Learning model for electronic component classification (Anas Mohammad Ishfaqul Muktadir Osmani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Mohammad Ishfaqul Muktadir Osmani, Taimur Rahman, Salekul Islam. (2024)<br><strong>VoltaVision: A Transfer Learning model for electronic component classification</strong><br><button class=copy-to-clipboard title="VoltaVision: A Transfer Learning model for electronic component classification" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03898v1.pdf filename=2404.03898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we analyze the effectiveness of <b>transfer</b> <b>learning</b> on classifying electronic components. <b>Transfer</b> <b>learning</b> reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight <b>CNN,</b> coined as VoltaVision, and compares its performance against more complex models. We test the hypothesis that transferring knowledge from a similar task to our target domain yields better results than state-of-the-art models trained on general datasets. Our dataset and code for this work are available at <a href=https://github.com/AnasIshfaque/VoltaVision>https://github.com/AnasIshfaque/VoltaVision</a>.</p></p class="citation"></blockquote><h3 id=1926--19125-increasing-fairness-in-classification-of-out-of-distribution-data-for-facial-recognition-gianluca-barone-et-al-2024>(19/26 | 19/125) Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition (Gianluca Barone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Barone, Aashrit Cunchala, Rudy Nunez. (2024)<br><strong>Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition</strong><br><button class=copy-to-clipboard title="Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Fairness, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03876v1.pdf filename=2404.03876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data <b>(&ldquo;out-of-distribution</b> data&rdquo;) which is different from data in the training distribution(&ldquo;in-distribution&rdquo;). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of <b>out-of-distribution</b> data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model&rsquo;s performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine&rsquo;s emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. We also tested the datasets in reverse order to see whether a more fair dataset with balanced features has an effect on the model&rsquo;s accuracy.</p></p class="citation"></blockquote><h3 id=2026--20125-improving-detection-in-aerial-images-by-capturing-inter-object-relationships-botao-ren-et-al-2024>(20/26 | 20/125) Improving Detection in Aerial Images by Capturing Inter-Object Relationships (Botao Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botao Ren, Botian Xu, Yifan Pu, Jingyi Wang, Zhidong Deng. (2024)<br><strong>Improving Detection in Aerial Images by Capturing Inter-Object Relationships</strong><br><button class=copy-to-clipboard title="Improving Detection in Aerial Images by Capturing Inter-Object Relationships" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04140v1.pdf filename=2404.04140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many image domains, the spatial distribution of objects in a scene exhibits meaningful patterns governed by their semantic relationships. In most modern detection pipelines, however, the detection proposals are processed independently, overlooking the underlying relationships between objects. In this work, we introduce a <b>transformer-based</b> approach to capture these inter-object relationships to refine classification and regression outcomes for detected objects. Building on two-stage detectors, we tokenize the region of interest (RoI) proposals to be processed by a <b>transformer</b> encoder. Specific spatial and geometric relations are incorporated into the attention weights and adaptively modulated and regularized. Experimental results demonstrate that the proposed method achieves consistent performance improvement on three <b>benchmarks</b> including DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on both DOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59 mAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016, respectively, compared to the baselines.</p></p class="citation"></blockquote><h3 id=2126--21125-scaresnet-a-resnet-variant-optimized-for-tiny-object-detection-in-transmission-and-distribution-towers-weile-li-et-al-2024>(21/26 | 21/125) SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers (Weile Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weile Li, Muqing Shi, Zhonghua Hong. (2024)<br><strong>SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers</strong><br><button class=copy-to-clipboard title="SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04179v1.pdf filename=2404.04179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional deep learning-based <b>object</b> <b>detection</b> networks often resize images during the data preprocessing stage to achieve a uniform size and scale in the feature map. Resizing is done to facilitate model propagation and fully connected classification. However, resizing inevitably leads to <b>object</b> <b>deformation</b> and loss of valuable information in the images. This drawback becomes particularly pronounced for tiny <b>objects</b> <b>like</b> distribution towers with linear shapes and few pixels. To address this issue, we propose abandoning the resizing operation. Instead, we introduce Positional-Encoding Multi-head Criss-Cross Attention. This allows the model to capture contextual information and learn from multiple representation subspaces, effectively enriching the semantics of distribution towers. Additionally, we enhance Spatial Pyramid Pooling by reshaping three pooled feature maps into a new unified one while also reducing the computational burden. This approach allows images of different sizes and scales to generate feature maps with uniform dimensions and can be employed in feature map propagation. Our SCAResNet incorporates these aforementioned improvements into the backbone network ResNet. We evaluated our SCAResNet using the Electric Transmission and Distribution Infrastructure Imagery dataset from Duke University. Without any additional tricks, we employed various <b>object</b> <b>detection</b> models with Gaussian Receptive Field based Label Assignment as the baseline. When incorporating the SCAResNet into the baseline model, we achieved a 2.1% improvement in mAPs. This demonstrates the advantages of our SCAResNet in detecting transmission and distribution towers and its value in tiny <b>object</b> <b>detection.</b> The source code is available at <a href=https://github.com/LisavilaLee/SCAResNet_mmdet>https://github.com/LisavilaLee/SCAResNet_mmdet</a>.</p></p class="citation"></blockquote><h3 id=2226--22125-marsseg-mars-surface-semantic-segmentation-with-multi-level-extractor-and-connector-junbo-li-et-al-2024>(22/26 | 22/125) MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector (Junbo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbo Li, Keyan Chen, Gengju Tian, Lu Li, Zhenwei Shi. (2024)<br><strong>MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector</strong><br><button class=copy-to-clipboard title="MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04155v1.pdf filename=2404.04155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The segmentation and interpretation of the Martian surface play a pivotal role in Mars exploration, providing essential data for the trajectory planning and obstacle avoidance of rovers. However, the complex topography, similar surface features, and the lack of extensive annotated data pose significant challenges to the high-precision semantic segmentation of the Martian surface. To address these challenges, we propose a novel encoder-decoder based Mars segmentation network, termed MarsSeg. Specifically, we employ an encoder-decoder structure with a minimized number of down-sampling layers to preserve local details. To facilitate a high-level semantic understanding across the shadow multi-level feature maps, we introduce a feature enhancement connection layer situated between the encoder and decoder. This layer incorporates Mini Atrous Spatial Pyramid Pooling (Mini-ASPP), Polarized <b>Self-Attention</b> (PSA), and Strip Pyramid Pooling Module (SPPM). The Mini-ASPP and PSA are specifically designed for shadow feature enhancement, thereby enabling the expression of local details and small objects. Conversely, the SPPM is employed for deep feature enhancement, facilitating the extraction of high-level semantic category-related information. Experimental results derived from the Mars-Seg and AI4Mars datasets substantiate that the proposed MarsSeg outperforms other state-of-the-art methods in segmentation performance, validating the efficacy of each proposed component.</p></p class="citation"></blockquote><h3 id=2326--23125-dynamic-risk-assessment-methodology-with-an-ldm-based-system-for-parking-scenarios-paola-natalia-cañas-et-al-2024>(23/26 | 23/125) Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios (Paola Natalia Cañas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paola Natalia Cañas, Mikel García, Nerea Aranjuelo, Marcos Nieto, Aitor Iglesias, Igor Rodríguez. (2024)<br><strong>Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios</strong><br><button class=copy-to-clipboard title="Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SY, cs.CV, eess-SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04040v1.pdf filename=2404.04040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the methodology for building a dynamic risk assessment for ADAS (Advanced Driving Assistance Systems) algorithms in parking scenarios, fusing exterior and interior perception for a better understanding of the scene and a more comprehensive risk estimation. This includes the definition of a dynamic risk methodology that depends on the situation from inside and outside the vehicle, the creation of a multi-sensor dataset of risk assessment for ADAS <b>benchmarking</b> purposes, and a Local Dynamic Map (LDM) that fuses data from the exterior and interior of the car to build an LDM-based Dynamic Risk Assessment System (DRAS).</p></p class="citation"></blockquote><h3 id=2426--24125-finsler-laplace-beltrami-operators-with-application-to-shape-analysis-simon-weber-et-al-2024>(24/26 | 24/125) Finsler-Laplace-Beltrami Operators with Application to Shape Analysis (Simon Weber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Weber, Thomas Dagès, Maolin Gao, Daniel Cremers. (2024)<br><strong>Finsler-Laplace-Beltrami Operators with Application to Shape Analysis</strong><br><button class=copy-to-clipboard title="Finsler-Laplace-Beltrami Operators with Application to Shape Analysis" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03999v1.pdf filename=2404.03999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Laplace-Beltrami operator (LBO) emerges from studying manifolds equipped with a Riemannian metric. It is often called the Swiss army knife of <b>geometry</b> processing as it allows to capture intrinsic shape information and gives rise to heat diffusion, geodesic distances, and a multitude of shape descriptors. It also plays a central role in geometric deep learning. In this work, we explore Finsler manifolds as a generalization of Riemannian manifolds. We revisit the Finsler heat equation and derive a Finsler heat kernel and a Finsler-Laplace-Beltrami Operator (FLBO): a novel theoretically justified anisotropic Laplace-Beltrami operator (ALBO). In experimental evaluations we demonstrate that the proposed FLBO is a valuable alternative to the traditional Riemannian-based LBO and ALBOs for spatial filtering and shape correspondence estimation. We hope that the proposed Finsler heat kernel and the FLBO will inspire further exploration of Finsler <b>geometry</b> in the computer vision community.</p></p class="citation"></blockquote><h3 id=2526--25125-robust-gaussian-splatting-françois-darmon-et-al-2024>(25/26 | 25/125) Robust Gaussian Splatting (François Darmon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder. (2024)<br><strong>Robust Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Robust Gaussian Splatting" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04211v1.pdf filename=2404.04211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant <b>benchmark</b> datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.</p></p class="citation"></blockquote><h3 id=2626--26125-noisy-label-processing-for-classification-a-survey-mengting-li-et-al-2024>(26/26 | 26/125) Noisy Label Processing for Classification: A Survey (Mengting Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengting Li, Chuang Zhu. (2024)<br><strong>Noisy Label Processing for Classification: A Survey</strong><br><button class=copy-to-clipboard title="Noisy Label Processing for Classification: A Survey" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04159v1.pdf filename=2404.04159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep neural networks (DNNs) have gained remarkable achievement in computer vision tasks, and the success of DNNs often depends greatly on the richness of data. However, the acquisition process of data and high-quality ground truth requires a lot of manpower and money. In the long, tedious process of data annotation, annotators are prone to make mistakes, resulting in incorrect labels of images, i.e., noisy labels. The emergence of noisy labels is inevitable. Moreover, since research shows that DNNs can easily fit noisy labels, the existence of noisy labels will cause significant damage to the model training process. Therefore, it is crucial to combat noisy labels for computer vision tasks, especially for classification tasks. In this survey, we first comprehensively review the evolution of different deep learning approaches for noisy label combating in the image classification task. In addition, we also review different noise patterns that have been proposed to design robust algorithms. Furthermore, we explore the inner pattern of real-world label noise and propose an algorithm to generate a synthetic label noise pattern guided by real-world data. We test the algorithm on the well-known real-world dataset CIFAR-10N to form a new real-world data-guided synthetic <b>benchmark</b> and evaluate some typical noise-robust methods on the <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=cscl-23>cs.CL (23)</h2><h3 id=123--27125-teaching-llama-a-new-language-through-cross-lingual-knowledge-transfer-hele-andra-kuulmets-et-al-2024>(1/23 | 27/125) Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer (Hele-Andra Kuulmets et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hele-Andra Kuulmets, Taido Purason, Agnes Luhtaru, Mark Fishel. (2024)<br><strong>Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer</strong><br><button class=copy-to-clipboard title="Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Knowledge Transfer, Alpaca, LLaMA, Common-sense Reasoning, Instruction Following, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04042v1.pdf filename=2404.04042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores cost-efficient methods to adapt pretrained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to new lower-resource languages, with a specific focus on Estonian. Leveraging the <b>Llama</b> 2 model, we investigate the impact of combining cross-lingual <b>instruction-tuning</b> <b>with</b> additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual <b>instruction-tuning</b> <b>significantly</b> enhances results on Estonian. Furthermore, we showcase cross-lingual <b>knowledge</b> <b>transfer</b> from high-quality English <b>instructions</b> <b>to</b> Estonian, resulting in improvements in <b>commonsense</b> <b>reasoning</b> and multi-turn conversation capabilities. Our best model, named \textsc{Llammas}, represents the first open-source <b>instruction-following</b> <b>LLM</b> for Estonian. Additionally, we publish <b>Alpaca-est,</b> the first general task <b>instruction</b> <b>dataset</b> for Estonia. These contributions mark the initial progress in the direction of developing open-source <b>LLMs</b> for Estonian.</p></p class="citation"></blockquote><h3 id=223--28125-extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction-bowen-zhang-et-al-2024>(2/23 | 28/125) Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction (Bowen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhang, Harold Soh. (2024)<br><strong>Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</strong><br><button class=copy-to-clipboard title="Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 86<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Open Information Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03868v1.pdf filename=2404.03868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we are interested in automated methods for <b>knowledge</b> <b>graph</b> creation (KGC) from input text. Progress on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has <b>prompted</b> a series of recent works applying them to KGC, e.g., via zero/few-shot <b>prompting.</b> Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the <b>KG</b> schema has to be included in the <b>LLM</b> <b>prompt</b> to generate valid triplets; larger and more complex schema easily exceed the <b>LLMs&rsquo;</b> context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): <b>open</b> <b>information</b> <b>extraction</b> followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the <b>LLMs&rsquo;</b> extraction performance in a <b>retrieval-augmented</b> <b>generation-like</b> <b>manner.</b> We demonstrate on three KGC <b>benchmarks</b> that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.</p></p class="citation"></blockquote><h3 id=323--29125-simple-techniques-for-enhancing-sentence-embeddings-in-generative-language-models-bowen-zhang-et-al-2024>(3/23 | 29/125) Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models (Bowen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhang, Kehua Chang, Chunping Li. (2024)<br><strong>Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models</strong><br><button class=copy-to-clipboard title="Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, LLaMA, Mistral, Sentence Embedding, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03921v1.pdf filename=2404.03921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentence</b> <b>Embedding</b> stands as a fundamental task within the realm of Natural Language Processing, finding extensive application in search engines, expert systems, and question-and-answer platforms. With the continuous evolution of <b>large</b> <b>language</b> <b>models</b> such as <b>LLaMA</b> and <b>Mistral,</b> research on <b>sentence</b> <b>embedding</b> has recently achieved notable breakthroughs. However, these advancements mainly pertain to <b>fine-tuning</b> scenarios, leaving explorations into computationally efficient direct inference methods for <b>sentence</b> <b>representation</b> in a nascent stage. This paper endeavors to bridge this research gap. Through comprehensive experimentation, we challenge the widely held belief in the necessity of an Explicit One-word Limitation for deriving <b>sentence</b> <b>embeddings</b> from <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs).</b> We demonstrate that this approach, while beneficial for generative models under direct inference scenario, is not imperative for discriminative models or the <b>fine-tuning</b> of generative <b>PLMs.</b> This discovery sheds new light on the design of manual templates in future studies. Building upon this insight, we propose two innovative <b>prompt</b> engineering techniques capable of further enhancing the expressive power of <b>PLMs&rsquo;</b> raw embeddings: Pretended Chain of Thought and Knowledge Enhancement. We confirm their effectiveness across various <b>PLM</b> types and provide a detailed exploration of the underlying factors contributing to their success.</p></p class="citation"></blockquote><h3 id=423--30125-cleared-for-takeoff-compositional--conditional-reasoning-may-be-the-achilles-heel-to-flight-booking-language-agents-harsh-kohli-et-al-2024>(4/23 | 30/125) Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents (Harsh Kohli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Kohli, Huan Sun. (2024)<br><strong>Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents</strong><br><button class=copy-to-clipboard title="Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, GPT-4 turbo, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04237v1.pdf filename=2404.04237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid progress of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has seen them excel and frequently surpass human performance on standard <b>benchmarks.</b> This has enabled many downstream applications, such as <b>LLM</b> agents, to rely on their sophisticated <b>reasoning</b> to navigate complex task requirements. However, <b>LLMs</b> are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional <b>reasoning,</b> two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse <b>benchmark</b> connecting these <b>reasoning</b> skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art <b>LLMs</b> with even the best performing model, <b>GPT-4</b> <b>Turbo,</b> not exceeding 67% accuracy despite advanced <b>prompting</b> techniques.</p></p class="citation"></blockquote><h3 id=523--31125-ffn-skipllm-a-hidden-gem-for-autoregressive-decoding-with-adaptive-feed-forward-skipping-ajay-jaiswal-et-al-2024>(5/23 | 31/125) FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping (Ajay Jaiswal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajay Jaiswal, Bodun Hu, Lu Yin, Yeonju Ro, Shiwei Liu, Tianlong Chen, Aditya Akella. (2024)<br><strong>FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping</strong><br><button class=copy-to-clipboard title="FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, LLaMA, Neural Machine Translation, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03865v1.pdf filename=2404.03865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive <b>Large</b> <b>Language</b> <b>Models</b> (e.g., <b>LLaMa,</b> <b>GPTs)</b> are omnipresent achieving remarkable success in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges for autoregressive token-by-token generation. To mitigate computation overload incurred during generation, several early-exit and layer-dropping strategies have been proposed. Despite some promising success due to the redundancy across <b>LLMs</b> layers on metrics like Rough-L/BLUE, our careful knowledge-intensive evaluation unveils issues such as generation collapse, hallucination of wrong facts, and noticeable performance drop even at the trivial exit ratio of 10-15% of layers. We attribute these errors primarily to ineffective handling of the KV cache through state copying during early-exit. In this work, we observed the saturation of computationally expensive feed-forward blocks of <b>LLM</b> layers and proposed FFN-SkipLLM, which is a novel fine-grained skip strategy of autoregressive <b>LLMs.</b> More specifically, FFN-SkipLLM is an input-adaptive feed-forward skipping strategy that can skip 25-30% of FFN blocks of <b>LLMs</b> with marginal change in performance on knowledge-intensive generation tasks without any requirement to handle KV cache. Our extensive experiments and ablation across <b>benchmarks</b> like <b>MT-Bench,</b> Factoid-QA, and variable-length <b>text</b> <b>summarization</b> illustrate how our simple and ease-at-use method can facilitate faster autoregressive decoding.</p></p class="citation"></blockquote><h3 id=623--32125-seme-at-semeval-2024-task-2-comparing-masked-and-generative-language-models-on-natural-language-inference-for-clinical-trials-mathilde-aguiar-et-al-2024>(6/23 | 32/125) SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials (Mathilde Aguiar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi. (2024)<br><strong>SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials</strong><br><button class=copy-to-clipboard title="SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Natural Language Inference, Natural Language Inference, Textual Entailment, Large Language Model, Masked Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03977v1.pdf filename=2404.03977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trials. The Multi-evidence <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trial Data (NLI4CT) consists of a <b>Textual</b> <b>Entailment</b> (TE) task focused on the evaluation of the consistency and faithfulness of <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> models applied to Clinical Trial Reports (CTR). We test 2 distinct approaches, one based on <b>finetuning</b> and ensembling <b>Masked</b> <b>Language</b> <b>Models</b> and the other based on <b>prompting</b> <b>Large</b> <b>Language</b> <b>Models</b> using templates, in particular, using Chain-Of-Thought and Contrastive Chain-Of-Thought. <b>Prompting</b> Flan-T5-large in a 2-shot setting leads to our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56 Consistency.</p></p class="citation"></blockquote><h3 id=723--33125-buddie-a-business-document-dataset-for-multi-task-information-extraction-ran-zmigrod-et-al-2024>(7/23 | 33/125) BuDDIE: A Business Document Dataset for Multi-task Information Extraction (Ran Zmigrod et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Zmigrod, Dongsheng Wang, Mathieu Sibue, Yulong Pei, Petr Babkin, Ivan Brugere, Xiaomo Liu, Nacho Navarro, Antony Papadimitriou, William Watson, Zhiqiang Ma, Armineh Nourbakhsh, Sameena Shah. (2024)<br><strong>BuDDIE: A Business Document Dataset for Multi-task Information Extraction</strong><br><button class=copy-to-clipboard title="BuDDIE: A Business Document Dataset for Multi-task Information Extraction" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Multi-modal, Document Classification, Information Retrieval, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04003v1.pdf filename=2404.04003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of visually rich <b>document</b> <b>understanding</b> (VRDU) aims to solve a multitude of well-researched NLP tasks in a <b>multi-modal</b> domain. Several datasets exist for research on specific tasks of VRDU such as <b>document</b> <b>classification</b> (DC), key entity extraction (KEE), entity linking, <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA),</b> inter alia. These datasets cover <b>documents</b> <b>like</b> invoices and receipts with sparse annotations such that they support one or two co-related tasks (e.g., entity extraction and entity linking). Unfortunately, only focusing on a single specific of <b>documents</b> <b>or</b> task is not representative of how <b>documents</b> <b>often</b> need to be processed in the wild - where variety in style and requirements is expected. In this paper, we introduce BuDDIE (Business <b>Document</b> <b>Dataset</b> for <b>Information</b> <b>Extraction),</b> the first multi-task dataset of 1,665 real-world business <b>documents</b> <b>that</b> contains rich and dense annotations for DC, KEE, and <b>VQA.</b> Our dataset consists of publicly available business entity <b>documents</b> <b>from</b> US state government websites. The <b>documents</b> <b>are</b> structured and vary in their style and layout across states and types (e.g., forms, certificates, reports, etc.). We provide data variety and quality metrics for BuDDIE as well as a series of baselines for each task. Our baselines cover traditional textual, <b>multi-modal,</b> and <b>large</b> <b>language</b> <b>model</b> approaches to VRDU.</p></p class="citation"></blockquote><h3 id=823--34125-unlocking-parameter-efficient-fine-tuning-for-low-resource-language-translation-tong-su-et-al-2024>(8/23 | 34/125) Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation (Tong Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Su, Xin Peng, Sarubi Thillainathan, David Guzmán, Surangika Ranathunga, En-Shiun Annie Lee. (2024)<br><strong>Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation</strong><br><button class=copy-to-clipboard title="Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Low-Resource, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04212v1.pdf filename=2404.04212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) methods are increasingly vital in adapting large-scale <b>pre-trained</b> <b>language</b> <b>models</b> for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in <b>Low-Resource</b> Language (LRL) <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods.</p></p class="citation"></blockquote><h3 id=923--35125-investigating-the-robustness-of-modelling-decisions-for-few-shot-cross-topic-stance-detection-a-preregistered-study-myrthe-reuver-et-al-2024>(9/23 | 35/125) Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study (Myrthe Reuver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myrthe Reuver, Suzan Verberne, Antske Fokkens. (2024)<br><strong>Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study</strong><br><button class=copy-to-clipboard title="Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, RoBERTa, Natural Language Inference, Natural Language Inference, Stance Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03987v1.pdf filename=2404.03987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a viewpoint-diverse news recommender, identifying whether two news articles express the same viewpoint is essential. One way to determine &ldquo;same or different&rdquo; viewpoint is <b>stance</b> <b>detection.</b> In this paper, we investigate the robustness of operationalization choices for <b>few-shot</b> <b>stance</b> <b>detection,</b> with special attention to modelling <b>stance</b> <b>across</b> different topics. Our experiments test pre-registered hypotheses on <b>stance</b> <b>detection.</b> Specifically, we compare two <b>stance</b> <b>task</b> definitions (Pro/Con versus Same Side <b>Stance),</b> <b>two</b> <b>LLM</b> architectures (bi-encoding versus cross-encoding), and adding <b>Natural</b> <b>Language</b> <b>Inference</b> knowledge, with pre-trained <b>RoBERTa</b> models trained with shots of 100 examples from 7 different <b>stance</b> <b>detection</b> datasets. Some of our hypotheses and claims from earlier work can be confirmed, while others give more inconsistent results. The effect of the Same Side <b>Stance</b> <b>definition</b> on performance differs per dataset and is influenced by other modelling choices. We found no relationship between the number of training topics in the training shots and performance. In general, cross-encoding out-performs bi-encoding, and adding <b>NLI</b> training to our models gives considerable improvement, but these results are not consistent across all datasets. Our results indicate that it is essential to include multiple datasets and systematic modelling experiments when aiming to find robust modelling choices for the concept `stance'.</p></p class="citation"></blockquote><h3 id=1023--36125-do-sentence-transformers-learn-quasi-geospatial-concepts-from-general-text-ilya-ilyankou-et-al-2024>(10/23 | 36/125) Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text? (Ilya Ilyankou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilya Ilyankou, Aldo Lipani, Stefano Cavazzi, Xiaowei Gao, James Haworth. (2024)<br><strong>Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text?</strong><br><button class=copy-to-clipboard title="Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text?" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Recommendation, Zero-shot, Transformer, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04169v1.pdf filename=2404.04169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence <b>transformers</b> are language models designed to perform semantic search. This study investigates the capacity of sentence <b>transformers,</b> <b>fine-tuned</b> on general <b>question-answering</b> <b>datasets</b> for asymmetric semantic search, to associate descriptions of human-generated routes across Great Britain with queries often used to describe hiking experiences. We find that sentence <b>transformers</b> have some <b>zero-shot</b> capabilities to understand quasi-geospatial concepts, such as route types and difficulty, suggesting their potential utility for routing <b>recommendation</b> systems.</p></p class="citation"></blockquote><h3 id=1123--37125-data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving-gulsum-yigit-et-al-2024>(11/23 | 37/125) Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving (Gulsum Yigit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gulsum Yigit, Mehmet Fatih Amasyali. (2024)<br><strong>Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving</strong><br><button class=copy-to-clipboard title="Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, LLaMA, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03938v1.pdf filename=2404.03938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Math Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for <b>data</b> <b>augmentation</b> by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new <b>in-context</b> <b>learning</b> augmentation method, employing the <b>Llama-7b</b> language model. This approach involves instruction-based <b>prompting</b> for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.</p></p class="citation"></blockquote><h3 id=1223--38125-forget-nli-use-a-dictionary-zero-shot-topic-classification-for-low-resource-languages-with-application-to-luxembourgish-fred-philippy-et-al-2024>(12/23 | 38/125) Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish (Fred Philippy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fred Philippy, Shohreh Haddadan, Siwen Guo. (2024)<br><strong>Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish</strong><br><button class=copy-to-clipboard title="Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Low-Resource, Zero-shot, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03912v1.pdf filename=2404.03912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In NLP, <b>zero-shot</b> classification (ZSC) is the task of assigning labels to textual data without any labeled examples for the target classes. A common method for ZSC is to <b>fine-tune</b> a language model on a <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> dataset and then use it to infer the entailment between the input document and the target labels. However, this approach faces certain challenges, particularly for languages with limited resources. In this paper, we propose an alternative solution that leverages dictionaries as a source of data for ZSC. We focus on Luxembourgish, a <b>low-resource</b> language spoken in Luxembourg, and construct two new topic relevance classification datasets based on a dictionary that provides various synonyms, word translations and example sentences. We evaluate the usability of our dataset and compare it with the <b>NLI-based</b> approach on two topic classification tasks in a <b>zero-shot</b> manner. Our results show that by using the dictionary-based dataset, the trained models outperform the ones following the <b>NLI-based</b> approach for ZSC. While we focus on a single <b>low-resource</b> language in this study, we believe that the efficacy of our approach can also transfer to other languages where such a dictionary is available.</p></p class="citation"></blockquote><h3 id=1323--39125-saas-solving-ability-amplification-strategy-for-enhanced-mathematical-reasoning-in-large-language-models-hyeonwoo-kim-et-al-2024>(13/23 | 39/125) SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models (Hyeonwoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim, Wonseok Lee, Chanjun Park. (2024)<br><strong>SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03887v1.pdf filename=2404.03887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel learning approach designed to enhance both <b>mathematical</b> <b>reasoning</b> and problem-solving abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We focus on integrating the Chain-of-Thought (CoT) and the Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning of <b>mathematical</b> <b>reasoning</b> ability is helpful for the amplification of problem-solving ability. Thus, the initial learning with CoT is essential for solving challenging <b>mathematical</b> <b>problems.</b> To this end, we propose a sequential learning approach, named SAAS (Solving Ability Amplification Strategy), which strategically transitions from CoT learning to PoT learning. Our empirical study, involving an extensive performance comparison using several <b>benchmarks,</b> demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The results underscore the effectiveness of our sequential learning approach, marking a significant advancement in the field of <b>mathematical</b> <b>reasoning</b> in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1423--40125-verifiable-by-design-aligning-language-models-to-quote-from-pre-training-data-jingyu-zhang-et-al-2024>(14/23 | 40/125) Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data (Jingyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi. (2024)<br><strong>Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data</strong><br><button class=copy-to-clipboard title="Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-domain, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03862v1.pdf filename=2404.03862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For humans to trust the fluent generations of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post-hoc provenance. However, such citations are prone to mistakes that further complicate their verifiability. To address these limitations, we tackle the verifiability goal with a different philosophy: we trivialize the verification process by developing models that quote verbatim statements from trusted sources in pre-training data. We propose Quote-Tuning, which demonstrates the feasibility of aligning <b>LLMs</b> to leverage memorized information and quote from pre-training data. Quote-Tuning quantifies quoting against <b>large</b> <b>corpora</b> <b>with</b> efficient membership inference tools, and uses the amount of quotes as an implicit reward signal to construct a synthetic preference dataset for quoting, without any human annotation. Next, the target model is aligned to quote using preference optimization algorithms. Experimental results show that Quote-Tuning significantly increases the percentage of <b>LLM</b> generation quoted verbatim from high-quality pre-training documents by 55% to 130% relative to untuned models while maintaining response quality. Further experiments demonstrate that Quote-Tuning generalizes quoting to <b>out-of-domain</b> data, is applicable in different tasks, and provides additional benefits to truthfulness. Quote-Tuning not only serves as a hassle-free method to increase quoting but also opens up avenues for improving <b>LLM</b> trustworthiness through better verifiability.</p></p class="citation"></blockquote><h3 id=1523--41125-benchmarking-and-improving-compositional-generalization-of-multi-aspect-controllable-text-generation-tianqi-zhong-et-al-2024>(15/23 | 41/125) Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation (Tianqi Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, Zhendong Mao. (2024)<br><strong>Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation</strong><br><button class=copy-to-clipboard title="Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Meta Learning, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04232v1.pdf filename=2404.04232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional generalization, representing the model&rsquo;s ability to generate <b>text</b> <b>with</b> new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable <b>text</b> <b>generation</b> (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation <b>benchmark</b> of MCTG is still lacking. We propose CompMCTG, a <b>benchmark</b> encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce <b>Meta-MCTG,</b> <b>a</b> training framework incorporating <b>meta-learning,</b> <b>where</b> we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of <b>Meta-MCTG</b> <b>through</b> achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4% cases.</p></p class="citation"></blockquote><h3 id=1623--42125-chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model-xinrun-du-et-al-2024>(16/23 | 42/125) Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model (Xinrun Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang. (2024)<br><strong>Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</strong><br><button class=copy-to-clipboard title="Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04167v1.pdf filename=2404.04167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce CT-LLM, a 2B <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> that illustrates a pivotal shift towards prioritizing the Chinese language in developing <b>LLMs.</b> Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model&rsquo;s exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training <b>LLMs</b> predominantly on English corpora and then adapting them to other languages, broadening the horizons for <b>LLM</b> training methodologies. By open-sourcing the full process of training a Chinese <b>LLM,</b> including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case <b>Benchmark</b> (CHC-Bench), and the 2B-size Chinese Tiny <b>LLM</b> (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.</p></p class="citation"></blockquote><h3 id=1723--43125-clue-a-clinical-language-understanding-evaluation-for-llms-amin-dada-et-al-2024>(17/23 | 43/125) CLUE: A Clinical Language Understanding Evaluation for LLMs (Amin Dada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek. (2024)<br><strong>CLUE: A Clinical Language Understanding Evaluation for LLMs</strong><br><button class=copy-to-clipboard title="CLUE: A Clinical Language Understanding Evaluation for LLMs" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04067v1.pdf filename=2404.04067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical <b>LLMs</b> address healthcare-specific challenges, including privacy demands and computational constraints. However, evaluation of these models has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. Additionally, there has been no thorough comparison between biomedical and general-domain <b>LLMs</b> for clinical tasks. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a <b>benchmark</b> tailored to evaluate <b>LLMs</b> on real-world clinical tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters and four existing tasks designed to test the practical applicability of <b>LLMs</b> in healthcare settings. Our evaluation covers several biomedical and general domain <b>LLMs,</b> providing insights into their clinical performance and applicability. CLUE represents a step towards a standardized approach to evaluating and developing <b>LLMs</b> in healthcare to align future model development with the real-world needs of clinical application. We publish our evaluation and data generation scripts: <a href=https://github.com/dadaamin/CLUE>https://github.com/dadaamin/CLUE</a></p></p class="citation"></blockquote><h3 id=1823--44125-assessing-the-quality-of-information-extraction-filip-seitl-et-al-2024>(18/23 | 44/125) Assessing the quality of information extraction (Filip Seitl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Seitl, Tomáš Kovářík, Soheyla Mirshahi, Jan Kryštůfek, Rastislav Dujava, Matúš Ondreička, Herbert Ullrich, Petr Gronat. (2024)<br><strong>Assessing the quality of information extraction</strong><br><button class=copy-to-clipboard title="Assessing the quality of information extraction" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04068v1.pdf filename=2404.04068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in <b>large</b> <b>language</b> <b>models</b> have notably enhanced the efficiency of <b>information</b> <b>extraction</b> from unstructured and semi-structured data sources. As these technologies become integral to various applications, establishing an objective measure for the quality of <b>information</b> <b>extraction</b> becomes imperative. However, the scarcity of labeled data presents significant challenges to this endeavor. In this paper, we introduce an automatic framework to assess the quality of the <b>information</b> <b>extraction</b> and its completeness. The framework focuses on <b>information</b> <b>extraction</b> in the form of entity and its properties. We discuss how to handle the input/output size limitations of the <b>large</b> <b>language</b> <b>models</b> and analyze their performance when iteratively extracting the <b>information.</b> <b>Finally,</b> we introduce metrics to evaluate the quality of the extraction and provide an extensive discussion on how to interpret the metrics.</p></p class="citation"></blockquote><h3 id=1923--45125-willkommens-merkel-chaos-johnson-and-tore-klose-modeling-the-evaluative-meaning-of-german-personal-name-compounds-annerose-eichel-et-al-2024>(19/23 | 45/125) Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds (Annerose Eichel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annerose Eichel, Tana Deeg, André Blessing, Milena Belosevic, Sabine Arndt-Lappe, Sabine Schulte im Walde. (2024)<br><strong>Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds</strong><br><button class=copy-to-clipboard title="Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04031v1.pdf filename=2404.04031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a comprehensive computational study of the under-investigated phenomenon of personal name compounds (PNCs) in German such as Willkommens-Merkel (&lsquo;Welcome-Merkel&rsquo;). Prevalent in news, social media, and political discourse, PNCs are hypothesized to exhibit an evaluative function that is reflected in a more positive or negative perception as compared to the respective personal full name (such as Angela Merkel). We model 321 PNCs and their corresponding full names at discourse level, and show that PNCs bear an evaluative nature that can be captured through a variety of computational methods. Specifically, we assess through valence information whether a PNC is more positively or negatively evaluative than the person&rsquo;s name, by applying and comparing two approaches using (i) valence norms and (ii) <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> We further enrich our data with personal, domain-specific, and extra-linguistic information and perform a range of regression analyses revealing that factors including compound and modifier valence, domain, and political party membership influence how a PNC is evaluated.</p></p class="citation"></blockquote><h3 id=2023--46125-how-lexical-is-bilingual-lexicon-induction-harsh-kohli-et-al-2024>(20/23 | 46/125) How Lexical is Bilingual Lexicon Induction? (Harsh Kohli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Kohli, Helian Feng, Nicholas Dronen, Calvin McCarter, Sina Moeini, Ali Kebarighotbi. (2024)<br><strong>How Lexical is Bilingual Lexicon Induction?</strong><br><button class=copy-to-clipboard title="How Lexical is Bilingual Lexicon Induction?" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04221v1.pdf filename=2404.04221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contemporary machine learning approaches to bilingual lexicon induction (BLI), a model learns a mapping between the embedding spaces of a language pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art results on the task. However, the problem remains challenging in <b>low-resource</b> settings, due to the paucity of data. The task is complicated by factors such as lexical variation across languages. We argue that the incorporation of additional lexical information into the recent retrieve-and-rank approach should improve lexicon induction. We demonstrate the efficacy of our proposed approach on XLING, improving over the previous state of the art by an average of 2% across all language pairs.</p></p class="citation"></blockquote><h3 id=2123--47125-social-skill-training-with-large-language-models-diyi-yang-et-al-2024>(21/23 | 47/125) Social Skill Training with Large Language Models (Diyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S. Bernstein, John Mitchell. (2024)<br><strong>Social Skill Training with Large Language Models</strong><br><button class=copy-to-clipboard title="Social Skill Training with Large Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04204v1.pdf filename=2404.04204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages <b>large</b> <b>language</b> <b>models</b> for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.</p></p class="citation"></blockquote><h3 id=2223--48125-bear-a-unified-framework-for-evaluating-relational-knowledge-in-causal-and-masked-language-models-jacek-wiland-et-al-2024>(22/23 | 48/125) BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models (Jacek Wiland et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacek Wiland, Max Ploner, Alan Akbik. (2024)<br><strong>BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</strong><br><button class=copy-to-clipboard title="BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04113v1.pdf filename=2404.04113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to <b>masked</b> <b>or</b> <b>causal</b> LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM&rsquo;s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.</p></p class="citation"></blockquote><h3 id=2323--49125-a-bi-consolidating-model-for-joint-relational-triple-extraction-xiaocheng-luo-et-al-2024>(23/23 | 49/125) A Bi-consolidating Model for Joint Relational Triple Extraction (Xiaocheng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaocheng Luo, Yanping Chen, Ruixue Tang, Ruizhang Huang, Yongbin Qin. (2024)<br><strong>A Bi-consolidating Model for Joint Relational Triple Extraction</strong><br><button class=copy-to-clipboard title="A Bi-consolidating Model for Joint Relational Triple Extraction" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03881v1.pdf filename=2404.03881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current methods to extract relational triples directly make a prediction based on a possible entity pair in a raw sentence without depending on entity recognition. The task suffers from a serious semantic overlapping problem, in which several relation triples may share one or two entities in a sentence. It is weak to learn discriminative semantic features relevant to a relation triple. In this paper, based on a two-dimensional sentence representation, a bi-consolidating model is proposed to address this problem by simultaneously reinforcing the local and global semantic features relevant to a relation triple. This model consists of a local consolidation component and a global consolidation component. The first component uses a pixel difference <b>convolution</b> to enhance semantic information of a possible triple representation from adjacent regions and mitigate noise in neighbouring neighbours. The second component strengthens the triple representation based a channel attention and a spatial attention, which has the advantage to learn remote semantic dependencies in a sentence. They are helpful to improve the performance of both entity identification and relation type classification in relation triple extraction. After evaluated on several publish datasets, it achieves competitive performance. Analytical experiments demonstrate the effectiveness of our model for relational triple extraction and give motivation for other natural language processing tasks.</p></p class="citation"></blockquote><h2 id=csro-10>cs.RO (10)</h2><h3 id=110--50125-can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning-gawon-choi-et-al-2024>(1/10 | 50/125) Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning (Gawon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gawon Choi, Hyemin Ahn. (2024)<br><strong>Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning</strong><br><button class=copy-to-clipboard title="Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 90<br>Keywords: Fine-tuning, GPT-2, GPT-3, GPT-3.5, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03891v1.pdf filename=2404.03891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotics, the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is becoming prevalent, especially for understanding human commands. In particular, <b>LLMs</b> are utilized as domain-agnostic task planners for high-level human commands. <b>LLMs</b> are capable of Chain-of-Thought (CoT) <b>reasoning,</b> and this allows <b>LLMs</b> to be task planners. However, we need to consider that modern robots still struggle to perform complex actions, and the domains where robots can be deployed are limited in practice. This leads us to pose a question: If small LMs can be trained to reason in chains within a single domain, would even small LMs be good task planners for the robots? To train smaller LMs to reason in chains, we build `COmmand-STeps datasets&rsquo; (COST) consisting of high-level commands along with corresponding actionable low-level steps, via <b>LLMs.</b> We release not only our datasets but also the <b>prompt</b> templates used to generate them, to allow anyone to build datasets for their domain. We compare <b>GPT3.5</b> and <b>GPT4</b> with the <b>finetuned</b> <b>GPT2</b> for task domains, in tabletop and kitchen environments, and the result shows that <b>GPT2-medium</b> is comparable to <b>GPT3.5</b> for task planning in a specific domain. Our dataset, code, and more output samples can be found in <a href=https://github.com/Gawon-Choi/small-LMs-Task-Planning>https://github.com/Gawon-Choi/small-LMs-Task-Planning</a></p></p class="citation"></blockquote><h3 id=210--51125-voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots-akhil-padmanabha-et-al-2024>(2/10 | 51/125) VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots (Akhil Padmanabha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson. (2024)<br><strong>VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots</strong><br><button class=copy-to-clipboard title="VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CL, cs-HC, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04066v1.pdf filename=2404.04066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating <b>LLMs</b> as interfaces to robots for high level task planning and <b>code</b> <b>generation</b> have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating <b>LLMs</b> as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using <b>LLMs</b> as speech interfaces for assistive robots. Videos and supporting files are located on our project website: <a href=https://sites.google.com/andrew.cmu.edu/voicepilot/>https://sites.google.com/andrew.cmu.edu/voicepilot/</a></p></p class="citation"></blockquote><h3 id=310--52125-pomdp-guided-active-force-based-search-for-robotic-insertion-chen-wang-et-al-2024>(3/10 | 52/125) POMDP-Guided Active Force-Based Search for Robotic Insertion (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Haoxiang Luo, Kun Zhang, Hua Chen, Jia Pan, Wei Zhang. (2024)<br><strong>POMDP-Guided Active Force-Based Search for Robotic Insertion</strong><br><button class=copy-to-clipboard title="POMDP-Guided Active Force-Based Search for Robotic Insertion" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03943v1.pdf filename=2404.03943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic insertion tasks where the uncertainty exceeds the allowable tolerance, a good search strategy is essential for successful insertion and significantly influences efficiency. The commonly used blind search method is time-consuming and does not exploit the rich contact information. In this paper, we propose a novel search strategy that actively utilizes the information contained in the contact configuration and shows high efficiency. In particular, we formulate this problem as a Partially Observable <b>Markov</b> <b>Decision</b> <b>Process</b> (POMDP) with carefully designed primitives based on an in-depth analysis of the contact configuration&rsquo;s static stability. From the formulated POMDP, we can derive a novel search strategy. Thanks to its simplicity, this search strategy can be incorporated into a Finite-State-Machine (FSM) controller. The behaviors of the FSM controller are realized through a low-level Cartesian Impedance Controller. Our method is based purely on the robot&rsquo;s proprioceptive sensing and does not need visual or tactile sensors. To evaluate the effectiveness of our proposed strategy and control framework, we conduct extensive comparison experiments in <b>simulation,</b> where we compare our method with the baseline approach. The results demonstrate that our proposed method achieves a higher success rate with a shorter search time and search trajectory length compared to the baseline method. Additionally, we show that our method is robust to various initial displacement errors.</p></p class="citation"></blockquote><h3 id=410--53125-continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation-lanpei-li-et-al-2024>(4/10 | 53/125) Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation (Lanpei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanpei Li, Enrico Donato, Vincenzo Lomonaco, Egidio Falotico. (2024)<br><strong>Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation</strong><br><button class=copy-to-clipboard title="Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04219v1.pdf filename=2404.04219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dexterous manipulation, often facilitated by multi-fingered robotic hands, holds solid impact for real-world applications. Soft robotic hands, due to their compliant nature, offer flexibility and adaptability during object grasping and manipulation. Yet, benefits come with challenges, particularly in the control development for finger coordination. <b>Reinforcement</b> <b>Learning</b> (RL) can be employed to train object-specific in-hand manipulation policies, but limiting adaptability and generalizability. We introduce a Continual Policy <b>Distillation</b> (CPD) framework to acquire a versatile controller for in-hand manipulation, to rotate different objects in shape and size within a four-fingered soft gripper. The framework leverages Policy <b>Distillation</b> (PD) to transfer knowledge from expert policies to a continually evolving student policy network. Exemplar-based rehearsal methods are then integrated to mitigate catastrophic forgetting and enhance generalization. The performance of the CPD framework over various replay strategies demonstrates its effectiveness in consolidating knowledge from multiple experts and achieving versatile and adaptive behaviours for in-hand manipulation tasks.</p></p class="citation"></blockquote><h3 id=510--54125-scaling-motion-forecasting-models-with-ensemble-distillation-scott-ettinger-et-al-2024>(5/10 | 54/125) Scaling Motion Forecasting Models with Ensemble Distillation (Scott Ettinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Ettinger, Kratarth Goel, Avikalp Srivastava, Rami Al-Rfou. (2024)<br><strong>Scaling Motion Forecasting Models with Ensemble Distillation</strong><br><button class=copy-to-clipboard title="Scaling Motion Forecasting Models with Ensemble Distillation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03843v1.pdf filename=2404.03843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion forecasting has become an increasingly critical component of autonomous robotic systems. Onboard compute budgets typically limit the accuracy of real-time systems. In this work we propose methods of improving motion forecasting systems subject to limited compute budgets by combining model ensemble and <b>distillation</b> techniques. The use of ensembles of deep neural networks has been shown to improve generalization accuracy in many application domains. We first demonstrate significant performance gains by creating a large ensemble of optimized single models. We then develop a generalized framework to <b>distill</b> motion forecasting model ensembles into small student models which retain high performance with a fraction of the computing cost. For this study we focus on the task of motion forecasting using real world data from autonomous driving systems. We develop ensemble models that are very competitive on the Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards. From these ensembles, we train <b>distilled</b> student models which have high performance at a fraction of the compute costs. These experiments demonstrate <b>distillation</b> from ensembles as an effective method for improving accuracy of predictive models for robotic systems with limited compute budgets.</p></p class="citation"></blockquote><h3 id=610--55125-tooleenet-tool-affordance-6d-pose-estimation-yunlong-wang-et-al-2024>(6/10 | 55/125) ToolEENet: Tool Affordance 6D Pose Estimation (Yunlong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Wang, Lei Zhang, Yuyang Tu, Hui Zhang, Kaixin Bai, Zhaopeng Chen, Jianwei Zhang. (2024)<br><strong>ToolEENet: Tool Affordance 6D Pose Estimation</strong><br><button class=copy-to-clipboard title="ToolEENet: Tool Affordance 6D Pose Estimation" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04193v1.pdf filename=2404.04193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of robotic dexterous hands utilizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tool&rsquo;s pose when grasped, as occlusion by the hand often degrades the quality of the estimation. Additionally, the tool&rsquo;s overall pose often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided, contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool&rsquo;s end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for accurate 6D pose estimation of the tool&rsquo;s EE. This framework begins by segmenting the tool&rsquo;s EE from raw RGBD data, then uses a <b>diffusion</b> <b>model-based</b> pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. All data and codes are available on the project website: <a href=https://yuyangtu.github.io/projectToolEENet.html>https://yuyangtu.github.io/projectToolEENet.html</a></p></p class="citation"></blockquote><h3 id=710--56125-designing-robots-to-help-women-martin-cooney-et-al-2024>(7/10 | 56/125) Designing Robots to Help Women (Martin Cooney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Cooney, Lena Klasén, Fernando Alonso-Fernandez. (2024)<br><strong>Designing Robots to Help Women</strong><br><button class=copy-to-clipboard title="Designing Robots to Help Women" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04123v1.pdf filename=2404.04123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots are being designed to help people in an increasing variety of settings&ndash;but seemingly little attention has been given so far to the specific needs of women, who represent roughly half of the world&rsquo;s population but are highly underrepresented in robotics. Here we used a speculative prototyping approach to explore this expansive design space: First, we identified some potential challenges of interest, including crimes and illnesses that disproportionately affect women, as well as potential opportunities for designers, which were visualized in five sketches. Then, one of the sketched scenarios was further explored by developing a prototype, of a robotic helper drone equipped with computer vision to detect hidden cameras that could be used to spy on women. While <b>object</b> <b>detection</b> introduced some errors, hidden cameras were identified with a reasonable accuracy of 80% (Intersection over Union (IoU) score: 0.40). Our aim is that the identified challenges and opportunities could help spark discussion and inspire designers, toward realizing a safer, more inclusive future through responsible use of technology.</p></p class="citation"></blockquote><h3 id=810--57125-modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks-jordan-thompson-et-al-2024>(8/10 | 57/125) Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks (Jordan Thompson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Thompson, Brian Y. Cho, Daniel S. Brown, Alan Kuntz. (2024)<br><strong>Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks</strong><br><button class=copy-to-clipboard title="Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04241v1.pdf filename=2404.04241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tendon-driven continuum robot kinematic models are frequently computationally expensive, inaccurate due to unmodeled effects, or both. In particular, unmodeled effects produce uncertainties that arise during the robot&rsquo;s operation that lead to variability in the resulting <b>geometry.</b> We propose a novel solution to these issues through the development of a Gaussian mixture kinematic model. We train a mixture density network to output a Gaussian mixture model representation of the robot <b>geometry</b> given the current tendon displacements. This model computes a probability distribution that is more representative of the true distribution of geometries at a given configuration than a model that outputs a single <b>geometry,</b> while also reducing the computation time. We demonstrate one use of this model through a trajectory optimization method that explicitly reasons about the workspace uncertainty to minimize the probability of collision.</p></p class="citation"></blockquote><h3 id=910--58125-multi-modal-perception-for-soft-robotic-interactions-using-generative-models-enrico-donato-et-al-2024>(9/10 | 58/125) Multi-modal perception for soft robotic interactions using generative models (Enrico Donato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrico Donato, Egidio Falotico, Thomas George Thuruthel. (2024)<br><strong>Multi-modal perception for soft robotic interactions using generative models</strong><br><button class=copy-to-clipboard title="Multi-modal perception for soft robotic interactions using generative models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04220v1.pdf filename=2404.04220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perception is essential for the active interaction of physical agents with the external environment. The integration of multiple sensory modalities, such as touch and vision, enhances this perceptual process, creating a more comprehensive and robust understanding of the world. Such fusion is particularly useful for highly deformable bodies such as soft robots. Developing a compact, yet comprehensive state representation from multi-sensory inputs can pave the way for the development of complex control strategies. This paper introduces a perception model that harmonizes data from diverse modalities to build a holistic state representation and assimilate essential information. The model relies on the causality between sensory input and robotic actions, employing a generative model to efficiently compress fused information and predict the next observation. We present, for the first time, a study on how touch can be predicted from vision and proprioception on soft robots, the importance of the cross-modal generation and why this is essential for soft robotic interactions in unstructured environments.</p></p class="citation"></blockquote><h3 id=1010--59125-mm-gaussian-3d-gaussian-based-multi-modal-fusion-for-localization-and-reconstruction-in-unbounded-scenes-chenyang-wu-et-al-2024>(10/10 | 59/125) MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes (Chenyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang. (2024)<br><strong>MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes</strong><br><button class=copy-to-clipboard title="MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04026v1.pdf filename=2404.04026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera <b>multi-modal</b> fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h2 id=csir-2>cs.IR (2)</h2><h3 id=12--60125-a-comparison-of-methods-for-evaluating-generative-ir-negar-arabzadeh-et-al-2024>(1/2 | 60/125) A Comparison of Methods for Evaluating Generative IR (Negar Arabzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Negar Arabzadeh, Charles L. A. Clarke. (2024)<br><strong>A Comparison of Methods for Evaluating Generative IR</strong><br><button class=copy-to-clipboard title="A Comparison of Methods for Evaluating Generative IR" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04044v1.pdf filename=2404.04044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> <b>systems</b> <b>increasingly</b> incorporate generative components. For example, in a <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> system, a <b>retrieval</b> <b>component</b> <b>might</b> provide a source of ground truth, while a generative component <b>summarizes</b> and augments its responses. In other systems, a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> might directly generate responses without consulting a <b>retrieval</b> <b>component.</b> <b>While</b> there are multiple definitions of generative <b>information</b> <b>retrieval</b> <b>(Gen-IR)</b> <b>systems,</b> in this paper we focus on those systems where the system&rsquo;s response is not drawn from a fixed collection of documents or passages. The response to a query may be entirely new text never. Since traditional IR evaluation methods break down under this model, we explore various methods that extend traditional offline evaluation approaches to the Gen-IR context. Offline IR evaluation traditionally employs paid human assessors, but increasingly <b>LLMs</b> are replacing human assessment, demonstrating capabilities similar or superior to crowdsourced labels. Given that Gen-IR systems do not generate responses from a fixed set, we assume that methods for Gen-IR evaluation must largely depend on <b>LLM-generated</b> labels. Along with methods based on binary and graded relevance, we explore methods based on explicit subtopics, pairwise preferences, and embeddings. We first validate these methods against human assessments on several TREC Deep Learning Track tasks; we then apply these methods to evaluate the output of several purely generative systems. For each method we consider both its ability to act autonomously, without the need for human labels or other input, and its ability to support human auditing. To trust these methods, we must be assured that their results align with human assessments. In order to do so, evaluation criteria must be transparent, so that outcomes can be audited by human assessors.</p></p class="citation"></blockquote><h3 id=22--61125-dwell-in-the-beginning-how-language-models-embed-long-documents-for-dense-retrieval-joão-coelho-et-al-2024>(2/2 | 61/125) Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval (João Coelho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Coelho, Bruno Martins, João Magalhães, Jamie Callan, Chenyan Xiong. (2024)<br><strong>Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval</strong><br><button class=copy-to-clipboard title="Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 35<br>Keywords: Dense Retrieval, Fine-tuning, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04163v1.pdf filename=2404.04163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the existence of positional biases in <b>Transformer-based</b> models for text <b>representation</b> <b>learning,</b> particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of <b>representation</b> <b>learning.</b> We examine positional biases at various stages of training for an encoder-decoder model, including language model pre-training, contrastive pre-training, and contrastive <b>fine-tuning.</b> Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture early contents of the input, with <b>fine-tuning</b> further aggravating this effect.</p></p class="citation"></blockquote><h2 id=csai-4>cs.AI (4)</h2><h3 id=14--62125-kgexplainer-towards-exploring-connected-subgraph-explanations-for-knowledge-graph-completion-tengfei-ma-et-al-2024>(1/4 | 62/125) KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion (Tengfei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tengfei Ma, Xiang song, Wen Tao, Mufei Li, Jiani Zhang, Xiaoqin Pan, Jianxin Lin, Bosheng Song, xiangxiang Zeng. (2024)<br><strong>KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 51<br>Keywords: Graph, Graph Embedding, Benchmarking, Black Box, Knowledge Distillation, Knowledge Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03893v1.pdf filename=2404.03893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> <b>completion</b> (KGC) aims to alleviate the inherent incompleteness of <b>knowledge</b> <b>graphs</b> <b>(KGs),</b> which is a critical task for various applications, such as <b>recommendations</b> on the web. Although <b>knowledge</b> <b>graph</b> <b>embedding</b> (KGE) models have demonstrated superior predictive performance on KGC tasks, these models infer missing links in a <b>black-box</b> <b>manner</b> that lacks transparency and accountability, preventing researchers from developing accountable models. Existing KGE-based explanation methods focus on exploring key paths or isolated edges as explanations, which is information-less to reason target prediction. Additionally, the missing ground truth leads to these explanation methods being ineffective in quantitatively evaluating explored explanations. To overcome these limitations, we propose KGExplainer, a model-agnostic method that identifies connected subgraph explanations and <b>distills</b> an evaluator to assess them quantitatively. KGExplainer employs a perturbation-based greedy search algorithm to find key connected subgraphs as explanations within the local structure of target predictions. To evaluate the quality of the explored explanations, KGExplainer <b>distills</b> an evaluator from the target KGE model. By forwarding the explanations to the evaluator, our method can examine the fidelity of them. Extensive experiments on <b>benchmark</b> datasets demonstrate that KGExplainer yields promising improvement and achieves an optimal ratio of 83.3% in human evaluation.</p></p class="citation"></blockquote><h3 id=24--63125-intervention-assisted-policy-gradient-methods-for-online-stochastic-queuing-network-optimization-technical-report-jerrod-wigmore-et-al-2024>(2/4 | 63/125) Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report (Jerrod Wigmore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerrod Wigmore, Brooke Shrader, Eytan Modiano. (2024)<br><strong>Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report</strong><br><button class=copy-to-clipboard title="Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: F-2-2; I-2-6, cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04106v1.pdf filename=2404.04106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) offers a powerful approach to training neural network control policies for stochastic queuing networks (SQN). However, traditional DRL methods rely on offline <b>simulations</b> or static datasets, limiting their real-world application in SQN control. This work proposes Online Deep <b>Reinforcement</b> <b>Learning-based</b> Controls (ODRLC) as an alternative, where an intelligent agent interacts directly with a real environment and learns an optimal control policy from these online interactions. SQNs present a challenge for ODRLC due to the unbounded nature of the queues within the network resulting in an unbounded state-space. An unbounded state-space is particularly challenging for neural network policies as neural networks are notoriously poor at extrapolating to unseen states. To address this challenge, we propose an intervention-assisted framework that leverages strategic interventions from known stable policies to ensure the queue sizes remain bounded. This framework combines the learning power of neural networks with the guaranteed stability of classical control policies for SQNs. We introduce a method to design these intervention-assisted policies to ensure strong stability of the network. Furthermore, we extend foundational DRL theorems for intervention-assisted policies and develop two practical algorithms specifically for ODRLC of SQNs. Finally, we demonstrate through experiments that our proposed algorithms outperform both classical control approaches and prior ODRLC algorithms.</p></p class="citation"></blockquote><h3 id=34--64125-random-walk-in-random-permutation-set-theory-jiefeng-zhou-et-al-2024>(3/4 | 64/125) Random Walk in Random Permutation Set Theory (Jiefeng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiefeng Zhou, Zhen Li, Yong Deng. (2024)<br><strong>Random Walk in Random Permutation Set Theory</strong><br><button class=copy-to-clipboard title="Random Walk in Random Permutation Set Theory" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-IT, cs.AI, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03978v1.pdf filename=2404.03978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random walk is an explainable approach for modeling natural processes at the molecular level. The Random Permutation Set Theory (RPST) serves as a framework for uncertainty <b>reasoning,</b> extending the applicability of Dempster-Shafer Theory. Recent explorations indicate a promising link between RPST and random walk. In this study, we conduct an analysis and construct a random walk model based on the properties of RPST, with Monte Carlo <b>simulations</b> of such random walk. Our findings reveal that the random walk generated through RPST exhibits characteristics similar to those of a Gaussian random walk and can be transformed into a Wiener process through a specific limiting scaling procedure. This investigation establishes a novel connection between RPST and random walk theory, thereby not only expanding the applicability of RPST, but also demonstrating the potential for combining the strengths of both approaches to improve problem-solving abilities.</p></p class="citation"></blockquote><h3 id=44--65125-large-language-models-as-oracles-for-instantiating-ontologies-with-domain-specific-knowledge-giovanni-ciatto-et-al-2024>(4/4 | 65/125) Large language models as oracles for instantiating ontologies with domain-specific knowledge (Giovanni Ciatto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini. (2024)<br><strong>Large language models as oracles for instantiating ontologies with domain-specific knowledge</strong><br><button class=copy-to-clipboard title="Large language models as oracles for instantiating ontologies with domain-specific knowledge" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04108v1.pdf filename=2404.04108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background. Endowing intelligent systems with semantic data commonly requires designing and instantiating ontologies with domain-specific knowledge. Especially in the early phases, those activities are typically performed manually by human experts possibly leveraging on their own experience. The resulting process is therefore time-consuming, error-prone, and often biased by the personal background of the ontology designer. Objective. To mitigate that issue, we propose a novel domain-independent approach to automatically instantiate ontologies with domain-specific knowledge, by leveraging on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as oracles. Method. Starting from (i) an initial schema composed by inter-related classes andproperties and (ii) a set of query templates, our method queries the <b>LLM</b> multi- ple times, and generates instances for both classes and properties from its replies. Thus, the ontology is automatically filled with domain-specific knowledge, compliant to the initial schema. As a result, the ontology is quickly and automatically enriched with manifold instances, which experts may consider to keep, adjust, discard, or complement according to their own needs and expertise. Contribution. We formalise our method in general way and instantiate it over various <b>LLMs,</b> as well as on a concrete case study. We report experiments rooted in the nutritional domain where an ontology of food meals and their ingredients is semi-automatically instantiated from scratch, starting from a categorisation of meals and their relationships. There, we analyse the quality of the generated ontologies and compare ontologies attained by exploiting different <b>LLMs.</b> Finally, we provide a SWOT analysis of the proposed method.</p></p class="citation"></blockquote><h2 id=cslg-24>cs.LG (24)</h2><h3 id=124--66125-player2vec-a-language-modeling-approach-to-understand-player-behavior-in-games-tianze-wang-et-al-2024>(1/24 | 66/125) player2vec: A Language Modeling Approach to Understand Player Behavior in Games (Tianze Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianze Wang, Maryam Honari-Jahromi, Styliani Katsarou, Olga Mikheeva, Theodoros Panagiotakopoulos, Sahar Asadi, Oleg Smirnov. (2024)<br><strong>player2vec: A Language Modeling Approach to Understand Player Behavior in Games</strong><br><button class=copy-to-clipboard title="player2vec: A Language Modeling Approach to Understand Player Behavior in Games" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Recommendation, Self-supervised Learning, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04234v1.pdf filename=2404.04234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Methods for learning latent user representations from historical behavior logs have gained traction for <b>recommendation</b> tasks in e-commerce, content streaming, and other settings. However, this area still remains relatively underexplored in video and mobile gaming contexts. In this work, we present a novel method for overcoming this limitation by extending a long-range <b>Transformer</b> model from the natural language processing domain to player behavior data. We discuss specifics of behavior tracking in games and propose preprocessing and <b>tokenization</b> approaches by viewing in-game events in an analogous way to words in sentences, thus enabling learning player representations in a <b>self-supervised</b> manner in the absence of ground-truth annotations. We experimentally demonstrate the efficacy of the proposed approach in fitting the distribution of behavior events by evaluating intrinsic language modeling metrics. Furthermore, we qualitatively analyze the emerging structure of the learned embedding space and show its value for generating insights into behavior patterns to inform downstream applications.</p></p class="citation"></blockquote><h3 id=224--67125-robust-preference-optimization-with-provable-noise-tolerance-for-llms-xize-liang-et-al-2024>(2/24 | 67/125) Robust Preference Optimization with Provable Noise Tolerance for LLMs (Xize Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye. (2024)<br><strong>Robust Preference Optimization with Provable Noise Tolerance for LLMs</strong><br><button class=copy-to-clipboard title="Robust Preference Optimization with Provable Noise Tolerance for LLMs" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Noise-tolerant, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04102v1.pdf filename=2404.04102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The preference alignment aims to enable <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate responses that conform to human values, which is essential for developing general AI systems. Ranking-based methods &ndash; a promising class of alignment approaches &ndash; learn human preferences from datasets containing response pairs by optimizing the log-likelihood margins between preferred and dis-preferred responses. However, due to the inherent differences in annotators&rsquo; preferences, ranking labels of comparisons for response pairs are unavoidably noisy. This seriously hurts the reliability of existing ranking-based methods. To address this problem, we propose a provably <b>noise-tolerant</b> preference alignment method, namely RObust Preference Optimization (ROPO). To the best of our knowledge, ROPO is the first preference alignment method with noise-tolerance guarantees. The key idea of ROPO is to dynamically assign conservative gradient weights to response pairs with high label uncertainty, based on the log-likelihood margins between the responses. By effectively suppressing the gradients of noisy samples, our weighting strategy ensures that the expected risk has the same gradient direction independent of the presence and proportion of noise. Experiments on three open-ended <b>text</b> <b>generation</b> tasks with four base models ranging in size from 2.8B to 13B demonstrate that ROPO significantly outperforms existing ranking-based methods.</p></p class="citation"></blockquote><h3 id=324--68125-enhancing-iot-intelligence-a-transformer-based-reinforcement-learning-methodology-gaith-rjoub-et-al-2024>(3/24 | 68/125) Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology (Gaith Rjoub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaith Rjoub, Saidul Islam, Jamal Bentahar, Mohammed Amin Almaiah, Rana Alrawashdeh. (2024)<br><strong>Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology</strong><br><button class=copy-to-clipboard title="Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04205v1.pdf filename=2404.04205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of the Internet of Things (IoT) has led to an explosion of data generated by interconnected devices, presenting both opportunities and challenges for intelligent decision-making in complex environments. Traditional <b>Reinforcement</b> <b>Learning</b> (RL) approaches often struggle to fully harness this data due to their limited ability to process and interpret the intricate patterns and dependencies inherent in IoT applications. This paper introduces a novel framework that integrates <b>transformer</b> architectures with Proximal Policy Optimization (PPO) to address these challenges. By leveraging the <b>self-attention</b> mechanism of <b>transformers,</b> our approach enhances RL agents&rsquo; capacity for understanding and acting within dynamic IoT environments, leading to improved decision-making processes. We demonstrate the effectiveness of our method across various IoT scenarios, from smart home automation to industrial control systems, showing marked improvements in decision-making efficiency and adaptability. Our contributions include a detailed exploration of the <b>transformer&rsquo;s</b> role in processing heterogeneous IoT data, a comprehensive evaluation of the framework&rsquo;s performance in diverse environments, and a <b>benchmark</b> against traditional RL methods. The results indicate significant advancements in enabling RL agents to navigate the complexities of IoT ecosystems, highlighting the potential of our approach to revolutionize intelligent automation and decision-making in the IoT landscape.</p></p class="citation"></blockquote><h3 id=424--69125-score-identity-distillation-exponentially-fast-distillation-of-pretrained-diffusion-models-for-one-step-generation-mingyuan-zhou-et-al-2024>(4/24 | 69/125) Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation (Mingyuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, Hai Huang. (2024)<br><strong>Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation</strong><br><button class=copy-to-clipboard title="Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04057v1.pdf filename=2404.04057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Score identity <b>Distillation</b> (SiD), an innovative data-free method that <b>distills</b> the generative capabilities of pretrained <b>diffusion</b> <b>models</b> into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fr'echet inception distance (FID) during <b>distillation</b> but also approaches or even exceeds the FID performance of the original teacher <b>diffusion</b> <b>models.</b> By reformulating forward <b>diffusion</b> <b>processes</b> as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four <b>benchmark</b> datasets, the SiD algorithm demonstrates high iteration efficiency during <b>distillation</b> and surpasses competing <b>distillation</b> approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the <b>benchmarks</b> for efficiency and effectiveness in <b>diffusion</b> <b>distillation</b> but also in the broader field of <b>diffusion-based</b> <b>generation.</b> Our PyTorch implementation will be publicly accessible on GitHub.</p></p class="citation"></blockquote><h3 id=524--70125-mitigating-heterogeneity-in-federated-multimodal-learning-with-biomedical-vision-language-pre-training-zitao-shuai-et-al-2024>(5/24 | 70/125) Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training (Zitao Shuai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zitao Shuai, Liyue Shen. (2024)<br><strong>Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training</strong><br><button class=copy-to-clipboard title="Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 31<br>Keywords: Federated Learning, Multi-modal, Multi-modal, Representation Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03854v1.pdf filename=2404.03854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-training (VLP) has arised as an efficient scheme for <b>multimodal</b> <b>representation</b> <b>learning,</b> but it requires large-scale <b>multimodal</b> data for pre-training, making it an obstacle especially for biomedical applications. To overcome the data limitation, <b>federated</b> <b>learning</b> (FL) can be a promising strategy to scale up the dataset for biomedical VLP while protecting data privacy. However, client data are often heterogeneous in real-world scenarios, and we observe that local training on heterogeneous client data would distort the <b>multimodal</b> <b>representation</b> <b>learning</b> and lead to biased cross-modal alignment. To address this challenge, we propose <b>Federated</b> <b>distributional</b> Robust Guidance-Based (FedRGB) learning framework for <b>federated</b> <b>VLP</b> with robustness to data heterogeneity. Specifically, we utilize a guidance-based local training scheme to reduce feature distortions, and employ a distribution-based min-max optimization to learn unbiased cross-modal alignment. The experiments on real-world datasets show our method successfully promotes efficient <b>federated</b> <b>multimodal</b> learning for biomedical VLP with data heterogeneity.</p></p class="citation"></blockquote><h3 id=624--71125-transformers-for-molecular-property-prediction-lessons-learned-from-the-past-five-years-afnan-sultan-et-al-2024>(6/24 | 71/125) Transformers for molecular property prediction: Lessons learned from the past five years (Afnan Sultan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afnan Sultan, Jochen Sieg, Miriam Mathea, Andrea Volkamer. (2024)<br><strong>Transformers for molecular property prediction: Lessons learned from the past five years</strong><br><button class=copy-to-clipboard title="Transformers for molecular property prediction: Lessons learned from the past five years" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03969v1.pdf filename=2404.03969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Molecular Property Prediction (MPP) is vital for drug discovery, crop protection, and environmental science. Over the last decades, diverse computational techniques have been developed, from using simple physical and chemical properties and molecular fingerprints in statistical models and classical machine learning to advanced deep learning approaches. In this review, we aim to <b>distill</b> insights from current research on employing <b>transformer</b> models for MPP. We analyze the currently available models and explore key questions that arise when training and <b>fine-tuning</b> a <b>transformer</b> model for MPP. These questions encompass the choice and scale of the pre-training data, optimal architecture selections, and promising pre-training objectives. Our analysis highlights areas not yet covered in current research, inviting further exploration to enhance the field&rsquo;s understanding. Additionally, we address the challenges in comparing different models, emphasizing the need for standardized data splitting and robust statistical analysis.</p></p class="citation"></blockquote><h3 id=724--72125-optimizing-convolutional-neural-networks-for-identifying-invasive-pollinator-apis-mellifera-and-finding-a-ligand-drug-to-protect-californias-biodiversity-arnav-swaroop-2024>(7/24 | 72/125) Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity (Arnav Swaroop, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnav Swaroop. (2024)<br><strong>Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity</strong><br><button class=copy-to-clipboard title="Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California's Biodiversity" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM, q-bio-QM<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03870v1.pdf filename=2404.03870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In North America, there are many diverse species of native bees crucial for the environment, who are the primary pollinators of most native floral species. The Californian agriculture industry imports European honeybees (Apis Mellifera) primarily for pollinating almonds. Unfortunately, this has resulted in the unintended consequence of disrupting the native ecosystem and threatening many native bee species as they are outcompeted for food. Our first step for protecting the native species is identification with the use of a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> to differentiate common native bee species from invasive ones. Removing invasive colonies efficiently without harming native species is difficult as pesticides cause myriad diseases in native species. Our approach seeks to prevent the formation of new queens, causing the colony&rsquo;s collapse. Workers secrete royal jelly, a substance that causes fertility and longevity; it is fed to future honeybee queens. Targeting the production of this substance is safe as no native species use it; small organic molecules (ligands) prevent the proteins Apisimin and MRJP1 from combining and producing an oligomer used to form the substance. Ideal ligands bind to only one of these proteins preventing them from joining together: they have a high affinity for one receptor and a significantly lower affinity for the other. We optimized the <b>CNN</b> to provide a framework for creating Machine Learning models that excel at differentiating between subspecies of insects by measuring the effects of image alteration and class grouping on model performance. The <b>CNN</b> is able to achieve an accuracy of 82% in differentiating between invasive and native bee species; 3 ligands have been identified as effective. Our new approach offers a promising solution to curb the spread of invasive bees within California through an identification and neutralization method.</p></p class="citation"></blockquote><h3 id=824--73125-dynamic-conditional-optimal-transport-through-simulation-free-flows-gavin-kerrigan-et-al-2024>(8/24 | 73/125) Dynamic Conditional Optimal Transport through Simulation-Free Flows (Gavin Kerrigan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gavin Kerrigan, Giosue Migliorini, Padhraic Smyth. (2024)<br><strong>Dynamic Conditional Optimal Transport through Simulation-Free Flows</strong><br><button class=copy-to-clipboard title="Dynamic Conditional Optimal Transport through Simulation-Free Flows" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04240v1.pdf filename=2404.04240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the <b>geometry</b> of conditional optimal transport (COT) and prove a dynamical formulation which generalizes the Benamou-Brenier Theorem. With these tools, we propose a <b>simulation-free</b> flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan. We build on the framework of flow matching to train a conditional generative model by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in the infinite-dimensional setting, making them well suited for inverse problems. Empirically, we demonstrate our proposed method on two image-to-image translation tasks and an infinite-dimensional Bayesian inverse problem.</p></p class="citation"></blockquote><h3 id=924--74125-model-selection-with-model-zoo-via-graph-learning-ziyu-li-et-al-2024>(9/24 | 74/125) Model Selection with Model Zoo via Graph Learning (Ziyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Li, Hilco van der Wilk, Danning Zhan, Megha Khosla, Alessandro Bozzon, Rihan Hai. (2024)<br><strong>Model Selection with Model Zoo via Graph Learning</strong><br><button class=copy-to-clipboard title="Model Selection with Model Zoo via Graph Learning" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03988v1.pdf filename=2404.03988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to <b>fine-tune</b> can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a <b>graph</b> learning problem. TransferGraph constructs a <b>graph</b> using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph&rsquo;s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual <b>fine-tuning</b> results compared to the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1024--75125-exploring-probabilistic-models-for-semi-supervised-learning-jianfeng-wang-2024>(10/24 | 75/125) Exploring Probabilistic Models for Semi-supervised Learning (Jianfeng Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfeng Wang. (2024)<br><strong>Exploring Probabilistic Models for Semi-supervised Learning</strong><br><button class=copy-to-clipboard title="Exploring Probabilistic Models for Semi-supervised Learning" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04199v1.pdf filename=2404.04199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This thesis studies advanced <b>probabilistic</b> <b>models,</b> including both their theoretical foundations and practical applications, for different <b>semi-supervised</b> <b>learning</b> (SSL) tasks. The proposed <b>probabilistic</b> <b>methods</b> are able to improve the safety of AI systems in real applications by providing reliable uncertainty estimates quickly, and at the same time, achieve competitive performance compared to their deterministic counterparts. The experimental results indicate that the methods proposed in the thesis have great value in safety-critical areas, such as the autonomous driving or medical imaging analysis domain, and pave the way for the future discovery of highly effective and efficient <b>probabilistic</b> <b>approaches</b> in the SSL sector.</p></p class="citation"></blockquote><h3 id=1124--76125-fusing-dictionary-learning-and-support-vector-machines-for-unsupervised-anomaly-detection-paul-irofti-et-al-2024>(11/24 | 76/125) Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection (Paul Irofti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Irofti, Iulian-Andrei Hîji, Andrei Pătraşcu, Nicolae Cleju. (2024)<br><strong>Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04064v1.pdf filename=2404.04064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study in this paper the improvement of one-class support vector machines (OC-SVM) through sparse representation techniques for <b>unsupervised</b> <b>anomaly</b> <b>detection.</b> As Dictionary Learning (DL) became recently a common analysis technique that reveals hidden sparse patterns of data, our approach uses this insight to endow <b>unsupervised</b> detection with more control on pattern finding and dimensions. We introduce a new <b>anomaly</b> <b>detection</b> model that unifies the OC-SVM and DL residual functions into a single composite objective, subsequently solved through K-SVD-type iterative algorithms. A closed-form of the alternating K-SVD iteration is explicitly derived for the new composite model and practical implementable schemes are discussed. The standard DL model is adapted for the Dictionary Pair Learning (DPL) context, where the usual sparsity constraints are naturally eliminated. Finally, we extend both objectives to the more general setting that allows the use of kernel functions. The empirical convergence properties of the resulting algorithms are provided and an in-depth analysis of their parametrization is performed while also demonstrating their numerical performance in comparison with existing methods.</p></p class="citation"></blockquote><h3 id=1224--77125-rolling-the-dice-for-better-deep-learning-performance-a-study-of-randomness-techniques-in-deep-neural-networks-mohammed-ghaith-altarabichi-et-al-2024>(12/24 | 77/125) Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks (Mohammed Ghaith Altarabichi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Ghaith Altarabichi, Sławomir Nowaczyk, Sepideh Pashami, Peyman Sheikholharam Mashhadi, Julia Handl. (2024)<br><strong>Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks</strong><br><button class=copy-to-clipboard title="Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: MNIST, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03992v1.pdf filename=2404.03992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates how various randomization techniques impact Deep Neural Networks (DNNs). Randomization, like weight noise and dropout, aids in reducing overfitting and enhancing generalization, but their interactions are poorly understood. The study categorizes randomness techniques into four types and proposes new methods: adding noise to the loss function and random masking of gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter optimization, it explores optimal configurations across <b>MNIST,</b> FASHION-MNIST, CIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated, revealing <b>data</b> <b>augmentation</b> and weight initialization randomness as main performance contributors. Correlation analysis shows different optimizers prefer distinct randomization types. The complete implementation and dataset are available on GitHub.</p></p class="citation"></blockquote><h3 id=1324--78125-multi-task-learning-for-lung-sound--lung-disease-classification-suma-k-v-et-al-2024>(13/24 | 78/125) Multi-Task Learning for Lung sound & Lung disease classification (Suma K V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suma K V, Deepali Koppad, Preethi Kumar, Neha A Kantikar, Surabhi Ramesh. (2024)<br><strong>Multi-Task Learning for Lung sound & Lung disease classification</strong><br><button class=copy-to-clipboard title="Multi-Task Learning for Lung sound & Lung disease classification" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SD, cs.LG<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03908v1.pdf filename=2404.03908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, advancements in deep learning techniques have considerably enhanced the efficiency and accuracy of medical diagnostics. In this work, a novel approach using multi-task learning (MTL) for the simultaneous classification of lung sounds and lung diseases is proposed. Our proposed model leverages MTL with four different deep learning models such as 2D <b>CNN,</b> ResNet50, MobileNet and Densenet to extract relevant features from the lung sound recordings. The ICBHI 2017 Respiratory Sound Database was employed in the current study. The MTL for MobileNet model performed better than the other models considered, with an accuracy of74% for lung sound analysis and 91% for lung diseases classification. Results of the experimentation demonstrate the efficacy of our approach in classifying both lung sounds and lung diseases concurrently. In this study,using the demographic data of the patients from the database, risk level computation for Chronic Obstructive Pulmonary Disease is also carried out. For this computation, three machine learning algorithms namely <b>Logistic</b> <b>Regression,</b> SVM and Random Forest classifierswere employed. Among these ML algorithms, the Random Forest classifier had the highest accuracy of 92%.This work helps in considerably reducing the physician&rsquo;s burden of not just diagnosing the pathology but also effectively communicating to the patient about the possible causes or outcomes.</p></p class="citation"></blockquote><h3 id=1424--79125-a-proximal-policy-optimization-based-intelligent-home-solar-management-kode-creer-et-al-2024>(14/24 | 79/125) A proximal policy optimization based intelligent home solar management (Kode Creer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kode Creer, Imitiaz Parvez. (2024)<br><strong>A proximal policy optimization based intelligent home solar management</strong><br><button class=copy-to-clipboard title="A proximal policy optimization based intelligent home solar management" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03888v1.pdf filename=2404.03888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the smart grid, the prosumers can sell unused electricity back to the power grid, assuming the prosumers own renewable energy sources and storage units. The maximizing of their profits under a dynamic electricity market is a problem that requires intelligent planning. To address this, we propose a framework based on Proximal Policy Optimization (PPO) using recurrent rewards. By using the information about the rewards modeled effectively with PPO to maximize our objective, we were able to get over 30% improvement over the other naive algorithms in accumulating total profits. This shows promise in getting <b>reinforcement</b> <b>learning</b> algorithms to perform tasks required to plan their actions in complex domains like financial markets. We also introduce a novel method for embedding longs based on soliton waves that outperformed normal embedding in our use case with random floating point <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=1524--80125-heterogeneous-multi-agent-reinforcement-learning-for-zero-shot-scalable-collaboration-xudong-guo-et-al-2024>(15/24 | 80/125) Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration (Xudong Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Guo, Daming Shi, Junjie Yu, Wenhui Fan. (2024)<br><strong>Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration</strong><br><button class=copy-to-clipboard title="Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03869v1.pdf filename=2404.03869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of multi-agent systems, especially the success of multi-agent <b>reinforcement</b> <b>learning</b> (MARL), is reshaping our future across diverse domains like autonomous vehicle networks. However, MARL still faces significant challenges, particularly in achieving <b>zero-shot</b> scalability, which allows trained MARL models to be directly applied to unseen tasks with varying numbers of agents. In addition, real-world multi-agent systems usually contain agents with different functions and strategies, while the existing scalable MARL methods only have limited heterogeneity. To address this, we propose a novel MARL framework named Scalable and Heterogeneous Proximal Policy Optimization (SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL networks. we first leverage a latent network to adaptively learn strategy patterns for each agent. Second, we introduce a heterogeneous layer for decision-making, whose parameters are specifically generated by the learned latent variables. Our approach is scalable as all the parameters are shared except for the heterogeneous layer, and gains both inter-individual and temporal heterogeneity at the same time. We implement our approach based on the state-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is agnostic to the backbone and can be seamlessly plugged into any parameter-shared MARL method. SHPPO exhibits superior performance over the baselines such as MAPPO and HAPPO in classic MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing enhanced <b>zero-shot</b> scalability and offering insights into the learned latent representation&rsquo;s impact on team performance by visualization.</p></p class="citation"></blockquote><h3 id=1624--81125-gnnbench-fair-and-productive-benchmarking-for-single-gpu-gnn-system-yidong-gong-et-al-2024>(16/24 | 81/125) GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System (Yidong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidong Gong, Pradeep Kumar. (2024)<br><strong>GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System</strong><br><button class=copy-to-clipboard title="GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph Neural Network, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04118v1.pdf filename=2404.04118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We hypothesize that the absence of a standardized <b>benchmark</b> has allowed several fundamental pitfalls in <b>GNN</b> System design and evaluation that the community has overlooked. In this work, we propose GNNBench, a plug-and-play <b>benchmarking</b> platform focused on system innovation. GNNBench presents a new protocol to exchange their captive tensor data, supports custom classes in System APIs, and allows automatic integration of the same system module to many deep learning frameworks, such as PyTorch and TensorFlow. To demonstrate the importance of such a <b>benchmark</b> framework, we integrated several <b>GNN</b> systems. Our results show that integration with GNNBench helped us identify several measurement issues that deserve attention from the community.</p></p class="citation"></blockquote><h3 id=1724--82125-growing-q-networks-solving-continuous-control-tasks-with-adaptive-control-resolution-tim-seyde-et-al-2024>(17/24 | 82/125) Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution (Tim Seyde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela Rus. (2024)<br><strong>Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution</strong><br><button class=copy-to-clipboard title="Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04253v1.pdf filename=2404.04253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>reinforcement</b> <b>learning</b> approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control <b>benchmarks.</b> The underlying coarse action space discretizations often yield favourable exploration characteristics while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and energy efficiency, but action costs can be detrimental to exploration during early training. In this work, we aim to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution, taking advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that yield surprisingly strong performance on continuous control tasks.</p></p class="citation"></blockquote><h3 id=1824--83125-active-causal-learning-for-decoding-chemical-complexities-with-targeted-interventions-zachary-r-fox-et-al-2024>(18/24 | 83/125) Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions (Zachary R. Fox et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zachary R. Fox, Ayana Ghosh. (2024)<br><strong>Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions</strong><br><button class=copy-to-clipboard title="Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, physics-data-an, q-bio-BM<br>Keyword Score: 13<br>Keywords: Graph, Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04224v1.pdf filename=2404.04224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting and enhancing inherent properties based on molecular structures is paramount to design tasks in medicine, materials science, and environmental management. Most of the current machine learning and deep learning approaches have become standard for predictions, but they face challenges when applied across different datasets due to reliance on correlations between molecular representation and target properties. These approaches typically depend on large datasets to capture the diversity within the chemical space, facilitating a more accurate approximation, interpolation, or extrapolation of the chemical behavior of molecules. In our research, we introduce an <b>active</b> <b>learning</b> approach that discerns underlying cause-effect relationships through strategic sampling with the use of a <b>graph</b> loss function. This method identifies the smallest subset of the dataset capable of encoding the most information representative of a much larger chemical space. The identified causal relations are then leveraged to conduct systematic interventions, optimizing the design task within a chemical space that the models have not encountered previously. While our implementation focused on the QM9 quantum-chemical dataset for a specific design task-finding molecules with a large dipole moment-our <b>active</b> <b>causal</b> learning approach, driven by intelligent sampling and interventions, holds potential for broader applications in molecular, materials design and discovery.</p></p class="citation"></blockquote><h3 id=1924--84125-generalizable-temperature-nowcasting-with-physics-constrained-rnns-for-predictive-maintenance-of-wind-turbine-components-johannes-exenberger-et-al-2024>(19/24 | 84/125) Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components (Johannes Exenberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Exenberger, Matteo Di Salvo, Thomas Hirsch, Franz Wotawa, Gerald Schweiger. (2024)<br><strong>Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components</strong><br><button class=copy-to-clipboard title="Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04126v1.pdf filename=2404.04126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning plays an important role in the operation of current wind energy production systems. One central application is predictive maintenance to increase efficiency and lower electricity costs by reducing downtimes. Integrating physics-based knowledge in neural networks to enforce their physical plausibilty is a promising method to improve current approaches, but incomplete system information often impedes their application in real world scenarios. We describe a simple and efficient way for physics-constrained deep learning-based predictive maintenance for wind turbine gearbox bearings with partial system knowledge. The approach is based on temperature nowcasting constrained by physics, where unknown system coefficients are treated as learnable neural network parameters. Results show improved generalization performance to unseen environments compared to a baseline neural network, which is especially important in low data scenarios often encountered in real-world applications.</p></p class="citation"></blockquote><h3 id=2024--85125-continual-learning-with-weight-interpolation-jędrzej-kozal-et-al-2024>(20/24 | 85/125) Continual Learning with Weight Interpolation (Jędrzej Kozal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jędrzej Kozal, Jan Wasilewski, Bartosz Krawczyk, Michał Woźniak. (2024)<br><strong>Continual Learning with Weight Interpolation</strong><br><button class=copy-to-clipboard title="Continual Learning with Weight Interpolation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04002v1.pdf filename=2404.04002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> poses a fundamental challenge for modern machine learning systems, requiring models to adapt to new tasks while retaining knowledge from previous ones. Addressing this challenge necessitates the development of efficient algorithms capable of learning from data streams and accumulating knowledge over time. This paper proposes a novel approach to <b>continual</b> <b>learning</b> utilizing the weight consolidation method. Our method, a simple yet powerful technique, enhances robustness against catastrophic forgetting by interpolating between old and new model weights after each novel task, effectively merging two models to facilitate exploration of local minima emerging after arrival of new concepts. Moreover, we demonstrate that our approach can complement existing rehearsal-based replay approaches, improving their accuracy and further mitigating the forgetting phenomenon. Additionally, our method provides an intuitive mechanism for controlling the stability-plasticity trade-off. Experimental results showcase the significant performance enhancement to state-of-the-art experience replay algorithms the proposed weight consolidation approach offers. Our algorithm can be downloaded from <a href=https://github.com/jedrzejkozal/weight-interpolation-cl>https://github.com/jedrzejkozal/weight-interpolation-cl</a>.</p></p class="citation"></blockquote><h3 id=2124--86125-demonstration-guided-multi-objective-reinforcement-learning-junlin-lu-et-al-2024>(21/24 | 86/125) Demonstration Guided Multi-Objective Reinforcement Learning (Junlin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Lu, Patrick Mannion, Karl Mason. (2024)<br><strong>Demonstration Guided Multi-Objective Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Demonstration Guided Multi-Objective Reinforcement Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03997v1.pdf filename=2404.03997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-objective <b>reinforcement</b> <b>learning</b> (MORL) is increasingly relevant due to its resemblance to real-world scenarios requiring trade-offs between multiple objectives. Catering to diverse user preferences, traditional <b>reinforcement</b> <b>learning</b> faces amplified challenges in MORL. To address the difficulty of training policies from scratch in MORL, we introduce demonstration-guided multi-objective <b>reinforcement</b> <b>learning</b> (DG-MORL). This novel approach utilizes prior demonstrations, aligns them with user preferences via corner weight support, and incorporates a self-evolving mechanism to refine suboptimal demonstrations. Our empirical studies demonstrate DG-MORL&rsquo;s superiority over existing MORL algorithms, establishing its robustness and efficacy, particularly under challenging conditions. We also provide an upper bound of the algorithm&rsquo;s sample complexity.</p></p class="citation"></blockquote><h3 id=2224--87125-hierarchical-neural-additive-models-for-interpretable-demand-forecasts-leif-feddersen-et-al-2024>(22/24 | 87/125) Hierarchical Neural Additive Models for Interpretable Demand Forecasts (Leif Feddersen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leif Feddersen, Catherine Cleophas. (2024)<br><strong>Hierarchical Neural Additive Models for Interpretable Demand Forecasts</strong><br><button class=copy-to-clipboard title="Hierarchical Neural Additive Models for Interpretable Demand Forecasts" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04070v1.pdf filename=2404.04070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Demand forecasts are the crucial basis for numerous business decisions, ranging from inventory management to strategic facility planning. While machine learning (ML) approaches offer accuracy gains, their interpretability and acceptance are notoriously lacking. Addressing this dilemma, we introduce Hierarchical Neural Additive Models for time series (HNAM). HNAM expands upon Neural Additive Models (NAM) by introducing a time-series specific additive model with a level and interacting covariate components. Covariate interactions are only allowed according to a user-specified interaction hierarchy. For example, weekday effects may be estimated independently of other covariates, whereas a holiday effect may depend on the weekday and an additional promotion may depend on both former covariates that are lower in the interaction hierarchy. Thereby, HNAM yields an intuitive forecasting interface in which analysts can observe the contribution for each known covariate. We evaluate the proposed approach and <b>benchmark</b> its performance against other state-of-the-art machine learning and statistical models extensively on real-world retail data. The results reveal that HNAM offers competitive prediction performance whilst providing plausible explanations.</p></p class="citation"></blockquote><h3 id=2324--88125-derivative-free-tree-optimization-for-complex-systems-ye-wei-et-al-2024>(23/24 | 88/125) Derivative-free tree optimization for complex systems (Ye Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Wei, Bo Peng, Ruiwen Xie, Yangtao Chen, Yu Qin, Peng Wen, Stefan Bauer, Po-Yen Tung. (2024)<br><strong>Derivative-free tree optimization for complex systems</strong><br><button class=copy-to-clipboard title="Derivative-free tree optimization for complex systems" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04062v1.pdf filename=2404.04062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A tremendous range of design tasks in materials, physics, and biology can be formulated as finding the optimum of an objective function depending on many parameters without knowing its closed-form expression or the derivative. Traditional derivative-free optimization techniques often rely on strong assumptions about objective functions, thereby failing at optimizing non-convex systems beyond 100 dimensions. Here, we present a tree search method for derivative-free optimization that enables accelerated optimal design of high-dimensional complex systems. Specifically, we introduce stochastic tree expansion, dynamic upper confidence bound, and short-range backpropagation mechanism to evade local optimum, iteratively approximating the global optimum using machine learning models. This development effectively confronts the dimensionally challenging problems, achieving convergence to global optima across various <b>benchmark</b> functions up to 2,000 dimensions, surpassing the existing methods by 10- to 20-fold. Our method demonstrates wide applicability to a wide range of real-world complex systems spanning materials, physics, and biology, considerably outperforming state-of-the-art algorithms. This enables efficient autonomous knowledge discovery and facilitates self-driving virtual laboratories. Although we focus on problems within the realm of natural science, the advancements in optimization techniques achieved herein are applicable to a broader spectrum of challenges across all quantitative disciplines.</p></p class="citation"></blockquote><h3 id=2424--89125-approximate-umap-allows-for-high-rate-online-visualization-of-high-dimensional-data-streams-peter-wassenaar-et-al-2024>(24/24 | 89/125) Approximate UMAP allows for high-rate online visualization of high-dimensional data streams (Peter Wassenaar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Wassenaar, Pierre Guetschel, Michael Tangermann. (2024)<br><strong>Approximate UMAP allows for high-rate online visualization of high-dimensional data streams</strong><br><button class=copy-to-clipboard title="Approximate UMAP allows for high-rate online visualization of high-dimensional data streams" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-5-3; I-5-3; J-4, cs-AI, cs-HC, cs-LG, cs.LG, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04001v1.pdf filename=2404.04001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the BCI field, introspection and interpretation of brain signals are desired for providing feedback or to guide rapid paradigm prototyping but are challenging due to the high noise level and dimensionality of the signals. Deep neural networks are often introspected by transforming their learned feature representations into 2- or 3-dimensional subspace visualizations using projection algorithms like Uniform Manifold Approximation and Projection (UMAP). Unfortunately, these methods are computationally expensive, making the projection of data streams in real-time a non-trivial task. In this study, we introduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at generating rapid projections for real-time introspection. To study its suitability for real-time projecting, we <b>benchmark</b> the methods against standard UMAP and its neural network counterpart parametric UMAP. Our results show that approximate UMAP delivers projections that replicate the projection space of standard UMAP while decreasing projection speed by an order of magnitude and maintaining the same training time.</p></p class="citation"></blockquote><h2 id=cshc-2>cs.HC (2)</h2><h3 id=12--90125-open-vocabulary-keyword-spotting-through-transfer-learning-from-speech-synthesis-kesavaraj-v-et-al-2024>(1/2 | 90/125) Open vocabulary keyword spotting through transfer learning from speech synthesis (Kesavaraj V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kesavaraj V, Anil Kumar Vuppala. (2024)<br><strong>Open vocabulary keyword spotting through transfer learning from speech synthesis</strong><br><button class=copy-to-clipboard title="Open vocabulary keyword spotting through transfer learning from speech synthesis" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SD, cs.HC, eess-AS<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Transfer Learning, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03914v1.pdf filename=2404.03914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying keywords in an open-vocabulary context is crucial for personalizing interactions with smart devices. Previous approaches to open vocabulary keyword spotting dependon a shared embedding space created by audio and text encoders. However, these approaches suffer from heterogeneous modality representations (i.e., audio-text mismatch). To address this issue, our proposed framework leverages <b>knowledge</b> <b>acquired</b> from a pre-trained <b>text-to-speech</b> <b>(TTS)</b> system. This <b>knowledge</b> <b>transfer</b> <b>allows</b> for the incorporation of awareness of audio projections into the text representations derived from the text encoder. The performance of the proposed approach is compared with various baseline methods across four different datasets. The robustness of our proposed model is evaluated by assessing its performance across different word lengths and in an Out-of-Vocabulary (OOV) scenario. Additionally, the effectiveness of <b>transfer</b> <b>learning</b> from the <b>TTS</b> system is investigated by analyzing its different intermediate representations. The experimental results indicate that, in the challenging LibriPhrase Hard dataset, the proposed approach outperformed the cross-modality correspondence detector (CMCD) method by a significant improvement of 8.22% in area under the curve (AUC) and 12.56% in equal error rate (EER).</p></p class="citation"></blockquote><h3 id=22--91125-which-experimental-design-is-better-suited-for-vqa-tasks-eye-tracking-study-on-cognitive-load-performance-and-gaze-allocations-sita-a-vriend-et-al-2024>(2/2 | 91/125) Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations (Sita A. Vriend et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sita A. Vriend, Sandeep Vidyapu, Amer Rama, Kun-Ting Chen, Daniel Weiskopf. (2024)<br><strong>Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations</strong><br><button class=copy-to-clipboard title="Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04036v1.pdf filename=2404.04036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conducted an eye-tracking user study with 13 participants to investigate the influence of stimulus-question ordering and <b>question</b> <b>modality</b> on participants using <b>visual</b> <b>question-answering</b> <b>(VQA)</b> tasks. We examined cognitive load, task performance, and gaze allocations across five distinct experimental designs, aiming to identify setups that minimize the cognitive burden on participants. The collected performance and gaze data were analyzed using quantitative and qualitative methods. Our results indicate a significant impact of stimulus-question ordering on cognitive load and task performance, as well as a noteworthy effect of <b>question</b> <b>modality</b> on task performance. These findings offer insights for the experimental design of controlled user studies in visualization research.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--92125-superior-genetic-algorithms-for-the-target-set-selection-problem-based-on-power-law-parameter-choices-and-simple-greedy-heuristics-benjamin-doerr-et-al-2024>(1/1 | 92/125) Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics (Benjamin Doerr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Doerr, Martin S. Krejca, Nguyen Vu. (2024)<br><strong>Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics</strong><br><button class=copy-to-clipboard title="Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 36<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04018v1.pdf filename=2404.04018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The target set selection problem (TSS) asks for a set of vertices such that an influence spreading process started in these vertices reaches the whole <b>graph.</b> <b>The</b> <b>current</b> state of the art for this NP-hard problem are three recently proposed randomized search heuristics, namely a biased random-key genetic algorithm (BRKGA) obtained from extensive parameter tuning, a max-min ant system (MMAS), and a MMAS using Q-learning with a <b>graph</b> <b>convolutional</b> <b>network.</b> We show that the BRKGA with two simple modifications and without the costly parameter tuning obtains significantly better results. Our first modification is to simply choose all parameters of the BRKGA in each iteration randomly from a power-law distribution. The resulting parameterless BRKGA is already competitive with the tuned BRKGA, as our experiments on the previously used <b>benchmarks</b> show. We then add a natural greedy heuristic, namely to repeatedly discard small-degree vertices that are not necessary for reaching the whole <b>graph.</b> <b>The</b> <b>resulting</b> algorithm consistently outperforms all of the state-of-the-art algorithms. Besides providing a superior algorithm for the TSS problem, this work shows that randomized parameter choices and elementary greedy heuristics can give better results than complex algorithms and costly parameter tuning.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--93125-evaluating-adversarial-robustness-a-comparison-of-fgsm-carlini-wagner-attacks-and-the-role-of-distillation-as-defense-mechanism-trilokesh-ranjan-sarkar-et-al-2024>(1/9 | 93/125) Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism (Trilokesh Ranjan Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen. (2024)<br><strong>Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism</strong><br><button class=copy-to-clipboard title="Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04245v1.pdf filename=2404.04245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report delves into an in-depth exploration of <b>adversarial</b> <b>attacks</b> specifically targeted at Deep Neural Networks (DNNs) utilized for image classification. The study also investigates defense mechanisms aimed at bolstering the robustness of machine learning models. The research focuses on comprehending the ramifications of two prominent attack methodologies: the Fast Gradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks are examined concerning three pre-trained image classifiers: Resnext50_32x4d, DenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the study proposes the robustness of defensive <b>distillation</b> as a defense mechanism to counter FGSM and CW attacks. This defense mechanism is evaluated using the CIFAR-10 dataset, where <b>CNN</b> models, specifically resnet101 and Resnext50_32x4d, serve as the teacher and student models, respectively. The proposed defensive <b>distillation</b> model exhibits effectiveness in thwarting attacks such as FGSM. However, it is noted to remain susceptible to more sophisticated techniques like the CW attack. The document presents a meticulous validation of the proposed scheme. It provides detailed and comprehensive results, elucidating the efficacy and limitations of the defense mechanisms employed. Through rigorous experimentation and analysis, the study offers insights into the dynamics of <b>adversarial</b> <b>attacks</b> on DNNs, as well as the effectiveness of defensive strategies in mitigating their impact.</p></p class="citation"></blockquote><h3 id=29--94125-re-pseudonymization-strategies-for-smart-meter-data-are-not-robust-to-deep-learning-profiling-attacks-ana-maria-cretu-et-al-2024>(2/9 | 94/125) Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks (Ana-Maria Cretu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye. (2024)<br><strong>Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks</strong><br><button class=copy-to-clipboard title="Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03948v1.pdf filename=2404.03948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart meters, devices measuring the electricity and gas consumption of a household, are currently being deployed at a fast rate throughout the world. The data they collect are extremely useful, including in the fight against climate change. However, these data and the information that can be inferred from them are highly sensitive. Re-pseudonymization, i.e., the frequent replacement of random identifiers over time, is widely used to share smart meter data while mitigating the risk of re-identification. We here show how, in spite of re-pseudonymization, households&rsquo; consumption records can be pieced together with high accuracy in large-scale datasets. We propose the first deep learning-based profiling attack against re-pseudonymized smart meter data. Our attack combines neural network embeddings, which are used to extract features from weekly consumption records and are tailored to the smart meter identification task, with a nearest neighbor classifier. We evaluate six neural networks architectures as the embedding model. Our results suggest that the <b>Transformer</b> and <b>CNN-LSTM</b> architectures vastly outperform previous methods as well as other architectures, successfully identifying the correct household 73.4% of the time among 5139 households based on electricity and gas consumption records (54.5% for electricity only). We further show that the features extracted by the embedding model maintain their effectiveness when transferred to a set of users disjoint from the one used to train the model. Finally, we extensively evaluate the robustness of our results. Taken together, our results strongly suggest that even frequent re-pseudonymization strategies can be reversed, strongly limiting their ability to prevent re-identification in practice.</p></p class="citation"></blockquote><h3 id=39--95125-reliable-feature-selection-for-adversarially-robust-cyber-attack-detection-joão-vitorino-et-al-2024>(3/9 | 95/125) Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection (João Vitorino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Vitorino, Miguel Silva, Eva Maia, Isabel Praça. (2024)<br><strong>Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection</strong><br><button class=copy-to-clipboard title="Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-NI, cs.CR<br>Keyword Score: 13<br>Keywords: Adversarial Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04188v1.pdf filename=2404.04188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing cybersecurity threats make it essential to use high-quality data to train Machine Learning (ML) models for network traffic analysis, without noisy or missing data. By selecting the most relevant features for cyber-attack detection, it is possible to improve both the robustness and computational efficiency of the models used in a cybersecurity system. This work presents a feature selection and consensus process that combines multiple methods and applies them to several network datasets. Two different feature sets were selected and were used to train multiple ML models with regular and <b>adversarial</b> <b>training.</b> Finally, an <b>adversarial</b> <b>evasion</b> robustness <b>benchmark</b> was performed to analyze the reliability of the different feature sets and their impact on the susceptibility of the models to <b>adversarial</b> <b>examples.</b> By using an improved dataset with more data diversity, selecting the best time-related features and a more specific feature set, and performing <b>adversarial</b> <b>training,</b> the ML models were able to achieve a better adversarially robust generalization. The robustness of the models was significantly improved without their generalization to regular traffic flows being affected, without increases of false alarms, and without requiring too many computational resources, which enables a reliable detection of suspicious activity and perturbed traffic flows in enterprise computer networks.</p></p class="citation"></blockquote><h3 id=49--96125-watermark-based-detection-and-attribution-of-ai-generated-content-zhengyuan-jiang-et-al-2024>(4/9 | 96/125) Watermark-based Detection and Attribution of AI-Generated Content (Zhengyuan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong. (2024)<br><strong>Watermark-based Detection and Attribution of AI-Generated Content</strong><br><button class=copy-to-clipboard title="Watermark-based Detection and Attribution of AI-Generated Content" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04254v1.pdf filename=2404.04254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several companies&ndash;such as Google, Microsoft, and OpenAI&ndash;have deployed techniques to watermark AI-generated content to enable proactive detection. However, existing literature mainly focuses on user-agnostic detection. Attribution aims to further trace back the user of a <b>generative-AI</b> <b>service</b> who generated a given content detected as AI-generated. Despite its growing importance, attribution is largely unexplored. In this work, we aim to bridge this gap by providing the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content. Specifically, we theoretically study the detection and attribution performance via rigorous probabilistic analysis. Moreover, we develop an efficient algorithm to select watermarks for the users to enhance attribution performance. Both our theoretical and empirical results show that watermark-based detection and attribution inherit the accuracy and (non-)robustness properties of the watermarking method.</p></p class="citation"></blockquote><h3 id=59--97125-precision-guided-approach-to-mitigate-data-poisoning-attacks-in-federated-learning-k-naveen-kumar-et-al-2024>(5/9 | 97/125) Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning (K Naveen Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>K Naveen Kumar, C Krishna Mohan, Aravind Machiry. (2024)<br><strong>Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning</strong><br><button class=copy-to-clipboard title="Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04139v1.pdf filename=2404.04139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a collaborative learning paradigm enabling participants to collectively train a shared machine learning model while preserving the privacy of their sensitive data. Nevertheless, the inherent decentralized and data-opaque characteristics of FL render its susceptibility to data poisoning attacks. These attacks introduce malformed or malicious inputs during local model training, subsequently influencing the global model and resulting in erroneous predictions. Current FL defense strategies against data poisoning attacks either involve a trade-off between accuracy and robustness or necessitate the presence of a uniformly distributed root dataset at the server. To overcome these limitations, we present FedZZ, which harnesses a zone-based deviating update (ZBDU) mechanism to effectively counter data poisoning attacks in FL. Further, we introduce a precision-guided methodology that actively characterizes these client clusters (zones), which in turn aids in recognizing and discarding malicious updates at the server. Our evaluation of FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate its efficacy in mitigating data poisoning attacks, surpassing the performance of prevailing state-of-the-art methodologies in both single and multi-client attack scenarios and varying attack volumes. Notably, FedZZ also functions as a robust client selection strategy, even in highly non-IID and attack-free scenarios. Moreover, in the face of escalating poisoning rates, the model accuracy attained by FedZZ displays superior resilience compared to existing techniques. For instance, when confronted with a 50% presence of malicious clients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the second-best solution, FL-Defender, diminishes to 43.36%.</p></p class="citation"></blockquote><h3 id=69--98125-you-can-use-but-cannot-recognize-preserving-visual-privacy-in-deep-neural-networks-qiushi-li-et-al-2024>(6/9 | 98/125) You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks (Qiushi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiushi Li, Yan Zhang, Ju Ren, Qi Li, Yaoxue Zhang. (2024)<br><strong>You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks</strong><br><button class=copy-to-clipboard title="You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04098v1.pdf filename=2404.04098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image data have been extensively used in Deep Neural Network (DNN) tasks in various scenarios, e.g., autonomous driving and medical image analysis, which incurs significant privacy concerns. Existing privacy protection techniques are unable to efficiently protect such data. For example, <b>Differential</b> <b>Privacy</b> (DP) that is an emerging technique protects data with strong privacy guarantee cannot effectively protect visual features of exposed image dataset. In this paper, we propose a novel privacy-preserving framework VisualMixer that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises. VisualMixer utilizes a new privacy metric called Visual Feature Entropy (VFE) to effectively quantify the visual features of an image from both biological and machine vision aspects. In VisualMixer, we devise a task-agnostic image obfuscation method to protect the visual privacy of data for DNN training and inference. For each image, it determines regions for pixel shuffling in the image and the sizes of these regions according to the desired VFE. It shuffles pixels both in the spatial domain and in the chromatic channel space in the regions without injecting noises so that it can prevent visual features from being discerned and recognized, while incurring negligible accuracy loss. Extensive experiments on real-world datasets demonstrate that VisualMixer can effectively preserve the visual privacy with negligible accuracy loss, i.e., at average 2.35 percentage points of model accuracy loss, and almost no performance degradation on model training.</p></p class="citation"></blockquote><h3 id=79--99125-from-theory-to-comprehension-a-comparative-study-of-differential-privacy-and-k-anonymity-saskia-nuñez-von-voigt-et-al-2024>(7/9 | 99/125) From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity (Saskia Nuñez von Voigt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saskia Nuñez von Voigt, Luise Mehner, Florian Tschorsch. (2024)<br><strong>From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity</strong><br><button class=copy-to-clipboard title="From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04006v1.pdf filename=2404.04006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The notion of $\varepsilon$-differential privacy is a widely used concept of providing quantifiable privacy to individuals. However, it is unclear how to explain the level of privacy protection provided by a <b>differential</b> <b>privacy</b> mechanism with a set $\varepsilon$. In this study, we focus on users&rsquo; comprehension of the privacy protection provided by a <b>differential</b> <b>privacy</b> mechanism. To do so, we study three variants of explaining the privacy protection provided by <b>differential</b> <b>privacy:</b> (1) the original mathematical definition; (2) $\varepsilon$ translated into a specific privacy risk; and (3) an explanation using the randomized response technique. We compare users&rsquo; comprehension of privacy protection employing these explanatory models with their comprehension of privacy protection of $k$-anonymity as baseline comprehensibility. Our findings suggest that participants&rsquo; comprehension of <b>differential</b> <b>privacy</b> protection is enhanced by the privacy risk model and the randomized response-based model. Moreover, our results confirm our intuition that privacy protection provided by $k$-anonymity is more comprehensible.</p></p class="citation"></blockquote><h3 id=89--100125-privshape-extracting-shapes-in-time-series-under-user-level-local-differential-privacy-yulian-mao-et-al-2024>(8/9 | 100/125) PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy (Yulian Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulian Mao, Qingqing Ye, Haibo Hu, Qi Wang, Kai Huang. (2024)<br><strong>PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy</strong><br><button class=copy-to-clipboard title="PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03873v1.pdf filename=2404.03873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series have numerous applications in finance, healthcare, IoT, and smart city. In many of these applications, time series typically contain personal data, so privacy infringement may occur if they are released directly to the public. Recently, local <b>differential</b> <b>privacy</b> (LDP) has emerged as the state-of-the-art approach to protecting data privacy. However, existing works on LDP-based collections cannot preserve the shape of time series. A recent work, PatternLDP, attempts to address this problem, but it can only protect a finite group of elements in a time series due to {\omega}-event level privacy guarantee. In this paper, we propose PrivShape, a trie-based mechanism under user-level LDP to protect all elements. PrivShape first transforms a time series to reduce its length, and then adopts trie-expansion and two-level refinement to improve utility. By extensive experiments on real-world datasets, we demonstrate that PrivShape outperforms PatternLDP when adapted for offline use, and can effectively extract frequent shapes.</p></p class="citation"></blockquote><h3 id=99--101125-smart-contract-languages-a-comparative-analysis-massimo-bartoletti-et-al-2024>(9/9 | 101/125) Smart Contract Languages: a comparative analysis (Massimo Bartoletti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Massimo Bartoletti, Lorenzo Benetollo, Michele Bugliesi, Silvia Crafa, Giacomo Dal Sasso, Roberto Pettinau, Andrea Pinna, Mattia Piras, Sabina Rossi, Stefano Salis, Alvise Spanò, Viacheslav Tkachenko, Roberto Tonelli, Roberto Zunino. (2024)<br><strong>Smart Contract Languages: a comparative analysis</strong><br><button class=copy-to-clipboard title="Smart Contract Languages: a comparative analysis" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-PL, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04129v1.pdf filename=2404.04129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized blockchain platforms support the secure exchange of assets among users without relying on trusted third parties. These exchanges are programmed with smart contracts, computer programs directly executed by blockchain nodes. Multiple smart contract languages are available nowadays to developers, each with its own distinctive features, strengths, and weaknesses. In this paper, we examine the smart contract languages used in six major blockchain platforms: Ethereum, Solana, Cardano, Algorand, Aptos, and Tezos. Starting with a high-level overview of their design choices, we provide a comprehensive assessment that focuses on programming style, security, code readability, and usability, drawing on an original <b>benchmark</b> that encompasses a common set of use cases across all the smart contract languages under examination.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=14--102125-nonlinear-kalman-filtering-based-on-self-attention-mechanism-and-lattice-trajectory-piecewise-linear-approximation-jiaming-wang-et-al-2024>(1/4 | 102/125) Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation (Jiaming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaming Wang, Xinyu Geng, Jun Xu. (2024)<br><strong>Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation</strong><br><button class=copy-to-clipboard title="Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03915v1.pdf filename=2404.03915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The traditional Kalman filter (KF) is widely applied in control systems, but it relies heavily on the accuracy of the system model and noise parameters, leading to potential performance degradation when facing inaccuracies. To address this issue, introducing neural networks into the KF framework offers a data-driven solution to compensate for these inaccuracies, improving the filter&rsquo;s performance while maintaining interpretability. Nevertheless, existing studies mostly employ <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN),</b> which fails to fully capture the dependencies among state sequences and lead to an unstable training process. In this paper, we propose a novel Kalman filtering algorithm named the attention Kalman filter (AtKF), which incorporates a <b>self-attention</b> network to capture the dependencies among state sequences. To address the instability in the recursive training process, a parallel pre-training strategy is devised. Specifically, this strategy involves piecewise linearizing the system via lattice trajectory piecewise linear (LTPWL) expression, and generating pre-training data through a batch estimation algorithm, which exploits the <b>self-attention</b> mechanism&rsquo;s parallel processing ability. Experimental results on a two-dimensional nonlinear system demonstrate that AtKF outperforms other filters under noise disturbances and model mismatches.</p></p class="citation"></blockquote><h3 id=24--103125-torque-minimizing-control-allocation-for-overactuated-quadrupedal-locomotion-mads-erlend-bøe-lysø-et-al-2024>(2/4 | 103/125) Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion (Mads Erlend Bøe Lysø et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mads Erlend Bøe Lysø, Esten Ingar Grøtli, Kristin Ytterstad Pettersen. (2024)<br><strong>Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion</strong><br><button class=copy-to-clipboard title="Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04156v1.pdf filename=2404.04156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we improve upon a method for optimal control of quadrupedal robots which utilizes a full-order model of the system. The original method utilizes offline nonlinear optimal control to synthesize a control scheme which exponentially orbitally stabilizes the closed-loop system. However, it is not able to handle the overactuated phases which frequently occur during quadrupedal locomotion as a result of the multi-contact nature of the system. We propose a modified method, which handles overactuated gait phases in a way that utilizes the full range of available actuators to minimize torque expenditure without requiring output trajectories to be modified. It is shown that the system under the proposed controller exhibits the same properties, i.e. exponential orbital stability, with the same or lower point-wise torque magnitude. A <b>simulation</b> study demonstrates that the reduction in torque may in certain cases be substantial.</p></p class="citation"></blockquote><h3 id=34--104125-field-teams-coordination-for-earthquake-damaged-distribution-system-energization-ilker-işık-et-al-2024>(3/4 | 104/125) Field Teams Coordination for Earthquake-Damaged Distribution System Energization (İlker Işık et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>İlker Işık, Ebru Aydin Gol. (2024)<br><strong>Field Teams Coordination for Earthquake-Damaged Distribution System Energization</strong><br><button class=copy-to-clipboard title="Field Teams Coordination for Earthquake-Damaged Distribution System Energization" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04087v1.pdf filename=2404.04087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The re-energization of electrical distribution systems in a post-disaster scenario is of grave importance as most modern infrastructure systems rely heavily on the presence of electricity. This paper introduces a method to coordinate the field teams for the optimal energization of an electrical distribution system after an earthquake-induced blackout. The proposed method utilizes a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) to create an optimal energization strategy, which aims to minimize the expected time to energize each distribution system component. The travel duration of each team and the possible outcomes of the energization attempts are considered in the state transitions. The failure probabilities of the system components are computed using the fragility curves of structures and the Peak Ground Acceleration (PGA) values which are encoded to the MDP model via transition probabilities. Furthermore, the proposed solution offers several methods to determine the non-optimal actions during the construction of the MDP and eliminate them in order to improve the run-time performance without sacrificing the optimality of the solution.</p></p class="citation"></blockquote><h3 id=44--105125-queue-aware-network-control-algorithm-with-a-high-quantum-computing-readiness-evaluated-in-discrete-time-flow-simulator-for-fat-pipe-networks-arthur-witt-2024>(4/4 | 105/125) Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks (Arthur Witt, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Witt. (2024)<br><strong>Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks</strong><br><button class=copy-to-clipboard title="Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 90C11, 94-10, 81P68, I-6; C-2-3; C-2-5, cs-ET, cs-SY, eess-SY, eess.SY, quant-ph<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04080v1.pdf filename=2404.04080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging technology of quantum computing has the potential to change the way how problems will be solved in the future. This work presents a centralized network control algorithm executable on already existing quantum computer which are based on the principle of quantum annealing like the D-Wave Advantage. We introduce a resource reoccupation algorithm for traffic engineering in wide-area networks. The proposed optimization algorithm changes traffic steering and resource allocation in case of overloaded transceivers. Settings of active components like fiber amplifiers and transceivers are not changed for the reason of stability. This algorithm is beneficial in situations when the network traffic is fluctuating in time scales of seconds or spontaneous bursts occur. Further, we developed a <b>discrete-time</b> <b>flow</b> simulator to study the algorithm&rsquo;s performance in wide-area networks. Our network simulator considers backlog and loss modeling of buffered transmission lines. Concurring flows are handled equally in case of a backlog. This work provides an ILP-based network configuring algorithm that is applicable on quantum annealing computers. We showcase, that traffic losses can be reduced significantly by a factor of 2 if a resource reoccupation algorithm is applied in a network with bursty traffic. As resources are used more efficiently by reoccupation in heavy load situations, overprovisioning of networks can be reduced. Thus, this new form of network operation leads toward a zero-margin network. We show that our newly introduced network simulator enables analyses of short-time effects like buffering within fat-pipe networks. As the calculation of network configurations in real-sized networks is typically time-consuming, quantum computing can enable the proposed network configuration algorithm for application in real-sized wide-area networks.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--106125-quantum-informed-simulations-for-mechanics-of-materials-dftbmbd-framework-zhaoxiang-shen-et-al-2024>(1/1 | 106/125) Quantum-informed simulations for mechanics of materials: DFTB+MBD framework (Zhaoxiang Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxiang Shen, Raúl I. Sosa, Stéphane P. A. Bordas, Alexandre Tkatchenko, Jakub Lengiewicz. (2024)<br><strong>Quantum-informed simulations for mechanics of materials: DFTB+MBD framework</strong><br><button class=copy-to-clipboard title="Quantum-informed simulations for mechanics of materials: DFTB+MBD framework" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04216v1.pdf filename=2404.04216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The macroscopic behaviors of materials are determined by interactions that occur at multiple lengths and time scales. Depending on the application, describing, predicting, and understanding these behaviors require models that rely on insights from electronic and atomic scales. In such cases, classical simplified approximations at those scales are insufficient, and quantum-based modeling is required. In this paper, we study how quantum effects can modify the mechanical properties of systems relevant to materials engineering. We base our study on a high-fidelity modeling framework that combines two computationally efficient models rooted in quantum first principles: Density Functional Tight Binding (DFTB) and many-body dispersion (MBD). The MBD model is applied to accurately describe non-covalent van der Waals interactions. Through various <b>benchmark</b> applications, we demonstrate the capabilities of this framework and the limitations of simplified modeling. We provide an open-source repository containing all codes, datasets, and examples presented in this work. This repository serves as a practical toolkit that we hope will support the development of future research in effective large-scale and multiscale modeling with quantum-mechanical fidelity.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--107125-a-posteriori-error-analysis-of-a-space-time-hybridizable-discontinuous-galerkin-method-for-the-advection-diffusion-problem-yuan-wang-et-al-2024>(1/2 | 107/125) A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem (Yuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wang, Sander Rhebergen. (2024)<br><strong>A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem</strong><br><button class=copy-to-clipboard title="A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04130v1.pdf filename=2404.04130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present and analyze an a posteriori error estimator for a space-time hybridizable discontinuous Galerkin discretization of the time-dependent advection-diffusion problem. The residual-based error estimator is proven to be reliable and locally efficient. In the reliability analysis we combine a Peclet-robust coercivity type result and a saturation assumption, while local efficiency analysis is based on using bubble functions. The analysis considers both local space and time adaptivity and is verified by numerical <b>simulations</b> on problems which include boundary and interior layers.</p></p class="citation"></blockquote><h3 id=22--108125-highly-efficient-nurbs-based-isogeometric-analysis-for-coupled-nonlinear-diffusion-reaction-equations-with-and-without-advection-ilham-asmouh-et-al-2024>(2/2 | 108/125) Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection (Ilham Asmouh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilham Asmouh, Alexander Ostermann. (2024)<br><strong>Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection</strong><br><button class=copy-to-clipboard title="Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04017v1.pdf filename=2404.04017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinear diffusion-reaction systems model a multitude of physical phenomena. A common situation is biological development modeling where such systems have been widely used to study spatiotemporal phenomena in cell biology. Systems of coupled diffusion-reaction equations are usually subject to some complicated features directly related to their multiphysics nature. Moreover, the presence of advection is source of numerical instabilities, in general, and adds another challenge to these systems. In this study, we propose a NURBS-based isogeometric analysis (IgA) combined with a second-order Strang operator splitting to deal with the multiphysics nature of the problem. The advection part is treated in a semi-Lagrangian framework and the resulting diffusion-reaction equations are then solved using an efficient time-stepping algorithm based on operator splitting. The accuracy of the method is studied by means of a advection-diffusion-reaction system with analytical solution. To further examine the performance of the new method on complex geometries, the well-known Schnakenberg-Turing problem is considered with and without advection. Finally, a Gray-Scott system on a circular domain is also presented. The results obtained demonstrate the efficiency of our new algorithm to accurately reproduce the solution in the presence of complex patterns on complex geometries. Moreover, the new method clarifies the effect of <b>geometry</b> on Turing patterns.</p></p class="citation"></blockquote><h2 id=csse-1>cs.SE (1)</h2><h3 id=11--109125-pros-and-cons-evaluating-chatgpt-on-software-vulnerability-xin-yin-2024>(1/1 | 109/125) Pros and Cons! Evaluating ChatGPT on Software Vulnerability (Xin Yin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Yin. (2024)<br><strong>Pros and Cons! Evaluating ChatGPT on Software Vulnerability</strong><br><button class=copy-to-clipboard title="Pros and Cons! Evaluating ChatGPT on Software Vulnerability" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03994v1.pdf filename=2404.03994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a pipeline for quantitatively evaluating interactive <b>LLMs</b> such as <b>ChatGPT</b> using publicly available dataset. We carry out an extensive technical evaluation of <b>ChatGPT</b> using Big-Vul covering five different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of <b>ChatGPT</b> based on this dataset. We found that the existing state-of-the-art methods are generally superior to <b>ChatGPT</b> in software vulnerability detection. Although <b>ChatGPT</b> improves accuracy when providing context information, it still has limitations in accurately predicting severity ratings for certain CWE types. In addition, <b>ChatGPT</b> demonstrates some ability in locating vulnerabilities for certain CWE types, but its performance varies among different CWE types. <b>ChatGPT</b> exhibits limited vulnerability repair capabilities in both providing and not providing context information. Finally, <b>ChatGPT</b> shows uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in detailed information. Overall, though <b>ChatGPT</b> performs well in some aspects, it still needs improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities in order to fully realize its potential. Our evaluation framework provides valuable insights for further enhancing <b>ChatGPT&rsquo;</b> s software vulnerability handling capabilities.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--110125-quand-rechercher-cest-faire-des-vagues--dans-et-à-partir-des-images-algorithmiques-gaëtan-robillard-2024>(1/1 | 110/125) Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques (Gaëtan Robillard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaëtan Robillard. (2024)<br><strong>Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques</strong><br><button class=copy-to-clipboard title="Quand rechercher c'est faire des vagues : Dans et {à} partir des images algorithmiques" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03923v1.pdf filename=2404.03923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Search of the Wave is a computer-generated film made in 2013, highlighting the computation of images through computer <b>simulation,</b> and through text and voice. Originating from a screening of the film at the Gustave Eiffel University, the article presents a reflection on research-creation in and from algorithmic images. Fundamentally, what is it in this research-creation &ndash; especially in research on algorithmic imagery &ndash; that can be set in motion? Without fully distinguishing between what would be research on one hand and creation on the other, we focus on characterizing forms, aesthetics, or theories that contribute to possible shifts. The inventory of these possibilities is precisely the challenge of the text: from mathematics to image and visualization, from the birth of generative aesthetics to the coding related to pioneering works (recoding), or from indexing new aesthetics to new forms of critical production.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--111125-deep-phase-coded-image-prior-nimrod-shabtay-et-al-2024>(1/2 | 111/125) Deep Phase Coded Image Prior (Nimrod Shabtay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nimrod Shabtay, Eli Schwartz, Raja Giryes. (2024)<br><strong>Deep Phase Coded Image Prior</strong><br><button class=copy-to-clipboard title="Deep Phase Coded Image Prior" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03906v1.pdf filename=2404.03906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phase-coded imaging is a computational imaging method designed to tackle tasks such as passive depth estimation and extended depth of field (EDOF) using depth cues inserted during image capture. Most of the current deep learning-based methods for depth estimation or all-in-focus imaging require a training dataset with high-quality depth maps and an optimal focus point at infinity for all-in-focus images. Such datasets are difficult to create, usually synthetic, and require external graphic programs. We propose a new method named &ldquo;Deep Phase Coded Image Prior&rdquo; (DPCIP) for jointly recovering the depth map and all-in-focus image from a coded-phase image using solely the captured image and the optical information of the imaging system. Our approach does not depend on any specific dataset and surpasses prior <b>supervised</b> techniques utilizing the same imaging system. This improvement is achieved through the utilization of a problem formulation based on implicit neural representation (INR) and deep image prior (DIP). Due to our <b>zero-shot</b> method, we overcome the barrier of acquiring accurate ground-truth data of depth maps and all-in-focus images for each new phase-coded system introduced. This allows focusing mainly on developing the imaging system, and not on ground-truth data collection.</p></p class="citation"></blockquote><h3 id=22--112125-lidar-guided-cross-attention-fusion-for-hyperspectral-band-selection-and-image-classification-judy-x-yang-et-al-2024>(2/2 | 112/125) LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification (Judy X Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Judy X Yang, Jun Zhou, Jing Wang, Hui Tian, Wee Chung Liew. (2024)<br><strong>LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification</strong><br><button class=copy-to-clipboard title="LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: F-2-2, I-2-7, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03883v1.pdf filename=2404.03883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fusion of hyperspectral and LiDAR data has been an active research topic. Existing fusion methods have ignored the high-dimensionality and redundancy challenges in hyperspectral images, despite that band selection methods have been intensively studied for hyperspectral image (HSI) processing. This paper addresses this significant gap by introducing a cross-attention mechanism from the <b>transformer</b> architecture for the selection of HSI bands guided by LiDAR data. LiDAR provides high-resolution vertical structural information, which can be useful in distinguishing different types of land cover that may have similar spectral signatures but different structural profiles. In our approach, the LiDAR data are used as the &ldquo;query&rdquo; to search and identify the &ldquo;key&rdquo; from the HSI to choose the most pertinent bands for LiDAR. This method ensures that the selected HSI bands drastically reduce redundancy and computational requirements while working optimally with the LiDAR data. Extensive experiments have been undertaken on three paired HSI and LiDAR data sets: Houston 2013, Trento and MUUFL. The results highlight the superiority of the cross-attention mechanism, underlining the enhanced classification accuracy of the identified HSI bands when fused with the LiDAR features. The results also show that the use of fewer bands combined with LiDAR surpasses the performance of state-of-the-art fusion models.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--113125-semantic-sql----combining-and-optimizing-semantic-predicates-in-sql-akash-mittal-et-al-2024>(1/1 | 113/125) Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL (Akash Mittal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Mittal, Anshul Bheemreddy, Huili Tao. (2024)<br><strong>Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL</strong><br><button class=copy-to-clipboard title="Semantic SQL -- Combining and optimizing semantic predicates in SQL" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: human-in-the-loop, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03880v1.pdf filename=2404.03880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the surge in unstructured data analysis, facilitated by advancements in Machine Learning (ML), has <b>prompted</b> diverse approaches for handling images, text documents, and videos. Analysts, leveraging ML models, can extract meaningful information from unstructured data and store it in relational databases, allowing the execution of SQL queries for further analysis. Simultaneously, vector databases have emerged, embedding unstructured data for efficient top-k queries based on textual queries. This paper introduces a novel framework SSQL - Semantic SQL that utilizes these two approaches, enabling the incorporation of semantic queries within SQL statements. Our approach extends SQL queries with dedicated keywords for specifying semantic queries alongside predicates related to ML model results and metadata. Our experimental results show that using just semantic queries fails catastrophically to answer count and spatial queries in more than 60% of the cases. Our proposed method jointly optimizes the queries containing both semantic predicates and predicates on structured tables, such as those generated by ML models or other metadata. Further, to improve the query results, we incorporated <b>human-in-the-loop</b> feedback to determine the optimal similarity score threshold for returning results.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--114125-simplifying-explicit-subtyping-coercions-in-a-polymorphic-calculus-with-effects-filip-koprivec-et-al-2024>(1/2 | 114/125) Simplifying explicit subtyping coercions in a polymorphic calculus with effects (Filip Koprivec et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Koprivec, Matija Pretnar. (2024)<br><strong>Simplifying explicit subtyping coercions in a polymorphic calculus with effects</strong><br><button class=copy-to-clipboard title="Simplifying explicit subtyping coercions in a polymorphic calculus with effects" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04218v1.pdf filename=2404.04218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algebraic effect handlers are becoming increasingly popular way of structuring and <b>reasoning</b> about effectful computations, and their performance is often a concern. One of the proposed approaches towards efficient compilation is tracking effect information through explicit subtyping coercions. However, in the presence of polymorphism, these coercions are compiled to additional arguments of compiled functions, incurring significant overhead. In this paper, we present a polymorphic effectful calculus, identify simplification phases needed to reduce the number of unnecessary constraints, and prove they preserve the semantics. In addition, we implement the simplification algorithm in the Eff language, and evaluate its performance on a number of <b>benchmarks.</b> Though we do not prove optimality of presented simplifications, the results show that the algorithm eliminates all the coercions, resulting in a code as efficient as manually monomorphised one.</p></p class="citation"></blockquote><h3 id=22--115125-v-star-learning-visibly-pushdown-grammars-from-program-inputs-xiaodong-jia-et-al-2024>(2/2 | 115/125) V-Star: Learning Visibly Pushdown Grammars from Program Inputs (Xiaodong Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodong Jia, Gang Tan. (2024)<br><strong>V-Star: Learning Visibly Pushdown Grammars from Program Inputs</strong><br><button class=copy-to-clipboard title="V-Star: Learning Visibly Pushdown Grammars from Program Inputs" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-FL, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04201v1.pdf filename=2404.04201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate description of program inputs remains a critical challenge in the field of programming languages. <b>Active</b> <b>learning,</b> as a well-established field, achieves exact learning for regular languages. We offer an innovative grammar inference tool, V-Star, based on the <b>active</b> <b>learning</b> of visibly pushdown automata. V-Star deduces nesting structures of program input languages from sample inputs, employing a novel inference mechanism based on nested patterns. This mechanism identifies token boundaries and converts languages such as XML documents into VPLs. We then adapted Angluin&rsquo;s L-Star, an exact learning algorithm, for VPA learning, which improves the precision of our tool. Our evaluation demonstrates that V-Star effectively and efficiently learns a variety of practical grammars, including S-Expressions, JSON, and XML, and outperforms other state-of-the-art tools.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--116125-h3dfact-heterogeneous-3d-integrated-cim-for-factorization-with-holographic-perceptual-representations-zishen-wan-et-al-2024>(1/1 | 116/125) H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations (Zishen Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zishen Wan, Che-Kai Liu, Mohamed Ibrahim, Hanchen Yang, Samuel Spetalnick, Tushar Krishna, Arijit Raychowdhury. (2024)<br><strong>H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations</strong><br><button class=copy-to-clipboard title="H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04173v1.pdf filename=2404.04173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Disentangling attributes of various sensory signals is central to human-like perception and <b>reasoning</b> and a critical task for higher-order cognitive and neuro-symbolic AI systems. An elegant approach to represent this intricate factorization is via high-dimensional holographic vectors drawing on brain-inspired vector symbolic architectures. However, holographic factorization involves iterative computation with high-dimensional matrix-vector multiplications and suffers from non-convergence problems. In this paper, we present H3DFact, a heterogeneous 3D integrated in-memory compute engine capable of efficiently factorizing high-dimensional holographic representations. H3DFact exploits the computation-in-superposition capability of holographic vectors and the intrinsic stochasticity associated with memristive-based 3D compute-in-memory. Evaluated on large-scale factorization and perceptual problems, H3DFact demonstrates superior capability in factorization accuracy and operational capacity by up to five orders of magnitude, with 5.5x compute density, 1.2x energy efficiency improvements, and 5.9x less silicon footprint compared to iso-capacity 2D designs.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--117125-on-the-quest-for-effectiveness-in-human-oversight-interdisciplinary-perspectives-sarah-sterz-et-al-2024>(1/2 | 117/125) On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives (Sarah Sterz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah Sterz, Kevin Baum, Sebastian Biewer, Holger Hermanns, Anne Lauber-Rönsberg, Philip Meinel, Markus Langer. (2024)<br><strong>On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives</strong><br><button class=copy-to-clipboard title="On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04059v1.pdf filename=2404.04059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human oversight is currently discussed as a potential safeguard to counter some of the negative aspects of high-risk AI applications. This <b>prompts</b> a critical examination of the role and conditions necessary for what is prominently termed effective or meaningful human oversight of these systems. This paper investigates effective human oversight by synthesizing insights from psychological, legal, philosophical, and technical domains. Based on the claim that the main objective of human oversight is risk mitigation, we propose a viable understanding of effectiveness in human oversight: for human oversight to be effective, the human overseer has to have (a) sufficient causal power with regards to the system and its effects, (b) suitable epistemic access to relevant aspects of the situation, (c) self-control over their own actions, and (d) fitting intentions for their role. Furthermore, we argue that this is equivalent to saying that a human overseer is effective if and only if they are morally responsible and have fitting intentions. Against this backdrop, we suggest facilitators and inhibitors of effectiveness in human oversight when striving for practical applicability. We discuss factors in three domains, namely, the technical design of the system, individual factors of human overseers, and the environmental circumstances in which the overseer operates. Finally, this paper scrutinizes the upcoming AI Act of the European Union &ndash; in particular Article 14 on Human Oversight &ndash; as an exemplary regulatory framework in which we study the practicality of our understanding of effective human oversight. By analyzing the provisions and implications of the European AI Act proposal, we pinpoint in how far that proposal aligns with our analyses regarding effective human oversight as well as how it might get enriched by our conceptual understanding of effectiveness in human oversight.</p></p class="citation"></blockquote><h3 id=22--118125-a-conceptual-design-of-in-game-real-and-virtual-currency-tracker-dennis-barzanoff-et-al-2024>(2/2 | 118/125) A Conceptual Design of In-Game Real and Virtual Currency Tracker (Dennis Barzanoff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Barzanoff, Amna Asif. (2024)<br><strong>A Conceptual Design of In-Game Real and Virtual Currency Tracker</strong><br><button class=copy-to-clipboard title="A Conceptual Design of In-Game Real and Virtual Currency Tracker" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03951v1.pdf filename=2404.03951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The gaming industry is earning huge revenues from incorporating virtual currencies into the game design experience. Even if it is a useful approach for the game industry to boost up their earnings, the unidirectional and bidirectional in-game virtual currencies can invoke inadequate gaming behaviors and additions among players. The market lacks gaming and customer protection regulations to avoid the financial, behavioral, and psychological exploitation of users. Therefore, it is needed to develop visual or textual interface design <b>recommendations</b> that help the game players keep balance in their spending and improve their gaming behavior. This paper presents a conceptual design of an in-game purchasing module that allows the user to observe their real time spendings in relation to virtual currency buying.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--119125-nonparametric-modern-hopfield-models-jerry-yao-chieh-hu-et-al-2024>(1/1 | 119/125) Nonparametric Modern Hopfield Models (Jerry Yao-Chieh Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, Han Liu. (2024)<br><strong>Nonparametric Modern Hopfield Models</strong><br><button class=copy-to-clipboard title="Nonparametric Modern Hopfield Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, cs-NE, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03900v1.pdf filename=2404.03900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a nonparametric construction for deep learning compatible modern Hopfield models and utilize this framework to debut an efficient variant. Our key contribution stems from interpreting the memory storage and retrieval processes in modern Hopfield models as a nonparametric regression problem subject to a set of query-memory pairs. Crucially, our framework not only recovers the known results from the original dense modern Hopfield model but also fills the void in the literature regarding efficient modern Hopfield models, by introducing \textit{sparse-structured} modern Hopfield models with sub-quadratic complexity. We establish that this sparse model inherits the appealing theoretical properties of its dense analogue &ndash; connection with <b>transformer</b> attention, fixed point convergence and exponential memory capacity &ndash; even without knowing details of the Hopfield energy function. Additionally, we showcase the versatility of our framework by constructing a family of modern Hopfield models as extensions, including linear, random masked, top-$K$ and positive random feature modern Hopfield models. Empirically, we validate the efficacy of our framework in both synthetic and realistic settings.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--120125-wireless-resource-optimization-in-hybrid-semanticbit-communication-networks-le-xia-et-al-2024>(1/1 | 120/125) Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks (Le Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Xia, Yao Sun, Dusit Niyato, Lan Zhang, Muhammad Ali Imran. (2024)<br><strong>Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks</strong><br><button class=copy-to-clipboard title="Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04162v1.pdf filename=2404.04162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, semantic communication (SemCom) has shown great potential in significant resource savings and efficient information exchanges, thus naturally introducing a novel and practical cellular network paradigm where two modes of SemCom and conventional bit communication (BitCom) coexist. Nevertheless, the involved wireless resource management becomes rather complicated and challenging, given the unique background knowledge matching and time-consuming semantic coding requirements in SemCom. To this end, this paper jointly investigates user association (UA), mode selection (MS), and bandwidth allocation (BA) problems in a hybrid semantic/bit communication network (HSB-Net). Concretely, we first identify a unified performance metric of message throughput for both SemCom and BitCom links. Next, we specially develop a knowledge matching-aware two-stage tandem packet queuing model and theoretically derive the average packet loss ratio and queuing latency. Combined with practical constraints, we then formulate a joint optimization problem for UA, MS, and BA to maximize the overall message throughput of HSB-Net. Afterward, we propose an optimal resource management strategy by utilizing a Lagrange primal-dual transformation method and a preference list-based heuristic algorithm with polynomial-time complexity. Numerical results not only demonstrate the accuracy of our analytical queuing model, but also validate the performance superiority of our proposed strategy compared with different <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--121125-hardness-of-circuit-and-monotone-diameters-of-polytopes-christian-nöbel-et-al-2024>(1/1 | 121/125) Hardness of circuit and monotone diameters of polytopes (Christian Nöbel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Nöbel, Raphael Steiner. (2024)<br><strong>Hardness of circuit and monotone diameters of polytopes</strong><br><button class=copy-to-clipboard title="Hardness of circuit and monotone diameters of polytopes" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-DM, cs-DS, math-CO, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04158v1.pdf filename=2404.04158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Circuit diameter of polytopes was introduced by Borgwardt, Finhold and Hemmecke as a fundamental tool for the study of circuit augmentation schemes for linear programming and for estimating combinatorial diameters. Determining the complexity of computing the circuit diameter of polytopes was posed as an open problem by Sanit`a as well as by Kafer, and was recently reiterated by Borgwardt, Grewe, Kafer, Lee and Sanit`a. In this paper, we solve this problem by showing that computing the circuit diameter of a polytope given in halfspace-description is strongly NP-hard. To prove this result, we show that computing the combinatorial diameter of the perfect matching polytope of a bipartite <b>graph</b> is NP-hard. This complements a result by Sanit`a (FOCS 2018) on the NP-hardness of computing the diameter of fractional matching polytopes and implies the new result that computing the diameter of a ${0,1}$-polytope is strongly NP-hard, which may be of independent interest. In our second main result, we give a precise <b>graph-theoretic</b> description of the monotone diameter of perfect matching polytopes and use this description to prove that computing the monotone (circuit) diameter of a given input polytope is strongly NP-hard as well.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--122125-discrete-fréchet-distance-oracles-boris-aronov-et-al-2024>(1/1 | 122/125) Discrete Fréchet Distance Oracles (Boris Aronov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boris Aronov, Tsuri Farhana, Matthew J. Katz, Indu Ramesh. (2024)<br><strong>Discrete Fréchet Distance Oracles</strong><br><button class=copy-to-clipboard title="Discrete Fréchet Distance Oracles" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04065v1.pdf filename=2404.04065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is unlikely that the discrete Fr'echet distance between two curves of length $n$ can be computed in strictly subquadratic time. We thus consider the setting where one of the curves, $P$, is known in advance. In particular, we wish to construct data structures (distance oracles) of near-linear size that support efficient distance queries with respect to $P$ in sublinear time. Since there is evidence that this is impossible for query curves of length $\Theta(n^\alpha)$, for any $\alpha > 0$, we focus on query curves of (small) constant length, for which we are able to devise distance oracles with the desired bounds. We extend our tools to handle subcurves of the given curve, and even arbitrary vertex-to-vertex subcurves of a given geometric tree. That is, we construct an oracle that can quickly compute the distance between a short polygonal path (the query) and a path in the preprocessed tree between two query-specified vertices. Moreover, we define a new family of geometric <b>graphs,</b> $t$-local <b>graphs</b> (which strictly contains the family of geometric spanners with constant stretch), for which a similar oracle exists: we can preprocess a <b>graph</b> $G$ in the family, so that, given a query segment and a pair $u,v$ of vertices in $G$, one can quickly compute the smallest discrete Fr'echet distance between the segment and any $(u,v)$-path in $G$. The answer is exact, if $t=1$, and approximate if $t>1$.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--123125-stability-in-graphs-with-matroid-constraints-fedor-v-fomin-et-al-2024>(1/2 | 123/125) Stability in Graphs with Matroid Constraints (Fedor V. Fomin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fedor V. Fomin, Petr A. Golovach, Tuukka Korhonen, Saket Saurabh. (2024)<br><strong>Stability in Graphs with Matroid Constraints</strong><br><button class=copy-to-clipboard title="Stability in Graphs with Matroid Constraints" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03979v1.pdf filename=2404.03979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the following Independent Stable Set problem. Let G be an undirected <b>graph</b> and M = (V(G),I) be a matroid whose elements are the vertices of G. For an integer k\geq 1, the task is to decide whether G contains a set S\subseteq V(G) of size at least k which is independent (stable) in G and independent in M. This problem generalizes several well-studied algorithmic problems, including Rainbow Independent Set, Rainbow Matching, and Bipartite Matching with Separation. We show that - When the matroid M is represented by the independence oracle, then for any computable function f, no algorithm can solve Independent Stable Set using f(k)n^{o(k)} calls to the oracle. - On the other hand, when the <b>graph</b> G is of degeneracy d, then the problem is solvable in time O((d+1)^kn), and hence is FPT parameterized by d+k. Moreover, when the degeneracy d is a constant (which is not a part of the input), the problem admits a kernel polynomial in k. More precisely, we prove that for every integer d\geq 0, the problem admits a kernelization algorithm that in time n^{O(d)} outputs an equivalent framework with a <b>graph</b> on dk^{O(d)} vertices. A lower bound complements this when d is part of the input: Independent Stable Set does not admit a polynomial kernel when parameterized by k+d unless NP \subseteq coNP/poly. This lower bound holds even when M is a partition matroid. - Another set of results concerns the scenario when the <b>graph</b> G is chordal. In this case, our computational lower bound excludes an FPT algorithm when the input matroid is given by its independence oracle. However, we demonstrate that Independent Stable Set can be solved in 2^{O(k)}||M||^{O(1)} time when M is a linear matroid given by its representation. In the same setting, Independent Stable Set does not have a polynomial kernel when parameterized by k unless NP\subseteq coNP/poly.</p></p class="citation"></blockquote><h3 id=22--124125-minor-containment-and-disjoint-paths-in-almost-linear-time-tuukka-korhonen-et-al-2024>(2/2 | 124/125) Minor Containment and Disjoint Paths in almost-linear time (Tuukka Korhonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuukka Korhonen, Michał Pilipczuk, Giannos Stamoulis. (2024)<br><strong>Minor Containment and Disjoint Paths in almost-linear time</strong><br><button class=copy-to-clipboard title="Minor Containment and Disjoint Paths in almost-linear time" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03958v1.pdf filename=2404.03958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give an algorithm that, given <b>graphs</b> $G$ and $H$, tests whether $H$ is a minor of $G$ in time ${\cal O}<em>H(n^{1+o(1)})$; here, $n$ is the number of vertices of $G$ and the ${\cal O}<em>H(\cdot)$-notation hides factors that depend on $H$ and are computable. By the <b>Graph</b> Minor Theorem, this implies the existence of an $n^{1+o(1)}$-time membership test for every minor-closed class of <b>graphs.</b> More generally, we give an ${\cal O}</em>{H,|X|}(m^{1+o(1)})$-time algorithm for the rooted version of the problem, in which $G$ comes with a set of roots $X\subseteq V(G)$ and some of the branch sets of the sought minor model of $H$ are required to contain prescribed subsets of $X$; here, $m$ is the total number of vertices and edges of $G$. This captures the Disjoint Paths problem, for which we obtain an ${\cal O}</em>{k}(m^{1+o(1)})$-time algorithm, where $k$ is the number of terminal pairs. For all the mentioned problems, the fastest algorithms known before are due to Kawarabayashi, Kobayashi, and Reed [JCTB 2012], and have a time complexity that is quadratic in the number of vertices of $G$. Our algorithm has two main ingredients: First, we show that by using the dynamic treewidth data structure of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\l}owski [FOCS 2023], the irrelevant vertex technique of Robertson and Seymour can be implemented in almost-linear time on apex-minor-free <b>graphs.</b> Then, we apply the recent advances in almost-linear time flow/cut algorithms to give an almost-linear time implementation of the recursive understanding technique, which effectively reduces the problem to apex-minor-free <b>graphs.</b></p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--125125-the-low-degree-hardness-of-finding-large-independent-sets-in-sparse-random-hypergraphs-abhishek-dhawan-et-al-2024>(1/1 | 125/125) The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs (Abhishek Dhawan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Dhawan, Yuzhou Wang. (2024)<br><strong>The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs</strong><br><button class=copy-to-clipboard title="The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC, math-CO, math-PR, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03842v1.pdf filename=2404.03842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the algorithmic task of finding large independent sets in Erdos-Renyi $r$-uniform hypergraphs on $n$ vertices having average degree $d$. Krivelevich and Sudakov showed that the maximum independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$. We show that the class of low-degree polynomial algorithms can find independent sets of density $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$ but no larger. This extends and generalizes earlier results of Gamarnik and Sudan, Rahman and Virag, and Wein on <b>graphs,</b> and answers a question of Bal and Bennett. We conjecture that this statistical-computational gap holds for this problem. Additionally, we explore the universality of this gap by examining $r$-partite hypergraphs. A hypergraph $H=(V,E)$ is $r$-partite if there is a partition $V=V_1\cup\cdots\cup V_r$ such that each edge contains exactly one vertex from each set $V_i$. We consider the problem of finding large balanced independent sets (independent sets containing the same number of vertices in each partition) in random $r$-partite hypergraphs with $n$ vertices in each partition and average degree $d$. We prove that the maximum balanced independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$ asymptotically. Furthermore, we prove an analogous low-degree computational threshold of $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$. Our results recover and generalize recent work of Perkins and the second author on bipartite <b>graphs.</b> While the <b>graph</b> case has been extensively studied, this work is the first to consider statistical-computational gaps of optimization problems on random hypergraphs. Our results suggest that these gaps persist for larger uniformities as well as across many models. A somewhat surprising aspect of the gap for balanced independent sets is that the algorithm achieving the lower bound is a simple degree-1 polynomial.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.06</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscv-26>cs.CV (26)</a><ul><li><a href=#126--1125-enhancing-breast-cancer-diagnosis-in-mammography-evaluation-and-integration-of-convolutional-neural-networks-and-explainable-ai-maryam-ahmed-et-al-2024>(1/26 | 1/125) Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI (Maryam Ahmed et al., 2024)</a></li><li><a href=#226--2125-image-text-co-decomposition-for-text-supervised-semantic-segmentation-ji-jia-wu-et-al-2024>(2/26 | 2/125) Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation (Ji-Jia Wu et al., 2024)</a></li><li><a href=#326--3125-dynamic-prompt-optimizing-for-text-to-image-generation-wenyi-mo-et-al-2024>(3/26 | 3/125) Dynamic Prompt Optimizing for Text-to-Image Generation (Wenyi Mo et al., 2024)</a></li><li><a href=#426--4125-learning-correlation-structures-for-vision-transformers-manjin-kim-et-al-2024>(4/26 | 4/125) Learning Correlation Structures for Vision Transformers (Manjin Kim et al., 2024)</a></li><li><a href=#526--5125-sigma-siamese-mamba-network-for-multi-modal-semantic-segmentation-zifu-wan-et-al-2024>(5/26 | 5/125) Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation (Zifu Wan et al., 2024)</a></li><li><a href=#626--6125-diffop-net-a-differential-operator-based-fully-convolutional-network-for-unsupervised-deformable-image-registration-jiong-wu-2024>(6/26 | 6/125) DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration (Jiong Wu, 2024)</a></li><li><a href=#726--7125-who-evaluates-the-evaluations-objectively-scoring-text-to-image-prompt-coherence-metrics-with-t2iscorescore-ts2-michael-saxon-et-al-2024>(7/26 | 7/125) Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2) (Michael Saxon et al., 2024)</a></li><li><a href=#826--8125-identity-decoupling-for-multi-subject-personalization-of-text-to-image-models-sangwon-jang-et-al-2024>(8/26 | 8/125) Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models (Sangwon Jang et al., 2024)</a></li><li><a href=#926--9125-physical-property-understanding-from-language-embedded-feature-fields-albert-j-zhai-et-al-2024>(9/26 | 9/125) Physical Property Understanding from Language-Embedded Feature Fields (Albert J. Zhai et al., 2024)</a></li><li><a href=#1026--10125-rasim-a-range-aware-high-fidelity-rgb-d-data-simulation-pipeline-for-real-world-applications-xingyu-liu-et-al-2024>(10/26 | 10/125) RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications (Xingyu Liu et al., 2024)</a></li><li><a href=#1126--11125-concept-weaver-enabling-multi-concept-fusion-in-text-to-image-models-gihyun-kwon-et-al-2024>(11/26 | 11/125) Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models (Gihyun Kwon et al., 2024)</a></li><li><a href=#1226--12125-3d-facial-expressions-through-analysis-by-neural-synthesis-george-retsinas-et-al-2024>(12/26 | 12/125) 3D Facial Expressions through Analysis-by-Neural-Synthesis (George Retsinas et al., 2024)</a></li><li><a href=#1326--13125-label-propagation-for-zero-shot-classification-with-vision-language-models-vladan-stojnić-et-al-2024>(13/26 | 13/125) Label Propagation for Zero-shot Classification with Vision-Language Models (Vladan Stojnić et al., 2024)</a></li><li><a href=#1426--14125-neural-symbolic-videoqa-learning-compositional-spatio-temporal-reasoning-for-real-world-video-question-answering-lili-liang-et-al-2024>(14/26 | 14/125) Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering (Lili Liang et al., 2024)</a></li><li><a href=#1526--15125-physics-inspired-synthesized-underwater-image-dataset-reina-kaneko-et-al-2024>(15/26 | 15/125) Physics-Inspired Synthesized Underwater Image Dataset (Reina Kaneko et al., 2024)</a></li><li><a href=#1626--16125-no-time-to-train-empowering-non-parametric-networks-for-few-shot-3d-scene-segmentation-xiangyang-zhu-et-al-2024>(16/26 | 16/125) No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation (Xiangyang Zhu et al., 2024)</a></li><li><a href=#1726--17125-instructhumans-editing-animated-3d-human-textures-with-instructions-jiayin-zhu-et-al-2024>(17/26 | 17/125) InstructHumans: Editing Animated 3D Human Textures with Instructions (Jiayin Zhu et al., 2024)</a></li><li><a href=#1826--18125-voltavision-a-transfer-learning-model-for-electronic-component-classification-anas-mohammad-ishfaqul-muktadir-osmani-et-al-2024>(18/26 | 18/125) VoltaVision: A Transfer Learning model for electronic component classification (Anas Mohammad Ishfaqul Muktadir Osmani et al., 2024)</a></li><li><a href=#1926--19125-increasing-fairness-in-classification-of-out-of-distribution-data-for-facial-recognition-gianluca-barone-et-al-2024>(19/26 | 19/125) Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition (Gianluca Barone et al., 2024)</a></li><li><a href=#2026--20125-improving-detection-in-aerial-images-by-capturing-inter-object-relationships-botao-ren-et-al-2024>(20/26 | 20/125) Improving Detection in Aerial Images by Capturing Inter-Object Relationships (Botao Ren et al., 2024)</a></li><li><a href=#2126--21125-scaresnet-a-resnet-variant-optimized-for-tiny-object-detection-in-transmission-and-distribution-towers-weile-li-et-al-2024>(21/26 | 21/125) SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers (Weile Li et al., 2024)</a></li><li><a href=#2226--22125-marsseg-mars-surface-semantic-segmentation-with-multi-level-extractor-and-connector-junbo-li-et-al-2024>(22/26 | 22/125) MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector (Junbo Li et al., 2024)</a></li><li><a href=#2326--23125-dynamic-risk-assessment-methodology-with-an-ldm-based-system-for-parking-scenarios-paola-natalia-cañas-et-al-2024>(23/26 | 23/125) Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios (Paola Natalia Cañas et al., 2024)</a></li><li><a href=#2426--24125-finsler-laplace-beltrami-operators-with-application-to-shape-analysis-simon-weber-et-al-2024>(24/26 | 24/125) Finsler-Laplace-Beltrami Operators with Application to Shape Analysis (Simon Weber et al., 2024)</a></li><li><a href=#2526--25125-robust-gaussian-splatting-françois-darmon-et-al-2024>(25/26 | 25/125) Robust Gaussian Splatting (François Darmon et al., 2024)</a></li><li><a href=#2626--26125-noisy-label-processing-for-classification-a-survey-mengting-li-et-al-2024>(26/26 | 26/125) Noisy Label Processing for Classification: A Survey (Mengting Li et al., 2024)</a></li></ul></li><li><a href=#cscl-23>cs.CL (23)</a><ul><li><a href=#123--27125-teaching-llama-a-new-language-through-cross-lingual-knowledge-transfer-hele-andra-kuulmets-et-al-2024>(1/23 | 27/125) Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer (Hele-Andra Kuulmets et al., 2024)</a></li><li><a href=#223--28125-extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction-bowen-zhang-et-al-2024>(2/23 | 28/125) Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction (Bowen Zhang et al., 2024)</a></li><li><a href=#323--29125-simple-techniques-for-enhancing-sentence-embeddings-in-generative-language-models-bowen-zhang-et-al-2024>(3/23 | 29/125) Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models (Bowen Zhang et al., 2024)</a></li><li><a href=#423--30125-cleared-for-takeoff-compositional--conditional-reasoning-may-be-the-achilles-heel-to-flight-booking-language-agents-harsh-kohli-et-al-2024>(4/23 | 30/125) Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents (Harsh Kohli et al., 2024)</a></li><li><a href=#523--31125-ffn-skipllm-a-hidden-gem-for-autoregressive-decoding-with-adaptive-feed-forward-skipping-ajay-jaiswal-et-al-2024>(5/23 | 31/125) FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping (Ajay Jaiswal et al., 2024)</a></li><li><a href=#623--32125-seme-at-semeval-2024-task-2-comparing-masked-and-generative-language-models-on-natural-language-inference-for-clinical-trials-mathilde-aguiar-et-al-2024>(6/23 | 32/125) SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials (Mathilde Aguiar et al., 2024)</a></li><li><a href=#723--33125-buddie-a-business-document-dataset-for-multi-task-information-extraction-ran-zmigrod-et-al-2024>(7/23 | 33/125) BuDDIE: A Business Document Dataset for Multi-task Information Extraction (Ran Zmigrod et al., 2024)</a></li><li><a href=#823--34125-unlocking-parameter-efficient-fine-tuning-for-low-resource-language-translation-tong-su-et-al-2024>(8/23 | 34/125) Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation (Tong Su et al., 2024)</a></li><li><a href=#923--35125-investigating-the-robustness-of-modelling-decisions-for-few-shot-cross-topic-stance-detection-a-preregistered-study-myrthe-reuver-et-al-2024>(9/23 | 35/125) Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study (Myrthe Reuver et al., 2024)</a></li><li><a href=#1023--36125-do-sentence-transformers-learn-quasi-geospatial-concepts-from-general-text-ilya-ilyankou-et-al-2024>(10/23 | 36/125) Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text? (Ilya Ilyankou et al., 2024)</a></li><li><a href=#1123--37125-data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving-gulsum-yigit-et-al-2024>(11/23 | 37/125) Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving (Gulsum Yigit et al., 2024)</a></li><li><a href=#1223--38125-forget-nli-use-a-dictionary-zero-shot-topic-classification-for-low-resource-languages-with-application-to-luxembourgish-fred-philippy-et-al-2024>(12/23 | 38/125) Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish (Fred Philippy et al., 2024)</a></li><li><a href=#1323--39125-saas-solving-ability-amplification-strategy-for-enhanced-mathematical-reasoning-in-large-language-models-hyeonwoo-kim-et-al-2024>(13/23 | 39/125) SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models (Hyeonwoo Kim et al., 2024)</a></li><li><a href=#1423--40125-verifiable-by-design-aligning-language-models-to-quote-from-pre-training-data-jingyu-zhang-et-al-2024>(14/23 | 40/125) Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data (Jingyu Zhang et al., 2024)</a></li><li><a href=#1523--41125-benchmarking-and-improving-compositional-generalization-of-multi-aspect-controllable-text-generation-tianqi-zhong-et-al-2024>(15/23 | 41/125) Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation (Tianqi Zhong et al., 2024)</a></li><li><a href=#1623--42125-chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model-xinrun-du-et-al-2024>(16/23 | 42/125) Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model (Xinrun Du et al., 2024)</a></li><li><a href=#1723--43125-clue-a-clinical-language-understanding-evaluation-for-llms-amin-dada-et-al-2024>(17/23 | 43/125) CLUE: A Clinical Language Understanding Evaluation for LLMs (Amin Dada et al., 2024)</a></li><li><a href=#1823--44125-assessing-the-quality-of-information-extraction-filip-seitl-et-al-2024>(18/23 | 44/125) Assessing the quality of information extraction (Filip Seitl et al., 2024)</a></li><li><a href=#1923--45125-willkommens-merkel-chaos-johnson-and-tore-klose-modeling-the-evaluative-meaning-of-german-personal-name-compounds-annerose-eichel-et-al-2024>(19/23 | 45/125) Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds (Annerose Eichel et al., 2024)</a></li><li><a href=#2023--46125-how-lexical-is-bilingual-lexicon-induction-harsh-kohli-et-al-2024>(20/23 | 46/125) How Lexical is Bilingual Lexicon Induction? (Harsh Kohli et al., 2024)</a></li><li><a href=#2123--47125-social-skill-training-with-large-language-models-diyi-yang-et-al-2024>(21/23 | 47/125) Social Skill Training with Large Language Models (Diyi Yang et al., 2024)</a></li><li><a href=#2223--48125-bear-a-unified-framework-for-evaluating-relational-knowledge-in-causal-and-masked-language-models-jacek-wiland-et-al-2024>(22/23 | 48/125) BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models (Jacek Wiland et al., 2024)</a></li><li><a href=#2323--49125-a-bi-consolidating-model-for-joint-relational-triple-extraction-xiaocheng-luo-et-al-2024>(23/23 | 49/125) A Bi-consolidating Model for Joint Relational Triple Extraction (Xiaocheng Luo et al., 2024)</a></li></ul></li><li><a href=#csro-10>cs.RO (10)</a><ul><li><a href=#110--50125-can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning-gawon-choi-et-al-2024>(1/10 | 50/125) Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning (Gawon Choi et al., 2024)</a></li><li><a href=#210--51125-voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots-akhil-padmanabha-et-al-2024>(2/10 | 51/125) VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots (Akhil Padmanabha et al., 2024)</a></li><li><a href=#310--52125-pomdp-guided-active-force-based-search-for-robotic-insertion-chen-wang-et-al-2024>(3/10 | 52/125) POMDP-Guided Active Force-Based Search for Robotic Insertion (Chen Wang et al., 2024)</a></li><li><a href=#410--53125-continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation-lanpei-li-et-al-2024>(4/10 | 53/125) Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation (Lanpei Li et al., 2024)</a></li><li><a href=#510--54125-scaling-motion-forecasting-models-with-ensemble-distillation-scott-ettinger-et-al-2024>(5/10 | 54/125) Scaling Motion Forecasting Models with Ensemble Distillation (Scott Ettinger et al., 2024)</a></li><li><a href=#610--55125-tooleenet-tool-affordance-6d-pose-estimation-yunlong-wang-et-al-2024>(6/10 | 55/125) ToolEENet: Tool Affordance 6D Pose Estimation (Yunlong Wang et al., 2024)</a></li><li><a href=#710--56125-designing-robots-to-help-women-martin-cooney-et-al-2024>(7/10 | 56/125) Designing Robots to Help Women (Martin Cooney et al., 2024)</a></li><li><a href=#810--57125-modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks-jordan-thompson-et-al-2024>(8/10 | 57/125) Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks (Jordan Thompson et al., 2024)</a></li><li><a href=#910--58125-multi-modal-perception-for-soft-robotic-interactions-using-generative-models-enrico-donato-et-al-2024>(9/10 | 58/125) Multi-modal perception for soft robotic interactions using generative models (Enrico Donato et al., 2024)</a></li><li><a href=#1010--59125-mm-gaussian-3d-gaussian-based-multi-modal-fusion-for-localization-and-reconstruction-in-unbounded-scenes-chenyang-wu-et-al-2024>(10/10 | 59/125) MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes (Chenyang Wu et al., 2024)</a></li></ul></li><li><a href=#csir-2>cs.IR (2)</a><ul><li><a href=#12--60125-a-comparison-of-methods-for-evaluating-generative-ir-negar-arabzadeh-et-al-2024>(1/2 | 60/125) A Comparison of Methods for Evaluating Generative IR (Negar Arabzadeh et al., 2024)</a></li><li><a href=#22--61125-dwell-in-the-beginning-how-language-models-embed-long-documents-for-dense-retrieval-joão-coelho-et-al-2024>(2/2 | 61/125) Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval (João Coelho et al., 2024)</a></li></ul></li><li><a href=#csai-4>cs.AI (4)</a><ul><li><a href=#14--62125-kgexplainer-towards-exploring-connected-subgraph-explanations-for-knowledge-graph-completion-tengfei-ma-et-al-2024>(1/4 | 62/125) KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion (Tengfei Ma et al., 2024)</a></li><li><a href=#24--63125-intervention-assisted-policy-gradient-methods-for-online-stochastic-queuing-network-optimization-technical-report-jerrod-wigmore-et-al-2024>(2/4 | 63/125) Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report (Jerrod Wigmore et al., 2024)</a></li><li><a href=#34--64125-random-walk-in-random-permutation-set-theory-jiefeng-zhou-et-al-2024>(3/4 | 64/125) Random Walk in Random Permutation Set Theory (Jiefeng Zhou et al., 2024)</a></li><li><a href=#44--65125-large-language-models-as-oracles-for-instantiating-ontologies-with-domain-specific-knowledge-giovanni-ciatto-et-al-2024>(4/4 | 65/125) Large language models as oracles for instantiating ontologies with domain-specific knowledge (Giovanni Ciatto et al., 2024)</a></li></ul></li><li><a href=#cslg-24>cs.LG (24)</a><ul><li><a href=#124--66125-player2vec-a-language-modeling-approach-to-understand-player-behavior-in-games-tianze-wang-et-al-2024>(1/24 | 66/125) player2vec: A Language Modeling Approach to Understand Player Behavior in Games (Tianze Wang et al., 2024)</a></li><li><a href=#224--67125-robust-preference-optimization-with-provable-noise-tolerance-for-llms-xize-liang-et-al-2024>(2/24 | 67/125) Robust Preference Optimization with Provable Noise Tolerance for LLMs (Xize Liang et al., 2024)</a></li><li><a href=#324--68125-enhancing-iot-intelligence-a-transformer-based-reinforcement-learning-methodology-gaith-rjoub-et-al-2024>(3/24 | 68/125) Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology (Gaith Rjoub et al., 2024)</a></li><li><a href=#424--69125-score-identity-distillation-exponentially-fast-distillation-of-pretrained-diffusion-models-for-one-step-generation-mingyuan-zhou-et-al-2024>(4/24 | 69/125) Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation (Mingyuan Zhou et al., 2024)</a></li><li><a href=#524--70125-mitigating-heterogeneity-in-federated-multimodal-learning-with-biomedical-vision-language-pre-training-zitao-shuai-et-al-2024>(5/24 | 70/125) Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training (Zitao Shuai et al., 2024)</a></li><li><a href=#624--71125-transformers-for-molecular-property-prediction-lessons-learned-from-the-past-five-years-afnan-sultan-et-al-2024>(6/24 | 71/125) Transformers for molecular property prediction: Lessons learned from the past five years (Afnan Sultan et al., 2024)</a></li><li><a href=#724--72125-optimizing-convolutional-neural-networks-for-identifying-invasive-pollinator-apis-mellifera-and-finding-a-ligand-drug-to-protect-californias-biodiversity-arnav-swaroop-2024>(7/24 | 72/125) Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity (Arnav Swaroop, 2024)</a></li><li><a href=#824--73125-dynamic-conditional-optimal-transport-through-simulation-free-flows-gavin-kerrigan-et-al-2024>(8/24 | 73/125) Dynamic Conditional Optimal Transport through Simulation-Free Flows (Gavin Kerrigan et al., 2024)</a></li><li><a href=#924--74125-model-selection-with-model-zoo-via-graph-learning-ziyu-li-et-al-2024>(9/24 | 74/125) Model Selection with Model Zoo via Graph Learning (Ziyu Li et al., 2024)</a></li><li><a href=#1024--75125-exploring-probabilistic-models-for-semi-supervised-learning-jianfeng-wang-2024>(10/24 | 75/125) Exploring Probabilistic Models for Semi-supervised Learning (Jianfeng Wang, 2024)</a></li><li><a href=#1124--76125-fusing-dictionary-learning-and-support-vector-machines-for-unsupervised-anomaly-detection-paul-irofti-et-al-2024>(11/24 | 76/125) Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection (Paul Irofti et al., 2024)</a></li><li><a href=#1224--77125-rolling-the-dice-for-better-deep-learning-performance-a-study-of-randomness-techniques-in-deep-neural-networks-mohammed-ghaith-altarabichi-et-al-2024>(12/24 | 77/125) Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks (Mohammed Ghaith Altarabichi et al., 2024)</a></li><li><a href=#1324--78125-multi-task-learning-for-lung-sound--lung-disease-classification-suma-k-v-et-al-2024>(13/24 | 78/125) Multi-Task Learning for Lung sound & Lung disease classification (Suma K V et al., 2024)</a></li><li><a href=#1424--79125-a-proximal-policy-optimization-based-intelligent-home-solar-management-kode-creer-et-al-2024>(14/24 | 79/125) A proximal policy optimization based intelligent home solar management (Kode Creer et al., 2024)</a></li><li><a href=#1524--80125-heterogeneous-multi-agent-reinforcement-learning-for-zero-shot-scalable-collaboration-xudong-guo-et-al-2024>(15/24 | 80/125) Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration (Xudong Guo et al., 2024)</a></li><li><a href=#1624--81125-gnnbench-fair-and-productive-benchmarking-for-single-gpu-gnn-system-yidong-gong-et-al-2024>(16/24 | 81/125) GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System (Yidong Gong et al., 2024)</a></li><li><a href=#1724--82125-growing-q-networks-solving-continuous-control-tasks-with-adaptive-control-resolution-tim-seyde-et-al-2024>(17/24 | 82/125) Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution (Tim Seyde et al., 2024)</a></li><li><a href=#1824--83125-active-causal-learning-for-decoding-chemical-complexities-with-targeted-interventions-zachary-r-fox-et-al-2024>(18/24 | 83/125) Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions (Zachary R. Fox et al., 2024)</a></li><li><a href=#1924--84125-generalizable-temperature-nowcasting-with-physics-constrained-rnns-for-predictive-maintenance-of-wind-turbine-components-johannes-exenberger-et-al-2024>(19/24 | 84/125) Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components (Johannes Exenberger et al., 2024)</a></li><li><a href=#2024--85125-continual-learning-with-weight-interpolation-jędrzej-kozal-et-al-2024>(20/24 | 85/125) Continual Learning with Weight Interpolation (Jędrzej Kozal et al., 2024)</a></li><li><a href=#2124--86125-demonstration-guided-multi-objective-reinforcement-learning-junlin-lu-et-al-2024>(21/24 | 86/125) Demonstration Guided Multi-Objective Reinforcement Learning (Junlin Lu et al., 2024)</a></li><li><a href=#2224--87125-hierarchical-neural-additive-models-for-interpretable-demand-forecasts-leif-feddersen-et-al-2024>(22/24 | 87/125) Hierarchical Neural Additive Models for Interpretable Demand Forecasts (Leif Feddersen et al., 2024)</a></li><li><a href=#2324--88125-derivative-free-tree-optimization-for-complex-systems-ye-wei-et-al-2024>(23/24 | 88/125) Derivative-free tree optimization for complex systems (Ye Wei et al., 2024)</a></li><li><a href=#2424--89125-approximate-umap-allows-for-high-rate-online-visualization-of-high-dimensional-data-streams-peter-wassenaar-et-al-2024>(24/24 | 89/125) Approximate UMAP allows for high-rate online visualization of high-dimensional data streams (Peter Wassenaar et al., 2024)</a></li></ul></li><li><a href=#cshc-2>cs.HC (2)</a><ul><li><a href=#12--90125-open-vocabulary-keyword-spotting-through-transfer-learning-from-speech-synthesis-kesavaraj-v-et-al-2024>(1/2 | 90/125) Open vocabulary keyword spotting through transfer learning from speech synthesis (Kesavaraj V et al., 2024)</a></li><li><a href=#22--91125-which-experimental-design-is-better-suited-for-vqa-tasks-eye-tracking-study-on-cognitive-load-performance-and-gaze-allocations-sita-a-vriend-et-al-2024>(2/2 | 91/125) Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations (Sita A. Vriend et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--92125-superior-genetic-algorithms-for-the-target-set-selection-problem-based-on-power-law-parameter-choices-and-simple-greedy-heuristics-benjamin-doerr-et-al-2024>(1/1 | 92/125) Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics (Benjamin Doerr et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--93125-evaluating-adversarial-robustness-a-comparison-of-fgsm-carlini-wagner-attacks-and-the-role-of-distillation-as-defense-mechanism-trilokesh-ranjan-sarkar-et-al-2024>(1/9 | 93/125) Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism (Trilokesh Ranjan Sarkar et al., 2024)</a></li><li><a href=#29--94125-re-pseudonymization-strategies-for-smart-meter-data-are-not-robust-to-deep-learning-profiling-attacks-ana-maria-cretu-et-al-2024>(2/9 | 94/125) Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks (Ana-Maria Cretu et al., 2024)</a></li><li><a href=#39--95125-reliable-feature-selection-for-adversarially-robust-cyber-attack-detection-joão-vitorino-et-al-2024>(3/9 | 95/125) Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection (João Vitorino et al., 2024)</a></li><li><a href=#49--96125-watermark-based-detection-and-attribution-of-ai-generated-content-zhengyuan-jiang-et-al-2024>(4/9 | 96/125) Watermark-based Detection and Attribution of AI-Generated Content (Zhengyuan Jiang et al., 2024)</a></li><li><a href=#59--97125-precision-guided-approach-to-mitigate-data-poisoning-attacks-in-federated-learning-k-naveen-kumar-et-al-2024>(5/9 | 97/125) Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning (K Naveen Kumar et al., 2024)</a></li><li><a href=#69--98125-you-can-use-but-cannot-recognize-preserving-visual-privacy-in-deep-neural-networks-qiushi-li-et-al-2024>(6/9 | 98/125) You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks (Qiushi Li et al., 2024)</a></li><li><a href=#79--99125-from-theory-to-comprehension-a-comparative-study-of-differential-privacy-and-k-anonymity-saskia-nuñez-von-voigt-et-al-2024>(7/9 | 99/125) From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity (Saskia Nuñez von Voigt et al., 2024)</a></li><li><a href=#89--100125-privshape-extracting-shapes-in-time-series-under-user-level-local-differential-privacy-yulian-mao-et-al-2024>(8/9 | 100/125) PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy (Yulian Mao et al., 2024)</a></li><li><a href=#99--101125-smart-contract-languages-a-comparative-analysis-massimo-bartoletti-et-al-2024>(9/9 | 101/125) Smart Contract Languages: a comparative analysis (Massimo Bartoletti et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#14--102125-nonlinear-kalman-filtering-based-on-self-attention-mechanism-and-lattice-trajectory-piecewise-linear-approximation-jiaming-wang-et-al-2024>(1/4 | 102/125) Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation (Jiaming Wang et al., 2024)</a></li><li><a href=#24--103125-torque-minimizing-control-allocation-for-overactuated-quadrupedal-locomotion-mads-erlend-bøe-lysø-et-al-2024>(2/4 | 103/125) Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion (Mads Erlend Bøe Lysø et al., 2024)</a></li><li><a href=#34--104125-field-teams-coordination-for-earthquake-damaged-distribution-system-energization-ilker-işık-et-al-2024>(3/4 | 104/125) Field Teams Coordination for Earthquake-Damaged Distribution System Energization (İlker Işık et al., 2024)</a></li><li><a href=#44--105125-queue-aware-network-control-algorithm-with-a-high-quantum-computing-readiness-evaluated-in-discrete-time-flow-simulator-for-fat-pipe-networks-arthur-witt-2024>(4/4 | 105/125) Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks (Arthur Witt, 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--106125-quantum-informed-simulations-for-mechanics-of-materials-dftbmbd-framework-zhaoxiang-shen-et-al-2024>(1/1 | 106/125) Quantum-informed simulations for mechanics of materials: DFTB+MBD framework (Zhaoxiang Shen et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--107125-a-posteriori-error-analysis-of-a-space-time-hybridizable-discontinuous-galerkin-method-for-the-advection-diffusion-problem-yuan-wang-et-al-2024>(1/2 | 107/125) A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem (Yuan Wang et al., 2024)</a></li><li><a href=#22--108125-highly-efficient-nurbs-based-isogeometric-analysis-for-coupled-nonlinear-diffusion-reaction-equations-with-and-without-advection-ilham-asmouh-et-al-2024>(2/2 | 108/125) Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection (Ilham Asmouh et al., 2024)</a></li></ul></li><li><a href=#csse-1>cs.SE (1)</a><ul><li><a href=#11--109125-pros-and-cons-evaluating-chatgpt-on-software-vulnerability-xin-yin-2024>(1/1 | 109/125) Pros and Cons! Evaluating ChatGPT on Software Vulnerability (Xin Yin, 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--110125-quand-rechercher-cest-faire-des-vagues--dans-et-à-partir-des-images-algorithmiques-gaëtan-robillard-2024>(1/1 | 110/125) Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques (Gaëtan Robillard, 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--111125-deep-phase-coded-image-prior-nimrod-shabtay-et-al-2024>(1/2 | 111/125) Deep Phase Coded Image Prior (Nimrod Shabtay et al., 2024)</a></li><li><a href=#22--112125-lidar-guided-cross-attention-fusion-for-hyperspectral-band-selection-and-image-classification-judy-x-yang-et-al-2024>(2/2 | 112/125) LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification (Judy X Yang et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--113125-semantic-sql----combining-and-optimizing-semantic-predicates-in-sql-akash-mittal-et-al-2024>(1/1 | 113/125) Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL (Akash Mittal et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--114125-simplifying-explicit-subtyping-coercions-in-a-polymorphic-calculus-with-effects-filip-koprivec-et-al-2024>(1/2 | 114/125) Simplifying explicit subtyping coercions in a polymorphic calculus with effects (Filip Koprivec et al., 2024)</a></li><li><a href=#22--115125-v-star-learning-visibly-pushdown-grammars-from-program-inputs-xiaodong-jia-et-al-2024>(2/2 | 115/125) V-Star: Learning Visibly Pushdown Grammars from Program Inputs (Xiaodong Jia et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--116125-h3dfact-heterogeneous-3d-integrated-cim-for-factorization-with-holographic-perceptual-representations-zishen-wan-et-al-2024>(1/1 | 116/125) H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations (Zishen Wan et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--117125-on-the-quest-for-effectiveness-in-human-oversight-interdisciplinary-perspectives-sarah-sterz-et-al-2024>(1/2 | 117/125) On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives (Sarah Sterz et al., 2024)</a></li><li><a href=#22--118125-a-conceptual-design-of-in-game-real-and-virtual-currency-tracker-dennis-barzanoff-et-al-2024>(2/2 | 118/125) A Conceptual Design of In-Game Real and Virtual Currency Tracker (Dennis Barzanoff et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--119125-nonparametric-modern-hopfield-models-jerry-yao-chieh-hu-et-al-2024>(1/1 | 119/125) Nonparametric Modern Hopfield Models (Jerry Yao-Chieh Hu et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--120125-wireless-resource-optimization-in-hybrid-semanticbit-communication-networks-le-xia-et-al-2024>(1/1 | 120/125) Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks (Le Xia et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--121125-hardness-of-circuit-and-monotone-diameters-of-polytopes-christian-nöbel-et-al-2024>(1/1 | 121/125) Hardness of circuit and monotone diameters of polytopes (Christian Nöbel et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--122125-discrete-fréchet-distance-oracles-boris-aronov-et-al-2024>(1/1 | 122/125) Discrete Fréchet Distance Oracles (Boris Aronov et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--123125-stability-in-graphs-with-matroid-constraints-fedor-v-fomin-et-al-2024>(1/2 | 123/125) Stability in Graphs with Matroid Constraints (Fedor V. Fomin et al., 2024)</a></li><li><a href=#22--124125-minor-containment-and-disjoint-paths-in-almost-linear-time-tuukka-korhonen-et-al-2024>(2/2 | 124/125) Minor Containment and Disjoint Paths in almost-linear time (Tuukka Korhonen et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--125125-the-low-degree-hardness-of-finding-large-independent-sets-in-sparse-random-hypergraphs-abhishek-dhawan-et-al-2024>(1/1 | 125/125) The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs (Abhishek Dhawan et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>