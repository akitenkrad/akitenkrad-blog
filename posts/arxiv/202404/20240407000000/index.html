<!doctype html><html><head><title>arXiv @ 2024.04.07</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.07"><meta property="og:description" content="Primary Categories cs.AI (8) cs.AR (1) cs.CC (1) cs.CE (1) cs.CG (1) cs.CL (27) cs.CR (13) cs.CV (35) cs.CY (2) cs.DB (1) cs.DC (2) cs.DS (3) cs.GR (1) cs.HC (4) cs.IR (3) cs.LG (33) cs.LO (1) cs.NE (2) cs.NI (1) cs.PL (2) cs.RO (15) cs.SD (2) cs.SE (1) econ.TH (1) eess.IV (2) eess.SY (5) math.NA (2) math.OC (1) nlin.PS (1) physics.data-an (1) stat.ME (1) stat.ML (4) Keywords keyword cs.CL cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240407000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-07T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-07T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.07"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240407000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Apr 7, 2024</p></div><div class=title><h1>arXiv @ 2024.04.07</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csai-8>cs.AI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscl-27>cs.CL (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscr-13>cs.CR (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscv-35>cs.CV (35)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csir-3>cs.IR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cslg-33>cs.LG (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#csse-1>cs.SE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#nlinps-1>nlin.PS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#physicsdata-an-1>physics.data-an (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/#statml-4>stat.ML (4)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Alpaca</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>8</td><td>2</td><td>12</td><td>9</td><td></td></tr><tr><td>Black Box</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Common-sense Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td>1</td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>2</td><td>5</td><td>3</td><td></td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>3</td><td>2</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Document Classification</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>3</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>6</td><td>2</td><td>2</td><td>5</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>GPT</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-3</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>2</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT-4 turbo</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>2</td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>1</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>1</td><td>2</td><td>3</td><td>3</td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>27</td><td>4</td><td>3</td><td>4</td><td>4</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Masked Language Model</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>1</td><td></td><td>6</td><td>2</td><td>2</td></tr><tr><td>Natural Language Inference</td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Noise-tolerant</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Open Information Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Phihsing</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>5</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>7</td><td>2</td><td>7</td><td>1</td><td>1</td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Question Answering</td><td>2</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td></td><td>3</td><td></td><td>1</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>1</td><td>6</td><td>1</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Security</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>1</td><td>1</td><td>3</td></tr><tr><td>Simulator</td><td></td><td></td><td>1</td><td>1</td><td>3</td></tr><tr><td>Stance Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td>1</td><td>1</td><td>6</td><td>3</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td></td><td>3</td><td>3</td><td></td></tr><tr><td>Virtual Reality (VR)</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>4</td><td></td><td>3</td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscr-13>cs.CR (13)</h2><h3 id=113--1178-increased-llm-vulnerabilities-from-fine-tuning-and-quantization-divyanshu-kumar-et-al-2024>(1/13 | 1/178) Increased LLM Vulnerabilities from Fine-tuning and Quantization (Divyanshu Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, Prashanth Harshangi. (2024)<br><strong>Increased LLM Vulnerabilities from Fine-tuning and Quantization</strong><br><button class=copy-to-clipboard title="Increased LLM Vulnerabilities from Fine-tuning and Quantization" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Quantization, LLaMA, Mistral, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04392v1.pdf filename=2404.04392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become very popular and have found use cases in many domains, such as <b>chatbots,</b> auto-task completion agents, and much more. However, <b>LLMs</b> are vulnerable to different types of attacks, such as jailbreaking, <b>prompt</b> injection attacks, and privacy leakage attacks. <b>Foundational</b> <b>LLMs</b> undergo adversarial and alignment training to learn not to generate malicious and toxic content. For specialized use cases, these <b>foundational</b> <b>LLMs</b> are subjected to <b>fine-tuning</b> or <b>quantization</b> for better performance and efficiency. We examine the impact of downstream tasks such as <b>fine-tuning</b> and <b>quantization</b> on <b>LLM</b> vulnerability. We test <b>foundation</b> <b>models</b> like <b>Mistral,</b> <b>Llama,</b> MosaicML, and their <b>fine-tuned</b> versions. Our research shows that <b>fine-tuning</b> and <b>quantization</b> reduces jailbreak resistance significantly, leading to increased <b>LLM</b> vulnerabilities. Finally, we demonstrate the utility of external guardrails in reducing <b>LLM</b> vulnerabilities.</p></p class="citation"></blockquote><h3 id=213--2178-auditgpt-auditing-smart-contracts-with-chatgpt-shihao-xia-et-al-2024>(2/13 | 2/178) AuditGPT: Auditing Smart Contracts with ChatGPT (Shihao Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shihao Xia, Shuai Shao, Mengting He, Tingting Yu, Linhai Song, Yiying Zhang. (2024)<br><strong>AuditGPT: Auditing Smart Contracts with ChatGPT</strong><br><button class=copy-to-clipboard title="AuditGPT: Auditing Smart Contracts with ChatGPT" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-CY, cs.CR<br>Keyword Score: 50<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04306v1.pdf filename=2404.04306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To govern smart contracts running on Ethereum, multiple Ethereum Request for Comment (ERC) standards have been developed, each containing a set of rules to guide the behaviors of smart contracts. Violating the ERC rules could cause serious <b>security</b> issues and financial loss, signifying the importance of verifying smart contracts follow ERCs. Today&rsquo;s practices of such verification are to either manually audit each single contract or use expert-developed, limited-scope program-analysis tools, both of which are far from being effective in identifying ERC rule violations. This paper presents a tool named AuditGPT that leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to automatically and comprehensively verify ERC rules against smart contracts. To build AuditGPT, we first conduct an empirical study on 222 ERC rules specified in four popular ERCs to understand their content, their <b>security</b> impacts, their specification in natural language, and their implementation in Solidity. Guided by the study, we construct AuditGPT by separating the <b>large,</b> <b>complex</b> <b>auditing</b> process into small, manageable tasks and design <b>prompts</b> specialized for each ERC rule type to enhance <b>LLMs&rsquo;</b> auditing performance. In the evaluation, AuditGPT successfully pinpoints 418 ERC rule violations and only reports 18 false positives, showcasing its effectiveness and accuracy. Moreover, AuditGPT beats an auditing service provided by <b>security</b> experts in effectiveness, accuracy, and cost, demonstrating its advancement over state-of-the-art smart-contract auditing practices.</p></p class="citation"></blockquote><h3 id=313--3178-evaluating-adversarial-robustness-a-comparison-of-fgsm-carlini-wagner-attacks-and-the-role-of-distillation-as-defense-mechanism-trilokesh-ranjan-sarkar-et-al-2024>(3/13 | 3/178) Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism (Trilokesh Ranjan Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen. (2024)<br><strong>Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism</strong><br><button class=copy-to-clipboard title="Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04245v1.pdf filename=2404.04245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report delves into an in-depth exploration of <b>adversarial</b> <b>attacks</b> specifically targeted at Deep Neural Networks (DNNs) utilized for image classification. The study also investigates defense mechanisms aimed at bolstering the robustness of machine learning models. The research focuses on comprehending the ramifications of two prominent attack methodologies: the Fast Gradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks are examined concerning three pre-trained image classifiers: Resnext50_32x4d, DenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the study proposes the robustness of defensive <b>distillation</b> as a defense mechanism to counter FGSM and CW attacks. This defense mechanism is evaluated using the CIFAR-10 dataset, where <b>CNN</b> models, specifically resnet101 and Resnext50_32x4d, serve as the teacher and student models, respectively. The proposed defensive <b>distillation</b> model exhibits effectiveness in thwarting attacks such as FGSM. However, it is noted to remain susceptible to more sophisticated techniques like the CW attack. The document presents a meticulous validation of the proposed scheme. It provides detailed and comprehensive results, elucidating the efficacy and limitations of the defense mechanisms employed. Through rigorous experimentation and analysis, the study offers insights into the dynamics of <b>adversarial</b> <b>attacks</b> on DNNs, as well as the effectiveness of defensive strategies in mitigating their impact.</p></p class="citation"></blockquote><h3 id=413--4178-re-pseudonymization-strategies-for-smart-meter-data-are-not-robust-to-deep-learning-profiling-attacks-ana-maria-cretu-et-al-2024>(4/13 | 4/178) Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks (Ana-Maria Cretu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye. (2024)<br><strong>Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks</strong><br><button class=copy-to-clipboard title="Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03948v1.pdf filename=2404.03948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart meters, devices measuring the electricity and gas consumption of a household, are currently being deployed at a fast rate throughout the world. The data they collect are extremely useful, including in the fight against climate change. However, these data and the information that can be inferred from them are highly sensitive. Re-pseudonymization, i.e., the frequent replacement of random identifiers over time, is widely used to share smart meter data while mitigating the risk of re-identification. We here show how, in spite of re-pseudonymization, households&rsquo; consumption records can be pieced together with high accuracy in large-scale datasets. We propose the first deep learning-based profiling attack against re-pseudonymized smart meter data. Our attack combines neural network embeddings, which are used to extract features from weekly consumption records and are tailored to the smart meter identification task, with a nearest neighbor classifier. We evaluate six neural networks architectures as the embedding model. Our results suggest that the <b>Transformer</b> and <b>CNN-LSTM</b> architectures vastly outperform previous methods as well as other architectures, successfully identifying the correct household 73.4% of the time among 5139 households based on electricity and gas consumption records (54.5% for electricity only). We further show that the features extracted by the embedding model maintain their effectiveness when transferred to a set of users disjoint from the one used to train the model. Finally, we extensively evaluate the robustness of our results. Taken together, our results strongly suggest that even frequent re-pseudonymization strategies can be reversed, strongly limiting their ability to prevent re-identification in practice.</p></p class="citation"></blockquote><h3 id=513--5178-vellet-verifiable-embedded-wallet-for-securing-authenticity-and-integrity-hiroki-watanabe-et-al-2024>(5/13 | 5/178) VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity (Hiroki Watanabe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroki Watanabe, Kohei Ichihara, Takumi Aita. (2024)<br><strong>VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity</strong><br><button class=copy-to-clipboard title="VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs.CR<br>Keyword Score: 20<br>Keywords: Phihsing, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03874v1.pdf filename=2404.03874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The blockchain ecosystem, particularly with the rise of Web3 and Non-Fungible Tokens (NFTs), has experienced a significant increase in users and applications. However, this expansion is challenged by the need to connect early adopters with a wider user base. A notable difficulty in this process is the complex interfaces of blockchain wallets, which can be daunting for those familiar with traditional payment methods. To address this issue, the category of &ldquo;embedded wallets&rdquo; has emerged as a promising solution. These wallets are seamlessly integrated into the front-end of decentralized applications (Dapps), simplifying the onboarding process for users and making access more widely available. However, our insights indicate that this simplification introduces a trade-off between ease of use and <b>security.</b> Embedded wallets lack transparency and auditability, leading to obscured transactions by the front end and a pronounced risk of fraud and <b>phishing</b> attacks. This paper proposes a new protocol to enhance the <b>security</b> of embedded wallets. Our VELLET protocol introduces a wallet verifier that can match the audit trail of embedded wallets on smart contracts, incorporating a process to verify authenticity and integrity. In the implementation architecture of the VELLET protocol, we suggest using the Text Record feature of the Ethereum Name Service (ENS), known as a decentralized domain name service, to serve as a repository for managing the audit trails of smart contracts. This approach has been demonstrated to reduce the necessity for new smart contract development and operational costs, proving cost-effective through a proof-of-concept. This protocol is a vital step in reducing <b>security</b> risks associated with embedded wallets, ensuring their convenience does not undermine user <b>security</b> and trust.</p></p class="citation"></blockquote><h3 id=613--6178-reliable-feature-selection-for-adversarially-robust-cyber-attack-detection-joão-vitorino-et-al-2024>(6/13 | 6/178) Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection (João Vitorino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Vitorino, Miguel Silva, Eva Maia, Isabel Praça. (2024)<br><strong>Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection</strong><br><button class=copy-to-clipboard title="Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-NI, cs.CR<br>Keyword Score: 13<br>Keywords: Adversarial Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04188v1.pdf filename=2404.04188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing cybersecurity threats make it essential to use high-quality data to train Machine Learning (ML) models for network traffic analysis, without noisy or missing data. By selecting the most relevant features for cyber-attack detection, it is possible to improve both the robustness and computational efficiency of the models used in a cybersecurity system. This work presents a feature selection and consensus process that combines multiple methods and applies them to several network datasets. Two different feature sets were selected and were used to train multiple ML models with regular and <b>adversarial</b> <b>training.</b> Finally, an <b>adversarial</b> <b>evasion</b> robustness <b>benchmark</b> was performed to analyze the reliability of the different feature sets and their impact on the susceptibility of the models to <b>adversarial</b> <b>examples.</b> By using an improved dataset with more data diversity, selecting the best time-related features and a more specific feature set, and performing <b>adversarial</b> <b>training,</b> the ML models were able to achieve a better adversarially robust generalization. The robustness of the models was significantly improved without their generalization to regular traffic flows being affected, without increases of false alarms, and without requiring too many computational resources, which enables a reliable detection of suspicious activity and perturbed traffic flows in enterprise computer networks.</p></p class="citation"></blockquote><h3 id=713--7178-smart-contract-languages-a-comparative-analysis-massimo-bartoletti-et-al-2024>(7/13 | 7/178) Smart Contract Languages: a comparative analysis (Massimo Bartoletti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Massimo Bartoletti, Lorenzo Benetollo, Michele Bugliesi, Silvia Crafa, Giacomo Dal Sasso, Roberto Pettinau, Andrea Pinna, Mattia Piras, Sabina Rossi, Stefano Salis, Alvise Spanò, Viacheslav Tkachenko, Roberto Tonelli, Roberto Zunino. (2024)<br><strong>Smart Contract Languages: a comparative analysis</strong><br><button class=copy-to-clipboard title="Smart Contract Languages: a comparative analysis" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-PL, cs.CR<br>Keyword Score: 13<br>Keywords: Benchmarking, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04129v1.pdf filename=2404.04129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized blockchain platforms support the secure exchange of assets among users without relying on trusted third parties. These exchanges are programmed with smart contracts, computer programs directly executed by blockchain nodes. Multiple smart contract languages are available nowadays to developers, each with its own distinctive features, strengths, and weaknesses. In this paper, we examine the smart contract languages used in six major blockchain platforms: Ethereum, Solana, Cardano, Algorand, Aptos, and Tezos. Starting with a high-level overview of their design choices, we provide a comprehensive assessment that focuses on programming style, <b>security,</b> code readability, and usability, drawing on an original <b>benchmark</b> that encompasses a common set of use cases across all the smart contract languages under examination.</p></p class="citation"></blockquote><h3 id=813--8178-reconfigurable-and-scalable-honeynet-for-cyber-physical-systems-luís-sousa-et-al-2024>(8/13 | 8/178) Reconfigurable and Scalable Honeynet for Cyber-Physical Systems (Luís Sousa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luís Sousa, José Cecílio, Pedro Ferreira, Alan Oliveira. (2024)<br><strong>Reconfigurable and Scalable Honeynet for Cyber-Physical Systems</strong><br><button class=copy-to-clipboard title="Reconfigurable and Scalable Honeynet for Cyber-Physical Systems" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04385v1.pdf filename=2404.04385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industrial Control Systems (ICS) constitute the backbone of contemporary industrial operations, ranging from modest heating, ventilation, and air conditioning systems to expansive national power grids. Given their pivotal role in critical infrastructure, there has been a concerted effort to enhance <b>security</b> measures and deepen our comprehension of potential cyber threats within this domain. To address these challenges, numerous implementations of Honeypots and Honeynets intended to detect and understand attacks have been employed for ICS. This approach diverges from conventional methods by focusing on making a scalable and reconfigurable honeynet for cyber-physical systems. It will also automatically generate attacks on the honeynet to test and validate it. With the development of a scalable and reconfigurable Honeynet and automatic attack generation tools, it is also expected that the system will serve as a basis for producing datasets for training algorithms for detecting and classifying attacks in cyber-physical honeynets.</p></p class="citation"></blockquote><h3 id=913--9178-watermark-based-detection-and-attribution-of-ai-generated-content-zhengyuan-jiang-et-al-2024>(9/13 | 9/178) Watermark-based Detection and Attribution of AI-Generated Content (Zhengyuan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong. (2024)<br><strong>Watermark-based Detection and Attribution of AI-Generated Content</strong><br><button class=copy-to-clipboard title="Watermark-based Detection and Attribution of AI-Generated Content" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04254v1.pdf filename=2404.04254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several companies&ndash;such as Google, Microsoft, and OpenAI&ndash;have deployed techniques to watermark AI-generated content to enable proactive detection. However, existing literature mainly focuses on user-agnostic detection. Attribution aims to further trace back the user of a <b>generative-AI</b> <b>service</b> who generated a given content detected as AI-generated. Despite its growing importance, attribution is largely unexplored. In this work, we aim to bridge this gap by providing the first systematic study on watermark-based, user-aware detection and attribution of AI-generated content. Specifically, we theoretically study the detection and attribution performance via rigorous probabilistic analysis. Moreover, we develop an efficient algorithm to select watermarks for the users to enhance attribution performance. Both our theoretical and empirical results show that watermark-based detection and attribution inherit the accuracy and (non-)robustness properties of the watermarking method.</p></p class="citation"></blockquote><h3 id=1013--10178-precision-guided-approach-to-mitigate-data-poisoning-attacks-in-federated-learning-k-naveen-kumar-et-al-2024>(10/13 | 10/178) Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning (K Naveen Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>K Naveen Kumar, C Krishna Mohan, Aravind Machiry. (2024)<br><strong>Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning</strong><br><button class=copy-to-clipboard title="Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04139v1.pdf filename=2404.04139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a collaborative learning paradigm enabling participants to collectively train a shared machine learning model while preserving the privacy of their sensitive data. Nevertheless, the inherent decentralized and data-opaque characteristics of FL render its susceptibility to data poisoning attacks. These attacks introduce malformed or malicious inputs during local model training, subsequently influencing the global model and resulting in erroneous predictions. Current FL defense strategies against data poisoning attacks either involve a trade-off between accuracy and robustness or necessitate the presence of a uniformly distributed root dataset at the server. To overcome these limitations, we present FedZZ, which harnesses a zone-based deviating update (ZBDU) mechanism to effectively counter data poisoning attacks in FL. Further, we introduce a precision-guided methodology that actively characterizes these client clusters (zones), which in turn aids in recognizing and discarding malicious updates at the server. Our evaluation of FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate its efficacy in mitigating data poisoning attacks, surpassing the performance of prevailing state-of-the-art methodologies in both single and multi-client attack scenarios and varying attack volumes. Notably, FedZZ also functions as a robust client selection strategy, even in highly non-IID and attack-free scenarios. Moreover, in the face of escalating poisoning rates, the model accuracy attained by FedZZ displays superior resilience compared to existing techniques. For instance, when confronted with a 50% presence of malicious clients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the second-best solution, FL-Defender, diminishes to 43.36%.</p></p class="citation"></blockquote><h3 id=1113--11178-you-can-use-but-cannot-recognize-preserving-visual-privacy-in-deep-neural-networks-qiushi-li-et-al-2024>(11/13 | 11/178) You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks (Qiushi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiushi Li, Yan Zhang, Ju Ren, Qi Li, Yaoxue Zhang. (2024)<br><strong>You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks</strong><br><button class=copy-to-clipboard title="You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04098v1.pdf filename=2404.04098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image data have been extensively used in Deep Neural Network (DNN) tasks in various scenarios, e.g., autonomous driving and medical image analysis, which incurs significant privacy concerns. Existing privacy protection techniques are unable to efficiently protect such data. For example, <b>Differential</b> <b>Privacy</b> (DP) that is an emerging technique protects data with strong privacy guarantee cannot effectively protect visual features of exposed image dataset. In this paper, we propose a novel privacy-preserving framework VisualMixer that protects the training data of visual DNN tasks by pixel shuffling, while not injecting any noises. VisualMixer utilizes a new privacy metric called Visual Feature Entropy (VFE) to effectively quantify the visual features of an image from both biological and machine vision aspects. In VisualMixer, we devise a task-agnostic image obfuscation method to protect the visual privacy of data for DNN training and inference. For each image, it determines regions for pixel shuffling in the image and the sizes of these regions according to the desired VFE. It shuffles pixels both in the spatial domain and in the chromatic channel space in the regions without injecting noises so that it can prevent visual features from being discerned and recognized, while incurring negligible accuracy loss. Extensive experiments on real-world datasets demonstrate that VisualMixer can effectively preserve the visual privacy with negligible accuracy loss, i.e., at average 2.35 percentage points of model accuracy loss, and almost no performance degradation on model training.</p></p class="citation"></blockquote><h3 id=1213--12178-from-theory-to-comprehension-a-comparative-study-of-differential-privacy-and-k-anonymity-saskia-nuñez-von-voigt-et-al-2024>(12/13 | 12/178) From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity (Saskia Nuñez von Voigt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saskia Nuñez von Voigt, Luise Mehner, Florian Tschorsch. (2024)<br><strong>From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity</strong><br><button class=copy-to-clipboard title="From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04006v1.pdf filename=2404.04006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The notion of $\varepsilon$-differential privacy is a widely used concept of providing quantifiable privacy to individuals. However, it is unclear how to explain the level of privacy protection provided by a <b>differential</b> <b>privacy</b> mechanism with a set $\varepsilon$. In this study, we focus on users&rsquo; comprehension of the privacy protection provided by a <b>differential</b> <b>privacy</b> mechanism. To do so, we study three variants of explaining the privacy protection provided by <b>differential</b> <b>privacy:</b> (1) the original mathematical definition; (2) $\varepsilon$ translated into a specific privacy risk; and (3) an explanation using the randomized response technique. We compare users&rsquo; comprehension of privacy protection employing these explanatory models with their comprehension of privacy protection of $k$-anonymity as baseline comprehensibility. Our findings suggest that participants&rsquo; comprehension of <b>differential</b> <b>privacy</b> protection is enhanced by the privacy risk model and the randomized response-based model. Moreover, our results confirm our intuition that privacy protection provided by $k$-anonymity is more comprehensible.</p></p class="citation"></blockquote><h3 id=1313--13178-privshape-extracting-shapes-in-time-series-under-user-level-local-differential-privacy-yulian-mao-et-al-2024>(13/13 | 13/178) PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy (Yulian Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulian Mao, Qingqing Ye, Haibo Hu, Qi Wang, Kai Huang. (2024)<br><strong>PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy</strong><br><button class=copy-to-clipboard title="PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03873v1.pdf filename=2404.03873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series have numerous applications in finance, healthcare, IoT, and smart city. In many of these applications, time series typically contain personal data, so privacy infringement may occur if they are released directly to the public. Recently, local <b>differential</b> <b>privacy</b> (LDP) has emerged as the state-of-the-art approach to protecting data privacy. However, existing works on LDP-based collections cannot preserve the shape of time series. A recent work, PatternLDP, attempts to address this problem, but it can only protect a finite group of elements in a time series due to {\omega}-event level privacy guarantee. In this paper, we propose PrivShape, a trie-based mechanism under user-level LDP to protect all elements. PrivShape first transforms a time series to reduce its length, and then adopts trie-expansion and two-level refinement to improve utility. By extensive experiments on real-world datasets, we demonstrate that PrivShape outperforms PatternLDP when adapted for offline use, and can effectively extract frequent shapes.</p></p class="citation"></blockquote><h2 id=cscl-27>cs.CL (27)</h2><h3 id=127--14178-deciphering-political-entity-sentiment-in-news-with-large-language-models-zero-shot-and-few-shot-strategies-alapan-kuila-et-al-2024>(1/27 | 14/178) Deciphering Political Entity Sentiment in News with Large Language Models: Zero-Shot and Few-Shot Strategies (Alapan Kuila et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alapan Kuila, Sudeshna Sarkar. (2024)<br><strong>Deciphering Political Entity Sentiment in News with Large Language Models: Zero-Shot and Few-Shot Strategies</strong><br><button class=copy-to-clipboard title="Deciphering Political Entity Sentiment in News with Large Language Models: Zero-Shot and Few-Shot Strategies" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Fine-tuning, Zero-shot, BERT, Sentiment Analysis, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04361v1.pdf filename=2404.04361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentiment</b> <b>analysis</b> plays a pivotal role in understanding public opinion, particularly in the political domain where the portrayal of entities in news articles influences public perception. In this paper, we investigate the effectiveness of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in predicting entity-specific <b>sentiment</b> <b>from</b> political news articles. Leveraging <b>zero-shot</b> and <b>few-shot</b> strategies, we explore the capability of <b>LLMs</b> to discern <b>sentiment</b> <b>towards</b> political entities in news content. Employing a chain-of-thought (COT) approach augmented with rationale in <b>few-shot</b> <b>in-context</b> <b>learning,</b> we assess whether this method enhances <b>sentiment</b> <b>prediction</b> accuracy. Our evaluation on <b>sentiment-labeled</b> <b>datasets</b> demonstrates that <b>LLMs,</b> outperform <b>fine-tuned</b> <b>BERT</b> models in capturing entity-specific <b>sentiment.</b> <b>We</b> find that learning <b>in-context</b> <b>significantly</b> improves model performance, while the self-consistency mechanism enhances consistency in <b>sentiment</b> <b>prediction.</b> Despite the promising results, we observe inconsistencies in the effectiveness of the COT <b>prompting</b> method. Overall, our findings underscore the potential of <b>LLMs</b> in entity-centric <b>sentiment</b> <b>analysis</b> within the political news domain and highlight the importance of suitable <b>prompting</b> strategies and model architectures.</p></p class="citation"></blockquote><h3 id=227--15178-assisting-humans-in-complex-comparisons-automated-information-comparison-at-scale-truman-yuen-et-al-2024>(2/27 | 15/178) Assisting humans in complex comparisons: automated information comparison at scale (Truman Yuen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Truman Yuen, Graham A. Watt, Yuri Lawryshyn. (2024)<br><strong>Assisting humans in complex comparisons: automated information comparison at scale</strong><br><button class=copy-to-clipboard title="Assisting humans in complex comparisons: automated information comparison at scale" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-8, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Zero-shot, Reasoning, In-context Learning, Large Language Model, Large Language Model, Prompt, Rouge, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04351v1.pdf filename=2404.04351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>Large</b> <b>Language</b> <b>Models</b> enable efficient analytics across knowledge domains, rivalling human experts in information comparisons. However, the applications of <b>LLMs</b> for information comparisons face scalability challenges due to the difficulties in maintaining information across <b>large</b> <b>contexts</b> <b>and</b> overcoming model token limitations. To address these challenges, we developed the novel Abstractive <b>Summarization</b> & Criteria-driven Comparison Endpoint (ASC$^2$End) system to automate information comparison at scale. Our system employs Semantic Text Similarity comparisons for generating evidence-supported analyses. We utilize proven data-handling strategies such as abstractive <b>summarization</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> to overcome token limitations and retain relevant information during model inference. <b>Prompts</b> were designed using <b>zero-shot</b> strategies to contextualize information for improved model <b>reasoning.</b> We evaluated abstractive <b>summarization</b> using <b>ROUGE</b> scoring and assessed the generated comparison quality using survey responses. Models evaluated on the ASC$^2$End system show desirable results providing insights on the expected performance of the system. ASC$^2$End is a novel system and tool that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval.</p></p class="citation"></blockquote><h3 id=327--16178-teaching-llama-a-new-language-through-cross-lingual-knowledge-transfer-hele-andra-kuulmets-et-al-2024>(3/27 | 16/178) Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer (Hele-Andra Kuulmets et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hele-Andra Kuulmets, Taido Purason, Agnes Luhtaru, Mark Fishel. (2024)<br><strong>Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer</strong><br><button class=copy-to-clipboard title="Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Knowledge Transfer, Alpaca, LLaMA, Common-sense Reasoning, Instruction Following, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04042v1.pdf filename=2404.04042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores cost-efficient methods to adapt pretrained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to new lower-resource languages, with a specific focus on Estonian. Leveraging the <b>Llama</b> 2 model, we investigate the impact of combining cross-lingual <b>instruction-tuning</b> <b>with</b> additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual <b>instruction-tuning</b> <b>significantly</b> enhances results on Estonian. Furthermore, we showcase cross-lingual <b>knowledge</b> <b>transfer</b> from high-quality English <b>instructions</b> <b>to</b> Estonian, resulting in improvements in <b>commonsense</b> <b>reasoning</b> and multi-turn conversation capabilities. Our best model, named \textsc{Llammas}, represents the first open-source <b>instruction-following</b> <b>LLM</b> for Estonian. Additionally, we publish <b>Alpaca-est,</b> the first general task <b>instruction</b> <b>dataset</b> for Estonia. These contributions mark the initial progress in the direction of developing open-source <b>LLMs</b> for Estonian.</p></p class="citation"></blockquote><h3 id=427--17178-extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction-bowen-zhang-et-al-2024>(4/27 | 17/178) Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction (Bowen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhang, Harold Soh. (2024)<br><strong>Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction</strong><br><button class=copy-to-clipboard title="Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 86<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Open Information Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03868v1.pdf filename=2404.03868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we are interested in automated methods for <b>knowledge</b> <b>graph</b> creation (KGC) from input text. Progress on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has <b>prompted</b> a series of recent works applying them to KGC, e.g., via zero/few-shot <b>prompting.</b> Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that in prior methods, the <b>KG</b> schema has to be included in the <b>LLM</b> <b>prompt</b> to generate valid triplets; larger and more complex schema easily exceed the <b>LLMs&rsquo;</b> context window length. To address this problem, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): <b>open</b> <b>information</b> <b>extraction</b> followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the <b>LLMs&rsquo;</b> extraction performance in a <b>retrieval-augmented</b> <b>generation-like</b> <b>manner.</b> We demonstrate on three KGC <b>benchmarks</b> that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works.</p></p class="citation"></blockquote><h3 id=527--18178-simple-techniques-for-enhancing-sentence-embeddings-in-generative-language-models-bowen-zhang-et-al-2024>(5/27 | 18/178) Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models (Bowen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhang, Kehua Chang, Chunping Li. (2024)<br><strong>Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models</strong><br><button class=copy-to-clipboard title="Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, LLaMA, Mistral, Sentence Embedding, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03921v1.pdf filename=2404.03921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Sentence</b> <b>Embedding</b> stands as a fundamental task within the realm of Natural Language Processing, finding extensive application in search engines, expert systems, and question-and-answer platforms. With the continuous evolution of <b>large</b> <b>language</b> <b>models</b> such as <b>LLaMA</b> and <b>Mistral,</b> research on <b>sentence</b> <b>embedding</b> has recently achieved notable breakthroughs. However, these advancements mainly pertain to <b>fine-tuning</b> scenarios, leaving explorations into computationally efficient direct inference methods for <b>sentence</b> <b>representation</b> in a nascent stage. This paper endeavors to bridge this research gap. Through comprehensive experimentation, we challenge the widely held belief in the necessity of an Explicit One-word Limitation for deriving <b>sentence</b> <b>embeddings</b> from <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs).</b> We demonstrate that this approach, while beneficial for generative models under direct inference scenario, is not imperative for discriminative models or the <b>fine-tuning</b> of generative <b>PLMs.</b> This discovery sheds new light on the design of manual templates in future studies. Building upon this insight, we propose two innovative <b>prompt</b> engineering techniques capable of further enhancing the expressive power of <b>PLMs&rsquo;</b> raw embeddings: Pretended Chain of Thought and Knowledge Enhancement. We confirm their effectiveness across various <b>PLM</b> types and provide a detailed exploration of the underlying factors contributing to their success.</p></p class="citation"></blockquote><h3 id=627--19178-cleared-for-takeoff-compositional--conditional-reasoning-may-be-the-achilles-heel-to-flight-booking-language-agents-harsh-kohli-et-al-2024>(6/27 | 19/178) Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents (Harsh Kohli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Kohli, Huan Sun. (2024)<br><strong>Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents</strong><br><button class=copy-to-clipboard title="Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, GPT-4 turbo, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04237v1.pdf filename=2404.04237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid progress of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has seen them excel and frequently surpass human performance on standard <b>benchmarks.</b> This has enabled many downstream applications, such as <b>LLM</b> agents, to rely on their sophisticated <b>reasoning</b> to navigate complex task requirements. However, <b>LLMs</b> are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional <b>reasoning,</b> two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse <b>benchmark</b> connecting these <b>reasoning</b> skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art <b>LLMs</b> with even the best performing model, <b>GPT-4</b> <b>Turbo,</b> not exceeding 67% accuracy despite advanced <b>prompting</b> techniques.</p></p class="citation"></blockquote><h3 id=727--20178-ffn-skipllm-a-hidden-gem-for-autoregressive-decoding-with-adaptive-feed-forward-skipping-ajay-jaiswal-et-al-2024>(7/27 | 20/178) FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping (Ajay Jaiswal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ajay Jaiswal, Bodun Hu, Lu Yin, Yeonju Ro, Shiwei Liu, Tianlong Chen, Aditya Akella. (2024)<br><strong>FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping</strong><br><button class=copy-to-clipboard title="FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, LLaMA, Neural Machine Translation, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03865v1.pdf filename=2404.03865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive <b>Large</b> <b>Language</b> <b>Models</b> (e.g., <b>LLaMa,</b> <b>GPTs)</b> are omnipresent achieving remarkable success in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges for autoregressive token-by-token generation. To mitigate computation overload incurred during generation, several early-exit and layer-dropping strategies have been proposed. Despite some promising success due to the redundancy across <b>LLMs</b> layers on metrics like Rough-L/BLUE, our careful knowledge-intensive evaluation unveils issues such as generation collapse, hallucination of wrong facts, and noticeable performance drop even at the trivial exit ratio of 10-15% of layers. We attribute these errors primarily to ineffective handling of the KV cache through state copying during early-exit. In this work, we observed the saturation of computationally expensive feed-forward blocks of <b>LLM</b> layers and proposed FFN-SkipLLM, which is a novel fine-grained skip strategy of autoregressive <b>LLMs.</b> More specifically, FFN-SkipLLM is an input-adaptive feed-forward skipping strategy that can skip 25-30% of FFN blocks of <b>LLMs</b> with marginal change in performance on knowledge-intensive generation tasks without any requirement to handle KV cache. Our extensive experiments and ablation across <b>benchmarks</b> like <b>MT-Bench,</b> Factoid-QA, and variable-length <b>text</b> <b>summarization</b> illustrate how our simple and ease-at-use method can facilitate faster autoregressive decoding.</p></p class="citation"></blockquote><h3 id=827--21178-seme-at-semeval-2024-task-2-comparing-masked-and-generative-language-models-on-natural-language-inference-for-clinical-trials-mathilde-aguiar-et-al-2024>(8/27 | 21/178) SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials (Mathilde Aguiar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi. (2024)<br><strong>SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials</strong><br><button class=copy-to-clipboard title="SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Natural Language Inference, Natural Language Inference, Textual Entailment, Large Language Model, Masked Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03977v1.pdf filename=2404.03977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trials. The Multi-evidence <b>Natural</b> <b>Language</b> <b>Inference</b> for Clinical Trial Data (NLI4CT) consists of a <b>Textual</b> <b>Entailment</b> (TE) task focused on the evaluation of the consistency and faithfulness of <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> models applied to Clinical Trial Reports (CTR). We test 2 distinct approaches, one based on <b>finetuning</b> and ensembling <b>Masked</b> <b>Language</b> <b>Models</b> and the other based on <b>prompting</b> <b>Large</b> <b>Language</b> <b>Models</b> using templates, in particular, using Chain-Of-Thought and Contrastive Chain-Of-Thought. <b>Prompting</b> Flan-T5-large in a 2-shot setting leads to our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56 Consistency.</p></p class="citation"></blockquote><h3 id=927--22178-buddie-a-business-document-dataset-for-multi-task-information-extraction-ran-zmigrod-et-al-2024>(9/27 | 22/178) BuDDIE: A Business Document Dataset for Multi-task Information Extraction (Ran Zmigrod et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Zmigrod, Dongsheng Wang, Mathieu Sibue, Yulong Pei, Petr Babkin, Ivan Brugere, Xiaomo Liu, Nacho Navarro, Antony Papadimitriou, William Watson, Zhiqiang Ma, Armineh Nourbakhsh, Sameena Shah. (2024)<br><strong>BuDDIE: A Business Document Dataset for Multi-task Information Extraction</strong><br><button class=copy-to-clipboard title="BuDDIE: A Business Document Dataset for Multi-task Information Extraction" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Multi-modal, Document Classification, Information Retrieval, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04003v1.pdf filename=2404.04003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of visually rich <b>document</b> <b>understanding</b> (VRDU) aims to solve a multitude of well-researched NLP tasks in a <b>multi-modal</b> domain. Several datasets exist for research on specific tasks of VRDU such as <b>document</b> <b>classification</b> (DC), key entity extraction (KEE), entity linking, <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA),</b> inter alia. These datasets cover <b>documents</b> <b>like</b> invoices and receipts with sparse annotations such that they support one or two co-related tasks (e.g., entity extraction and entity linking). Unfortunately, only focusing on a single specific of <b>documents</b> <b>or</b> task is not representative of how <b>documents</b> <b>often</b> need to be processed in the wild - where variety in style and requirements is expected. In this paper, we introduce BuDDIE (Business <b>Document</b> <b>Dataset</b> for <b>Information</b> <b>Extraction),</b> the first multi-task dataset of 1,665 real-world business <b>documents</b> <b>that</b> contains rich and dense annotations for DC, KEE, and <b>VQA.</b> Our dataset consists of publicly available business entity <b>documents</b> <b>from</b> US state government websites. The <b>documents</b> <b>are</b> structured and vary in their style and layout across states and types (e.g., forms, certificates, reports, etc.). We provide data variety and quality metrics for BuDDIE as well as a series of baselines for each task. Our baselines cover traditional textual, <b>multi-modal,</b> and <b>large</b> <b>language</b> <b>model</b> approaches to VRDU.</p></p class="citation"></blockquote><h3 id=1027--23178-unlocking-parameter-efficient-fine-tuning-for-low-resource-language-translation-tong-su-et-al-2024>(10/27 | 23/178) Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation (Tong Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Su, Xin Peng, Sarubi Thillainathan, David Guzmán, Surangika Ranathunga, En-Shiun Annie Lee. (2024)<br><strong>Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation</strong><br><button class=copy-to-clipboard title="Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Low-Resource, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04212v1.pdf filename=2404.04212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) methods are increasingly vital in adapting large-scale <b>pre-trained</b> <b>language</b> <b>models</b> for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in <b>Low-Resource</b> Language (LRL) <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT)</b> to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods.</p></p class="citation"></blockquote><h3 id=1127--24178-investigating-the-robustness-of-modelling-decisions-for-few-shot-cross-topic-stance-detection-a-preregistered-study-myrthe-reuver-et-al-2024>(11/27 | 24/178) Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study (Myrthe Reuver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myrthe Reuver, Suzan Verberne, Antske Fokkens. (2024)<br><strong>Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study</strong><br><button class=copy-to-clipboard title="Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, RoBERTa, Natural Language Inference, Natural Language Inference, Stance Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03987v1.pdf filename=2404.03987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a viewpoint-diverse news recommender, identifying whether two news articles express the same viewpoint is essential. One way to determine &ldquo;same or different&rdquo; viewpoint is <b>stance</b> <b>detection.</b> In this paper, we investigate the robustness of operationalization choices for <b>few-shot</b> <b>stance</b> <b>detection,</b> with special attention to modelling <b>stance</b> <b>across</b> different topics. Our experiments test pre-registered hypotheses on <b>stance</b> <b>detection.</b> Specifically, we compare two <b>stance</b> <b>task</b> definitions (Pro/Con versus Same Side <b>Stance),</b> <b>two</b> <b>LLM</b> architectures (bi-encoding versus cross-encoding), and adding <b>Natural</b> <b>Language</b> <b>Inference</b> knowledge, with pre-trained <b>RoBERTa</b> models trained with shots of 100 examples from 7 different <b>stance</b> <b>detection</b> datasets. Some of our hypotheses and claims from earlier work can be confirmed, while others give more inconsistent results. The effect of the Same Side <b>Stance</b> <b>definition</b> on performance differs per dataset and is influenced by other modelling choices. We found no relationship between the number of training topics in the training shots and performance. In general, cross-encoding out-performs bi-encoding, and adding <b>NLI</b> training to our models gives considerable improvement, but these results are not consistent across all datasets. Our results indicate that it is essential to include multiple datasets and systematic modelling experiments when aiming to find robust modelling choices for the concept `stance'.</p></p class="citation"></blockquote><h3 id=1227--25178-scope-ambiguities-in-large-language-models-gaurav-kamath-et-al-2024>(12/27 | 25/178) Scope Ambiguities in Large Language Models (Gaurav Kamath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Kamath, Sebastian Schuster, Sowmya Vajjala, Siva Reddy. (2024)<br><strong>Scope Ambiguities in Large Language Models</strong><br><button class=copy-to-clipboard title="Scope Ambiguities in Large Language Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-2, GPT-4, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04332v1.pdf filename=2404.04332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentences containing multiple semantic operators with overlapping scope often create ambiguities in interpretation, known as scope ambiguities. These ambiguities offer rich insights into the interaction between semantic structure and world knowledge in language processing. Despite this, there has been little research into how modern <b>large</b> <b>language</b> <b>models</b> treat them. In this paper, we investigate how different versions of certain autoregressive language models &ndash; <b>GPT-2,</b> <b>GPT-3/3.5,</b> <b>Llama</b> 2 and <b>GPT-4</b> &ndash; treat scope ambiguous sentences, and compare this with human judgments. We introduce novel datasets that contain a joint total of almost 1,000 unique scope-ambiguous sentences, containing interactions between a range of semantic operators, and annotated for human judgments. Using these datasets, we find evidence that several models (i) are sensitive to the meaning ambiguity in these sentences, in a way that patterns well with human judgments, and (ii) can successfully identify human-preferred readings at a high level of accuracy (over 90% in some cases).</p></p class="citation"></blockquote><h3 id=1327--26178-do-sentence-transformers-learn-quasi-geospatial-concepts-from-general-text-ilya-ilyankou-et-al-2024>(13/27 | 26/178) Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text? (Ilya Ilyankou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilya Ilyankou, Aldo Lipani, Stefano Cavazzi, Xiaowei Gao, James Haworth. (2024)<br><strong>Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text?</strong><br><button class=copy-to-clipboard title="Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text?" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Recommendation, Zero-shot, Transformer, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04169v1.pdf filename=2404.04169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence <b>transformers</b> are language models designed to perform semantic search. This study investigates the capacity of sentence <b>transformers,</b> <b>fine-tuned</b> on general <b>question-answering</b> <b>datasets</b> for asymmetric semantic search, to associate descriptions of human-generated routes across Great Britain with queries often used to describe hiking experiences. We find that sentence <b>transformers</b> have some <b>zero-shot</b> capabilities to understand quasi-geospatial concepts, such as route types and difficulty, suggesting their potential utility for routing <b>recommendation</b> systems.</p></p class="citation"></blockquote><h3 id=1427--27178-data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving-gulsum-yigit-et-al-2024>(14/27 | 27/178) Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving (Gulsum Yigit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gulsum Yigit, Mehmet Fatih Amasyali. (2024)<br><strong>Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving</strong><br><button class=copy-to-clipboard title="Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, LLaMA, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03938v1.pdf filename=2404.03938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Math Word Problem (MWP) solving presents a challenging task in Natural Language Processing (NLP). This study aims to provide MWP solvers with a more diverse training set, ultimately improving their ability to solve various math problems. We propose several methods for <b>data</b> <b>augmentation</b> by modifying the problem texts and equations, such as synonym replacement, rule-based: question replacement, and rule based: reversing question methodologies over two English MWP datasets. This study extends by introducing a new <b>in-context</b> <b>learning</b> augmentation method, employing the <b>Llama-7b</b> language model. This approach involves instruction-based <b>prompting</b> for rephrasing the math problem texts. Performance evaluations are conducted on 9 baseline models, revealing that augmentation methods outperform baseline models. Moreover, concatenating examples generated by various augmentation methods further improves performance.</p></p class="citation"></blockquote><h3 id=1527--28178-forget-nli-use-a-dictionary-zero-shot-topic-classification-for-low-resource-languages-with-application-to-luxembourgish-fred-philippy-et-al-2024>(15/27 | 28/178) Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish (Fred Philippy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fred Philippy, Shohreh Haddadan, Siwen Guo. (2024)<br><strong>Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish</strong><br><button class=copy-to-clipboard title="Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Low-Resource, Zero-shot, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03912v1.pdf filename=2404.03912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In NLP, <b>zero-shot</b> classification (ZSC) is the task of assigning labels to textual data without any labeled examples for the target classes. A common method for ZSC is to <b>fine-tune</b> a language model on a <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> dataset and then use it to infer the entailment between the input document and the target labels. However, this approach faces certain challenges, particularly for languages with limited resources. In this paper, we propose an alternative solution that leverages dictionaries as a source of data for ZSC. We focus on Luxembourgish, a <b>low-resource</b> language spoken in Luxembourg, and construct two new topic relevance classification datasets based on a dictionary that provides various synonyms, word translations and example sentences. We evaluate the usability of our dataset and compare it with the <b>NLI-based</b> approach on two topic classification tasks in a <b>zero-shot</b> manner. Our results show that by using the dictionary-based dataset, the trained models outperform the ones following the <b>NLI-based</b> approach for ZSC. While we focus on a single <b>low-resource</b> language in this study, we believe that the efficacy of our approach can also transfer to other languages where such a dictionary is available.</p></p class="citation"></blockquote><h3 id=1627--29178-saas-solving-ability-amplification-strategy-for-enhanced-mathematical-reasoning-in-large-language-models-hyeonwoo-kim-et-al-2024>(16/27 | 29/178) SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models (Hyeonwoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim, Wonseok Lee, Chanjun Park. (2024)<br><strong>SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03887v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03887v2.pdf filename=2404.03887v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel learning approach designed to enhance both <b>mathematical</b> <b>reasoning</b> and problem-solving abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We focus on integrating the Chain-of-Thought (CoT) and the Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning of <b>mathematical</b> <b>reasoning</b> ability is helpful for the amplification of problem-solving ability. Thus, the initial learning with CoT is essential for solving challenging <b>mathematical</b> <b>problems.</b> To this end, we propose a sequential learning approach, named SAAS (Solving Ability Amplification Strategy), which strategically transitions from CoT learning to PoT learning. Our empirical study, involving an extensive performance comparison using several <b>benchmarks,</b> demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The results underscore the effectiveness of our sequential learning approach, marking a significant advancement in the field of <b>mathematical</b> <b>reasoning</b> in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1727--30178-towards-realistic-few-shot-relation-extraction-a-new-meta-dataset-and-evaluation-fahmida-alam-et-al-2024>(17/27 | 30/178) Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation (Fahmida Alam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahmida Alam, Md Asiful Islam, Robert Vacareanu, Mihai Surdeanu. (2024)<br><strong>Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation</strong><br><button class=copy-to-clipboard title="Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Few-shot, Supervised Learning, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04445v1.pdf filename=2404.04445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a meta dataset for <b>few-shot</b> <b>relation</b> <b>extraction,</b> which includes two datasets derived from existing <b>supervised</b> <b>relation</b> <b>extraction</b> datasets NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and Gurevych, 2017) as well as a <b>few-shot</b> form of the TACRED dataset (Sabo et al., 2021). Importantly, all these <b>few-shot</b> datasets were generated under realistic assumptions such as: the test <b>relations</b> <b>are</b> different from any <b>relations</b> <b>a</b> model might have seen before, limited training data, and a preponderance of candidate <b>relation</b> <b>mentions</b> that do not correspond to any of the <b>relations</b> <b>of</b> interest. Using this large resource, we conduct a comprehensive evaluation of six recent <b>few-shot</b> <b>relation</b> <b>extraction</b> methods, and observe that no method comes out as a clear winner. Further, the overall performance on this task is low, indicating substantial need for future research. We release all versions of the data, i.e., both <b>supervised</b> and <b>few-shot,</b> for future research.</p></p class="citation"></blockquote><h3 id=1827--31178-verifiable-by-design-aligning-language-models-to-quote-from-pre-training-data-jingyu-zhang-et-al-2024>(18/27 | 31/178) Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data (Jingyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel Khashabi. (2024)<br><strong>Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data</strong><br><button class=copy-to-clipboard title="Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-domain, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03862v1.pdf filename=2404.03862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For humans to trust the fluent generations of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> they must be able to verify their correctness against trusted, external sources. Recent efforts aim to increase verifiability through citations of retrieved documents or post-hoc provenance. However, such citations are prone to mistakes that further complicate their verifiability. To address these limitations, we tackle the verifiability goal with a different philosophy: we trivialize the verification process by developing models that quote verbatim statements from trusted sources in pre-training data. We propose Quote-Tuning, which demonstrates the feasibility of aligning <b>LLMs</b> to leverage memorized information and quote from pre-training data. Quote-Tuning quantifies quoting against <b>large</b> <b>corpora</b> <b>with</b> efficient membership inference tools, and uses the amount of quotes as an implicit reward signal to construct a synthetic preference dataset for quoting, without any human annotation. Next, the target model is aligned to quote using preference optimization algorithms. Experimental results show that Quote-Tuning significantly increases the percentage of <b>LLM</b> generation quoted verbatim from high-quality pre-training documents by 55% to 130% relative to untuned models while maintaining response quality. Further experiments demonstrate that Quote-Tuning generalizes quoting to <b>out-of-domain</b> data, is applicable in different tasks, and provides additional benefits to truthfulness. Quote-Tuning not only serves as a hassle-free method to increase quoting but also opens up avenues for improving <b>LLM</b> trustworthiness through better verifiability.</p></p class="citation"></blockquote><h3 id=1927--32178-benchmarking-and-improving-compositional-generalization-of-multi-aspect-controllable-text-generation-tianqi-zhong-et-al-2024>(19/27 | 32/178) Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation (Tianqi Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, Zhendong Mao. (2024)<br><strong>Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation</strong><br><button class=copy-to-clipboard title="Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Meta Learning, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04232v1.pdf filename=2404.04232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional generalization, representing the model&rsquo;s ability to generate <b>text</b> <b>with</b> new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable <b>text</b> <b>generation</b> (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation <b>benchmark</b> of MCTG is still lacking. We propose CompMCTG, a <b>benchmark</b> encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce <b>Meta-MCTG,</b> <b>a</b> training framework incorporating <b>meta-learning,</b> <b>where</b> we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of <b>Meta-MCTG</b> <b>through</b> achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4% cases.</p></p class="citation"></blockquote><h3 id=2027--33178-chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model-xinrun-du-et-al-2024>(20/27 | 33/178) Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model (Xinrun Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang. (2024)<br><strong>Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model</strong><br><button class=copy-to-clipboard title="Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04167v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04167v3.pdf filename=2404.04167v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce CT-LLM, a 2B <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> that illustrates a pivotal shift towards prioritizing the Chinese language in developing <b>LLMs.</b> Uniquely initiated from scratch, CT-LLM diverges from the conventional methodology by primarily incorporating Chinese textual data, utilizing an extensive corpus of 1,200 billion tokens, including 800 billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This strategic composition facilitates the model&rsquo;s exceptional proficiency in understanding and processing Chinese, a capability further enhanced through alignment techniques. Demonstrating remarkable performance on the CHC-Bench, CT-LLM excels in Chinese language tasks, and showcases its adeptness in English through SFT. This research challenges the prevailing paradigm of training <b>LLMs</b> predominantly on English corpora and then adapting them to other languages, broadening the horizons for <b>LLM</b> training methodologies. By open-sourcing the full process of training a Chinese <b>LLM,</b> including a detailed data processing procedure with the obtained Massive Appropriate Pretraining Chinese Corpus (MAP-CC), a well-chosen multidisciplinary Chinese Hard Case <b>Benchmark</b> (CHC-Bench), and the 2B-size Chinese Tiny <b>LLM</b> (CT-LLM), we aim to foster further exploration and innovation in both academia and industry, paving the way for more inclusive and versatile language models.</p></p class="citation"></blockquote><h3 id=2127--34178-clue-a-clinical-language-understanding-evaluation-for-llms-amin-dada-et-al-2024>(21/27 | 34/178) CLUE: A Clinical Language Understanding Evaluation for LLMs (Amin Dada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek. (2024)<br><strong>CLUE: A Clinical Language Understanding Evaluation for LLMs</strong><br><button class=copy-to-clipboard title="CLUE: A Clinical Language Understanding Evaluation for LLMs" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04067v1.pdf filename=2404.04067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown the potential to significantly contribute to patient care, diagnostics, and administrative processes. Emerging biomedical <b>LLMs</b> address healthcare-specific challenges, including privacy demands and computational constraints. However, evaluation of these models has primarily been limited to non-clinical tasks, which do not reflect the complexity of practical clinical applications. Additionally, there has been no thorough comparison between biomedical and general-domain <b>LLMs</b> for clinical tasks. To fill this gap, we present the Clinical Language Understanding Evaluation (CLUE), a <b>benchmark</b> tailored to evaluate <b>LLMs</b> on real-world clinical tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters and four existing tasks designed to test the practical applicability of <b>LLMs</b> in healthcare settings. Our evaluation covers several biomedical and general domain <b>LLMs,</b> providing insights into their clinical performance and applicability. CLUE represents a step towards a standardized approach to evaluating and developing <b>LLMs</b> in healthcare to align future model development with the real-world needs of clinical application. We publish our evaluation and data generation scripts: <a href=https://github.com/dadaamin/CLUE>https://github.com/dadaamin/CLUE</a></p></p class="citation"></blockquote><h3 id=2227--35178-assessing-the-quality-of-information-extraction-filip-seitl-et-al-2024>(22/27 | 35/178) Assessing the quality of information extraction (Filip Seitl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Seitl, Tomáš Kovářík, Soheyla Mirshahi, Jan Kryštůfek, Rastislav Dujava, Matúš Ondreička, Herbert Ullrich, Petr Gronat. (2024)<br><strong>Assessing the quality of information extraction</strong><br><button class=copy-to-clipboard title="Assessing the quality of information extraction" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04068v1.pdf filename=2404.04068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in <b>large</b> <b>language</b> <b>models</b> have notably enhanced the efficiency of <b>information</b> <b>extraction</b> from unstructured and semi-structured data sources. As these technologies become integral to various applications, establishing an objective measure for the quality of <b>information</b> <b>extraction</b> becomes imperative. However, the scarcity of labeled data presents significant challenges to this endeavor. In this paper, we introduce an automatic framework to assess the quality of the <b>information</b> <b>extraction</b> and its completeness. The framework focuses on <b>information</b> <b>extraction</b> in the form of entity and its properties. We discuss how to handle the input/output size limitations of the <b>large</b> <b>language</b> <b>models</b> and analyze their performance when iteratively extracting the <b>information.</b> <b>Finally,</b> we introduce metrics to evaluate the quality of the extraction and provide an extensive discussion on how to interpret the metrics.</p></p class="citation"></blockquote><h3 id=2327--36178-willkommens-merkel-chaos-johnson-and-tore-klose-modeling-the-evaluative-meaning-of-german-personal-name-compounds-annerose-eichel-et-al-2024>(23/27 | 36/178) Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds (Annerose Eichel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annerose Eichel, Tana Deeg, André Blessing, Milena Belosevic, Sabine Arndt-Lappe, Sabine Schulte im Walde. (2024)<br><strong>Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds</strong><br><button class=copy-to-clipboard title="Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04031v1.pdf filename=2404.04031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a comprehensive computational study of the under-investigated phenomenon of personal name compounds (PNCs) in German such as Willkommens-Merkel (&lsquo;Welcome-Merkel&rsquo;). Prevalent in news, social media, and political discourse, PNCs are hypothesized to exhibit an evaluative function that is reflected in a more positive or negative perception as compared to the respective personal full name (such as Angela Merkel). We model 321 PNCs and their corresponding full names at discourse level, and show that PNCs bear an evaluative nature that can be captured through a variety of computational methods. Specifically, we assess through valence information whether a PNC is more positively or negatively evaluative than the person&rsquo;s name, by applying and comparing two approaches using (i) valence norms and (ii) <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> We further enrich our data with personal, domain-specific, and extra-linguistic information and perform a range of regression analyses revealing that factors including compound and modifier valence, domain, and political party membership influence how a PNC is evaluated.</p></p class="citation"></blockquote><h3 id=2427--37178-how-lexical-is-bilingual-lexicon-induction-harsh-kohli-et-al-2024>(24/27 | 37/178) How Lexical is Bilingual Lexicon Induction? (Harsh Kohli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Kohli, Helian Feng, Nicholas Dronen, Calvin McCarter, Sina Moeini, Ali Kebarighotbi. (2024)<br><strong>How Lexical is Bilingual Lexicon Induction?</strong><br><button class=copy-to-clipboard title="How Lexical is Bilingual Lexicon Induction?" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04221v1.pdf filename=2404.04221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contemporary machine learning approaches to bilingual lexicon induction (BLI), a model learns a mapping between the embedding spaces of a language pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art results on the task. However, the problem remains challenging in <b>low-resource</b> settings, due to the paucity of data. The task is complicated by factors such as lexical variation across languages. We argue that the incorporation of additional lexical information into the recent retrieve-and-rank approach should improve lexicon induction. We demonstrate the efficacy of our proposed approach on XLING, improving over the previous state of the art by an average of 2% across all language pairs.</p></p class="citation"></blockquote><h3 id=2527--38178-social-skill-training-with-large-language-models-diyi-yang-et-al-2024>(25/27 | 38/178) Social Skill Training with Large Language Models (Diyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S. Bernstein, John Mitchell. (2024)<br><strong>Social Skill Training with Large Language Models</strong><br><button class=copy-to-clipboard title="Social Skill Training with Large Language Models" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04204v1.pdf filename=2404.04204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages <b>large</b> <b>language</b> <b>models</b> for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.</p></p class="citation"></blockquote><h3 id=2627--39178-bear-a-unified-framework-for-evaluating-relational-knowledge-in-causal-and-masked-language-models-jacek-wiland-et-al-2024>(26/27 | 39/178) BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models (Jacek Wiland et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacek Wiland, Max Ploner, Alan Akbik. (2024)<br><strong>BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models</strong><br><button class=copy-to-clipboard title="BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04113v1.pdf filename=2404.04113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to <b>masked</b> <b>or</b> <b>causal</b> LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM&rsquo;s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.</p></p class="citation"></blockquote><h3 id=2727--40178-a-bi-consolidating-model-for-joint-relational-triple-extraction-xiaocheng-luo-et-al-2024>(27/27 | 40/178) A Bi-consolidating Model for Joint Relational Triple Extraction (Xiaocheng Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaocheng Luo, Yanping Chen, Ruixue Tang, Ruizhang Huang, Yongbin Qin. (2024)<br><strong>A Bi-consolidating Model for Joint Relational Triple Extraction</strong><br><button class=copy-to-clipboard title="A Bi-consolidating Model for Joint Relational Triple Extraction" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03881v1.pdf filename=2404.03881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current methods to extract relational triples directly make a prediction based on a possible entity pair in a raw sentence without depending on entity recognition. The task suffers from a serious semantic overlapping problem, in which several relation triples may share one or two entities in a sentence. It is weak to learn discriminative semantic features relevant to a relation triple. In this paper, based on a two-dimensional sentence representation, a bi-consolidating model is proposed to address this problem by simultaneously reinforcing the local and global semantic features relevant to a relation triple. This model consists of a local consolidation component and a global consolidation component. The first component uses a pixel difference <b>convolution</b> to enhance semantic information of a possible triple representation from adjacent regions and mitigate noise in neighbouring neighbours. The second component strengthens the triple representation based a channel attention and a spatial attention, which has the advantage to learn remote semantic dependencies in a sentence. They are helpful to improve the performance of both entity identification and relation type classification in relation triple extraction. After evaluated on several publish datasets, it achieves competitive performance. Analytical experiments demonstrate the effectiveness of our model for relational triple extraction and give motivation for other natural language processing tasks.</p></p class="citation"></blockquote><h2 id=cscv-35>cs.CV (35)</h2><h3 id=135--41178-enhancing-breast-cancer-diagnosis-in-mammography-evaluation-and-integration-of-convolutional-neural-networks-and-explainable-ai-maryam-ahmed-et-al-2024>(1/35 | 41/178) Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI (Maryam Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir. (2024)<br><strong>Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI</strong><br><button class=copy-to-clipboard title="Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 91<br>Keywords: Black Box, Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Explainable AI, Fairness, Fine-tuning, Multi-modal, Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03892v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03892v2.pdf filename=2404.03892v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study introduces an integrated framework combining <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and <b>Explainable</b> <b>Artificial</b> Intelligence (XAI) for the enhanced diagnosis of breast cancer using the CBIS-DDSM dataset. Utilizing a <b>fine-tuned</b> ResNet50 architecture, our investigation not only provides effective differentiation of mammographic images into benign and malignant categories but also addresses the opaque <b>&ldquo;black-box&rdquo;</b> <b>nature</b> of deep learning models by employing XAI methodologies, namely Grad-CAM, LIME, and SHAP, to interpret <b>CNN</b> decision-making processes for healthcare professionals. Our methodology encompasses an elaborate <b>data</b> <b>preprocessing</b> pipeline and advanced <b>data</b> <b>augmentation</b> techniques to counteract dataset limitations, and <b>transfer</b> <b>learning</b> using pre-trained networks, such as VGG-16, DenseNet and ResNet was employed. A focal point of our study is the evaluation of XAI&rsquo;s effectiveness in interpreting model predictions, highlighted by utilising the Hausdorff measure to assess the alignment between AI-generated explanations and expert annotations quantitatively. This approach plays a critical role for XAI in promoting trustworthiness and ethical <b>fairness</b> in AI-assisted diagnostics. The findings from our research illustrate the effective collaboration between <b>CNNs</b> and XAI in advancing diagnostic methods for breast cancer, thereby facilitating a more seamless integration of advanced AI technologies within clinical settings. By enhancing the interpretability of AI-driven decisions, this work lays the groundwork for improved collaboration between AI systems and medical practitioners, ultimately enriching patient care. Furthermore, the implications of our research extend well beyond the current methodologies, advocating for subsequent inquiries into the integration of <b>multimodal</b> <b>data</b> <b>and</b> the refinement of AI explanations to satisfy the needs of clinical practice.</p></p class="citation"></blockquote><h3 id=235--42178-vision-transformers-in-domain-adaptation-and-generalization-a-study-of-robustness-shadi-alijani-et-al-2024>(2/35 | 42/178) Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness (Shadi Alijani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shadi Alijani, Jamil Fayyad, Homayoun Najjaran. (2024)<br><strong>Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness</strong><br><button class=copy-to-clipboard title="Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-10; I-4-9; I-4-7; I-5-1, cs-AI, cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Vision Transformer, Data Augmentation, Distribution Shift, Distribution Shift, Meta Learning, Transformer, Domain Adaptation, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04452v1.pdf filename=2404.04452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models are often evaluated in scenarios where the <b>data</b> <b>distribution</b> <b>is</b> different from those used in the training and validation phases. The discrepancy presents a challenge for accurately predicting the performance of models once deployed on the target <b>distribution.</b> <b>Domain</b> <b>adaptation</b> and generalization are widely recognized as effective strategies for addressing such shifts, thereby ensuring reliable performance. The recent promising results in applying <b>vision</b> <b>transformers</b> in computer <b>vision</b> <b>tasks,</b> coupled with advancements in <b>self-attention</b> mechanisms, have demonstrated their significant potential for robustness and generalization in handling <b>distribution</b> <b>shifts.</b> Motivated by the increased interest from the research community, our paper investigates the deployment of <b>vision</b> <b>transformers</b> in <b>domain</b> <b>adaptation</b> and <b>domain</b> <b>generalization</b> scenarios. For <b>domain</b> <b>adaptation</b> methods, we categorize research into feature-level, instance-level, model-level adaptations, and hybrid approaches, along with other categorizations with respect to diverse strategies for enhancing <b>domain</b> <b>adaptation.</b> Similarly, for <b>domain</b> <b>generalization,</b> we categorize research into multi-domain learning, <b>meta-learning,</b> <b>regularization</b> techniques, and <b>data</b> <b>augmentation</b> strategies. We further classify diverse strategies in research, underscoring the various approaches researchers have taken to address <b>distribution</b> <b>shifts</b> by integrating <b>vision</b> <b>transformers.</b> The inclusion of comprehensive tables summarizing these categories is a distinct feature of our work, offering valuable insights for researchers. These findings highlight the versatility of <b>vision</b> <b>transformers</b> in managing <b>distribution</b> <b>shifts,</b> crucial for real-world applications, especially in critical safety and decision-making scenarios.</p></p class="citation"></blockquote><h3 id=335--43178-koala-key-frame-conditioned-long-video-llm-reuben-tan-et-al-2024>(3/35 | 43/178) Koala: Key frame-conditioned long video-LLM (Reuben Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan A. Plummer, Bryan Russell, Kate Saenko. (2024)<br><strong>Koala: Key frame-conditioned long video-LLM</strong><br><button class=copy-to-clipboard title="Koala: Key frame-conditioned long video-LLM" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Self-supervised Learning, Zero-shot, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04346v1.pdf filename=2404.04346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long video <b>question</b> <b>answering</b> is a challenging task that involves recognizing short-term activities and <b>reasoning</b> about their fine-grained relationships. State-of-the-art video <b>Large</b> <b>Language</b> <b>Models</b> (vLLMs) hold promise as a viable solution due to their demonstrated emergent capabilities on new tasks. However, despite being trained on millions of short seconds-long videos, vLLMs are unable to understand minutes-long videos and accurately answer <b>questions</b> <b>about</b> them. To address this limitation, we propose a lightweight and <b>self-supervised</b> approach, Key frame-conditioned long video-LLM (Koala), that introduces learnable spatiotemporal queries to adapt pretrained vLLMs for generalizing to longer videos. Our approach introduces two new tokenizers that condition on visual tokens computed from sparse video key frames for understanding short and long video moments. We train our proposed approach on HowTo100M and demonstrate its effectiveness on <b>zero-shot</b> long video understanding <b>benchmarks,</b> where it outperforms state-of-the-art <b>large</b> <b>models</b> <b>by</b> 3 - 6% in absolute accuracy across all tasks. Surprisingly, we also empirically show that our approach not only helps a pretrained vLLM to understand long videos but also improves its accuracy on short-term action recognition.</p></p class="citation"></blockquote><h3 id=435--44178-image-text-co-decomposition-for-text-supervised-semantic-segmentation-ji-jia-wu-et-al-2024>(4/35 | 44/178) Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation (Ji-Jia Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, Chun-Pei Chen, Yu-Lun Liu, Min-Hung Chen, Hou-Ning Hu, Yung-Yu Chuang, Yen-Yu Lin. (2024)<br><strong>Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Image2text, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04231v1.pdf filename=2404.04231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses text-supervised semantic segmentation, aiming to learn a model capable of segmenting arbitrary visual concepts within images by using only <b>image-text</b> pairs without dense annotations. Existing methods have demonstrated that <b>contrastive</b> <b>learning</b> on <b>image-text</b> pairs effectively aligns visual segments with the meanings of texts. We notice that there is a discrepancy between text alignment and semantic segmentation: A text often consists of multiple semantic concepts, whereas semantic segmentation strives to create semantically homogeneous segments. To address this issue, we propose a novel framework, <b>Image-Text</b> Co-Decomposition (CoDe), where the paired image and text are jointly decomposed into a set of image regions and a set of word segments, respectively, and <b>contrastive</b> <b>learning</b> is developed to enforce region-word alignment. To work with a <b>vision-language</b> model, we present a <b>prompt</b> <b>learning</b> mechanism that derives an extra representation to highlight an image segment or a word segment of interest, with which more effective features can be extracted from that segment. Comprehensive experimental results demonstrate that our method performs favorably against existing text-supervised semantic segmentation methods on six <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=535--45178-dynamic-prompt-optimizing-for-text-to-image-generation-wenyi-mo-et-al-2024>(5/35 | 45/178) Dynamic Prompt Optimizing for Text-to-Image Generation (Wenyi Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, Qing Yang. (2024)<br><strong>Dynamic Prompt Optimizing for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Dynamic Prompt Optimizing for Text-to-Image Generation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Online Reinforcement Learning, Reinforcement Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04095v1.pdf filename=2404.04095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generative models, specifically those based on <b>diffusion</b> <b>models</b> like Imagen and Stable <b>Diffusion,</b> <b>have</b> made substantial advancements. Recently, there has been a surge of interest in the delicate refinement of text <b>prompts.</b> Users assign weights or alter the injection time steps of certain words in the text <b>prompts</b> to improve the quality of generated images. However, the success of fine-control <b>prompts</b> depends on the accuracy of the text <b>prompts</b> and the careful selection of weights and time steps, which requires significant manual intervention. To address this, we introduce the \textbf{P}rompt \textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the original <b>prompts</b> for image generation, we further employ an <b>online</b> <b>reinforcement</b> <b>learning</b> strategy to explore the weights and injection time steps of each word, leading to the dynamic fine-control <b>prompts.</b> The reward function during training encourages the model to consider aesthetic score, semantic consistency, and user preferences. Experimental results demonstrate that our proposed method effectively improves the original <b>prompts,</b> generating visually more appealing images while maintaining semantic alignment. Code is available at <a href=https://github.com/Mowenyii/PAE>https://github.com/Mowenyii/PAE</a>.</p></p class="citation"></blockquote><h3 id=635--46178-learning-correlation-structures-for-vision-transformers-manjin-kim-et-al-2024>(6/35 | 46/178) Learning Correlation Structures for Vision Transformers (Manjin Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manjin Kim, Paul Hongsuck Seo, Cordelia Schmid, Minsu Cho. (2024)<br><strong>Learning Correlation Structures for Vision Transformers</strong><br><button class=copy-to-clipboard title="Learning Correlation Structures for Vision Transformers" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03924v1.pdf filename=2404.03924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new attention mechanism, dubbed structural <b>self-attention</b> (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recognizing space-time structures of key-query correlations via <b>convolution</b> and uses them to dynamically aggregate local contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural <b>vision</b> <b>transformer</b> (StructViT) and evaluate its effectiveness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-1K, Kinetics-400, Something-Something V1 & V2, Diving-48, and FineGym.</p></p class="citation"></blockquote><h3 id=735--47178-sigma-siamese-mamba-network-for-multi-modal-semantic-segmentation-zifu-wan-et-al-2024>(7/35 | 47/178) Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation (Zifu Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zifu Wan, Yuhao Wang, Silong Yong, Pingping Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie. (2024)<br><strong>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Convolutional Neural Network, Multi-modal, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04256v1.pdf filename=2404.04256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> semantic segmentation significantly enhances AI agents&rsquo; perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable segmentation. In this work, we introduce Sigma, a Siamese Mamba network for <b>multi-modal</b> semantic segmentation, utilizing the Selective Structured State Space Model, Mamba. Unlike conventional methods that rely on <b>CNNs,</b> with their limited local receptive fields, or <b>Vision</b> <b>Transformers</b> (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields coverage with linear complexity. By employing a Siamese encoder and innovating a Mamba fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our method, Sigma, is rigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in <b>multi-modal</b> perception tasks. Code is available at <a href=https://github.com/zifuwan/Sigma>https://github.com/zifuwan/Sigma</a>.</p></p class="citation"></blockquote><h3 id=835--48178-diffop-net-a-differential-operator-based-fully-convolutional-network-for-unsupervised-deformable-image-registration-jiong-wu-2024>(8/35 | 48/178) DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration (Jiong Wu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiong Wu. (2024)<br><strong>DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration</strong><br><button class=copy-to-clipboard title="DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04244v1.pdf filename=2404.04244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>unsupervised</b> <b>deformable</b> image registration methods usually rely on metrics applied to the gradients of predicted displacement or velocity fields as a regularization term to ensure transformation smoothness, which potentially limits registration accuracy. In this study, we propose a novel approach to enhance <b>unsupervised</b> <b>deformable</b> image registration by introducing a new differential operator into the registration framework. This operator, acting on the velocity field and mapping it to a dual space, ensures the smoothness of the velocity field during optimization, facilitating accurate deformable registration. In addition, to tackle the challenge of capturing large deformations inside image pairs, we introduce a Cross-Coordinate Attention module (CCA) and embed it into a proposed Fully <b>Convolutional</b> <b>Networks</b> (FCNs)-based multi-resolution registration architecture. Evaluation experiments are conducted on two magnetic resonance imaging (MRI) datasets. Compared to various state-of-the-art registration approaches, including a traditional algorithm and three representative <b>unsupervised</b> <b>learning-based</b> methods, our method achieves superior accuracies, maintaining desirable diffeomorphic properties, and exhibiting promising registration speed.</p></p class="citation"></blockquote><h3 id=935--49178-who-evaluates-the-evaluations-objectively-scoring-text-to-image-prompt-coherence-metrics-with-t2iscorescore-ts2-michael-saxon-et-al-2024>(9/35 | 49/178) Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2) (Michael Saxon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang. (2024)<br><strong>Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)</strong><br><button class=copy-to-clipboard title="Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Graph, Benchmarking, Benchmarking, Text2image, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04251v1.pdf filename=2404.04251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With advances in the quality of <b>text-to-image</b> (T2I) models has come interest in <b>benchmarking</b> their <b>prompt</b> faithfulness-the semantic coherence of generated images to the <b>prompts</b> they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and <b>vision-language</b> models (VLMs). However, these metrics are not rigorously compared and <b>benchmarked,</b> instead presented against few weak baselines by correlation to human Likert scores over a set of easy-to-discriminate images. We introduce T2IScoreScore (TS2), a curated set of semantic error <b>graphs</b> containing a <b>prompt</b> and a set increasingly erroneous images. These allow us to rigorously judge whether a given <b>prompt</b> faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I <b>prompt</b> faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.</p></p class="citation"></blockquote><h3 id=1035--50178-robust-few-shot-ensemble-learning-with-focal-diversity-based-pruning-selim-furkan-tekin-et-al-2024>(10/35 | 50/178) Robust Few-Shot Ensemble Learning with Focal Diversity-Based Pruning (Selim Furkan Tekin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ka-Ho Chow, Margaret L. Loper, Ling Liu. (2024)<br><strong>Robust Few-Shot Ensemble Learning with Focal Diversity-Based Pruning</strong><br><button class=copy-to-clipboard title="Robust Few-Shot Ensemble Learning with Focal Diversity-Based Pruning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04434v1.pdf filename=2404.04434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents FusionShot, a focal diversity optimized <b>few-shot</b> <b>ensemble</b> learning approach for boosting the robustness and generalization performance of pre-trained <b>few-shot</b> <b>models.</b> The paper makes three original contributions. First, we explore the unique characteristics of <b>few-shot</b> <b>learning</b> to ensemble multiple <b>few-shot</b> <b>(FS)</b> models by creating three alternative fusion channels. Second, we introduce the concept of focal error diversity to learn the most efficient ensemble teaming strategy, rather than assuming that an ensemble of a larger number of base models will outperform those sub-ensembles of smaller size. We develop a focal-diversity ensemble <b>pruning</b> method to effectively prune out the candidate ensembles with low ensemble error diversity and recommend top-$K$ FS ensembles with the highest focal error diversity. Finally, we capture the complex non-linear patterns of ensemble <b>few-shot</b> <b>predictions</b> by designing the learn-to-combine algorithm, which can learn the diverse weight assignments for robust ensemble fusion over different member models. Extensive experiments on representative <b>few-shot</b> <b>benchmarks</b> show that the top-K ensembles recommended by FusionShot can outperform the representative SOTA <b>few-shot</b> <b>models</b> on novel tasks (different distributions and unknown at training), and can prevail over existing <b>few-shot</b> <b>learners</b> in both cross-domain settings and adversarial settings. For reproducibility purposes, FusionShot trained models, results, and code are made available at <a href=https://github.com/sftekin/fusionshot>https://github.com/sftekin/fusionshot</a></p></p class="citation"></blockquote><h3 id=1135--51178-identity-decoupling-for-multi-subject-personalization-of-text-to-image-models-sangwon-jang-et-al-2024>(11/35 | 51/178) Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models (Sangwon Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwon Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang. (2024)<br><strong>Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Data Augmentation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04243v1.pdf filename=2404.04243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have shown remarkable success in generating a personalized subject based on a few reference images. However, current methods struggle with handling multiple subjects simultaneously, often resulting in mixed identities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by the Segment Anything Model for both training and inference, as a form of <b>data</b> <b>augmentation</b> for training and initialization for the generation process. Our experiments demonstrate that MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. In human evaluation, MuDI shows twice as many successes for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% compared to the strongest baseline. More results are available at <a href=https://mudi-t2i.github.io/>https://mudi-t2i.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1235--52178-physical-property-understanding-from-language-embedded-feature-fields-albert-j-zhai-et-al-2024>(12/35 | 52/178) Physical Property Understanding from Language-Embedded Feature Fields (Albert J. Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, Shenlong Wang. (2024)<br><strong>Physical Property Understanding from Language-Embedded Feature Fields</strong><br><button class=copy-to-clipboard title="Physical Property Understanding from Language-Embedded Feature Fields" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Zero-shot, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04242v1.pdf filename=2404.04242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can computers perceive the physical properties of objects solely through vision? Research in cognitive science and vision science has shown that humans excel at identifying materials and estimating their physical properties based purely on visual appearance. In this paper, we present a novel approach for dense prediction of the physical properties of objects using a collection of images. Inspired by how humans reason about physics through vision, we leverage <b>large</b> <b>language</b> <b>models</b> to propose candidate materials for each object. We then construct a language-embedded point cloud and estimate the physical properties of each 3D point using a <b>zero-shot</b> kernel regression approach. Our method is accurate, annotation-free, and applicable to any object in the open world. Experiments demonstrate the effectiveness of the proposed approach in various physical property <b>reasoning</b> tasks, such as estimating the mass of common objects, as well as other properties like friction and hardness.</p></p class="citation"></blockquote><h3 id=1335--53178-rasim-a-range-aware-high-fidelity-rgb-d-data-simulation-pipeline-for-real-world-applications-xingyu-liu-et-al-2024>(13/35 | 53/178) RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications (Xingyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Liu, Chenyangguang Zhang, Gu Wang, Ruida Zhang, Xiangyang Ji. (2024)<br><strong>RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications</strong><br><button class=copy-to-clipboard title="RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03962v1.pdf filename=2404.03962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic vision, a de-facto paradigm is to learn in simulated environments and then transfer to real-world applications, which poses an essential challenge in bridging the sim-to-real domain gap. While mainstream works tackle this problem in the RGB domain, we focus on depth data synthesis and develop a range-aware RGB-D data <b>simulation</b> pipeline (RaSim). In particular, high-fidelity depth data is generated by imitating the imaging principle of real-world sensors. A range-aware rendering strategy is further introduced to enrich data diversity. Extensive experiments show that models trained with RaSim can be directly applied to real-world scenarios without any <b>finetuning</b> and excel at downstream RGB-D perception tasks.</p></p class="citation"></blockquote><h3 id=1435--54178-concept-weaver-enabling-multi-concept-fusion-in-text-to-image-models-gihyun-kwon-et-al-2024>(14/35 | 54/178) Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models (Gihyun Kwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, Fabian Caba Heilbron. (2024)<br><strong>Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models</strong><br><button class=copy-to-clipboard title="Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03913v1.pdf filename=2404.03913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While there has been significant progress in customizing <b>text-to-image</b> generation models, generating images that combine multiple personalized concepts remains challenging. In this work, we introduce Concept Weaver, a method for composing customized <b>text-to-image</b> <b>diffusion</b> <b>models</b> at inference time. Specifically, the method breaks the process into two steps: creating a template image aligned with the semantics of input <b>prompts,</b> and then personalizing the template using a concept fusion strategy. The fusion strategy incorporates the appearance of the target concepts into the template image while retaining its structural details. The results indicate that our method can generate multiple custom concepts with higher identity fidelity compared to alternative approaches. Furthermore, the method is shown to seamlessly handle more than two concepts and closely follow the semantic meaning of the input <b>prompt</b> without blending appearances across different subjects.</p></p class="citation"></blockquote><h3 id=1535--55178-idea-2-3d-collaborative-lmm-agents-enable-3d-model-generation-from-interleaved-multimodal-inputs-junhao-chen-et-al-2024>(15/35 | 55/178) Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs (Junhao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Chen, Xiang Li, Xiaojun Ye, Chao Li, Zhaoxin Fan, Hao Zhao. (2024)<br><strong>Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs</strong><br><button class=copy-to-clipboard title="Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Human Intervention, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04363v1.pdf filename=2404.04363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we pursue a novel 3D AIGC setting: generating 3D content from IDEAs. The definition of an IDEA is the composition of <b>multimodal</b> inputs including text, image, and 3D models. To our knowledge, this challenging and appealing 3D AIGC setting has not been studied before. We propose the novel framework called Idea-2-3D to achieve this goal, which consists of three agents based upon large multimodel models (LMMs) and several existing algorithmic tools for them to invoke. Specifically, these three LMM-based agents are <b>prompted</b> to do the jobs of <b>prompt</b> generation, model selection and feedback reflection. They work in a cycle that involves both mutual collaboration and criticism. Note that this cycle is done in a fully automatic manner, without any <b>human</b> <b>intervention.</b> The framework then outputs a text <b>prompt</b> to generate 3D models that well align with input IDEAs. We show impressive 3D AIGC results that are beyond any previous methods can achieve. For quantitative comparisons, we construct caption-based baselines using a whole bunch of state-of-the-art 3D AIGC models and demonstrate Idea-2-3D out-performs significantly. In 94.2% of cases, Idea-2-3D meets users&rsquo; requirements, marking a degree of match between IDEA and 3D models that is 2.3 times higher than baselines. Moreover, in 93.5% of the cases, users agreed that Idea-2-3D was better than baselines. Codes, data and models will made publicly available.</p></p class="citation"></blockquote><h3 id=1635--56178-3d-facial-expressions-through-analysis-by-neural-synthesis-george-retsinas-et-al-2024>(16/35 | 56/178) 3D Facial Expressions through Analysis-by-Neural-Synthesis (George Retsinas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>George Retsinas, Panagiotis P. Filntisis, Radek Danecek, Victoria F. Abrevaya, Anastasios Roussos, Timo Bolkart, Petros Maragos. (2024)<br><strong>3D Facial Expressions through Analysis-by-Neural-Synthesis</strong><br><button class=copy-to-clipboard title="3D Facial Expressions through Analysis-by-Neural-Synthesis" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Reconstruction Loss, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04104v1.pdf filename=2404.04104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While existing methods for 3D face <b>reconstruction</b> <b>from</b> in-the-wild images excel at recovering the overall face shape, they commonly miss subtle, extreme, asymmetric, or rarely observed expressions. We improve upon these methods with SMIRK (Spatial Modeling for Image-based <b>Reconstruction</b> <b>of</b> Kinesics), which faithfully reconstructs expressive 3D faces from images. We identify two key limitations in existing methods: shortcomings in their <b>self-supervised</b> training formulation, and a lack of expression diversity in the training images. For training, most methods employ differentiable rendering to compare a predicted face mesh with the input image, along with a plethora of additional loss functions. This differentiable rendering loss not only has to provide supervision to optimize for 3D face <b>geometry,</b> camera, albedo, and lighting, which is an ill-posed optimization problem, but the domain gap between rendering and input image further hinders the learning process. Instead, SMIRK replaces the differentiable rendering with a neural rendering module that, given the rendered predicted mesh <b>geometry,</b> and sparsely sampled pixels of the input image, generates a face image. As the neural rendering gets color information from sampled image pixels, supervising with neural rendering-based <b>reconstruction</b> <b>loss</b> can focus solely on the <b>geometry.</b> Further, it enables us to generate images of the input identity with varying expressions while training. These are then utilized as input to the <b>reconstruction</b> <b>model</b> and used as supervision with ground truth <b>geometry.</b> This effectively augments the training data and enhances the generalization for diverse expressions. Our qualitative, quantitative and particularly our perceptual evaluations demonstrate that SMIRK achieves the new state-of-the art performance on accurate expression <b>reconstruction.</b> <b>Project</b> webpage: <a href=https://georgeretsi.github.io/smirk/>https://georgeretsi.github.io/smirk/</a>.</p></p class="citation"></blockquote><h3 id=1735--57178-clickdiffusion-harnessing-llms-for-interactive-precise-image-editing-alec-helbling-et-al-2024>(17/35 | 57/178) ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing (Alec Helbling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alec Helbling, Seongmin Lee, Polo Chau. (2024)<br><strong>ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing</strong><br><button class=copy-to-clipboard title="ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04376v1.pdf filename=2404.04376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, researchers have proposed powerful systems for generating and manipulating images using natural language instructions. However, it is difficult to precisely specify many common classes of image transformations with text alone. For example, a user may wish to change the location and breed of a particular dog in an image with several similar dogs. This task is quite difficult with natural language alone, and would require a user to write a laboriously complex <b>prompt</b> that both disambiguates the target dog and describes the destination. We propose ClickDiffusion, a system for precise image manipulation and generation that combines natural language instructions with visual feedback provided by the user through a direct manipulation interface. We demonstrate that by serializing both an image and a <b>multi-modal</b> instruction into a textual representation it is possible to leverage <b>LLMs</b> to perform precise transformations of the layout and appearance of an image. Code available at <a href=https://github.com/poloclub/ClickDiffusion>https://github.com/poloclub/ClickDiffusion</a>.</p></p class="citation"></blockquote><h3 id=1835--58178-label-propagation-for-zero-shot-classification-with-vision-language-models-vladan-stojnić-et-al-2024>(18/35 | 58/178) Label Propagation for Zero-shot Classification with Vision-Language Models (Vladan Stojnić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladan Stojnić, Yannis Kalantidis, Giorgos Tolias. (2024)<br><strong>Label Propagation for Zero-shot Classification with Vision-Language Models</strong><br><button class=copy-to-clipboard title="Label Propagation for Zero-shot Classification with Vision-Language Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04072v1.pdf filename=2404.04072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs) have demonstrated impressive performance on <b>zero-shot</b> classification, i.e. classification when provided merely with a list of class names. In this paper, we tackle the case of <b>zero-shot</b> classification in the presence of unlabeled data. We leverage the <b>graph</b> structure of the unlabeled data and introduce ZLaP, a method based on label propagation (LP) that utilizes geodesic distances for classification. We tailor LP to <b>graphs</b> containing both text and image features and further propose an efficient method for performing inductive inference based on a dual solution and a sparsification step. We perform extensive experiments to evaluate the effectiveness of our method on 14 common datasets and show that ZLaP outperforms the latest related works. Code: <a href=https://github.com/vladan-stojnic/ZLaP>https://github.com/vladan-stojnic/ZLaP</a></p></p class="citation"></blockquote><h3 id=1935--59178-neural-symbolic-videoqa-learning-compositional-spatio-temporal-reasoning-for-real-world-video-question-answering-lili-liang-et-al-2024>(19/35 | 59/178) Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering (Lili Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lili Liang, Guanglu Sun, Jin Qiu, Lizhong Zhang. (2024)<br><strong>Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering</strong><br><button class=copy-to-clipboard title="Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04007v1.pdf filename=2404.04007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compositional spatio-temporal <b>reasoning</b> poses a significant challenge in the field of video <b>question</b> <b>answering</b> (VideoQA). Existing approaches struggle to establish effective symbolic <b>reasoning</b> structures, which are crucial for answering compositional spatio-temporal <b>questions.</b> <b>To</b> address this challenge, we propose a neural-symbolic framework called Neural-Symbolic VideoQA (NS-VideoQA), specifically designed for real-world VideoQA tasks. The uniqueness and superiority of NS-VideoQA are two-fold: 1) It proposes a Scene Parser Network (SPN) to transform static-dynamic video scenes into Symbolic Representation (SR), structuralizing persons, objects, relations, and action chronologies. 2) A Symbolic <b>Reasoning</b> Machine (SRM) is designed for top-down <b>question</b> <b>decompositions</b> and bottom-up compositional <b>reasonings.</b> Specifically, a polymorphic program executor is constructed for internally consistent <b>reasoning</b> from SR to the final answer. As a result, Our NS-VideoQA not only improves the compositional spatio-temporal <b>reasoning</b> in real-world VideoQA task, but also enables step-by-step error analysis by tracing the intermediate results. Experimental evaluations on the AGQA Decomp <b>benchmark</b> demonstrate the effectiveness of the proposed NS-VideoQA framework. Empirical studies further confirm that NS-VideoQA exhibits internal consistency in answering compositional <b>questions</b> <b>and</b> significantly improves the capability of spatio-temporal and logical inference for VideoQA tasks.</p></p class="citation"></blockquote><h3 id=2035--60178-physics-inspired-synthesized-underwater-image-dataset-reina-kaneko-et-al-2024>(20/35 | 60/178) Physics-Inspired Synthesized Underwater Image Dataset (Reina Kaneko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reina Kaneko, Hiroshi Higashi, Yuichi Tanaka. (2024)<br><strong>Physics-Inspired Synthesized Underwater Image Dataset</strong><br><button class=copy-to-clipboard title="Physics-Inspired Synthesized Underwater Image Dataset" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03998v1.pdf filename=2404.03998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the physics-inspired synthesized underwater image dataset (PHISWID), a dataset tailored for enhancing underwater image processing through physics-inspired image synthesis. Deep learning approaches to underwater image enhancement typically demand extensive datasets, yet acquiring paired clean and degraded underwater ones poses significant challenges. While several underwater image datasets have been proposed using physics-based synthesis, a publicly accessible collection has been lacking. Additionally, most underwater image synthesis approaches do not intend to reproduce atmospheric scenes, resulting in incomplete enhancement. PHISWID addresses this gap by offering a set of paired ground-truth (atmospheric) and synthetically degraded underwater images, showcasing not only color degradation but also the often-neglected effects of marine snow, a composite of organic matter and sand particles that considerably impairs underwater image clarity. The dataset applies these degradations to atmospheric RGB-D images, enhancing the dataset&rsquo;s realism and applicability. PHISWID is particularly valuable for training deep neural networks in a <b>supervised</b> <b>learning</b> setting and for objectively assessing image quality in <b>benchmark</b> analyses. Our results reveal that even a basic U-Net architecture, when trained with PHISWID, substantially outperforms existing methods in underwater image enhancement. We intend to release PHISWID publicly, contributing a significant resource to the advancement of underwater imaging technology.</p></p class="citation"></blockquote><h3 id=2135--61178-physpt-physics-aware-pretrained-transformer-for-estimating-human-dynamics-from-monocular-videos-yufei-zhang-et-al-2024>(21/35 | 61/178) PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos (Yufei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Zhang, Jeffrey O. Kephart, Zijun Cui, Qiang Ji. (2024)<br><strong>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</strong><br><button class=copy-to-clipboard title="PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04430v1.pdf filename=2404.04430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While current methods have shown promising progress on estimating 3D human motion from monocular videos, their motion estimates are often physically unrealistic because they mainly consider kinematics. In this paper, we introduce Physics-aware Pretrained <b>Transformer</b> (PhysPT), which improves kinematics-based motion estimates and infers motion forces. PhysPT exploits a <b>Transformer</b> encoder-decoder backbone to effectively learn human dynamics in a <b>self-supervised</b> manner. Moreover, it incorporates physics principles governing human motion. Specifically, we build a physics-based body representation and contact force model. We leverage them to impose novel physics-inspired training losses (i.e., force loss, contact loss, and Euler-Lagrange loss), enabling PhysPT to capture physical properties of the human body and the forces it experiences. Experiments demonstrate that, once trained, PhysPT can be directly applied to kinematics-based estimates to significantly enhance their physical plausibility and generate favourable motion forces. Furthermore, we show that these physically meaningful quantities translate into improved accuracy of an important downstream task: human action recognition.</p></p class="citation"></blockquote><h3 id=2235--62178-no-time-to-train-empowering-non-parametric-networks-for-few-shot-3d-scene-segmentation-xiangyang-zhu-et-al-2024>(22/35 | 62/178) No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation (Xiangyang Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Han Xiao, Chaoyou Fu, Hao Dong, Peng Gao. (2024)<br><strong>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</strong><br><button class=copy-to-clipboard title="No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04050v1.pdf filename=2404.04050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to <b>few-shot</b> <b>learning.</b> Current 3D <b>few-shot</b> <b>segmentation</b> methods first pre-train models on &lsquo;seen&rsquo; classes, and then evaluate their generalization performance on &lsquo;unseen&rsquo; classes. However, the prior pre-training stage not only introduces excessive time overhead but also incurs a significant domain gap on &lsquo;unseen&rsquo; classes. To tackle these issues, we propose a Non-parametric Network for <b>few-shot</b> <b>3D</b> Segmentation, Seg-NN, and its Parametric variant, Seg-PN. Without training, Seg-NN extracts dense representations by hand-crafted filters and achieves comparable performance to existing parametric models. Due to the elimination of pre-training, Seg-NN can alleviate the domain gap issue and save a substantial amount of time. Based on Seg-NN, Seg-PN only requires training a lightweight QUEry-Support Transferring (QUEST) module, which enhances the interaction between the support set and query set. Experiments suggest that Seg-PN outperforms previous state-of-the-art method by +4.19% and +7.71% mIoU on S3DIS and ScanNet datasets respectively, while reducing training time by -90%, indicating its effectiveness and efficiency.</p></p class="citation"></blockquote><h3 id=2335--63178-instructhumans-editing-animated-3d-human-textures-with-instructions-jiayin-zhu-et-al-2024>(23/35 | 63/178) InstructHumans: Editing Animated 3D Human Textures with Instructions (Jiayin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayin Zhu, Linlin Yang, Angela Yao. (2024)<br><strong>InstructHumans: Editing Animated 3D Human Textures with Instructions</strong><br><button class=copy-to-clipboard title="InstructHumans: Editing Animated 3D Human Textures with Instructions" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04037v1.pdf filename=2404.04037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present InstructHumans, a novel framework for instruction-driven 3D human texture editing. Existing text-based editing methods use Score <b>Distillation</b> Sampling (SDS) to <b>distill</b> guidance from generative models. This work shows that naively using such scores is harmful to editing as they destroy consistency with the source avatar. Instead, we propose an alternate SDS for Editing (SDS-E) that selectively incorporates subterms of SDS across diffusion timesteps. We further enhance SDS-E with spatial smoothness regularization and gradient-based viewpoint sampling to achieve high-quality edits with sharp and high-fidelity detailing. InstructHumans significantly outperforms existing 3D editing methods, consistent with the initial avatar while faithful to the textual instructions. Project page: <a href=https://jyzhu.top/instruct-humans>https://jyzhu.top/instruct-humans</a> .</p></p class="citation"></blockquote><h3 id=2435--64178-voltavision-a-transfer-learning-model-for-electronic-component-classification-anas-mohammad-ishfaqul-muktadir-osmani-et-al-2024>(24/35 | 64/178) VoltaVision: A Transfer Learning model for electronic component classification (Anas Mohammad Ishfaqul Muktadir Osmani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Mohammad Ishfaqul Muktadir Osmani, Taimur Rahman, Salekul Islam. (2024)<br><strong>VoltaVision: A Transfer Learning model for electronic component classification</strong><br><button class=copy-to-clipboard title="VoltaVision: A Transfer Learning model for electronic component classification" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03898v1.pdf filename=2404.03898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we analyze the effectiveness of <b>transfer</b> <b>learning</b> on classifying electronic components. <b>Transfer</b> <b>learning</b> reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight <b>CNN,</b> coined as VoltaVision, and compares its performance against more complex models. We test the hypothesis that transferring knowledge from a similar task to our target domain yields better results than state-of-the-art models trained on general datasets. Our dataset and code for this work are available at <a href=https://github.com/AnasIshfaque/VoltaVision>https://github.com/AnasIshfaque/VoltaVision</a>.</p></p class="citation"></blockquote><h3 id=2535--65178-increasing-fairness-in-classification-of-out-of-distribution-data-for-facial-recognition-gianluca-barone-et-al-2024>(25/35 | 65/178) Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition (Gianluca Barone et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca Barone, Aashrit Cunchala, Rudy Nunez. (2024)<br><strong>Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition</strong><br><button class=copy-to-clipboard title="Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-CY, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Fairness, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03876v1.pdf filename=2404.03876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data <b>(&ldquo;out-of-distribution</b> data&rdquo;) which is different from data in the training distribution(&ldquo;in-distribution&rdquo;). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of <b>out-of-distribution</b> data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model&rsquo;s performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine&rsquo;s emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. We also tested the datasets in reverse order to see whether a more fair dataset with balanced features has an effect on the model&rsquo;s accuracy.</p></p class="citation"></blockquote><h3 id=2635--66178-improving-detection-in-aerial-images-by-capturing-inter-object-relationships-botao-ren-et-al-2024>(26/35 | 66/178) Improving Detection in Aerial Images by Capturing Inter-Object Relationships (Botao Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botao Ren, Botian Xu, Yifan Pu, Jingyi Wang, Zhidong Deng. (2024)<br><strong>Improving Detection in Aerial Images by Capturing Inter-Object Relationships</strong><br><button class=copy-to-clipboard title="Improving Detection in Aerial Images by Capturing Inter-Object Relationships" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04140v1.pdf filename=2404.04140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many image domains, the spatial distribution of objects in a scene exhibits meaningful patterns governed by their semantic relationships. In most modern detection pipelines, however, the detection proposals are processed independently, overlooking the underlying relationships between objects. In this work, we introduce a <b>transformer-based</b> approach to capture these inter-object relationships to refine classification and regression outcomes for detected objects. Building on two-stage detectors, we tokenize the region of interest (RoI) proposals to be processed by a <b>transformer</b> encoder. Specific spatial and geometric relations are incorporated into the attention weights and adaptively modulated and regularized. Experimental results demonstrate that the proposed method achieves consistent performance improvement on three <b>benchmarks</b> including DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on both DOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59 mAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016, respectively, compared to the baselines.</p></p class="citation"></blockquote><h3 id=2735--67178-analyzing-participants-engagement-during-online-meetings-using-unsupervised-remote-photoplethysmography-with-behavioral-features-alexander-vedernikov-et-al-2024>(27/35 | 67/178) Analyzing Participants&rsquo; Engagement during Online Meetings Using Unsupervised Remote Photoplethysmography with Behavioral Features (Alexander Vedernikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Vedernikov, Zhaodong Sun, Virpi-Liisa Kykyri, Mikko Pohjola, Miriam Nokia, Xiaobai Li. (2024)<br><strong>Analyzing Participants&rsquo; Engagement during Online Meetings Using Unsupervised Remote Photoplethysmography with Behavioral Features</strong><br><button class=copy-to-clipboard title="Analyzing Participants' Engagement during Online Meetings Using Unsupervised Remote Photoplethysmography with Behavioral Features" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04394v1.pdf filename=2404.04394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Engagement measurement finds application in healthcare, education, advertisement, and services. The use of physiological and behavioral features is viable, but the impracticality of traditional physiological measurement arises due to the need for contact sensors. We demonstrate the feasibility of <b>unsupervised</b> remote photoplethysmography (rPPG) as an alternative for contact sensors in deriving heart rate variability (HRV) features, then fusing these with behavioral features to measure engagement in online group meetings. Firstly, a unique Engagement Dataset of online interactions among social workers is collected with granular engagement labels, offering insight into virtual meeting dynamics. Secondly, a pre-trained rPPG model is customized to reconstruct accurate rPPG signals from video meetings in an <b>unsupervised</b> manner, enabling the calculation of HRV features. Thirdly, the feasibility of estimating engagement from HRV features using short observation windows, with a notable enhancement when using longer observation windows of two to four minutes, is demonstrated. Fourthly, the effectiveness of behavioral cues is evaluated and fused with physiological data, which further enhances engagement estimation performance. An accuracy of 94% is achieved when only HRV features are used, eliminating the need for contact sensors or ground truth signals. The incorporation of behavioral cues raises the accuracy to 96%. Facial video analysis offers precise engagement measurement, beneficial for future applications.</p></p class="citation"></blockquote><h3 id=2835--68178-spatialtracker-tracking-any-2d-pixels-in-3d-space-yuxi-xiao-et-al-2024>(28/35 | 68/178) SpatialTracker: Tracking Any 2D Pixels in 3D Space (Yuxi Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, Xiaowei Zhou. (2024)<br><strong>SpatialTracker: Tracking Any 2D Pixels in 3D Space</strong><br><button class=copy-to-clipboard title="SpatialTracker: Tracking Any 2D Pixels in 3D Space" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04319v1.pdf filename=2404.04319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recovering dense and long-range pixel motion in videos is a challenging problem. Part of the difficulty arises from the 3D-to-2D projection process, leading to occlusions and discontinuities in the 2D motion domain. While 2D motion can be intricate, we posit that the underlying 3D motion can often be simple and low-dimensional. In this work, we propose to estimate point trajectories in 3D space to mitigate the issues caused by image projection. Our method, named SpatialTracker, lifts 2D pixels to 3D using monocular depth estimators, represents the 3D content of each frame efficiently using a triplane representation, and performs iterative updates using a <b>transformer</b> to estimate 3D trajectories. Tracking in 3D allows us to leverage as-rigid-as-possible (ARAP) constraints while simultaneously learning a rigidity embedding that clusters pixels into different rigid parts. Extensive evaluation shows that our approach achieves state-of-the-art tracking performance both qualitatively and quantitatively, particularly in challenging scenarios such as out-of-plane rotation.</p></p class="citation"></blockquote><h3 id=2935--69178-robust-depth-enhancement-via-polarization-prompt-fusion-tuning-kei-ikemura-et-al-2024>(29/35 | 69/178) Robust Depth Enhancement via Polarization Prompt Fusion Tuning (Kei Ikemura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kei Ikemura, Yiming Huang, Felix Heide, Zhaoxiang Zhang, Qifeng Chen, Chenyang Lei. (2024)<br><strong>Robust Depth Enhancement via Polarization Prompt Fusion Tuning</strong><br><button class=copy-to-clipboard title="Robust Depth Enhancement via Polarization Prompt Fusion Tuning" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04318v1.pdf filename=2404.04318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing depth sensors are imperfect and may provide inaccurate depth values in challenging scenarios, such as in the presence of transparent or reflective objects. In this work, we present a general framework that leverages polarization imaging to improve inaccurate depth measurements from various depth sensors. Previous polarization-based depth enhancement methods focus on utilizing pure physics-based formulas for a single sensor. In contrast, our method first adopts a learning-based strategy where a neural network is trained to estimate a dense and complete depth map from polarization data and a sensor depth map from different sensors. To further improve the performance, we propose a Polarization <b>Prompt</b> Fusion Tuning (PPFT) strategy to effectively utilize RGB-based models pre-trained on large-scale datasets, as the size of the polarization dataset is limited to train a strong model from scratch. We conducted extensive experiments on a public dataset, and the results demonstrate that the proposed method performs favorably compared to existing depth enhancement baselines. Code and demos are available at <a href=https://lastbasket.github.io/PPFT/>https://lastbasket.github.io/PPFT/</a>.</p></p class="citation"></blockquote><h3 id=3035--70178-scaresnet-a-resnet-variant-optimized-for-tiny-object-detection-in-transmission-and-distribution-towers-weile-li-et-al-2024>(30/35 | 70/178) SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers (Weile Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weile Li, Muqing Shi, Zhonghua Hong. (2024)<br><strong>SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers</strong><br><button class=copy-to-clipboard title="SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04179v1.pdf filename=2404.04179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional deep learning-based <b>object</b> <b>detection</b> networks often resize images during the data preprocessing stage to achieve a uniform size and scale in the feature map. Resizing is done to facilitate model propagation and fully connected classification. However, resizing inevitably leads to <b>object</b> <b>deformation</b> and loss of valuable information in the images. This drawback becomes particularly pronounced for tiny <b>objects</b> <b>like</b> distribution towers with linear shapes and few pixels. To address this issue, we propose abandoning the resizing operation. Instead, we introduce Positional-Encoding Multi-head Criss-Cross Attention. This allows the model to capture contextual information and learn from multiple representation subspaces, effectively enriching the semantics of distribution towers. Additionally, we enhance Spatial Pyramid Pooling by reshaping three pooled feature maps into a new unified one while also reducing the computational burden. This approach allows images of different sizes and scales to generate feature maps with uniform dimensions and can be employed in feature map propagation. Our SCAResNet incorporates these aforementioned improvements into the backbone network ResNet. We evaluated our SCAResNet using the Electric Transmission and Distribution Infrastructure Imagery dataset from Duke University. Without any additional tricks, we employed various <b>object</b> <b>detection</b> models with Gaussian Receptive Field based Label Assignment as the baseline. When incorporating the SCAResNet into the baseline model, we achieved a 2.1% improvement in mAPs. This demonstrates the advantages of our SCAResNet in detecting transmission and distribution towers and its value in tiny <b>object</b> <b>detection.</b> The source code is available at <a href=https://github.com/LisavilaLee/SCAResNet_mmdet>https://github.com/LisavilaLee/SCAResNet_mmdet</a>.</p></p class="citation"></blockquote><h3 id=3135--71178-marsseg-mars-surface-semantic-segmentation-with-multi-level-extractor-and-connector-junbo-li-et-al-2024>(31/35 | 71/178) MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector (Junbo Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbo Li, Keyan Chen, Gengju Tian, Lu Li, Zhenwei Shi. (2024)<br><strong>MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector</strong><br><button class=copy-to-clipboard title="MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04155v1.pdf filename=2404.04155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The segmentation and interpretation of the Martian surface play a pivotal role in Mars exploration, providing essential data for the trajectory planning and obstacle avoidance of rovers. However, the complex topography, similar surface features, and the lack of extensive annotated data pose significant challenges to the high-precision semantic segmentation of the Martian surface. To address these challenges, we propose a novel encoder-decoder based Mars segmentation network, termed MarsSeg. Specifically, we employ an encoder-decoder structure with a minimized number of down-sampling layers to preserve local details. To facilitate a high-level semantic understanding across the shadow multi-level feature maps, we introduce a feature enhancement connection layer situated between the encoder and decoder. This layer incorporates Mini Atrous Spatial Pyramid Pooling (Mini-ASPP), Polarized <b>Self-Attention</b> (PSA), and Strip Pyramid Pooling Module (SPPM). The Mini-ASPP and PSA are specifically designed for shadow feature enhancement, thereby enabling the expression of local details and small objects. Conversely, the SPPM is employed for deep feature enhancement, facilitating the extraction of high-level semantic category-related information. Experimental results derived from the Mars-Seg and AI4Mars datasets substantiate that the proposed MarsSeg outperforms other state-of-the-art methods in segmentation performance, validating the efficacy of each proposed component.</p></p class="citation"></blockquote><h3 id=3235--72178-dynamic-risk-assessment-methodology-with-an-ldm-based-system-for-parking-scenarios-paola-natalia-cañas-et-al-2024>(32/35 | 72/178) Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios (Paola Natalia Cañas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paola Natalia Cañas, Mikel García, Nerea Aranjuelo, Marcos Nieto, Aitor Iglesias, Igor Rodríguez. (2024)<br><strong>Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios</strong><br><button class=copy-to-clipboard title="Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-SY, cs.CV, eess-SY<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04040v1.pdf filename=2404.04040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the methodology for building a dynamic risk assessment for ADAS (Advanced Driving Assistance Systems) algorithms in parking scenarios, fusing exterior and interior perception for a better understanding of the scene and a more comprehensive risk estimation. This includes the definition of a dynamic risk methodology that depends on the situation from inside and outside the vehicle, the creation of a multi-sensor dataset of risk assessment for ADAS <b>benchmarking</b> purposes, and a Local Dynamic Map (LDM) that fuses data from the exterior and interior of the car to build an LDM-based Dynamic Risk Assessment System (DRAS).</p></p class="citation"></blockquote><h3 id=3335--73178-finsler-laplace-beltrami-operators-with-application-to-shape-analysis-simon-weber-et-al-2024>(33/35 | 73/178) Finsler-Laplace-Beltrami Operators with Application to Shape Analysis (Simon Weber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Weber, Thomas Dagès, Maolin Gao, Daniel Cremers. (2024)<br><strong>Finsler-Laplace-Beltrami Operators with Application to Shape Analysis</strong><br><button class=copy-to-clipboard title="Finsler-Laplace-Beltrami Operators with Application to Shape Analysis" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03999v1.pdf filename=2404.03999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Laplace-Beltrami operator (LBO) emerges from studying manifolds equipped with a Riemannian metric. It is often called the Swiss army knife of <b>geometry</b> processing as it allows to capture intrinsic shape information and gives rise to heat diffusion, geodesic distances, and a multitude of shape descriptors. It also plays a central role in geometric deep learning. In this work, we explore Finsler manifolds as a generalization of Riemannian manifolds. We revisit the Finsler heat equation and derive a Finsler heat kernel and a Finsler-Laplace-Beltrami Operator (FLBO): a novel theoretically justified anisotropic Laplace-Beltrami operator (ALBO). In experimental evaluations we demonstrate that the proposed FLBO is a valuable alternative to the traditional Riemannian-based LBO and ALBOs for spatial filtering and shape correspondence estimation. We hope that the proposed Finsler heat kernel and the FLBO will inspire further exploration of Finsler <b>geometry</b> in the computer vision community.</p></p class="citation"></blockquote><h3 id=3435--74178-robust-gaussian-splatting-françois-darmon-et-al-2024>(34/35 | 74/178) Robust Gaussian Splatting (François Darmon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>François Darmon, Lorenzo Porzi, Samuel Rota-Bulò, Peter Kontschieder. (2024)<br><strong>Robust Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Robust Gaussian Splatting" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04211v1.pdf filename=2404.04211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address common error sources for 3D Gaussian Splatting (3DGS) including blur, imperfect camera poses, and color inconsistencies, with the goal of improving its robustness for practical applications like reconstructions from handheld phone captures. Our main contribution involves modeling motion blur as a Gaussian distribution over camera poses, allowing us to address both camera pose refinement and motion blur correction in a unified way. Additionally, we propose mechanisms for defocus blur compensation and for addressing color in-consistencies caused by ambient light, shadows, or due to camera-related factors like varying white balancing settings. Our proposed solutions integrate in a seamless way with the 3DGS formulation while maintaining its benefits in terms of training efficiency and rendering speed. We experimentally validate our contributions on relevant <b>benchmark</b> datasets including Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and thus consistent improvements over relevant baselines.</p></p class="citation"></blockquote><h3 id=3535--75178-noisy-label-processing-for-classification-a-survey-mengting-li-et-al-2024>(35/35 | 75/178) Noisy Label Processing for Classification: A Survey (Mengting Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengting Li, Chuang Zhu. (2024)<br><strong>Noisy Label Processing for Classification: A Survey</strong><br><button class=copy-to-clipboard title="Noisy Label Processing for Classification: A Survey" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04159v1.pdf filename=2404.04159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep neural networks (DNNs) have gained remarkable achievement in computer vision tasks, and the success of DNNs often depends greatly on the richness of data. However, the acquisition process of data and high-quality ground truth requires a lot of manpower and money. In the long, tedious process of data annotation, annotators are prone to make mistakes, resulting in incorrect labels of images, i.e., noisy labels. The emergence of noisy labels is inevitable. Moreover, since research shows that DNNs can easily fit noisy labels, the existence of noisy labels will cause significant damage to the model training process. Therefore, it is crucial to combat noisy labels for computer vision tasks, especially for classification tasks. In this survey, we first comprehensively review the evolution of different deep learning approaches for noisy label combating in the image classification task. In addition, we also review different noise patterns that have been proposed to design robust algorithms. Furthermore, we explore the inner pattern of real-world label noise and propose an algorithm to generate a synthetic label noise pattern guided by real-world data. We test the algorithm on the well-known real-world dataset CIFAR-10N to form a new real-world data-guided synthetic <b>benchmark</b> and evaluate some typical noise-robust methods on the <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--76178-can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning-gawon-choi-et-al-2024>(1/15 | 76/178) Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning (Gawon Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gawon Choi, Hyemin Ahn. (2024)<br><strong>Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning</strong><br><button class=copy-to-clipboard title="Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 90<br>Keywords: Fine-tuning, GPT-2, GPT-3, GPT-3.5, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03891v1.pdf filename=2404.03891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotics, the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is becoming prevalent, especially for understanding human commands. In particular, <b>LLMs</b> are utilized as domain-agnostic task planners for high-level human commands. <b>LLMs</b> are capable of Chain-of-Thought (CoT) <b>reasoning,</b> and this allows <b>LLMs</b> to be task planners. However, we need to consider that modern robots still struggle to perform complex actions, and the domains where robots can be deployed are limited in practice. This leads us to pose a question: If small LMs can be trained to reason in chains within a single domain, would even small LMs be good task planners for the robots? To train smaller LMs to reason in chains, we build `COmmand-STeps datasets&rsquo; (COST) consisting of high-level commands along with corresponding actionable low-level steps, via <b>LLMs.</b> We release not only our datasets but also the <b>prompt</b> templates used to generate them, to allow anyone to build datasets for their domain. We compare <b>GPT3.5</b> and <b>GPT4</b> with the <b>finetuned</b> <b>GPT2</b> for task domains, in tabletop and kitchen environments, and the result shows that <b>GPT2-medium</b> is comparable to <b>GPT3.5</b> for task planning in a specific domain. Our dataset, code, and more output samples can be found in <a href=https://github.com/Gawon-Choi/small-LMs-Task-Planning>https://github.com/Gawon-Choi/small-LMs-Task-Planning</a></p></p class="citation"></blockquote><h3 id=215--77178-voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots-akhil-padmanabha-et-al-2024>(2/15 | 77/178) VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots (Akhil Padmanabha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla, Carmel Majidi, Henny Admoni, Zackory Erickson. (2024)<br><strong>VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots</strong><br><button class=copy-to-clipboard title="VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CL, cs-HC, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04066v1.pdf filename=2404.04066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living. Speech interfaces, especially ones that utilize <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. Frameworks for integrating <b>LLMs</b> as interfaces to robots for high level task planning and <b>code</b> <b>generation</b> have been proposed, but fail to incorporate human-centric considerations which are essential while developing assistive interfaces. In this work, we present a framework for incorporating <b>LLMs</b> as speech interfaces for physically assistive robots, constructed iteratively with 3 stages of testing involving a feeding robot, culminating in an evaluation with 11 older adults at an independent living facility. We use both quantitative and qualitative data from the final study to validate our framework and additionally provide design guidelines for using <b>LLMs</b> as speech interfaces for assistive robots. Videos and supporting files are located on our project website: <a href=https://sites.google.com/andrew.cmu.edu/voicepilot/>https://sites.google.com/andrew.cmu.edu/voicepilot/</a></p></p class="citation"></blockquote><h3 id=315--78178-pomdp-guided-active-force-based-search-for-robotic-insertion-chen-wang-et-al-2024>(3/15 | 78/178) POMDP-Guided Active Force-Based Search for Robotic Insertion (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Haoxiang Luo, Kun Zhang, Hua Chen, Jia Pan, Wei Zhang. (2024)<br><strong>POMDP-Guided Active Force-Based Search for Robotic Insertion</strong><br><button class=copy-to-clipboard title="POMDP-Guided Active Force-Based Search for Robotic Insertion" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03943v1.pdf filename=2404.03943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robotic insertion tasks where the uncertainty exceeds the allowable tolerance, a good search strategy is essential for successful insertion and significantly influences efficiency. The commonly used blind search method is time-consuming and does not exploit the rich contact information. In this paper, we propose a novel search strategy that actively utilizes the information contained in the contact configuration and shows high efficiency. In particular, we formulate this problem as a Partially Observable <b>Markov</b> <b>Decision</b> <b>Process</b> (POMDP) with carefully designed primitives based on an in-depth analysis of the contact configuration&rsquo;s static stability. From the formulated POMDP, we can derive a novel search strategy. Thanks to its simplicity, this search strategy can be incorporated into a Finite-State-Machine (FSM) controller. The behaviors of the FSM controller are realized through a low-level Cartesian Impedance Controller. Our method is based purely on the robot&rsquo;s proprioceptive sensing and does not need visual or tactile sensors. To evaluate the effectiveness of our proposed strategy and control framework, we conduct extensive comparison experiments in <b>simulation,</b> where we compare our method with the baseline approach. The results demonstrate that our proposed method achieves a higher success rate with a shorter search time and search trajectory length compared to the baseline method. Additionally, we show that our method is robust to various initial displacement errors.</p></p class="citation"></blockquote><h3 id=415--79178-hybrid-force-motion-control-with-estimated-surface-normal-for-manufacturing-applications-ehsan-nasiri-et-al-2024>(4/15 | 79/178) Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications (Ehsan Nasiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Nasiri, Long Wang. (2024)<br><strong>Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications</strong><br><button class=copy-to-clipboard title="Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04419v1.pdf filename=2404.04419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a hybrid force-motion framework that utilizes real-time surface normal updates. The surface normal is estimated via a novel method that leverages force sensing measurements and velocity commands to compensate the friction bias. This approach is critical for robust execution of precision force-controlled tasks in manufacturing, such as thermoplastic tape replacement that traces surfaces or paths on a workpiece subject to uncertainties deviated from the model. We formulated the proposed method and implemented the framework in ROS2 environment. The approach was validated using kinematic <b>simulations</b> and a hardware platform. Specifically, we demonstrated the approach on a 7-DoF manipulator equipped with a force/torque sensor at the end-effector.</p></p class="citation"></blockquote><h3 id=515--80178-admittance-control-for-adaptive-remote-center-of-motion-in-robotic-laparoscopic-surgery-ehsan-nasiri-et-al-2024>(5/15 | 80/178) Admittance Control for Adaptive Remote Center of Motion in Robotic Laparoscopic Surgery (Ehsan Nasiri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Nasiri, Long Wang. (2024)<br><strong>Admittance Control for Adaptive Remote Center of Motion in Robotic Laparoscopic Surgery</strong><br><button class=copy-to-clipboard title="Admittance Control for Adaptive Remote Center of Motion in Robotic Laparoscopic Surgery" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04416v1.pdf filename=2404.04416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In laparoscopic robot-assisted minimally invasive surgery, the kinematic control of the robot is subject to the remote center of motion (RCM) constraint at the port of entry (e.g., trocar) into the patient&rsquo;s body. During surgery, after the instrument is inserted through the trocar, intrinsic physiological movements such as the patient&rsquo;s heartbeat, breathing process, and/or other purposeful body repositioning may deviate the position of the port of entry. This can cause a conflict between the registered RCM and the moved port of entry. To mitigate this conflict, we seek to utilize the interaction forces at the RCM. We develop a novel framework that integrates admittance control into a redundancy resolution method for the RCM kinematic constraint. Using the force/torque sensory feedback at the base of the instrument driving mechanism (IDM), the proposed framework estimates the forces at RCM, rejects forces applied on other locations along the instrument, and uses them in the admittance controller. In this paper, we report analysis from kinematic <b>simulations</b> to validate the proposed framework. In addition, a hardware platform has been completed, and future work is planned for experimental validation.</p></p class="citation"></blockquote><h3 id=615--81178-continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation-lanpei-li-et-al-2024>(6/15 | 81/178) Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation (Lanpei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanpei Li, Enrico Donato, Vincenzo Lomonaco, Egidio Falotico. (2024)<br><strong>Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation</strong><br><button class=copy-to-clipboard title="Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04219v1.pdf filename=2404.04219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dexterous manipulation, often facilitated by multi-fingered robotic hands, holds solid impact for real-world applications. Soft robotic hands, due to their compliant nature, offer flexibility and adaptability during object grasping and manipulation. Yet, benefits come with challenges, particularly in the control development for finger coordination. <b>Reinforcement</b> <b>Learning</b> (RL) can be employed to train object-specific in-hand manipulation policies, but limiting adaptability and generalizability. We introduce a Continual Policy <b>Distillation</b> (CPD) framework to acquire a versatile controller for in-hand manipulation, to rotate different objects in shape and size within a four-fingered soft gripper. The framework leverages Policy <b>Distillation</b> (PD) to transfer knowledge from expert policies to a continually evolving student policy network. Exemplar-based rehearsal methods are then integrated to mitigate catastrophic forgetting and enhance generalization. The performance of the CPD framework over various replay strategies demonstrates its effectiveness in consolidating knowledge from multiple experts and achieving versatile and adaptive behaviours for in-hand manipulation tasks.</p></p class="citation"></blockquote><h3 id=715--82178-scaling-motion-forecasting-models-with-ensemble-distillation-scott-ettinger-et-al-2024>(7/15 | 82/178) Scaling Motion Forecasting Models with Ensemble Distillation (Scott Ettinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Ettinger, Kratarth Goel, Avikalp Srivastava, Rami Al-Rfou. (2024)<br><strong>Scaling Motion Forecasting Models with Ensemble Distillation</strong><br><button class=copy-to-clipboard title="Scaling Motion Forecasting Models with Ensemble Distillation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03843v1.pdf filename=2404.03843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion forecasting has become an increasingly critical component of autonomous robotic systems. Onboard compute budgets typically limit the accuracy of real-time systems. In this work we propose methods of improving motion forecasting systems subject to limited compute budgets by combining model ensemble and <b>distillation</b> techniques. The use of ensembles of deep neural networks has been shown to improve generalization accuracy in many application domains. We first demonstrate significant performance gains by creating a large ensemble of optimized single models. We then develop a generalized framework to <b>distill</b> motion forecasting model ensembles into small student models which retain high performance with a fraction of the computing cost. For this study we focus on the task of motion forecasting using real world data from autonomous driving systems. We develop ensemble models that are very competitive on the Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards. From these ensembles, we train <b>distilled</b> student models which have high performance at a fraction of the compute costs. These experiments demonstrate <b>distillation</b> from ensembles as an effective method for improving accuracy of predictive models for robotic systems with limited compute budgets.</p></p class="citation"></blockquote><h3 id=815--83178-a-ground-mobile-robot-for-autonomous-terrestrial-laser-scanning-based-field-phenotyping-javier-rodriguez-sanchez-et-al-2024>(8/15 | 83/178) A Ground Mobile Robot for Autonomous Terrestrial Laser Scanning-Based Field Phenotyping (Javier Rodriguez-Sanchez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Javier Rodriguez-Sanchez, Kyle Johnsen, Changying Li. (2024)<br><strong>A Ground Mobile Robot for Autonomous Terrestrial Laser Scanning-Based Field Phenotyping</strong><br><button class=copy-to-clipboard title="A Ground Mobile Robot for Autonomous Terrestrial Laser Scanning-Based Field Phenotyping" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04404v1.pdf filename=2404.04404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional field phenotyping methods are often manual, time-consuming, and destructive, posing a challenge for breeding progress. To address this bottleneck, robotics and automation technologies offer efficient sensing tools to monitor field evolution and crop development throughout the season. This study aimed to develop an autonomous ground robotic system for LiDAR-based field phenotyping in plant breeding trials. A Husky platform was equipped with a high-resolution three-dimensional (3D) laser scanner to collect in-field terrestrial laser scanning (TLS) data without <b>human</b> <b>intervention.</b> To automate the TLS process, a 3D ray casting analysis was implemented for optimal TLS site planning, and a route optimization algorithm was utilized to minimize travel distance during data collection. The platform was deployed in two cotton breeding fields for evaluation, where it autonomously collected TLS data. The system provided accurate pose information through RTK-GNSS positioning and sensor fusion techniques, with average errors of less than 0.6 cm for location and 0.38$^{\circ}$ for heading. The achieved localization accuracy allowed point cloud registration with mean point errors of approximately 2 cm, comparable to traditional TLS methods that rely on artificial targets and manual sensor deployment. This work presents an autonomous phenotyping platform that facilitates the quantitative assessment of plant traits under field conditions of both large agricultural fields and small breeding trials to contribute to the advancement of plant phenomics and breeding programs.</p></p class="citation"></blockquote><h3 id=915--84178-loss-slam-lightweight-open-set-semantic-simultaneous-localization-and-mapping-kurran-singh-et-al-2024>(9/15 | 84/178) LOSS-SLAM: Lightweight Open-Set Semantic Simultaneous Localization and Mapping (Kurran Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kurran Singh, Tim Magoun, John J. Leonard. (2024)<br><strong>LOSS-SLAM: Lightweight Open-Set Semantic Simultaneous Localization and Mapping</strong><br><button class=copy-to-clipboard title="LOSS-SLAM: Lightweight Open-Set Semantic Simultaneous Localization and Mapping" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04377v1.pdf filename=2404.04377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling robots to understand the world in terms of objects is a critical building block towards higher level autonomy. The success of <b>foundation</b> <b>models</b> in vision has created the ability to segment and identify nearly all objects in the world. However, utilizing such objects to localize the robot and build an open-set semantic map of the world remains an open research question. In this work, a system of identifying, localizing, and encoding objects is tightly coupled with probabilistic graphical models for performing open-set semantic simultaneous localization and mapping (SLAM). Results are presented demonstrating that the proposed lightweight object encoding can be used to perform more accurate object-based SLAM than existing open-set methods, closed-set methods, and geometric methods while incurring a lower computational overhead than existing open-set mapping methods.</p></p class="citation"></blockquote><h3 id=1015--85178-tooleenet-tool-affordance-6d-pose-estimation-yunlong-wang-et-al-2024>(10/15 | 85/178) ToolEENet: Tool Affordance 6D Pose Estimation (Yunlong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Wang, Lei Zhang, Yuyang Tu, Hui Zhang, Kaixin Bai, Zhaopeng Chen, Jianwei Zhang. (2024)<br><strong>ToolEENet: Tool Affordance 6D Pose Estimation</strong><br><button class=copy-to-clipboard title="ToolEENet: Tool Affordance 6D Pose Estimation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04193v1.pdf filename=2404.04193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exploration of robotic dexterous hands utilizing tools has recently attracted considerable attention. A significant challenge in this field is the precise awareness of a tool&rsquo;s pose when grasped, as occlusion by the hand often degrades the quality of the estimation. Additionally, the tool&rsquo;s overall pose often fails to accurately represent the contact interaction, thereby limiting the effectiveness of vision-guided, contact-dependent activities. To overcome this limitation, we present the innovative TOOLEE dataset, which, to the best of our knowledge, is the first to feature affordance segmentation of a tool&rsquo;s end-effector (EE) along with its defined 6D pose based on its usage. Furthermore, we propose the ToolEENet framework for accurate 6D pose estimation of the tool&rsquo;s EE. This framework begins by segmenting the tool&rsquo;s EE from raw RGBD data, then uses a <b>diffusion</b> <b>model-based</b> pose estimator for 6D pose estimation at a category-specific level. Addressing the issue of symmetry in pose estimation, we introduce a symmetry-aware pose representation that enhances the consistency of pose estimation. Our approach excels in this field, demonstrating high levels of precision and generalization. Furthermore, it shows great promise for application in contact-based manipulation scenarios. All data and codes are available on the project website: <a href=https://yuyangtu.github.io/projectToolEENet.html>https://yuyangtu.github.io/projectToolEENet.html</a></p></p class="citation"></blockquote><h3 id=1115--86178-designing-robots-to-help-women-martin-cooney-et-al-2024>(11/15 | 86/178) Designing Robots to Help Women (Martin Cooney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Cooney, Lena Klasén, Fernando Alonso-Fernandez. (2024)<br><strong>Designing Robots to Help Women</strong><br><button class=copy-to-clipboard title="Designing Robots to Help Women" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04123v1.pdf filename=2404.04123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots are being designed to help people in an increasing variety of settings&ndash;but seemingly little attention has been given so far to the specific needs of women, who represent roughly half of the world&rsquo;s population but are highly underrepresented in robotics. Here we used a speculative prototyping approach to explore this expansive design space: First, we identified some potential challenges of interest, including crimes and illnesses that disproportionately affect women, as well as potential opportunities for designers, which were visualized in five sketches. Then, one of the sketched scenarios was further explored by developing a prototype, of a robotic helper drone equipped with computer vision to detect hidden cameras that could be used to spy on women. While <b>object</b> <b>detection</b> introduced some errors, hidden cameras were identified with a reasonable accuracy of 80% (Intersection over Union (IoU) score: 0.40). Our aim is that the identified challenges and opportunities could help spark discussion and inspire designers, toward realizing a safer, more inclusive future through responsible use of technology.</p></p class="citation"></blockquote><h3 id=1215--87178-high-frequency-capacitive-sensing-for-electrohydraulic-soft-actuators-michel-r-vogt-et-al-2024>(12/15 | 87/178) High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators (Michel R. Vogt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michel R. Vogt, Maximilian Eberlein, Clemens C. Christoph, Felix Baumann, Fabrice Bourquin, Wim Wende, Fabio Schaub, Amirhossein Kazemipour, Robert K. Katzschmann. (2024)<br><strong>High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators</strong><br><button class=copy-to-clipboard title="High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04071v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04071v2.pdf filename=2404.04071v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The need for compliant and proprioceptive actuators has grown more evident in pursuing more adaptable and versatile robotic systems. Hydraulically Amplified Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with their inherent softness and flexibility, making them promising candidates for various robotic tasks, including delicate interactions with humans and animals, biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a growing interest in the capacitive self-sensing capabilities of HASEL actuators to create miniature displacement estimation circuitry that does not require external sensors. However, achieving HASEL self-sensing for actuation frequencies above 1 Hz and with miniature high-voltage power supplies has remained limited. In this paper, we introduce the F-HASEL actuator, which adds an additional electrode pair used exclusively for capacitive sensing to a Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL during high-frequency actuation up to 20 Hz and during external loading using miniaturized circuitry comprised of low-cost off-the-shelf components and a miniature high-voltage power supply. Finally, we propose a circuitry to estimate the displacement of multiple F-HASELs and demonstrate it in a wearable application to track joint rotations of a <b>virtual</b> <b>reality</b> user in real-time.</p></p class="citation"></blockquote><h3 id=1315--88178-modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks-jordan-thompson-et-al-2024>(13/15 | 88/178) Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks (Jordan Thompson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Thompson, Brian Y. Cho, Daniel S. Brown, Alan Kuntz. (2024)<br><strong>Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks</strong><br><button class=copy-to-clipboard title="Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04241v1.pdf filename=2404.04241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tendon-driven continuum robot kinematic models are frequently computationally expensive, inaccurate due to unmodeled effects, or both. In particular, unmodeled effects produce uncertainties that arise during the robot&rsquo;s operation that lead to variability in the resulting <b>geometry.</b> We propose a novel solution to these issues through the development of a Gaussian mixture kinematic model. We train a mixture density network to output a Gaussian mixture model representation of the robot <b>geometry</b> given the current tendon displacements. This model computes a probability distribution that is more representative of the true distribution of geometries at a given configuration than a model that outputs a single <b>geometry,</b> while also reducing the computation time. We demonstrate one use of this model through a trajectory optimization method that explicitly reasons about the workspace uncertainty to minimize the probability of collision.</p></p class="citation"></blockquote><h3 id=1415--89178-multi-modal-perception-for-soft-robotic-interactions-using-generative-models-enrico-donato-et-al-2024>(14/15 | 89/178) Multi-modal perception for soft robotic interactions using generative models (Enrico Donato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrico Donato, Egidio Falotico, Thomas George Thuruthel. (2024)<br><strong>Multi-modal perception for soft robotic interactions using generative models</strong><br><button class=copy-to-clipboard title="Multi-modal perception for soft robotic interactions using generative models" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04220v1.pdf filename=2404.04220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perception is essential for the active interaction of physical agents with the external environment. The integration of multiple sensory modalities, such as touch and vision, enhances this perceptual process, creating a more comprehensive and robust understanding of the world. Such fusion is particularly useful for highly deformable bodies such as soft robots. Developing a compact, yet comprehensive state representation from multi-sensory inputs can pave the way for the development of complex control strategies. This paper introduces a perception model that harmonizes data from diverse modalities to build a holistic state representation and assimilate essential information. The model relies on the causality between sensory input and robotic actions, employing a generative model to efficiently compress fused information and predict the next observation. We present, for the first time, a study on how touch can be predicted from vision and proprioception on soft robots, the importance of the cross-modal generation and why this is essential for soft robotic interactions in unstructured environments.</p></p class="citation"></blockquote><h3 id=1515--90178-mm-gaussian-3d-gaussian-based-multi-modal-fusion-for-localization-and-reconstruction-in-unbounded-scenes-chenyang-wu-et-al-2024>(15/15 | 90/178) MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes (Chenyang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyang Wu, Yifan Duan, Xinran Zhang, Yu Sheng, Jianmin Ji, Yanyong Zhang. (2024)<br><strong>MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes</strong><br><button class=copy-to-clipboard title="MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04026v1.pdf filename=2404.04026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localization and mapping are critical tasks for various applications such as autonomous vehicles and robotics. The challenges posed by outdoor environments present particular complexities due to their unbounded characteristics. In this work, we present MM-Gaussian, a LiDAR-camera <b>multi-modal</b> fusion system for localization and mapping in unbounded scenes. Our approach is inspired by the recently developed 3D Gaussians, which demonstrate remarkable capabilities in achieving high rendering quality and fast rendering speed. Specifically, our system fully utilizes the geometric structure information provided by solid-state LiDAR to address the problem of inaccurate depth encountered when relying solely on visual solutions in unbounded, outdoor scenarios. Additionally, we utilize 3D Gaussian point clouds, with the assistance of pixel-level gradient descent, to fully exploit the color information in photos, thereby achieving realistic rendering effects. To further bolster the robustness of our system, we designed a relocalization module, which assists in returning to the correct trajectory in the event of a localization failure. Experiments conducted in multiple scenarios demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h2 id=csai-8>cs.AI (8)</h2><h3 id=18--91178-hypothesis-generation-with-large-language-models-yangqiaoyu-zhou-et-al-2024>(1/8 | 91/178) Hypothesis Generation with Large Language Models (Yangqiaoyu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan. (2024)<br><strong>Hypothesis Generation with Large Language Models</strong><br><button class=copy-to-clipboard title="Hypothesis Generation with Large Language Models" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.AI<br>Keyword Score: 70<br>Keywords: Bandit Algorithm, Few-shot, Supervised Learning, Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04326v1.pdf filename=2404.04326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable <b>LLMs</b> to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed <b>bandits,</b> we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than <b>few-shot</b> <b>prompting</b> in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform <b>supervised</b> <b>learning</b> by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.</p></p class="citation"></blockquote><h3 id=28--92178-exploring-autonomous-agents-through-the-lens-of-large-language-models-a-review-saikat-barua-2024>(2/8 | 92/178) Exploring Autonomous Agents through the Lens of Large Language Models: A Review (Saikat Barua, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saikat Barua. (2024)<br><strong>Exploring Autonomous Agents through the Lens of Large Language Models: A Review</strong><br><button class=copy-to-clipboard title="Exploring Autonomous Agents through the Lens of Large Language Models: A Review" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 60<br>Keywords: Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04442v1.pdf filename=2404.04442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains. These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare. However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation. Techniques like <b>prompting,</b> <b>reasoning,</b> tool utilization, and <b>in-context</b> <b>learning</b> are being explored to enhance their capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM provide robust methods for assessing these agents in complex scenarios. These advancements are leading to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives, assisting in tasks from email responses to disease diagnosis. The future of AI, with <b>LLMs</b> at the forefront, is promising.</p></p class="citation"></blockquote><h3 id=38--93178-kgexplainer-towards-exploring-connected-subgraph-explanations-for-knowledge-graph-completion-tengfei-ma-et-al-2024>(3/8 | 93/178) KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion (Tengfei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tengfei Ma, Xiang song, Wen Tao, Mufei Li, Jiani Zhang, Xiaoqin Pan, Jianxin Lin, Bosheng Song, xiangxiang Zeng. (2024)<br><strong>KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion</strong><br><button class=copy-to-clipboard title="KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 51<br>Keywords: Graph, Graph Embedding, Benchmarking, Black Box, Knowledge Distillation, Knowledge Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03893v1.pdf filename=2404.03893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graph</b> <b>completion</b> (KGC) aims to alleviate the inherent incompleteness of <b>knowledge</b> <b>graphs</b> <b>(KGs),</b> which is a critical task for various applications, such as <b>recommendations</b> on the web. Although <b>knowledge</b> <b>graph</b> <b>embedding</b> (KGE) models have demonstrated superior predictive performance on KGC tasks, these models infer missing links in a <b>black-box</b> <b>manner</b> that lacks transparency and accountability, preventing researchers from developing accountable models. Existing KGE-based explanation methods focus on exploring key paths or isolated edges as explanations, which is information-less to reason target prediction. Additionally, the missing ground truth leads to these explanation methods being ineffective in quantitatively evaluating explored explanations. To overcome these limitations, we propose KGExplainer, a model-agnostic method that identifies connected subgraph explanations and <b>distills</b> an evaluator to assess them quantitatively. KGExplainer employs a perturbation-based greedy search algorithm to find key connected subgraphs as explanations within the local structure of target predictions. To evaluate the quality of the explored explanations, KGExplainer <b>distills</b> an evaluator from the target KGE model. By forwarding the explanations to the evaluator, our method can examine the fidelity of them. Extensive experiments on <b>benchmark</b> datasets demonstrate that KGExplainer yields promising improvement and achieves an optimal ratio of 83.3% in human evaluation.</p></p class="citation"></blockquote><h3 id=48--94178-intervention-assisted-policy-gradient-methods-for-online-stochastic-queuing-network-optimization-technical-report-jerrod-wigmore-et-al-2024>(4/8 | 94/178) Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report (Jerrod Wigmore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerrod Wigmore, Brooke Shrader, Eytan Modiano. (2024)<br><strong>Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report</strong><br><button class=copy-to-clipboard title="Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: F-2-2; I-2-6, cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04106v1.pdf filename=2404.04106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) offers a powerful approach to training neural network control policies for stochastic queuing networks (SQN). However, traditional DRL methods rely on offline <b>simulations</b> or static datasets, limiting their real-world application in SQN control. This work proposes Online Deep <b>Reinforcement</b> <b>Learning-based</b> Controls (ODRLC) as an alternative, where an intelligent agent interacts directly with a real environment and learns an optimal control policy from these online interactions. SQNs present a challenge for ODRLC due to the unbounded nature of the queues within the network resulting in an unbounded state-space. An unbounded state-space is particularly challenging for neural network policies as neural networks are notoriously poor at extrapolating to unseen states. To address this challenge, we propose an intervention-assisted framework that leverages strategic interventions from known stable policies to ensure the queue sizes remain bounded. This framework combines the learning power of neural networks with the guaranteed stability of classical control policies for SQNs. We introduce a method to design these intervention-assisted policies to ensure strong stability of the network. Furthermore, we extend foundational DRL theorems for intervention-assisted policies and develop two practical algorithms specifically for ODRLC of SQNs. Finally, we demonstrate through experiments that our proposed algorithms outperform both classical control approaches and prior ODRLC algorithms.</p></p class="citation"></blockquote><h3 id=58--95178-random-walk-in-random-permutation-set-theory-jiefeng-zhou-et-al-2024>(5/8 | 95/178) Random Walk in Random Permutation Set Theory (Jiefeng Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiefeng Zhou, Zhen Li, Yong Deng. (2024)<br><strong>Random Walk in Random Permutation Set Theory</strong><br><button class=copy-to-clipboard title="Random Walk in Random Permutation Set Theory" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-IT, cs.AI, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03978v1.pdf filename=2404.03978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random walk is an explainable approach for modeling natural processes at the molecular level. The Random Permutation Set Theory (RPST) serves as a framework for uncertainty <b>reasoning,</b> extending the applicability of Dempster-Shafer Theory. Recent explorations indicate a promising link between RPST and random walk. In this study, we conduct an analysis and construct a random walk model based on the properties of RPST, with Monte Carlo <b>simulations</b> of such random walk. Our findings reveal that the random walk generated through RPST exhibits characteristics similar to those of a Gaussian random walk and can be transformed into a Wiener process through a specific limiting scaling procedure. This investigation establishes a novel connection between RPST and random walk theory, thereby not only expanding the applicability of RPST, but also demonstrating the potential for combining the strengths of both approaches to improve problem-solving abilities.</p></p class="citation"></blockquote><h3 id=68--96178-large-language-models-as-oracles-for-instantiating-ontologies-with-domain-specific-knowledge-giovanni-ciatto-et-al-2024>(6/8 | 96/178) Large language models as oracles for instantiating ontologies with domain-specific knowledge (Giovanni Ciatto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanni Ciatto, Andrea Agiollo, Matteo Magnini, Andrea Omicini. (2024)<br><strong>Large language models as oracles for instantiating ontologies with domain-specific knowledge</strong><br><button class=copy-to-clipboard title="Large language models as oracles for instantiating ontologies with domain-specific knowledge" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04108v1.pdf filename=2404.04108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background. Endowing intelligent systems with semantic data commonly requires designing and instantiating ontologies with domain-specific knowledge. Especially in the early phases, those activities are typically performed manually by human experts possibly leveraging on their own experience. The resulting process is therefore time-consuming, error-prone, and often biased by the personal background of the ontology designer. Objective. To mitigate that issue, we propose a novel domain-independent approach to automatically instantiate ontologies with domain-specific knowledge, by leveraging on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as oracles. Method. Starting from (i) an initial schema composed by inter-related classes andproperties and (ii) a set of query templates, our method queries the <b>LLM</b> multi- ple times, and generates instances for both classes and properties from its replies. Thus, the ontology is automatically filled with domain-specific knowledge, compliant to the initial schema. As a result, the ontology is quickly and automatically enriched with manifold instances, which experts may consider to keep, adjust, discard, or complement according to their own needs and expertise. Contribution. We formalise our method in general way and instantiate it over various <b>LLMs,</b> as well as on a concrete case study. We report experiments rooted in the nutritional domain where an ontology of food meals and their ingredients is semi-automatically instantiated from scratch, starting from a categorisation of meals and their relationships. There, we analyse the quality of the generated ontologies and compare ontologies attained by exploiting different <b>LLMs.</b> Finally, we provide a SWOT analysis of the proposed method.</p></p class="citation"></blockquote><h3 id=78--97178-ai-knowledge-and-reasoning-emulating-expert-creativity-in-scientific-research-anirban-mukherjee-et-al-2024>(7/8 | 97/178) AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research (Anirban Mukherjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anirban Mukherjee, Hannah Hanwen Chang. (2024)<br><strong>AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research</strong><br><button class=copy-to-clipboard title="AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04436v1.pdf filename=2404.04436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate whether modern AI can emulate expert creativity in complex scientific endeavors. We introduce novel methodology that utilizes original research articles published after the AI&rsquo;s training cutoff, ensuring no prior exposure, mitigating concerns of rote memorization and prior training. The AI are tasked with redacting findings, predicting outcomes from redacted research, and assessing prediction accuracy against reported results. Analysis on 589 published studies in four leading psychology journals over a 28-month period, showcase the AI&rsquo;s proficiency in understanding specialized research, deductive <b>reasoning,</b> and evaluating evidentiary alignment&ndash;cognitive hallmarks of human subject matter expertise and creativity. These findings suggest the potential of general-purpose AI to transform academia, with roles requiring knowledge-based creativity become increasingly susceptible to technological substitution.</p></p class="citation"></blockquote><h3 id=88--98178-visual-knowledge-in-the-big-model-era-retrospect-and-prospect-wenguan-wang-et-al-2024>(8/8 | 98/178) Visual Knowledge in the Big Model Era: Retrospect and Prospect (Wenguan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenguan Wang, Yi Yang, Yunhe Pan. (2024)<br><strong>Visual Knowledge in the Big Model Era: Retrospect and Prospect</strong><br><button class=copy-to-clipboard title="Visual Knowledge in the Big Model Era: Retrospect and Prospect" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04308v1.pdf filename=2404.04308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual knowledge is a new form of knowledge representation that can encapsulate visual concepts and their relations in a succinct, comprehensive, and interpretable manner, with a deep root in cognitive psychology. As the knowledge about the visual world has been identified as an indispensable component of human cognition and intelligence, visual knowledge is poised to have a pivotal role in establishing machine intelligence. With the recent advance of Artificial Intelligence (AI) techniques, large AI models (or <b>foundation</b> <b>models)</b> have emerged as a potent tool capable of extracting versatile patterns from broad data as implicit knowledge, and abstracting them into an outrageous amount of numeric parameters. To pave the way for creating visual knowledge empowered AI machines in this coming wave, we present a timely review that investigates the origins and development of visual knowledge in the pre-big model era, and accentuates the opportunities and unique role of visual knowledge in the big model era.</p></p class="citation"></blockquote><h2 id=csir-3>cs.IR (3)</h2><h3 id=13--99178-a-comparison-of-methods-for-evaluating-generative-ir-negar-arabzadeh-et-al-2024>(1/3 | 99/178) A Comparison of Methods for Evaluating Generative IR (Negar Arabzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Negar Arabzadeh, Charles L. A. Clarke. (2024)<br><strong>A Comparison of Methods for Evaluating Generative IR</strong><br><button class=copy-to-clipboard title="A Comparison of Methods for Evaluating Generative IR" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04044v1.pdf filename=2404.04044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> <b>systems</b> <b>increasingly</b> incorporate generative components. For example, in a <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> system, a <b>retrieval</b> <b>component</b> <b>might</b> provide a source of ground truth, while a generative component <b>summarizes</b> and augments its responses. In other systems, a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> might directly generate responses without consulting a <b>retrieval</b> <b>component.</b> <b>While</b> there are multiple definitions of generative <b>information</b> <b>retrieval</b> <b>(Gen-IR)</b> <b>systems,</b> in this paper we focus on those systems where the system&rsquo;s response is not drawn from a fixed collection of documents or passages. The response to a query may be entirely new text never. Since traditional IR evaluation methods break down under this model, we explore various methods that extend traditional offline evaluation approaches to the Gen-IR context. Offline IR evaluation traditionally employs paid human assessors, but increasingly <b>LLMs</b> are replacing human assessment, demonstrating capabilities similar or superior to crowdsourced labels. Given that Gen-IR systems do not generate responses from a fixed set, we assume that methods for Gen-IR evaluation must largely depend on <b>LLM-generated</b> labels. Along with methods based on binary and graded relevance, we explore methods based on explicit subtopics, pairwise preferences, and embeddings. We first validate these methods against human assessments on several TREC Deep Learning Track tasks; we then apply these methods to evaluate the output of several purely generative systems. For each method we consider both its ability to act autonomously, without the need for human labels or other input, and its ability to support human auditing. To trust these methods, we must be assured that their results align with human assessments. In order to do so, evaluation criteria must be transparent, so that outcomes can be audited by human assessors.</p></p class="citation"></blockquote><h3 id=23--100178-dwell-in-the-beginning-how-language-models-embed-long-documents-for-dense-retrieval-joão-coelho-et-al-2024>(2/3 | 100/178) Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval (João Coelho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Coelho, Bruno Martins, João Magalhães, Jamie Callan, Chenyan Xiong. (2024)<br><strong>Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval</strong><br><button class=copy-to-clipboard title="Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 35<br>Keywords: Dense Retrieval, Fine-tuning, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04163v1.pdf filename=2404.04163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the existence of positional biases in <b>Transformer-based</b> models for text <b>representation</b> <b>learning,</b> particularly in the context of web document retrieval. We build on previous research that demonstrated loss of information in the middle of input sequences for causal language models, extending it to the domain of <b>representation</b> <b>learning.</b> We examine positional biases at various stages of training for an encoder-decoder model, including language model pre-training, contrastive pre-training, and contrastive <b>fine-tuning.</b> Experiments with the MS-MARCO document collection reveal that after contrastive pre-training the model already generates embeddings that better capture early contents of the input, with <b>fine-tuning</b> further aggravating this effect.</p></p class="citation"></blockquote><h3 id=33--101178-jobformer-skill-aware-job-recommendation-with-semantic-enhanced-transformer-zhihao-guan-et-al-2024>(3/3 | 101/178) JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer (Zhihao Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Guan, Jia-Qi Yang, Yang Yang, Hengshu Zhu, Wenjie Li, Hui Xiong. (2024)<br><strong>JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer</strong><br><button class=copy-to-clipboard title="JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 35<br>Keywords: Recommendation, Representation Learning, Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04313v1.pdf filename=2404.04313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Job <b>recommendation</b> aims to provide potential talents with suitable job descriptions (JDs) consistent with their career trajectory, which plays an essential role in proactive talent recruitment. In real-world management scenarios, the available JD-user records always consist of JDs, user profiles, and click data, in which the user profiles are typically <b>summarized</b> as the user&rsquo;s skill distribution for privacy reasons. Although existing sophisticated <b>recommendation</b> methods can be directly employed, effective <b>recommendation</b> still has challenges considering the information deficit of JD itself and the natural heterogeneous gap between JD and user profile. To address these challenges, we proposed a novel skill-aware <b>recommendation</b> model based on the designed semantic-enhanced <b>transformer</b> to parse JDs and complete personalized job <b>recommendation.</b> Specifically, we first model the relative items of each JD and then adopt an encoder with the local-global attention mechanism to better mine the intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a two-stage learning strategy for skill-aware <b>recommendation,</b> in which we utilize the skill distribution to guide JD <b>representation</b> <b>learning</b> in the recall stage, and then combine the user profiles for final prediction in the ranking stage. Consequently, we can embed rich contextual semantic <b>representations</b> <b>for</b> learning JDs, while skill-aware <b>recommendation</b> provides effective JD-user joint <b>representation</b> <b>for</b> click-through rate (CTR) prediction. To validate the superior performance of our method for job <b>recommendation,</b> we present a thorough empirical analysis of large-scale real-world and public datasets to demonstrate its effectiveness and interpretability.</p></p class="citation"></blockquote><h2 id=cslg-33>cs.LG (33)</h2><h3 id=133--102178-prompt-public-large-language-models-to-synthesize-data-for-private-on-device-applications-shanshan-wu-et-al-2024>(1/33 | 102/178) Prompt Public Large Language Models to Synthesize Data for Private On-device Applications (Shanshan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanshan Wu, Zheng Xu, Yanxiang Zhang, Yuanbo Zhang, Daniel Ramage. (2024)<br><strong>Prompt Public Large Language Models to Synthesize Data for Private On-device Applications</strong><br><button class=copy-to-clipboard title="Prompt Public Large Language Models to Synthesize Data for Private On-device Applications" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Federated Learning, Fine-tuning, Large Language Model, Large Language Model, Prompt, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04360v1.pdf filename=2404.04360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-training on public data is an effective method to improve the performance for <b>federated</b> <b>learning</b> (FL) with <b>differential</b> <b>privacy</b> (DP). This paper investigates how <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> trained on public data can improve the quality of pre-training data for the on-device language models trained with DP and FL. We carefully design <b>LLM</b> <b>prompts</b> to filter and transform existing public data, and generate new data to resemble the real user data distribution. The model pre-trained on our synthetic dataset achieves relative improvement of 19.0% and 22.8% in next word prediction accuracy compared to the baseline model pre-trained on a standard public dataset, when evaluated over the real user data in Gboard (Google Keyboard, a production mobile keyboard application). Furthermore, our method achieves evaluation accuracy better than or comparable to the baseline during the DP FL <b>fine-tuning</b> over millions of mobile devices, and our final model outperforms the baseline in production A/B testing. Our experiments demonstrate the strengths of <b>LLMs</b> in synthesizing data close to the private distribution even without accessing the private data, and also suggest future research directions to further reduce the distribution gap.</p></p class="citation"></blockquote><h3 id=233--103178-dynamic-switch-layers-for-unsupervised-learning-haiguang-li-et-al-2024>(2/33 | 103/178) Dynamic Switch Layers For Unsupervised Learning (Haiguang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiguang Li, Usama Pervaiz, Michał Matuszak, Robert Kamara, Gilles Roux, Trausti Thormundsson, Joseph Antognini. (2024)<br><strong>Dynamic Switch Layers For Unsupervised Learning</strong><br><button class=copy-to-clipboard title="Dynamic Switch Layers For Unsupervised Learning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Graph Attention Networks, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04405v1.pdf filename=2404.04405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On-device machine learning (ODML) enables intelligent applications on resource-constrained devices. However, power consumption poses a major challenge, forcing a trade-off between model accuracy and power efficiency that often limits model complexity. The previously established <b>Gated</b> Compression (GC) layers offer a solution, enabling power efficiency without sacrificing model performance by selectively <b>gating</b> samples that lack signals of interest. However, their reliance on ground truth labels limits GC layers to <b>supervised</b> tasks. This work introduces the Dynamic Switch Layer (DSL), extending the benefits of GC layers to <b>unsupervised</b> <b>learning</b> scenarios, and maintaining power efficiency without the need for labeled data. The DSL builds upon the GC architecture, leveraging a dynamic pathway selection, and adapting model complexity in response to the innate structure of the data. We integrate the DSL into the SoundStream architecture and demonstrate that by routing up to 80% of samples through a lightweight pass we achieve a 12.3x reduction in the amount of computation performed and a 20.9x reduction in model size. This reduces the on-device inference latency by up to 26.5% and improves power efficiency by up to 21.4% without impacting model performance.</p></p class="citation"></blockquote><h3 id=333--104178-player2vec-a-language-modeling-approach-to-understand-player-behavior-in-games-tianze-wang-et-al-2024>(3/33 | 104/178) player2vec: A Language Modeling Approach to Understand Player Behavior in Games (Tianze Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianze Wang, Maryam Honari-Jahromi, Styliani Katsarou, Olga Mikheeva, Theodoros Panagiotakopoulos, Sahar Asadi, Oleg Smirnov. (2024)<br><strong>player2vec: A Language Modeling Approach to Understand Player Behavior in Games</strong><br><button class=copy-to-clipboard title="player2vec: A Language Modeling Approach to Understand Player Behavior in Games" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Recommendation, Self-supervised Learning, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04234v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04234v2.pdf filename=2404.04234v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Methods for learning latent user representations from historical behavior logs have gained traction for <b>recommendation</b> tasks in e-commerce, content streaming, and other settings. However, this area still remains relatively underexplored in video and mobile gaming contexts. In this work, we present a novel method for overcoming this limitation by extending a long-range <b>Transformer</b> model from the natural language processing domain to player behavior data. We discuss specifics of behavior tracking in games and propose preprocessing and <b>tokenization</b> approaches by viewing in-game events in an analogous way to words in sentences, thus enabling learning player representations in a <b>self-supervised</b> manner in the absence of ground-truth annotations. We experimentally demonstrate the efficacy of the proposed approach in fitting the distribution of behavior events by evaluating intrinsic language modeling metrics. Furthermore, we qualitatively analyze the emerging structure of the learned embedding space and show its value for generating insights into behavior patterns to inform downstream applications.</p></p class="citation"></blockquote><h3 id=433--105178-robust-preference-optimization-with-provable-noise-tolerance-for-llms-xize-liang-et-al-2024>(4/33 | 105/178) Robust Preference Optimization with Provable Noise Tolerance for LLMs (Xize Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, Jieping Ye. (2024)<br><strong>Robust Preference Optimization with Provable Noise Tolerance for LLMs</strong><br><button class=copy-to-clipboard title="Robust Preference Optimization with Provable Noise Tolerance for LLMs" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Noise-tolerant, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04102v1.pdf filename=2404.04102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The preference alignment aims to enable <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate responses that conform to human values, which is essential for developing general AI systems. Ranking-based methods &ndash; a promising class of alignment approaches &ndash; learn human preferences from datasets containing response pairs by optimizing the log-likelihood margins between preferred and dis-preferred responses. However, due to the inherent differences in annotators&rsquo; preferences, ranking labels of comparisons for response pairs are unavoidably noisy. This seriously hurts the reliability of existing ranking-based methods. To address this problem, we propose a provably <b>noise-tolerant</b> preference alignment method, namely RObust Preference Optimization (ROPO). To the best of our knowledge, ROPO is the first preference alignment method with noise-tolerance guarantees. The key idea of ROPO is to dynamically assign conservative gradient weights to response pairs with high label uncertainty, based on the log-likelihood margins between the responses. By effectively suppressing the gradients of noisy samples, our weighting strategy ensures that the expected risk has the same gradient direction independent of the presence and proportion of noise. Experiments on three open-ended <b>text</b> <b>generation</b> tasks with four base models ranging in size from 2.8B to 13B demonstrate that ROPO significantly outperforms existing ranking-based methods.</p></p class="citation"></blockquote><h3 id=533--106178-enhancing-iot-intelligence-a-transformer-based-reinforcement-learning-methodology-gaith-rjoub-et-al-2024>(5/33 | 106/178) Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology (Gaith Rjoub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaith Rjoub, Saidul Islam, Jamal Bentahar, Mohammed Amin Almaiah, Rana Alrawashdeh. (2024)<br><strong>Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology</strong><br><button class=copy-to-clipboard title="Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04205v1.pdf filename=2404.04205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of the Internet of Things (IoT) has led to an explosion of data generated by interconnected devices, presenting both opportunities and challenges for intelligent decision-making in complex environments. Traditional <b>Reinforcement</b> <b>Learning</b> (RL) approaches often struggle to fully harness this data due to their limited ability to process and interpret the intricate patterns and dependencies inherent in IoT applications. This paper introduces a novel framework that integrates <b>transformer</b> architectures with Proximal Policy Optimization (PPO) to address these challenges. By leveraging the <b>self-attention</b> mechanism of <b>transformers,</b> our approach enhances RL agents&rsquo; capacity for understanding and acting within dynamic IoT environments, leading to improved decision-making processes. We demonstrate the effectiveness of our method across various IoT scenarios, from smart home automation to industrial control systems, showing marked improvements in decision-making efficiency and adaptability. Our contributions include a detailed exploration of the <b>transformer&rsquo;s</b> role in processing heterogeneous IoT data, a comprehensive evaluation of the framework&rsquo;s performance in diverse environments, and a <b>benchmark</b> against traditional RL methods. The results indicate significant advancements in enabling RL agents to navigate the complexities of IoT ecosystems, highlighting the potential of our approach to revolutionize intelligent automation and decision-making in the IoT landscape.</p></p class="citation"></blockquote><h3 id=633--107178-score-identity-distillation-exponentially-fast-distillation-of-pretrained-diffusion-models-for-one-step-generation-mingyuan-zhou-et-al-2024>(6/33 | 107/178) Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation (Mingyuan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, Hai Huang. (2024)<br><strong>Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation</strong><br><button class=copy-to-clipboard title="Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04057v1.pdf filename=2404.04057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Score identity <b>Distillation</b> (SiD), an innovative data-free method that <b>distills</b> the generative capabilities of pretrained <b>diffusion</b> <b>models</b> into a single-step generator. SiD not only facilitates an exponentially fast reduction in Fr'echet inception distance (FID) during <b>distillation</b> but also approaches or even exceeds the FID performance of the original teacher <b>diffusion</b> <b>models.</b> By reformulating forward <b>diffusion</b> <b>processes</b> as semi-implicit distributions, we leverage three score-related identities to create an innovative loss mechanism. This mechanism achieves rapid FID reduction by training the generator using its own synthesized images, eliminating the need for real data or reverse-diffusion-based generation, all accomplished within significantly shortened generation time. Upon evaluation across four <b>benchmark</b> datasets, the SiD algorithm demonstrates high iteration efficiency during <b>distillation</b> and surpasses competing <b>distillation</b> approaches, whether they are one-step or few-step, data-free, or dependent on training data, in terms of generation quality. This achievement not only redefines the <b>benchmarks</b> for efficiency and effectiveness in <b>diffusion</b> <b>distillation</b> but also in the broader field of <b>diffusion-based</b> <b>generation.</b> Our PyTorch implementation will be publicly accessible on GitHub.</p></p class="citation"></blockquote><h3 id=733--108178-mitigating-heterogeneity-in-federated-multimodal-learning-with-biomedical-vision-language-pre-training-zitao-shuai-et-al-2024>(7/33 | 108/178) Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training (Zitao Shuai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zitao Shuai, Liyue Shen. (2024)<br><strong>Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training</strong><br><button class=copy-to-clipboard title="Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 31<br>Keywords: Federated Learning, Multi-modal, Multi-modal, Representation Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03854v1.pdf filename=2404.03854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> pre-training (VLP) has arised as an efficient scheme for <b>multimodal</b> <b>representation</b> <b>learning,</b> but it requires large-scale <b>multimodal</b> data for pre-training, making it an obstacle especially for biomedical applications. To overcome the data limitation, <b>federated</b> <b>learning</b> (FL) can be a promising strategy to scale up the dataset for biomedical VLP while protecting data privacy. However, client data are often heterogeneous in real-world scenarios, and we observe that local training on heterogeneous client data would distort the <b>multimodal</b> <b>representation</b> <b>learning</b> and lead to biased cross-modal alignment. To address this challenge, we propose <b>Federated</b> <b>distributional</b> Robust Guidance-Based (FedRGB) learning framework for <b>federated</b> <b>VLP</b> with robustness to data heterogeneity. Specifically, we utilize a guidance-based local training scheme to reduce feature distortions, and employ a distribution-based min-max optimization to learn unbiased cross-modal alignment. The experiments on real-world datasets show our method successfully promotes efficient <b>federated</b> <b>multimodal</b> learning for biomedical VLP with data heterogeneity.</p></p class="citation"></blockquote><h3 id=833--109178-parameter-efficient-quasi-orthogonal-fine-tuning-via-givens-rotation-xinyu-ma-et-al-2024>(8/33 | 109/178) Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation (Xinyu Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Ma, Xu Chu, Zhibang Yang, Yang Lin, Xin Gao, Junfeng Zhao. (2024)<br><strong>Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation</strong><br><button class=copy-to-clipboard title="Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04316v1.pdf filename=2404.04316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasingly powerful performances and enormous scales of <b>Pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs),</b> promoting parameter efficiency in <b>fine-tuning</b> has become a crucial need for effective and efficient adaptation to various downstream tasks. One representative line of <b>fine-tuning</b> methods is Orthogonal <b>Fine-tuning</b> (OFT), which rigorously preserves the angular distances within the parameter space to preserve the <b>pretrained</b> <b>knowledge.</b> <b>Despite</b> the empirical effectiveness, OFT still suffers low parameter efficiency at $\mathcal{O}(d^2)$ and limited capability of downstream adaptation. Inspired by Givens rotation, in this paper, we proposed quasi-Givens Orthogonal <b>Fine-Tuning</b> (qGOFT) to address the problems. We first use $\mathcal{O}(d)$ Givens rotations to accomplish arbitrary orthogonal transformation in $SO(d)$ with provable equivalence, reducing parameter complexity from $\mathcal{O}(d^2)$ to $\mathcal{O}(d)$. Then we introduce flexible norm and relative angular adjustments under soft orthogonality regularization to enhance the adaptation capability of downstream semantic deviations. Extensive experiments on various tasks and <b>PLMs</b> validate the effectiveness of our methods.</p></p class="citation"></blockquote><h3 id=933--110178-a-real-time-anomaly-detection-using-convolutional-autoencoder-with-dynamic-threshold-sarit-maitra-et-al-2024>(9/33 | 110/178) A Real-time Anomaly Detection Using Convolutional Autoencoder with Dynamic Threshold (Sarit Maitra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarit Maitra, Sukanya Kundu, Aishwarya Shankar. (2024)<br><strong>A Real-time Anomaly Detection Using Convolutional Autoencoder with Dynamic Threshold</strong><br><button class=copy-to-clipboard title="A Real-time Anomaly Detection Using Convolutional Autoencoder with Dynamic Threshold" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Autoencoder, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04311v1.pdf filename=2404.04311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The majority of modern consumer-level energy is generated by real-time smart metering systems. These frequently contain anomalies, which prevent reliable estimates of the series&rsquo; evolution. This work introduces a hybrid modeling approach combining statistics and a <b>Convolutional</b> <b>Autoencoder</b> with a dynamic threshold. The threshold is determined based on Mahalanobis distance and moving averages. It has been tested using real-life energy consumption data collected from smart metering systems. The solution includes a real-time, meter-level <b>anomaly</b> <b>detection</b> system that connects to an advanced monitoring system. This makes a substantial contribution by detecting unusual data movements and delivering an early warning. Early detection and subsequent troubleshooting can financially benefit organizations and consumers and prevent disasters from occurring.</p></p class="citation"></blockquote><h3 id=1033--111178-transformers-for-molecular-property-prediction-lessons-learned-from-the-past-five-years-afnan-sultan-et-al-2024>(10/33 | 111/178) Transformers for molecular property prediction: Lessons learned from the past five years (Afnan Sultan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Afnan Sultan, Jochen Sieg, Miriam Mathea, Andrea Volkamer. (2024)<br><strong>Transformers for molecular property prediction: Lessons learned from the past five years</strong><br><button class=copy-to-clipboard title="Transformers for molecular property prediction: Lessons learned from the past five years" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03969v1.pdf filename=2404.03969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Molecular Property Prediction (MPP) is vital for drug discovery, crop protection, and environmental science. Over the last decades, diverse computational techniques have been developed, from using simple physical and chemical properties and molecular fingerprints in statistical models and classical machine learning to advanced deep learning approaches. In this review, we aim to <b>distill</b> insights from current research on employing <b>transformer</b> models for MPP. We analyze the currently available models and explore key questions that arise when training and <b>fine-tuning</b> a <b>transformer</b> model for MPP. These questions encompass the choice and scale of the pre-training data, optimal architecture selections, and promising pre-training objectives. Our analysis highlights areas not yet covered in current research, inviting further exploration to enhance the field&rsquo;s understanding. Additionally, we address the challenges in comparing different models, emphasizing the need for standardized data splitting and robust statistical analysis.</p></p class="citation"></blockquote><h3 id=1133--112178-optimizing-convolutional-neural-networks-for-identifying-invasive-pollinator-apis-mellifera-and-finding-a-ligand-drug-to-protect-californias-biodiversity-arnav-swaroop-2024>(11/33 | 112/178) Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity (Arnav Swaroop, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnav Swaroop. (2024)<br><strong>Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity</strong><br><button class=copy-to-clipboard title="Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California's Biodiversity" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM, q-bio-QM<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03870v1.pdf filename=2404.03870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In North America, there are many diverse species of native bees crucial for the environment, who are the primary pollinators of most native floral species. The Californian agriculture industry imports European honeybees (Apis Mellifera) primarily for pollinating almonds. Unfortunately, this has resulted in the unintended consequence of disrupting the native ecosystem and threatening many native bee species as they are outcompeted for food. Our first step for protecting the native species is identification with the use of a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> to differentiate common native bee species from invasive ones. Removing invasive colonies efficiently without harming native species is difficult as pesticides cause myriad diseases in native species. Our approach seeks to prevent the formation of new queens, causing the colony&rsquo;s collapse. Workers secrete royal jelly, a substance that causes fertility and longevity; it is fed to future honeybee queens. Targeting the production of this substance is safe as no native species use it; small organic molecules (ligands) prevent the proteins Apisimin and MRJP1 from combining and producing an oligomer used to form the substance. Ideal ligands bind to only one of these proteins preventing them from joining together: they have a high affinity for one receptor and a significantly lower affinity for the other. We optimized the <b>CNN</b> to provide a framework for creating Machine Learning models that excel at differentiating between subspecies of insects by measuring the effects of image alteration and class grouping on model performance. The <b>CNN</b> is able to achieve an accuracy of 82% in differentiating between invasive and native bee species; 3 ligands have been identified as effective. Our new approach offers a promising solution to curb the spread of invasive bees within California through an identification and neutralization method.</p></p class="citation"></blockquote><h3 id=1233--113178-dynamic-conditional-optimal-transport-through-simulation-free-flows-gavin-kerrigan-et-al-2024>(12/33 | 113/178) Dynamic Conditional Optimal Transport through Simulation-Free Flows (Gavin Kerrigan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gavin Kerrigan, Giosue Migliorini, Padhraic Smyth. (2024)<br><strong>Dynamic Conditional Optimal Transport through Simulation-Free Flows</strong><br><button class=copy-to-clipboard title="Dynamic Conditional Optimal Transport through Simulation-Free Flows" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04240v1.pdf filename=2404.04240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the <b>geometry</b> of conditional optimal transport (COT) and prove a dynamical formulation which generalizes the Benamou-Brenier Theorem. With these tools, we propose a <b>simulation-free</b> flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan. We build on the framework of flow matching to train a conditional generative model by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in the infinite-dimensional setting, making them well suited for inverse problems. Empirically, we demonstrate our proposed method on two image-to-image translation tasks and an infinite-dimensional Bayesian inverse problem.</p></p class="citation"></blockquote><h3 id=1333--114178-model-selection-with-model-zoo-via-graph-learning-ziyu-li-et-al-2024>(13/33 | 114/178) Model Selection with Model Zoo via Graph Learning (Ziyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyu Li, Hilco van der Wilk, Danning Zhan, Megha Khosla, Alessandro Bozzon, Rihan Hai. (2024)<br><strong>Model Selection with Model Zoo via Graph Learning</strong><br><button class=copy-to-clipboard title="Model Selection with Model Zoo via Graph Learning" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03988v1.pdf filename=2404.03988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained deep learning (DL) models are increasingly accessible in public repositories, i.e., model zoos. Given a new prediction task, finding the best model to <b>fine-tune</b> can be computationally intensive and costly, especially when the number of pre-trained models is large. Selecting the right pre-trained models is crucial, yet complicated by the diversity of models from various model families (like ResNet, Vit, Swin) and the hidden relationships between models and datasets. Existing methods, which utilize basic information from models and datasets to compute scores indicating model performance on target datasets, overlook the intrinsic relationships, limiting their effectiveness in model selection. In this study, we introduce TransferGraph, a novel framework that reformulates model selection as a <b>graph</b> learning problem. TransferGraph constructs a <b>graph</b> using extensive metadata extracted from models and datasets, while capturing their inherent relationships. Through comprehensive experiments across 16 real datasets, both images and texts, we demonstrate TransferGraph&rsquo;s effectiveness in capturing essential model-dataset relationships, yielding up to a 32% improvement in correlation between predicted performance and the actual <b>fine-tuning</b> results compared to the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1433--115178-pixel-wise-rl-on-diffusion-models-reinforcement-learning-from-rich-feedback-mo-kordzanganeh-et-al-2024>(14/33 | 115/178) Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback (Mo Kordzanganeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mo Kordzanganeh, Danial Keshvary, Nariman Arian. (2024)<br><strong>Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback</strong><br><button class=copy-to-clipboard title="Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04356v1.pdf filename=2404.04356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Latent <b>diffusion</b> <b>models</b> are the state-of-the-art for synthetic image generation. To align these models with human preferences, training the models using <b>reinforcement</b> <b>learning</b> on human feedback is crucial. Black et. al 2024 introduced denoising <b>diffusion</b> <b>policy</b> optimisation (DDPO), which accounts for the iterative denoising nature of the generation by modelling it as a Markov chain with a final reward. As the reward is a single value that determines the model&rsquo;s performance on the entire image, the model has to navigate a very sparse reward landscape and so requires a large sample count. In this work, we extend the DDPO by presenting the Pixel-wise Policy Optimisation (PXPO) algorithm, which can take feedback for each pixel, providing a more nuanced reward to the model.</p></p class="citation"></blockquote><h3 id=1533--116178-exploring-probabilistic-models-for-semi-supervised-learning-jianfeng-wang-2024>(15/33 | 116/178) Exploring Probabilistic Models for Semi-supervised Learning (Jianfeng Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfeng Wang. (2024)<br><strong>Exploring Probabilistic Models for Semi-supervised Learning</strong><br><button class=copy-to-clipboard title="Exploring Probabilistic Models for Semi-supervised Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04199v1.pdf filename=2404.04199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This thesis studies advanced <b>probabilistic</b> <b>models,</b> including both their theoretical foundations and practical applications, for different <b>semi-supervised</b> <b>learning</b> (SSL) tasks. The proposed <b>probabilistic</b> <b>methods</b> are able to improve the safety of AI systems in real applications by providing reliable uncertainty estimates quickly, and at the same time, achieve competitive performance compared to their deterministic counterparts. The experimental results indicate that the methods proposed in the thesis have great value in safety-critical areas, such as the autonomous driving or medical imaging analysis domain, and pave the way for the future discovery of highly effective and efficient <b>probabilistic</b> <b>approaches</b> in the SSL sector.</p></p class="citation"></blockquote><h3 id=1633--117178-fusing-dictionary-learning-and-support-vector-machines-for-unsupervised-anomaly-detection-paul-irofti-et-al-2024>(16/33 | 117/178) Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection (Paul Irofti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Irofti, Iulian-Andrei Hîji, Andrei Pătraşcu, Nicolae Cleju. (2024)<br><strong>Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04064v1.pdf filename=2404.04064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study in this paper the improvement of one-class support vector machines (OC-SVM) through sparse representation techniques for <b>unsupervised</b> <b>anomaly</b> <b>detection.</b> As Dictionary Learning (DL) became recently a common analysis technique that reveals hidden sparse patterns of data, our approach uses this insight to endow <b>unsupervised</b> detection with more control on pattern finding and dimensions. We introduce a new <b>anomaly</b> <b>detection</b> model that unifies the OC-SVM and DL residual functions into a single composite objective, subsequently solved through K-SVD-type iterative algorithms. A closed-form of the alternating K-SVD iteration is explicitly derived for the new composite model and practical implementable schemes are discussed. The standard DL model is adapted for the Dictionary Pair Learning (DPL) context, where the usual sparsity constraints are naturally eliminated. Finally, we extend both objectives to the more general setting that allows the use of kernel functions. The empirical convergence properties of the resulting algorithms are provided and an in-depth analysis of their parametrization is performed while also demonstrating their numerical performance in comparison with existing methods.</p></p class="citation"></blockquote><h3 id=1733--118178-rolling-the-dice-for-better-deep-learning-performance-a-study-of-randomness-techniques-in-deep-neural-networks-mohammed-ghaith-altarabichi-et-al-2024>(17/33 | 118/178) Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks (Mohammed Ghaith Altarabichi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Ghaith Altarabichi, Sławomir Nowaczyk, Sepideh Pashami, Peyman Sheikholharam Mashhadi, Julia Handl. (2024)<br><strong>Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks</strong><br><button class=copy-to-clipboard title="Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: MNIST, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03992v1.pdf filename=2404.03992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates how various randomization techniques impact Deep Neural Networks (DNNs). Randomization, like weight noise and dropout, aids in reducing overfitting and enhancing generalization, but their interactions are poorly understood. The study categorizes randomness techniques into four types and proposes new methods: adding noise to the loss function and random masking of gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter optimization, it explores optimal configurations across <b>MNIST,</b> FASHION-MNIST, CIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated, revealing <b>data</b> <b>augmentation</b> and weight initialization randomness as main performance contributors. Correlation analysis shows different optimizers prefer distinct randomization types. The complete implementation and dataset are available on GitHub.</p></p class="citation"></blockquote><h3 id=1833--119178-multi-task-learning-for-lung-sound--lung-disease-classification-suma-k-v-et-al-2024>(18/33 | 119/178) Multi-Task Learning for Lung sound & Lung disease classification (Suma K V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suma K V, Deepali Koppad, Preethi Kumar, Neha A Kantikar, Surabhi Ramesh. (2024)<br><strong>Multi-Task Learning for Lung sound & Lung disease classification</strong><br><button class=copy-to-clipboard title="Multi-Task Learning for Lung sound & Lung disease classification" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SD, cs.LG<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03908v1.pdf filename=2404.03908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, advancements in deep learning techniques have considerably enhanced the efficiency and accuracy of medical diagnostics. In this work, a novel approach using multi-task learning (MTL) for the simultaneous classification of lung sounds and lung diseases is proposed. Our proposed model leverages MTL with four different deep learning models such as 2D <b>CNN,</b> ResNet50, MobileNet and Densenet to extract relevant features from the lung sound recordings. The ICBHI 2017 Respiratory Sound Database was employed in the current study. The MTL for MobileNet model performed better than the other models considered, with an accuracy of74% for lung sound analysis and 91% for lung diseases classification. Results of the experimentation demonstrate the efficacy of our approach in classifying both lung sounds and lung diseases concurrently. In this study,using the demographic data of the patients from the database, risk level computation for Chronic Obstructive Pulmonary Disease is also carried out. For this computation, three machine learning algorithms namely <b>Logistic</b> <b>Regression,</b> SVM and Random Forest classifierswere employed. Among these ML algorithms, the Random Forest classifier had the highest accuracy of 92%.This work helps in considerably reducing the physician&rsquo;s burden of not just diagnosing the pathology but also effectively communicating to the patient about the possible causes or outcomes.</p></p class="citation"></blockquote><h3 id=1933--120178-a-proximal-policy-optimization-based-intelligent-home-solar-management-kode-creer-et-al-2024>(19/33 | 120/178) A proximal policy optimization based intelligent home solar management (Kode Creer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kode Creer, Imitiaz Parvez. (2024)<br><strong>A proximal policy optimization based intelligent home solar management</strong><br><button class=copy-to-clipboard title="A proximal policy optimization based intelligent home solar management" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03888v1.pdf filename=2404.03888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the smart grid, the prosumers can sell unused electricity back to the power grid, assuming the prosumers own renewable energy sources and storage units. The maximizing of their profits under a dynamic electricity market is a problem that requires intelligent planning. To address this, we propose a framework based on Proximal Policy Optimization (PPO) using recurrent rewards. By using the information about the rewards modeled effectively with PPO to maximize our objective, we were able to get over 30% improvement over the other naive algorithms in accumulating total profits. This shows promise in getting <b>reinforcement</b> <b>learning</b> algorithms to perform tasks required to plan their actions in complex domains like financial markets. We also introduce a novel method for embedding longs based on soliton waves that outperformed normal embedding in our use case with random floating point <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=2033--121178-heterogeneous-multi-agent-reinforcement-learning-for-zero-shot-scalable-collaboration-xudong-guo-et-al-2024>(20/33 | 121/178) Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration (Xudong Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Guo, Daming Shi, Junjie Yu, Wenhui Fan. (2024)<br><strong>Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration</strong><br><button class=copy-to-clipboard title="Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03869v1.pdf filename=2404.03869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of multi-agent systems, especially the success of multi-agent <b>reinforcement</b> <b>learning</b> (MARL), is reshaping our future across diverse domains like autonomous vehicle networks. However, MARL still faces significant challenges, particularly in achieving <b>zero-shot</b> scalability, which allows trained MARL models to be directly applied to unseen tasks with varying numbers of agents. In addition, real-world multi-agent systems usually contain agents with different functions and strategies, while the existing scalable MARL methods only have limited heterogeneity. To address this, we propose a novel MARL framework named Scalable and Heterogeneous Proximal Policy Optimization (SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL networks. we first leverage a latent network to adaptively learn strategy patterns for each agent. Second, we introduce a heterogeneous layer for decision-making, whose parameters are specifically generated by the learned latent variables. Our approach is scalable as all the parameters are shared except for the heterogeneous layer, and gains both inter-individual and temporal heterogeneity at the same time. We implement our approach based on the state-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is agnostic to the backbone and can be seamlessly plugged into any parameter-shared MARL method. SHPPO exhibits superior performance over the baselines such as MAPPO and HAPPO in classic MARL environments like Starcraft Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing enhanced <b>zero-shot</b> scalability and offering insights into the learned latent representation&rsquo;s impact on team performance by visualization.</p></p class="citation"></blockquote><h3 id=2133--122178-gnnbench-fair-and-productive-benchmarking-for-single-gpu-gnn-system-yidong-gong-et-al-2024>(21/33 | 122/178) GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System (Yidong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yidong Gong, Pradeep Kumar. (2024)<br><strong>GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System</strong><br><button class=copy-to-clipboard title="GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph Neural Network, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04118v1.pdf filename=2404.04118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We hypothesize that the absence of a standardized <b>benchmark</b> has allowed several fundamental pitfalls in <b>GNN</b> System design and evaluation that the community has overlooked. In this work, we propose GNNBench, a plug-and-play <b>benchmarking</b> platform focused on system innovation. GNNBench presents a new protocol to exchange their captive tensor data, supports custom classes in System APIs, and allows automatic integration of the same system module to many deep learning frameworks, such as PyTorch and TensorFlow. To demonstrate the importance of such a <b>benchmark</b> framework, we integrated several <b>GNN</b> systems. Our results show that integration with GNNBench helped us identify several measurement issues that deserve attention from the community.</p></p class="citation"></blockquote><h3 id=2233--123178-growing-q-networks-solving-continuous-control-tasks-with-adaptive-control-resolution-tim-seyde-et-al-2024>(22/33 | 123/178) Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution (Tim Seyde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela Rus. (2024)<br><strong>Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution</strong><br><button class=copy-to-clipboard title="Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04253v1.pdf filename=2404.04253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>reinforcement</b> <b>learning</b> approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control <b>benchmarks.</b> The underlying coarse action space discretizations often yield favourable exploration characteristics while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and energy efficiency, but action costs can be detrimental to exploration during early training. In this work, we aim to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution, taking advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim(A) = 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that yield surprisingly strong performance on continuous control tasks.</p></p class="citation"></blockquote><h3 id=2333--124178-active-causal-learning-for-decoding-chemical-complexities-with-targeted-interventions-zachary-r-fox-et-al-2024>(23/33 | 124/178) Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions (Zachary R. Fox et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zachary R. Fox, Ayana Ghosh. (2024)<br><strong>Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions</strong><br><button class=copy-to-clipboard title="Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, physics-data-an, q-bio-BM<br>Keyword Score: 13<br>Keywords: Graph, Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04224v1.pdf filename=2404.04224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting and enhancing inherent properties based on molecular structures is paramount to design tasks in medicine, materials science, and environmental management. Most of the current machine learning and deep learning approaches have become standard for predictions, but they face challenges when applied across different datasets due to reliance on correlations between molecular representation and target properties. These approaches typically depend on large datasets to capture the diversity within the chemical space, facilitating a more accurate approximation, interpolation, or extrapolation of the chemical behavior of molecules. In our research, we introduce an <b>active</b> <b>learning</b> approach that discerns underlying cause-effect relationships through strategic sampling with the use of a <b>graph</b> loss function. This method identifies the smallest subset of the dataset capable of encoding the most information representative of a much larger chemical space. The identified causal relations are then leveraged to conduct systematic interventions, optimizing the design task within a chemical space that the models have not encountered previously. While our implementation focused on the QM9 quantum-chemical dataset for a specific design task-finding molecules with a large dipole moment-our <b>active</b> <b>causal</b> learning approach, driven by intelligent sampling and interventions, holds potential for broader applications in molecular, materials design and discovery.</p></p class="citation"></blockquote><h3 id=2433--125178-half-space-feature-learning-in-neural-networks-mahesh-lorik-yadav-et-al-2024>(24/33 | 125/178) Half-Space Feature Learning in Neural Networks (Mahesh Lorik Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahesh Lorik Yadav, Harish Guruprasad Ramaswamy, Chandrashekar Lakshminarayanan. (2024)<br><strong>Half-Space Feature Learning in Neural Networks</strong><br><button class=copy-to-clipboard title="Half-Space Feature Learning in Neural Networks" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 13<br>Keywords: Graph Attention Networks, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04312v1.pdf filename=2404.04312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There currently exist two extreme viewpoints for neural network feature learning &ndash; (i) Neural networks simply implement a kernel method (a la NTK) and hence no features are learned (ii) Neural networks can represent (and hence learn) intricate hierarchical features suitable for the data. We argue in this paper neither interpretation is likely to be correct based on a novel viewpoint. Neural networks can be viewed as a mixture of experts, where each expert corresponds to a (number of layers length) path through a sequence of hidden units. We use this alternate interpretation to motivate a model, called the Deep Linearly <b>Gated</b> Network (DLGN), which sits midway between deep linear networks and ReLU networks. Unlike deep linear networks, the DLGN is capable of learning non-linear features (which are then linearly combined), and unlike ReLU networks these features are ultimately simple &ndash; each feature is effectively an indicator function for a region compactly described as an intersection of (number of layers) half-spaces in the input space. This viewpoint allows for a comprehensive global visualization of features, unlike the local visualizations for neurons based on saliency/activation/gradient maps. Feature learning in DLGNs is shown to happen and the mechanism with which this happens is through learning half-spaces in the input space that contain smooth regions of the target function. Due to the structure of DLGNs, the neurons in later layers are fundamentally the same as those in earlier layers &ndash; they all represent a half-space &ndash; however, the dynamics of gradient descent impart a distinct <b>clustering</b> to the later layer neurons. We hypothesize that ReLU networks also have similar feature learning behaviour.</p></p class="citation"></blockquote><h3 id=2533--126178-implicit-bias-of-adamw-ell_infty-norm-constrained-optimization-shuo-xie-et-al-2024>(25/33 | 126/178) Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization (Shuo Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Xie, Zhiyuan Li. (2024)<br><strong>Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization</strong><br><button class=copy-to-clipboard title="Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Karush-Kuhn-Tucker<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04454v1.pdf filename=2404.04454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adam with decoupled weight decay, also known as AdamW, is widely acclaimed for its superior performance in language modeling tasks, surpassing Adam with $\ell_2$ regularization in terms of generalization and optimization. However, this advantage is not theoretically well-understood. One challenge here is that though intuitively Adam with $\ell_2$ regularization optimizes the $\ell_2$ regularized loss, it is not clear if AdamW optimizes a specific objective. In this work, we make progress toward understanding the benefit of AdamW by showing that it implicitly performs constrained optimization. More concretely, we show in the full-batch setting, if AdamW converges with any non-increasing learning rate schedule whose partial sum diverges, it must converge to a <b>KKT</b> point of the original loss under the constraint that the $\ell_\infty$ norm of the parameter is bounded by the inverse of the weight decay factor. This result is built on the observation that Adam can be viewed as a smoothed version of SignGD, which is the normalized steepest descent with respect to $\ell_\infty$ norm, and a surprising connection between normalized steepest descent with weight decay and Frank-Wolfe.</p></p class="citation"></blockquote><h3 id=2633--127178-compositional-estimation-of-lipschitz-constants-for-deep-neural-networks-yuezhu-xu-et-al-2024>(26/33 | 127/178) Compositional Estimation of Lipschitz Constants for Deep Neural Networks (Yuezhu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuezhu Xu, S. Sivaranjani. (2024)<br><strong>Compositional Estimation of Lipschitz Constants for Deep Neural Networks</strong><br><button class=copy-to-clipboard title="Compositional Estimation of Lipschitz Constants for Deep Neural Networks" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04375v1.pdf filename=2404.04375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations and <b>adversarial</b> <b>attacks,</b> as well as the stability and safety of systems with neural network controllers. Therefore, estimation of tight bounds on the Lipschitz constant of neural networks is a well-studied topic. However, typical approaches involve solving a large matrix verification problem, the computational cost of which grows significantly for deeper networks. In this letter, we provide a compositional approach to estimate Lipschitz constants for deep feedforward neural networks by obtaining an exact decomposition of the large matrix verification problem into smaller sub-problems. We further obtain a closed-form solution that applies to most common neural network activation functions, which will enable rapid robustness and stability certificates for neural networks deployed in online control settings. Finally, we demonstrate through numerical experiments that our approach provides a steep reduction in computation time while yielding Lipschitz bounds that are very close to those achieved by state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=2733--128178-generalizable-temperature-nowcasting-with-physics-constrained-rnns-for-predictive-maintenance-of-wind-turbine-components-johannes-exenberger-et-al-2024>(27/33 | 128/178) Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components (Johannes Exenberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes Exenberger, Matteo Di Salvo, Thomas Hirsch, Franz Wotawa, Gerald Schweiger. (2024)<br><strong>Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components</strong><br><button class=copy-to-clipboard title="Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04126v1.pdf filename=2404.04126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning plays an important role in the operation of current wind energy production systems. One central application is predictive maintenance to increase efficiency and lower electricity costs by reducing downtimes. Integrating physics-based knowledge in neural networks to enforce their physical plausibilty is a promising method to improve current approaches, but incomplete system information often impedes their application in real world scenarios. We describe a simple and efficient way for physics-constrained deep learning-based predictive maintenance for wind turbine gearbox bearings with partial system knowledge. The approach is based on temperature nowcasting constrained by physics, where unknown system coefficients are treated as learnable neural network parameters. Results show improved generalization performance to unseen environments compared to a baseline neural network, which is especially important in low data scenarios often encountered in real-world applications.</p></p class="citation"></blockquote><h3 id=2833--129178-continual-learning-with-weight-interpolation-jędrzej-kozal-et-al-2024>(28/33 | 129/178) Continual Learning with Weight Interpolation (Jędrzej Kozal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jędrzej Kozal, Jan Wasilewski, Bartosz Krawczyk, Michał Woźniak. (2024)<br><strong>Continual Learning with Weight Interpolation</strong><br><button class=copy-to-clipboard title="Continual Learning with Weight Interpolation" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04002v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04002v2.pdf filename=2404.04002v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> poses a fundamental challenge for modern machine learning systems, requiring models to adapt to new tasks while retaining knowledge from previous ones. Addressing this challenge necessitates the development of efficient algorithms capable of learning from data streams and accumulating knowledge over time. This paper proposes a novel approach to <b>continual</b> <b>learning</b> utilizing the weight consolidation method. Our method, a simple yet powerful technique, enhances robustness against catastrophic forgetting by interpolating between old and new model weights after each novel task, effectively merging two models to facilitate exploration of local minima emerging after arrival of new concepts. Moreover, we demonstrate that our approach can complement existing rehearsal-based replay approaches, improving their accuracy and further mitigating the forgetting phenomenon. Additionally, our method provides an intuitive mechanism for controlling the stability-plasticity trade-off. Experimental results showcase the significant performance enhancement to state-of-the-art experience replay algorithms the proposed weight consolidation approach offers. Our algorithm can be downloaded from <a href=https://github.com/jedrzejkozal/weight-interpolation-cl>https://github.com/jedrzejkozal/weight-interpolation-cl</a>.</p></p class="citation"></blockquote><h3 id=2933--130178-demonstration-guided-multi-objective-reinforcement-learning-junlin-lu-et-al-2024>(29/33 | 130/178) Demonstration Guided Multi-Objective Reinforcement Learning (Junlin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Lu, Patrick Mannion, Karl Mason. (2024)<br><strong>Demonstration Guided Multi-Objective Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Demonstration Guided Multi-Objective Reinforcement Learning" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03997v1.pdf filename=2404.03997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-objective <b>reinforcement</b> <b>learning</b> (MORL) is increasingly relevant due to its resemblance to real-world scenarios requiring trade-offs between multiple objectives. Catering to diverse user preferences, traditional <b>reinforcement</b> <b>learning</b> faces amplified challenges in MORL. To address the difficulty of training policies from scratch in MORL, we introduce demonstration-guided multi-objective <b>reinforcement</b> <b>learning</b> (DG-MORL). This novel approach utilizes prior demonstrations, aligns them with user preferences via corner weight support, and incorporates a self-evolving mechanism to refine suboptimal demonstrations. Our empirical studies demonstrate DG-MORL&rsquo;s superiority over existing MORL algorithms, establishing its robustness and efficacy, particularly under challenging conditions. We also provide an upper bound of the algorithm&rsquo;s sample complexity.</p></p class="citation"></blockquote><h3 id=3033--131178-generating-synthetic-ground-truth-distributions-for-multi-step-trajectory-prediction-using-probabilistic-composite-bézier-curves-ronny-hug-et-al-2024>(30/33 | 131/178) Generating Synthetic Ground Truth Distributions for Multi-step Trajectory Prediction using Probabilistic Composite Bézier Curves (Ronny Hug et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ronny Hug, Stefan Becker, Wolfgang Hübner, Michael Arens. (2024)<br><strong>Generating Synthetic Ground Truth Distributions for Multi-step Trajectory Prediction using Probabilistic Composite Bézier Curves</strong><br><button class=copy-to-clipboard title="Generating Synthetic Ground Truth Distributions for Multi-step Trajectory Prediction using Probabilistic Composite Bézier Curves" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04397v1.pdf filename=2404.04397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An appropriate data basis grants one of the most important aspects for training and evaluating probabilistic trajectory prediction models based on neural networks. In this regard, a common shortcoming of current <b>benchmark</b> datasets is their limitation to sets of sample trajectories and a lack of actual ground truth distributions, which prevents the use of more expressive error metrics, such as the Wasserstein distance for model evaluation. Towards this end, this paper proposes a novel approach to synthetic dataset generation based on composite probabilistic B'ezier curves, which is capable of generating ground truth data in terms of probability distributions over full trajectories. This allows the calculation of arbitrary posterior distributions. The paper showcases an exemplary trajectory prediction model evaluation using generated ground truth distribution data.</p></p class="citation"></blockquote><h3 id=3133--132178-hierarchical-neural-additive-models-for-interpretable-demand-forecasts-leif-feddersen-et-al-2024>(31/33 | 132/178) Hierarchical Neural Additive Models for Interpretable Demand Forecasts (Leif Feddersen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leif Feddersen, Catherine Cleophas. (2024)<br><strong>Hierarchical Neural Additive Models for Interpretable Demand Forecasts</strong><br><button class=copy-to-clipboard title="Hierarchical Neural Additive Models for Interpretable Demand Forecasts" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04070v1.pdf filename=2404.04070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Demand forecasts are the crucial basis for numerous business decisions, ranging from inventory management to strategic facility planning. While machine learning (ML) approaches offer accuracy gains, their interpretability and acceptance are notoriously lacking. Addressing this dilemma, we introduce Hierarchical Neural Additive Models for time series (HNAM). HNAM expands upon Neural Additive Models (NAM) by introducing a time-series specific additive model with a level and interacting covariate components. Covariate interactions are only allowed according to a user-specified interaction hierarchy. For example, weekday effects may be estimated independently of other covariates, whereas a holiday effect may depend on the weekday and an additional promotion may depend on both former covariates that are lower in the interaction hierarchy. Thereby, HNAM yields an intuitive forecasting interface in which analysts can observe the contribution for each known covariate. We evaluate the proposed approach and <b>benchmark</b> its performance against other state-of-the-art machine learning and statistical models extensively on real-world retail data. The results reveal that HNAM offers competitive prediction performance whilst providing plausible explanations.</p></p class="citation"></blockquote><h3 id=3233--133178-derivative-free-tree-optimization-for-complex-systems-ye-wei-et-al-2024>(32/33 | 133/178) Derivative-free tree optimization for complex systems (Ye Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Wei, Bo Peng, Ruiwen Xie, Yangtao Chen, Yu Qin, Peng Wen, Stefan Bauer, Po-Yen Tung. (2024)<br><strong>Derivative-free tree optimization for complex systems</strong><br><button class=copy-to-clipboard title="Derivative-free tree optimization for complex systems" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04062v1.pdf filename=2404.04062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A tremendous range of design tasks in materials, physics, and biology can be formulated as finding the optimum of an objective function depending on many parameters without knowing its closed-form expression or the derivative. Traditional derivative-free optimization techniques often rely on strong assumptions about objective functions, thereby failing at optimizing non-convex systems beyond 100 dimensions. Here, we present a tree search method for derivative-free optimization that enables accelerated optimal design of high-dimensional complex systems. Specifically, we introduce stochastic tree expansion, dynamic upper confidence bound, and short-range backpropagation mechanism to evade local optimum, iteratively approximating the global optimum using machine learning models. This development effectively confronts the dimensionally challenging problems, achieving convergence to global optima across various <b>benchmark</b> functions up to 2,000 dimensions, surpassing the existing methods by 10- to 20-fold. Our method demonstrates wide applicability to a wide range of real-world complex systems spanning materials, physics, and biology, considerably outperforming state-of-the-art algorithms. This enables efficient autonomous knowledge discovery and facilitates self-driving virtual laboratories. Although we focus on problems within the realm of natural science, the advancements in optimization techniques achieved herein are applicable to a broader spectrum of challenges across all quantitative disciplines.</p></p class="citation"></blockquote><h3 id=3333--134178-approximate-umap-allows-for-high-rate-online-visualization-of-high-dimensional-data-streams-peter-wassenaar-et-al-2024>(33/33 | 134/178) Approximate UMAP allows for high-rate online visualization of high-dimensional data streams (Peter Wassenaar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Wassenaar, Pierre Guetschel, Michael Tangermann. (2024)<br><strong>Approximate UMAP allows for high-rate online visualization of high-dimensional data streams</strong><br><button class=copy-to-clipboard title="Approximate UMAP allows for high-rate online visualization of high-dimensional data streams" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-5-3; I-5-3; J-4, cs-AI, cs-HC, cs-LG, cs.LG, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04001v1.pdf filename=2404.04001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the BCI field, introspection and interpretation of brain signals are desired for providing feedback or to guide rapid paradigm prototyping but are challenging due to the high noise level and dimensionality of the signals. Deep neural networks are often introspected by transforming their learned feature representations into 2- or 3-dimensional subspace visualizations using projection algorithms like Uniform Manifold Approximation and Projection (UMAP). Unfortunately, these methods are computationally expensive, making the projection of data streams in real-time a non-trivial task. In this study, we introduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at generating rapid projections for real-time introspection. To study its suitability for real-time projecting, we <b>benchmark</b> the methods against standard UMAP and its neural network counterpart parametric UMAP. Our results show that approximate UMAP delivers projections that replicate the projection space of standard UMAP while decreasing projection speed by an order of magnitude and maintaining the same training time.</p></p class="citation"></blockquote><h2 id=statml-4>stat.ML (4)</h2><h3 id=14--135178-deeplink-t-deep-learning-inference-for-time-series-data-using-knockoffs-and-lstm-wenxuan-zuo-et-al-2024>(1/4 | 135/178) DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM (Wenxuan Zuo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxuan Zuo, Zifan Zhu, Yuxuan Du, Yi-Chun Yeh, Jed A. Fuhrman, Jinchi Lv, Yingying Fan, Fengzhu Sun. (2024)<br><strong>DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM</strong><br><button class=copy-to-clipboard title="DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, q-bio-QM, stat-ML, stat.ML<br>Keyword Score: 60<br>Keywords: Autoencoder, Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04317v1.pdf filename=2404.04317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional longitudinal time series data is prevalent across various real-world applications. Many such applications can be modeled as regression problems with high-dimensional time series covariates. Deep learning has been a popular and powerful tool for fitting these regression models. Yet, the development of interpretable and reproducible deep-learning models is challenging and remains underexplored. This study introduces a novel method, Deep Learning Inference using Knockoffs for Time series data (DeepLINK-T), focusing on the selection of significant time series variables in regression while controlling the false discovery rate (FDR) at a predetermined level. DeepLINK-T combines deep learning with knockoff inference to control FDR in feature selection for time series models, accommodating a wide variety of feature distributions. It addresses dependencies across time and features by leveraging a time-varying latent factor structure in time series covariates. Three key ingredients for DeepLINK-T are 1) a <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> <b>autoencoder</b> for generating time series knockoff variables, 2) an <b>LSTM</b> prediction network using both original and knockoff variables, and 3) the application of the knockoffs framework for variable selection with FDR control. Extensive <b>simulation</b> studies have been conducted to evaluate DeepLINK-T&rsquo;s performance, showing its capability to control FDR effectively while demonstrating superior feature selection power for high-dimensional longitudinal time series data compared to its non-time series counterpart. DeepLINK-T is further applied to three metagenomic data sets, validating its practical utility and effectiveness, and underscoring its potential in real-world applications.</p></p class="citation"></blockquote><h3 id=24--136178-longitudinal-targeted-minimum-loss-based-estimation-with-temporal-difference-heterogeneous-transformer-toru-shirakawa-et-al-2024>(2/4 | 136/178) Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer (Toru Shirakawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao, Hiroyasu Iso, Mark van der Laan. (2024)<br><strong>Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer</strong><br><button class=copy-to-clipboard title="Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-AP, stat-ME, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Counter-factual, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04399v1.pdf filename=2404.04399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the <b>counterfactual</b> mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a <b>transformer</b> architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the <b>transformer,</b> following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. <b>Simulation</b> results demonstrate our method&rsquo;s superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate <b>counterfactual</b> mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.</p></p class="citation"></blockquote><h3 id=34--137178-nonparametric-modern-hopfield-models-jerry-yao-chieh-hu-et-al-2024>(3/4 | 137/178) Nonparametric Modern Hopfield Models (Jerry Yao-Chieh Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, Han Liu. (2024)<br><strong>Nonparametric Modern Hopfield Models</strong><br><button class=copy-to-clipboard title="Nonparametric Modern Hopfield Models" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, cs-NE, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Hopfield Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03900v1.pdf filename=2404.03900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a nonparametric construction for deep learning compatible modern <b>Hopfield</b> <b>models</b> and utilize this framework to debut an efficient variant. Our key contribution stems from interpreting the memory storage and retrieval processes in modern <b>Hopfield</b> <b>models</b> as a nonparametric regression problem subject to a set of query-memory pairs. Crucially, our framework not only recovers the known results from the original dense modern <b>Hopfield</b> <b>model</b> but also fills the void in the literature regarding efficient modern <b>Hopfield</b> <b>models,</b> by introducing \textit{sparse-structured} modern <b>Hopfield</b> <b>models</b> with sub-quadratic complexity. We establish that this sparse model inherits the appealing theoretical properties of its dense analogue &ndash; connection with <b>transformer</b> attention, fixed point convergence and exponential memory capacity &ndash; even without knowing details of the <b>Hopfield</b> <b>energy</b> function. Additionally, we showcase the versatility of our framework by constructing a family of modern <b>Hopfield</b> <b>models</b> as extensions, including linear, random masked, top-$K$ and positive random feature modern <b>Hopfield</b> <b>models.</b> Empirically, we validate the efficacy of our framework in both synthetic and realistic settings.</p></p class="citation"></blockquote><h3 id=44--138178-bayesian-additive-regression-networks-danielle-van-boxel-2024>(4/4 | 138/178) Bayesian Additive Regression Networks (Danielle Van Boxel, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danielle Van Boxel. (2024)<br><strong>Bayesian Additive Regression Networks</strong><br><button class=copy-to-clipboard title="Bayesian Additive Regression Networks" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Benchmarking, BART<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04425v1.pdf filename=2404.04425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We apply Bayesian Additive Regression Tree <b>(BART)</b> principles to training an ensemble of small neural networks for regression tasks. Using Markov Chain Monte Carlo, we sample from the posterior distribution of neural networks that have a single hidden layer. To create an ensemble of these, we apply Gibbs sampling to update each network against the residual target value (i.e. subtracting the effect of the other networks). We demonstrate the effectiveness of this technique on several <b>benchmark</b> regression problems, comparing it to equivalent shallow neural networks, <b>BART,</b> and ordinary least squares. Our Bayesian Additive Regression Networks (BARN) provide more consistent and often more accurate results. On test data <b>benchmarks,</b> BARN averaged between 5 to 20 percent lower root mean square error. This error performance does come at the cost, however, of greater computation time. BARN sometimes takes on the order of a minute where competing methods take a second or less. But, BARN without cross-validated hyperparameter tuning takes about the same amount of computation time as tuned other methods. Yet BARN is still typically more accurate.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--139178-it-is-okay-to-be-uncommon-quantizing-sound-event-detection-networks-on-hardware-accelerators-with-uncommon-sub-byte-support-yushu-wu-et-al-2024>(1/2 | 139/178) &lsquo;It is okay to be uncommon&rsquo;: Quantizing Sound Event Detection Networks on Hardware Accelerators with Uncommon Sub-Byte Support (Yushu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushu Wu, Xiao Quan, Mohammad Rasool Izadi, Chuan-Che Huang. (2024)<br><strong>&lsquo;It is okay to be uncommon&rsquo;: Quantizing Sound Event Detection Networks on Hardware Accelerators with Uncommon Sub-Byte Support</strong><br><button class=copy-to-clipboard title="'It is okay to be uncommon': Quantizing Sound Event Detection Networks on Hardware Accelerators with Uncommon Sub-Byte Support" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Quantization, Quantization, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04386v1.pdf filename=2404.04386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>If our noise-canceling headphones can understand our audio environments, they can then inform us of important sound <b>events,</b> <b>tune</b> equalization based on the types of content we listen to, and dynamically adjust noise cancellation parameters based on audio scenes to further reduce distraction. However, running multiple audio understanding models on headphones with a limited energy budget and on-chip memory remains a challenging task. In this work, we identify a new class of neural network accelerators (e.g., NE16 on GAP9) that allows network weights to be <b>quantized</b> to different common (e.g., 8 bits) and uncommon bit-widths (e.g., 3 bits). We then applied a differentiable neural architecture search to search over the optimal bit-widths of a network on two different sound <b>event</b> <b>detection</b> tasks with potentially different requirements on <b>quantization</b> and prediction granularity (i.e., classification vs. embeddings for <b>few-shot</b> <b>learning).</b> We further evaluated our <b>quantized</b> models on actual hardware, showing that we reduce memory usage, inference latency, and energy consumption by an average of 62%, 46%, and 61% respectively compared to 8-bit models while maintaining floating point performance. Our work sheds light on the benefits of such accelerators on sound <b>event</b> <b>detection</b> tasks when combined with an appropriate search method.</p></p class="citation"></blockquote><h3 id=22--140178-the-nes-video-music-database-a-dataset-of-symbolic-video-game-music-paired-with-gameplay-videos-igor-cardoso-et-al-2024>(2/2 | 140/178) The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos (Igor Cardoso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Igor Cardoso, Rubens O. Moraes, Lucas N. Ferreira. (2024)<br><strong>The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos</strong><br><button class=copy-to-clipboard title="The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04420v1.pdf filename=2404.04420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural models are one of the most popular approaches for music generation, yet there aren&rsquo;t standard large datasets tailored for learning music directly from game data. To address this research gap, we introduce a novel dataset named NES-VMDB, containing 98,940 gameplay videos from 389 NES games, each paired with its original soundtrack in symbolic format (MIDI). NES-VMDB is built upon the Nintendo Entertainment System Music Database (NES-MDB), encompassing 5,278 music pieces from 397 NES games. Our approach involves collecting long-play videos for 389 games of the original dataset, slicing them into 15-second-long clips, and extracting the audio from each clip. Subsequently, we apply an audio fingerprinting algorithm (similar to Shazam) to automatically identify the corresponding piece in the NES-MDB dataset. Additionally, we introduce a baseline method based on the Controllable Music <b>Transformer</b> to generate NES music conditioned on gameplay clips. We evaluated this approach with objective metrics, and the results showed that the conditional CMT improves musical structural quality when compared to its unconditional counterpart. Moreover, we used a neural classifier to predict the game genre of the generated pieces. Results showed that the CMT generator can learn correlations between gameplay videos and game genres, but further research has to be conducted to achieve human-level performance.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--141178-open-vocabulary-keyword-spotting-through-transfer-learning-from-speech-synthesis-kesavaraj-v-et-al-2024>(1/4 | 141/178) Open vocabulary keyword spotting through transfer learning from speech synthesis (Kesavaraj V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kesavaraj V, Anil Kumar Vuppala. (2024)<br><strong>Open vocabulary keyword spotting through transfer learning from speech synthesis</strong><br><button class=copy-to-clipboard title="Open vocabulary keyword spotting through transfer learning from speech synthesis" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SD, cs.HC, eess-AS<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Transfer Learning, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03914v1.pdf filename=2404.03914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying keywords in an open-vocabulary context is crucial for personalizing interactions with smart devices. Previous approaches to open vocabulary keyword spotting dependon a shared embedding space created by audio and text encoders. However, these approaches suffer from heterogeneous modality representations (i.e., audio-text mismatch). To address this issue, our proposed framework leverages <b>knowledge</b> <b>acquired</b> from a pre-trained <b>text-to-speech</b> <b>(TTS)</b> system. This <b>knowledge</b> <b>transfer</b> <b>allows</b> for the incorporation of awareness of audio projections into the text representations derived from the text encoder. The performance of the proposed approach is compared with various baseline methods across four different datasets. The robustness of our proposed model is evaluated by assessing its performance across different word lengths and in an Out-of-Vocabulary (OOV) scenario. Additionally, the effectiveness of <b>transfer</b> <b>learning</b> from the <b>TTS</b> system is investigated by analyzing its different intermediate representations. The experimental results indicate that, in the challenging LibriPhrase Hard dataset, the proposed approach outperformed the cross-modality correspondence detector (CMCD) method by a significant improvement of 8.22% in area under the curve (AUC) and 12.56% in equal error rate (EER).</p></p class="citation"></blockquote><h3 id=24--142178-which-experimental-design-is-better-suited-for-vqa-tasks-eye-tracking-study-on-cognitive-load-performance-and-gaze-allocations-sita-a-vriend-et-al-2024>(2/4 | 142/178) Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations (Sita A. Vriend et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sita A. Vriend, Sandeep Vidyapu, Amer Rama, Kun-Ting Chen, Daniel Weiskopf. (2024)<br><strong>Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations</strong><br><button class=copy-to-clipboard title="Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04036v1.pdf filename=2404.04036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conducted an eye-tracking user study with 13 participants to investigate the influence of stimulus-question ordering and <b>question</b> <b>modality</b> on participants using <b>visual</b> <b>question-answering</b> <b>(VQA)</b> tasks. We examined cognitive load, task performance, and gaze allocations across five distinct experimental designs, aiming to identify setups that minimize the cognitive burden on participants. The collected performance and gaze data were analyzed using quantitative and qualitative methods. Our results indicate a significant impact of stimulus-question ordering on cognitive load and task performance, as well as a noteworthy effect of <b>question</b> <b>modality</b> on task performance. These findings offer insights for the experimental design of controlled user studies in visualization research.</p></p class="citation"></blockquote><h3 id=34--143178-hiv-client-perspectives-on-digital-health-in-malawi-lisa-orii-et-al-2024>(3/4 | 143/178) HIV Client Perspectives on Digital Health in Malawi (Lisa Orii et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Orii, Caryl Feldacker, Jacqueline Madalitso Huwa, Agness Thawani, Evelyn Viola, Christine Kiruthu-Kamamia, Odala Sande, Hannock Tweya, Richard Anderson. (2024)<br><strong>HIV Client Perspectives on Digital Health in Malawi</strong><br><button class=copy-to-clipboard title="HIV Client Perspectives on Digital Health in Malawi" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Recommendation, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04444v1.pdf filename=2404.04444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>eHealth has strong potential to advance HIV care in low- and middle-income countries. Given the sensitivity of HIV-related information and the risks associated with unintended HIV status disclosure, clients&rsquo; privacy perceptions towards eHealth applications should be examined to develop client-centered technologies. Through focus group discussions with antiretroviral therapy (ART) clients from Lighthouse Trust, Malawi&rsquo;s public HIV care program, we explored perceptions of data <b>security</b> and privacy, including their understanding of data flow and their concerns about data confidentiality across several layers of data use. Our findings highlight the broad privacy concerns that affect ART clients&rsquo; day-to-day choices, clients&rsquo; trust in Malawi&rsquo;s health system, and their acceptance of, and familiarity with, point-of-care technologies used in HIV care. Based on our findings, we provide <b>recommendations</b> for building robust digital health systems in low- and middle-income countries with limited resources, nascent privacy regulations, and political will to take action to protect client data.</p></p class="citation"></blockquote><h3 id=44--144178-effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction-hyunyoung-jang-et-al-2024>(4/4 | 144/178) Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction (Hyunyoung Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunyoung Jang, Jinwook Kim, Jeongmi Lee. (2024)<br><strong>Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction</strong><br><button class=copy-to-clipboard title="Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Virtual Reality (VR), Virtual Reality (VR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03899v1.pdf filename=2404.03899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retargeting methods that modify the visual representation of real movements have been widely used to expand the interaction space and create engaging <b>virtual</b> <b>reality</b> experiences. For optimal user experience and performance, it is essential to specify the perception of retargeting and utilize the appropriate range of modification parameters. However, previous studies mostly concentrated on whether users perceived the target sense or not and rarely examined the perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how the perception and performance in hand-retargeted interactions are influenced by multisensory feedback. In this study, we used rigorous psychophysical methods to specify users&rsquo; perceptual accuracy and sensitivity to hand-retargeting and provide acceptable ranges of retargeting parameters. We also presented different multisensory feedback simultaneously with the retargeting to probe its effect on users&rsquo; perception and task performance. The experimental results showed that providing continuous multisensory feedback, proportionate to the distance between the <b>virtual</b> <b>hand</b> and the targeted destination, heightened the accuracy of users&rsquo; perception of hand retargeting without altering their perceptual sensitivity. Furthermore, the utilization of multisensory feedback considerably improved the precision of task performance, particularly at lower gain factors. Based on these findings, we propose design guidelines and potential applications of <b>VR</b> hand-retargeted interactions and multisensory feedback for optimal user experience and performance.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--145178-superior-genetic-algorithms-for-the-target-set-selection-problem-based-on-power-law-parameter-choices-and-simple-greedy-heuristics-benjamin-doerr-et-al-2024>(1/2 | 145/178) Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics (Benjamin Doerr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Doerr, Martin S. Krejca, Nguyen Vu. (2024)<br><strong>Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics</strong><br><button class=copy-to-clipboard title="Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 36<br>Keywords: Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04018v1.pdf filename=2404.04018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The target set selection problem (TSS) asks for a set of vertices such that an influence spreading process started in these vertices reaches the whole <b>graph.</b> <b>The</b> <b>current</b> state of the art for this NP-hard problem are three recently proposed randomized search heuristics, namely a biased random-key genetic algorithm (BRKGA) obtained from extensive parameter tuning, a max-min ant system (MMAS), and a MMAS using Q-learning with a <b>graph</b> <b>convolutional</b> <b>network.</b> We show that the BRKGA with two simple modifications and without the costly parameter tuning obtains significantly better results. Our first modification is to simply choose all parameters of the BRKGA in each iteration randomly from a power-law distribution. The resulting parameterless BRKGA is already competitive with the tuned BRKGA, as our experiments on the previously used <b>benchmarks</b> show. We then add a natural greedy heuristic, namely to repeatedly discard small-degree vertices that are not necessary for reaching the whole <b>graph.</b> <b>The</b> <b>resulting</b> algorithm consistently outperforms all of the state-of-the-art algorithms. Besides providing a superior algorithm for the TSS problem, this work shows that randomized parameter choices and elementary greedy heuristics can give better results than complex algorithms and costly parameter tuning.</p></p class="citation"></blockquote><h3 id=22--146178-mining-potentially-explanatory-patterns-via-partial-solutions-giancarlo-catalano-et-al-2024>(2/2 | 146/178) Mining Potentially Explanatory Patterns via Partial Solutions (GianCarlo Catalano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>GianCarlo Catalano, Alexander E. I. Brownlee, David Cairns, John McCall, Russell Ainslie. (2024)<br><strong>Mining Potentially Explanatory Patterns via Partial Solutions</strong><br><button class=copy-to-clipboard title="Mining Potentially Explanatory Patterns via Partial Solutions" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: I-2-8, cs-LG, cs-NE, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04388v1.pdf filename=2404.04388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Genetic Algorithms have established their capability for solving many complex optimization problems. Even as good solutions are produced, the user&rsquo;s understanding of a problem is not necessarily improved, which can lead to a lack of confidence in the results. To mitigate this issue, explainability aims to give insight to the user by presenting them with the knowledge obtained by the algorithm. In this paper we introduce Partial Solutions in order to improve the explainability of solutions to combinatorial optimization problems. Partial Solutions represent beneficial traits found by analyzing a population, and are presented to the user for explainability, but also provide an explicit model from which new solutions can be generated. We present an algorithm that assembles a collection of Partial Solutions chosen to strike a balance between high fitness, simplicity and atomicity. Experiments with standard <b>benchmarks</b> show that the proposed algorithm is able to find Partial Solutions which improve explainability at reasonable computational cost without affecting search performance.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--147178-stable-blockchain-sharding-under-adversarial-transaction-generation-ramesh-adhikari-et-al-2024>(1/2 | 147/178) Stable Blockchain Sharding under Adversarial Transaction Generation (Ramesh Adhikari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramesh Adhikari, Costas Busch, Dariusz Kowalski. (2024)<br><strong>Stable Blockchain Sharding under Adversarial Transaction Generation</strong><br><button class=copy-to-clipboard title="Stable Blockchain Sharding under Adversarial Transaction Generation" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 33<br>Keywords: Clustering, Hierarchical Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04438v1.pdf filename=2404.04438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sharding is used to improve the scalability and performance of blockchain systems. We investigate the stability of blockchain sharding, where transactions are continuously generated by an adversarial model. The system consists of $n$ processing nodes that are divided into $s$ shards. Following the paradigm of classical adversarial queuing theory, transactions are continuously received at injection rate $\rho \leq 1$ and burstiness $b > 0$. We give an absolute upper bound $\max{ \frac{2}{k+1}, \frac{2}{ \left\lfloor\sqrt{2s}\right\rfloor}}$ on the maximum injection rate for which any scheduler could guarantee bounded queues and latency of transactions, where $k$ is the number of shards that each transaction accesses. We next give a basic distributed scheduling algorithm for uniform systems where shards are equally close to each other. To guarantee stability, the injection rate is limited to $\rho \leq \max{ \frac{1}{18k}, \frac{1}{\lceil 18 \sqrt{s} \rceil} }$. We then provide a fully distributed scheduling algorithm for non-uniform systems where shards are arbitrarily far from each other. By using a <b>hierarchical</b> <b>clustering</b> of the shards, stability is guaranteed with injection rate $\rho \leq \frac{1}{c_1d \log^2 s} \cdot \max{ \frac{1}{k}, \frac{1}{\sqrt{s}} }$, where $d$ is the worst distance of any transaction to the shards it will access, and $c_1$ is some positive constant. We also conduct <b>simulations</b> to evaluate the algorithms and measure the average queue sizes and latency throughout the system. To our knowledge, this is the first adversarial stability analysis of sharded blockchain systems.</p></p class="citation"></blockquote><h3 id=22--148178-evaluation-of-programming-models-and-performance-for-stencil-computation-on-current-gpu-architectures-baodi-shan-et-al-2024>(2/2 | 148/178) Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures (Baodi Shan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baodi Shan, Mauricio Araya-Polo. (2024)<br><strong>Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures</strong><br><button class=copy-to-clipboard title="Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04441v1.pdf filename=2404.04441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accelerated computing is widely used in high-performance computing. Therefore, it is crucial to experiment and discover how to better utilize GPUGPUs latest generations on relevant applications. In this paper, we present results and share insights about highly tuned stencil-based kernels for NVIDIA Ampere (A100) and Hopper (GH200) architectures. Performance results yield useful insights into the behavior of this type of algorithms for these new accelerators. This knowledge can be leveraged by many scientific applications which involve stencils computations. Further, evaluation of three different programming models: CUDA, OpenACC, and OpenMP target offloading is conducted on aforementioned accelerators. We extensively study the performance and portability of various kernels under each programming model and provide corresponding optimization <b>recommendations.</b> Furthermore, we compare the performance of different programming models on the mentioned architectures. Up to 58% performance improvement was achieved against the previous GPGPU&rsquo;s architecture generation for an highly optimized kernel of the same class, and up to 42% for all classes. In terms of programming models, and keeping portability in mind, optimized OpenACC implementation outperforms OpenMP implementation by 33%. If portability is not a factor, our best tuned CUDA implementation outperforms the optimized OpenACC one by 2.1x.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--149178-nonlinear-kalman-filtering-based-on-self-attention-mechanism-and-lattice-trajectory-piecewise-linear-approximation-jiaming-wang-et-al-2024>(1/5 | 149/178) Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation (Jiaming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaming Wang, Xinyu Geng, Jun Xu. (2024)<br><strong>Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation</strong><br><button class=copy-to-clipboard title="Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03915v1.pdf filename=2404.03915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The traditional Kalman filter (KF) is widely applied in control systems, but it relies heavily on the accuracy of the system model and noise parameters, leading to potential performance degradation when facing inaccuracies. To address this issue, introducing neural networks into the KF framework offers a data-driven solution to compensate for these inaccuracies, improving the filter&rsquo;s performance while maintaining interpretability. Nevertheless, existing studies mostly employ <b>recurrent</b> <b>neural</b> <b>network</b> <b>(RNN),</b> which fails to fully capture the dependencies among state sequences and lead to an unstable training process. In this paper, we propose a novel Kalman filtering algorithm named the attention Kalman filter (AtKF), which incorporates a <b>self-attention</b> network to capture the dependencies among state sequences. To address the instability in the recursive training process, a parallel pre-training strategy is devised. Specifically, this strategy involves piecewise linearizing the system via lattice trajectory piecewise linear (LTPWL) expression, and generating pre-training data through a batch estimation algorithm, which exploits the <b>self-attention</b> mechanism&rsquo;s parallel processing ability. Experimental results on a two-dimensional nonlinear system demonstrate that AtKF outperforms other filters under noise disturbances and model mismatches.</p></p class="citation"></blockquote><h3 id=25--150178-torque-minimizing-control-allocation-for-overactuated-quadrupedal-locomotion-mads-erlend-bøe-lysø-et-al-2024>(2/5 | 150/178) Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion (Mads Erlend Bøe Lysø et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mads Erlend Bøe Lysø, Esten Ingar Grøtli, Kristin Ytterstad Pettersen. (2024)<br><strong>Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion</strong><br><button class=copy-to-clipboard title="Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04156v1.pdf filename=2404.04156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we improve upon a method for optimal control of quadrupedal robots which utilizes a full-order model of the system. The original method utilizes offline nonlinear optimal control to synthesize a control scheme which exponentially orbitally stabilizes the closed-loop system. However, it is not able to handle the overactuated phases which frequently occur during quadrupedal locomotion as a result of the multi-contact nature of the system. We propose a modified method, which handles overactuated gait phases in a way that utilizes the full range of available actuators to minimize torque expenditure without requiring output trajectories to be modified. It is shown that the system under the proposed controller exhibits the same properties, i.e. exponential orbital stability, with the same or lower point-wise torque magnitude. A <b>simulation</b> study demonstrates that the reduction in torque may in certain cases be substantial.</p></p class="citation"></blockquote><h3 id=35--151178-optimal-policy-synthesis-from-a-sequence-of-goal-sets-with-an-application-to-electric-distribution-system-restoration-ilker-işık-et-al-2024>(3/5 | 151/178) Optimal Policy Synthesis from A Sequence of Goal Sets with An Application to Electric Distribution System Restoration (İlker Işık et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>İlker Işık, Onur Yigit Arpali, Ebru Aydin Gol. (2024)<br><strong>Optimal Policy Synthesis from A Sequence of Goal Sets with An Application to Electric Distribution System Restoration</strong><br><button class=copy-to-clipboard title="Optimal Policy Synthesis from A Sequence of Goal Sets with An Application to Electric Distribution System Restoration" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04338v1.pdf filename=2404.04338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the post-disaster distribution system restoration problem, in this paper, we study the problem of synthesizing the optimal policy for a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) from a sequence of goal sets. For each goal set, our aim is to both maximize the probability to reach and minimize the expected time to reach the goal set. The order of the goal sets represents their priority. In particular, our aim is to generate a policy that is optimal with respect to the first goal set, and it is optimal with respect to the second goal set among the policies that are optimal with respect to the first goal set and so on. To synthesize such a policy, we iteratively filter the applicable actions according to the goal sets. We illustrate the developed method over sample distribution systems and disaster scenarios.</p></p class="citation"></blockquote><h3 id=45--152178-field-teams-coordination-for-earthquake-damaged-distribution-system-energization-ilker-işık-et-al-2024>(4/5 | 152/178) Field Teams Coordination for Earthquake-Damaged Distribution System Energization (İlker Işık et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>İlker Işık, Ebru Aydin Gol. (2024)<br><strong>Field Teams Coordination for Earthquake-Damaged Distribution System Energization</strong><br><button class=copy-to-clipboard title="Field Teams Coordination for Earthquake-Damaged Distribution System Energization" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04087v1.pdf filename=2404.04087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The re-energization of electrical distribution systems in a post-disaster scenario is of grave importance as most modern infrastructure systems rely heavily on the presence of electricity. This paper introduces a method to coordinate the field teams for the optimal energization of an electrical distribution system after an earthquake-induced blackout. The proposed method utilizes a <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP) to create an optimal energization strategy, which aims to minimize the expected time to energize each distribution system component. The travel duration of each team and the possible outcomes of the energization attempts are considered in the state transitions. The failure probabilities of the system components are computed using the fragility curves of structures and the Peak Ground Acceleration (PGA) values which are encoded to the MDP model via transition probabilities. Furthermore, the proposed solution offers several methods to determine the non-optimal actions during the construction of the MDP and eliminate them in order to improve the run-time performance without sacrificing the optimality of the solution.</p></p class="citation"></blockquote><h3 id=55--153178-queue-aware-network-control-algorithm-with-a-high-quantum-computing-readiness-evaluated-in-discrete-time-flow-simulator-for-fat-pipe-networks-arthur-witt-2024>(5/5 | 153/178) Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks (Arthur Witt, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Witt. (2024)<br><strong>Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks</strong><br><button class=copy-to-clipboard title="Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 90C11, 94-10, 81P68, I-6; C-2-3; C-2-5, cs-ET, cs-SY, eess-SY, eess.SY, quant-ph<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04080v1.pdf filename=2404.04080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging technology of quantum computing has the potential to change the way how problems will be solved in the future. This work presents a centralized network control algorithm executable on already existing quantum computer which are based on the principle of quantum annealing like the D-Wave Advantage. We introduce a resource reoccupation algorithm for traffic engineering in wide-area networks. The proposed optimization algorithm changes traffic steering and resource allocation in case of overloaded transceivers. Settings of active components like fiber amplifiers and transceivers are not changed for the reason of stability. This algorithm is beneficial in situations when the network traffic is fluctuating in time scales of seconds or spontaneous bursts occur. Further, we developed a <b>discrete-time</b> <b>flow</b> simulator to study the algorithm&rsquo;s performance in wide-area networks. Our network simulator considers backlog and loss modeling of buffered transmission lines. Concurring flows are handled equally in case of a backlog. This work provides an ILP-based network configuring algorithm that is applicable on quantum annealing computers. We showcase, that traffic losses can be reduced significantly by a factor of 2 if a resource reoccupation algorithm is applied in a network with bursty traffic. As resources are used more efficiently by reoccupation in heavy load situations, overprovisioning of networks can be reduced. Thus, this new form of network operation leads toward a zero-margin network. We show that our newly introduced network simulator enables analyses of short-time effects like buffering within fat-pipe networks. As the calculation of network configurations in real-sized networks is typically time-consuming, quantum computing can enable the proposed network configuration algorithm for application in real-sized wide-area networks.</p></p class="citation"></blockquote><h2 id=physicsdata-an-1>physics.data-an (1)</h2><h3 id=11--154178-physics-event-classification-using-large-language-models-cristiano-fanelli-et-al-2024>(1/1 | 154/178) Physics Event Classification Using Large Language Models (Cristiano Fanelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristiano Fanelli, James Giroux, Patrick Moran, Hemalata Nayak, Karthik Suresh, Eric Walter. (2024)<br><strong>Physics Event Classification Using Large Language Models</strong><br><button class=copy-to-clipboard title="Physics Event Classification Using Large Language Models" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.data-an<br>Categories: cs-LG, hep-ex, physics-data-an, physics.data-an<br>Keyword Score: 30<br>Keywords: ChatGPT, Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.05752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.05752v1.pdf filename=2404.05752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC workshop at The Catholic University of America. This workshop brought together researchers from physics, data science and computer science to discuss the latest developments in Artificial Intelligence (AI) and Machine Learning (ML) for the Electron Ion Collider (EIC), including applications for detectors, accelerators, and experimental control. The hackathon, held on the final day of the workshop, involved using a <b>chatbot</b> powered by a <b>Large</b> <b>Language</b> <b>Model,</b> <b>ChatGPT-3.5,</b> to train a binary classifier neutrons and photons in simulated data from the \textsc{GlueX} Barrel Calorimeter. In total, six teams of up to four participants from all over the world took part in this intense educational and research event. This article highlights the hackathon challenge, the resources and methodology used, and the results and insights gained from analyzing physics data using the most cutting-edge tools in AI/ML.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--155178-low-rank-robust-subspace-tensor-clustering-for-metro-passenger-flow-modeling-jiuyun-hu-et-al-2024>(1/1 | 155/178) Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling (Jiuyun Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuyun Hu, Ziyue Li, Chen Zhang, Fugee Tsung, Hao Yan. (2024)<br><strong>Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling</strong><br><button class=copy-to-clipboard title="Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-AI, stat-ME, stat.ME<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04403v1.pdf filename=2404.04403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tensor <b>clustering</b> has become an important topic, specifically in spatio-temporal modeling, due to its ability to cluster spatial modes (e.g., stations or road segments) and temporal modes (e.g., time of the day or day of the week). Our motivating example is from subway passenger flow modeling, where similarities between stations are commonly found. However, the challenges lie in the innate high-dimensionality of tensors and also the potential existence of anomalies. This is because the three tasks, i.e., dimension reduction, <b>clustering,</b> and anomaly decomposition, are inter-correlated to each other, and treating them in a separate manner will render a suboptimal performance. Thus, in this work, we design a tensor-based subspace <b>clustering</b> and anomaly decomposition technique for simultaneously outlier-robust dimension reduction and <b>clustering</b> for high-dimensional tensors. To achieve this, a novel low-rank robust subspace <b>clustering</b> decomposition model is proposed by combining Tucker decomposition, sparse anomaly decomposition, and subspace <b>clustering.</b> An effective algorithm based on Block Coordinate Descent is proposed to update the parameters. Prudent experiments prove the effectiveness of the proposed framework via the <b>simulation</b> study, with a gain of +25% <b>clustering</b> accuracy than <b>benchmark</b> methods in a hard case. The interrelations of the three tasks are also analyzed via ablation studies, validating the interrelation assumption. Moreover, a case study in the station <b>clustering</b> based on real passenger flow data is conducted, with quite valuable insights discovered.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--156178-quantum-informed-simulations-for-mechanics-of-materials-dftbmbd-framework-zhaoxiang-shen-et-al-2024>(1/1 | 156/178) Quantum-informed simulations for mechanics of materials: DFTB+MBD framework (Zhaoxiang Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoxiang Shen, Raúl I. Sosa, Stéphane P. A. Bordas, Alexandre Tkatchenko, Jakub Lengiewicz. (2024)<br><strong>Quantum-informed simulations for mechanics of materials: DFTB+MBD framework</strong><br><button class=copy-to-clipboard title="Quantum-informed simulations for mechanics of materials: DFTB+MBD framework" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE, physics-comp-ph, quant-ph<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04216v1.pdf filename=2404.04216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The macroscopic behaviors of materials are determined by interactions that occur at multiple lengths and time scales. Depending on the application, describing, predicting, and understanding these behaviors require models that rely on insights from electronic and atomic scales. In such cases, classical simplified approximations at those scales are insufficient, and quantum-based modeling is required. In this paper, we study how quantum effects can modify the mechanical properties of systems relevant to materials engineering. We base our study on a high-fidelity modeling framework that combines two computationally efficient models rooted in quantum first principles: Density Functional Tight Binding (DFTB) and many-body dispersion (MBD). The MBD model is applied to accurately describe non-covalent van der Waals interactions. Through various <b>benchmark</b> applications, we demonstrate the capabilities of this framework and the limitations of simplified modeling. We provide an open-source repository containing all codes, datasets, and examples presented in this work. This repository serves as a practical toolkit that we hope will support the development of future research in effective large-scale and multiscale modeling with quantum-mechanical fidelity.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--157178-a-posteriori-error-analysis-of-a-space-time-hybridizable-discontinuous-galerkin-method-for-the-advection-diffusion-problem-yuan-wang-et-al-2024>(1/2 | 157/178) A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem (Yuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Wang, Sander Rhebergen. (2024)<br><strong>A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem</strong><br><button class=copy-to-clipboard title="A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04130v1.pdf filename=2404.04130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present and analyze an a posteriori error estimator for a space-time hybridizable discontinuous Galerkin discretization of the time-dependent advection-diffusion problem. The residual-based error estimator is proven to be reliable and locally efficient. In the reliability analysis we combine a Peclet-robust coercivity type result and a saturation assumption, while local efficiency analysis is based on using bubble functions. The analysis considers both local space and time adaptivity and is verified by numerical <b>simulations</b> on problems which include boundary and interior layers.</p></p class="citation"></blockquote><h3 id=22--158178-highly-efficient-nurbs-based-isogeometric-analysis-for-coupled-nonlinear-diffusion-reaction-equations-with-and-without-advection-ilham-asmouh-et-al-2024>(2/2 | 158/178) Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection (Ilham Asmouh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilham Asmouh, Alexander Ostermann. (2024)<br><strong>Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection</strong><br><button class=copy-to-clipboard title="Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04017v1.pdf filename=2404.04017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinear diffusion-reaction systems model a multitude of physical phenomena. A common situation is biological development modeling where such systems have been widely used to study spatiotemporal phenomena in cell biology. Systems of coupled diffusion-reaction equations are usually subject to some complicated features directly related to their multiphysics nature. Moreover, the presence of advection is source of numerical instabilities, in general, and adds another challenge to these systems. In this study, we propose a NURBS-based isogeometric analysis (IgA) combined with a second-order Strang operator splitting to deal with the multiphysics nature of the problem. The advection part is treated in a semi-Lagrangian framework and the resulting diffusion-reaction equations are then solved using an efficient time-stepping algorithm based on operator splitting. The accuracy of the method is studied by means of a advection-diffusion-reaction system with analytical solution. To further examine the performance of the new method on complex geometries, the well-known Schnakenberg-Turing problem is considered with and without advection. Finally, a Gray-Scott system on a circular domain is also presented. The results obtained demonstrate the efficiency of our new algorithm to accurately reproduce the solution in the presence of complex patterns on complex geometries. Moreover, the new method clarifies the effect of <b>geometry</b> on Turing patterns.</p></p class="citation"></blockquote><h2 id=csse-1>cs.SE (1)</h2><h3 id=11--159178-pros-and-cons-evaluating-chatgpt-on-software-vulnerability-xin-yin-2024>(1/1 | 159/178) Pros and Cons! Evaluating ChatGPT on Software Vulnerability (Xin Yin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Yin. (2024)<br><strong>Pros and Cons! Evaluating ChatGPT on Software Vulnerability</strong><br><button class=copy-to-clipboard title="Pros and Cons! Evaluating ChatGPT on Software Vulnerability" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03994v1.pdf filename=2404.03994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a pipeline for quantitatively evaluating interactive <b>LLMs</b> such as <b>ChatGPT</b> using publicly available dataset. We carry out an extensive technical evaluation of <b>ChatGPT</b> using Big-Vul covering five different common software vulnerability tasks. We evaluate the multitask and multilingual aspects of <b>ChatGPT</b> based on this dataset. We found that the existing state-of-the-art methods are generally superior to <b>ChatGPT</b> in software vulnerability detection. Although <b>ChatGPT</b> improves accuracy when providing context information, it still has limitations in accurately predicting severity ratings for certain CWE types. In addition, <b>ChatGPT</b> demonstrates some ability in locating vulnerabilities for certain CWE types, but its performance varies among different CWE types. <b>ChatGPT</b> exhibits limited vulnerability repair capabilities in both providing and not providing context information. Finally, <b>ChatGPT</b> shows uneven performance in generating CVE descriptions for various CWE types, with limited accuracy in detailed information. Overall, though <b>ChatGPT</b> performs well in some aspects, it still needs improvement in understanding the subtle differences in code vulnerabilities and the ability to describe vulnerabilities in order to fully realize its potential. Our evaluation framework provides valuable insights for further enhancing <b>ChatGPT&rsquo;</b> s software vulnerability handling capabilities.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--160178-quand-rechercher-cest-faire-des-vagues--dans-et-à-partir-des-images-algorithmiques-gaëtan-robillard-2024>(1/1 | 160/178) Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques (Gaëtan Robillard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaëtan Robillard. (2024)<br><strong>Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques</strong><br><button class=copy-to-clipboard title="Quand rechercher c'est faire des vagues : Dans et {à} partir des images algorithmiques" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03923v1.pdf filename=2404.03923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Search of the Wave is a computer-generated film made in 2013, highlighting the computation of images through computer <b>simulation,</b> and through text and voice. Originating from a screening of the film at the Gustave Eiffel University, the article presents a reflection on research-creation in and from algorithmic images. Fundamentally, what is it in this research-creation &ndash; especially in research on algorithmic imagery &ndash; that can be set in motion? Without fully distinguishing between what would be research on one hand and creation on the other, we focus on characterizing forms, aesthetics, or theories that contribute to possible shifts. The inventory of these possibilities is precisely the challenge of the text: from mathematics to image and visualization, from the birth of generative aesthetics to the coding related to pioneering works (recoding), or from indexing new aesthetics to new forms of critical production.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--161178-deep-phase-coded-image-prior-nimrod-shabtay-et-al-2024>(1/2 | 161/178) Deep Phase Coded Image Prior (Nimrod Shabtay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nimrod Shabtay, Eli Schwartz, Raja Giryes. (2024)<br><strong>Deep Phase Coded Image Prior</strong><br><button class=copy-to-clipboard title="Deep Phase Coded Image Prior" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03906v1.pdf filename=2404.03906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phase-coded imaging is a computational imaging method designed to tackle tasks such as passive depth estimation and extended depth of field (EDOF) using depth cues inserted during image capture. Most of the current deep learning-based methods for depth estimation or all-in-focus imaging require a training dataset with high-quality depth maps and an optimal focus point at infinity for all-in-focus images. Such datasets are difficult to create, usually synthetic, and require external graphic programs. We propose a new method named &ldquo;Deep Phase Coded Image Prior&rdquo; (DPCIP) for jointly recovering the depth map and all-in-focus image from a coded-phase image using solely the captured image and the optical information of the imaging system. Our approach does not depend on any specific dataset and surpasses prior <b>supervised</b> techniques utilizing the same imaging system. This improvement is achieved through the utilization of a problem formulation based on implicit neural representation (INR) and deep image prior (DIP). Due to our <b>zero-shot</b> method, we overcome the barrier of acquiring accurate ground-truth data of depth maps and all-in-focus images for each new phase-coded system introduced. This allows focusing mainly on developing the imaging system, and not on ground-truth data collection.</p></p class="citation"></blockquote><h3 id=22--162178-lidar-guided-cross-attention-fusion-for-hyperspectral-band-selection-and-image-classification-judy-x-yang-et-al-2024>(2/2 | 162/178) LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification (Judy X Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Judy X Yang, Jun Zhou, Jing Wang, Hui Tian, Wee Chung Liew. (2024)<br><strong>LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification</strong><br><button class=copy-to-clipboard title="LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: F-2-2, I-2-7, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03883v1.pdf filename=2404.03883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fusion of hyperspectral and LiDAR data has been an active research topic. Existing fusion methods have ignored the high-dimensionality and redundancy challenges in hyperspectral images, despite that band selection methods have been intensively studied for hyperspectral image (HSI) processing. This paper addresses this significant gap by introducing a cross-attention mechanism from the <b>transformer</b> architecture for the selection of HSI bands guided by LiDAR data. LiDAR provides high-resolution vertical structural information, which can be useful in distinguishing different types of land cover that may have similar spectral signatures but different structural profiles. In our approach, the LiDAR data are used as the &ldquo;query&rdquo; to search and identify the &ldquo;key&rdquo; from the HSI to choose the most pertinent bands for LiDAR. This method ensures that the selected HSI bands drastically reduce redundancy and computational requirements while working optimally with the LiDAR data. Extensive experiments have been undertaken on three paired HSI and LiDAR data sets: Houston 2013, Trento and MUUFL. The results highlight the superiority of the cross-attention mechanism, underlining the enhanced classification accuracy of the identified HSI bands when fused with the LiDAR features. The results also show that the use of fewer bands combined with LiDAR surpasses the performance of state-of-the-art fusion models.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--163178-semantic-sql----combining-and-optimizing-semantic-predicates-in-sql-akash-mittal-et-al-2024>(1/1 | 163/178) Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL (Akash Mittal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Mittal, Anshul Bheemreddy, Huili Tao. (2024)<br><strong>Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL</strong><br><button class=copy-to-clipboard title="Semantic SQL -- Combining and optimizing semantic predicates in SQL" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: human-in-the-loop, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03880v1.pdf filename=2404.03880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the surge in unstructured data analysis, facilitated by advancements in Machine Learning (ML), has <b>prompted</b> diverse approaches for handling images, text documents, and videos. Analysts, leveraging ML models, can extract meaningful information from unstructured data and store it in relational databases, allowing the execution of SQL queries for further analysis. Simultaneously, vector databases have emerged, embedding unstructured data for efficient top-k queries based on textual queries. This paper introduces a novel framework SSQL - Semantic SQL that utilizes these two approaches, enabling the incorporation of semantic queries within SQL statements. Our approach extends SQL queries with dedicated keywords for specifying semantic queries alongside predicates related to ML model results and metadata. Our experimental results show that using just semantic queries fails catastrophically to answer count and spatial queries in more than 60% of the cases. Our proposed method jointly optimizes the queries containing both semantic predicates and predicates on structured tables, such as those generated by ML models or other metadata. Further, to improve the query results, we incorporated <b>human-in-the-loop</b> feedback to determine the optimal similarity score threshold for returning results.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--164178-simplifying-explicit-subtyping-coercions-in-a-polymorphic-calculus-with-effects-filip-koprivec-et-al-2024>(1/2 | 164/178) Simplifying explicit subtyping coercions in a polymorphic calculus with effects (Filip Koprivec et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Koprivec, Matija Pretnar. (2024)<br><strong>Simplifying explicit subtyping coercions in a polymorphic calculus with effects</strong><br><button class=copy-to-clipboard title="Simplifying explicit subtyping coercions in a polymorphic calculus with effects" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04218v1.pdf filename=2404.04218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algebraic effect handlers are becoming increasingly popular way of structuring and <b>reasoning</b> about effectful computations, and their performance is often a concern. One of the proposed approaches towards efficient compilation is tracking effect information through explicit subtyping coercions. However, in the presence of polymorphism, these coercions are compiled to additional arguments of compiled functions, incurring significant overhead. In this paper, we present a polymorphic effectful calculus, identify simplification phases needed to reduce the number of unnecessary constraints, and prove they preserve the semantics. In addition, we implement the simplification algorithm in the Eff language, and evaluate its performance on a number of <b>benchmarks.</b> Though we do not prove optimality of presented simplifications, the results show that the algorithm eliminates all the coercions, resulting in a code as efficient as manually monomorphised one.</p></p class="citation"></blockquote><h3 id=22--165178-v-star-learning-visibly-pushdown-grammars-from-program-inputs-xiaodong-jia-et-al-2024>(2/2 | 165/178) V-Star: Learning Visibly Pushdown Grammars from Program Inputs (Xiaodong Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodong Jia, Gang Tan. (2024)<br><strong>V-Star: Learning Visibly Pushdown Grammars from Program Inputs</strong><br><button class=copy-to-clipboard title="V-Star: Learning Visibly Pushdown Grammars from Program Inputs" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-FL, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04201v1.pdf filename=2404.04201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate description of program inputs remains a critical challenge in the field of programming languages. <b>Active</b> <b>learning,</b> as a well-established field, achieves exact learning for regular languages. We offer an innovative grammar inference tool, V-Star, based on the <b>active</b> <b>learning</b> of visibly pushdown automata. V-Star deduces nesting structures of program input languages from sample inputs, employing a novel inference mechanism based on nested patterns. This mechanism identifies token boundaries and converts languages such as XML documents into VPLs. We then adapted Angluin&rsquo;s L-Star, an exact learning algorithm, for VPA learning, which improves the precision of our tool. Our evaluation demonstrates that V-Star effectively and efficiently learns a variety of practical grammars, including S-Expressions, JSON, and XML, and outperforms other state-of-the-art tools.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--166178-algorithmic-fairness-and-social-welfare-annie-liang-et-al-2024>(1/1 | 166/178) Algorithmic Fairness and Social Welfare (Annie Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annie Liang, Jay Lu. (2024)<br><strong>Algorithmic Fairness and Social Welfare</strong><br><button class=copy-to-clipboard title="Algorithmic Fairness and Social Welfare" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-GT, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04424v1.pdf filename=2404.04424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithms are increasingly used to guide high-stakes decisions about individuals. Consequently, substantial interest has developed around defining and measuring the <code>fairness'' of these algorithms. These definitions of fair algorithms share two features: First, they prioritize the role of a pre-defined group identity (e.g., race or gender) by focusing on how the algorithm's impact differs systematically across groups. Second, they are statistical in nature; for example, comparing false positive rates, or assessing whether group identity is independent of the decision (where both are viewed as random variables). These notions are facially distinct from a social welfare approach to &lt;b>fairness,&lt;/b> in particular one based on </code>veil of ignorance&rsquo;&rsquo; thought experiments in which individuals choose how to structure society prior to the realization of their social identity. In this paper, we seek to understand and organize the relationship between these different approaches to <b>fairness.</b> Can the optimization criteria proposed in the algorithmic <b>fairness</b> literature also be motivated as the choices of someone from behind the veil of ignorance? If not, what properties distinguish either approach to fairness?</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--167178-counting-like-transformers-compiling-temporal-counting-logic-into-softmax-transformers-andy-yang-et-al-2024>(1/1 | 167/178) Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers (Andy Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andy Yang, David Chiang. (2024)<br><strong>Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers</strong><br><button class=copy-to-clipboard title="Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CL, cs-FL, cs-LG, cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04393v1.pdf filename=2404.04393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deriving formal bounds on the expressivity of <b>transformers,</b> as well as studying <b>transformers</b> that are constructed to implement known algorithms, are both effective methods for better understanding the computational power of <b>transformers.</b> Towards both ends, we introduce the temporal counting logic $\textbf{K}<em>\text{t}$[#] alongside the RASP variant $\textbf{C-RASP}$. We show they are equivalent to each other, and that together they are the best-known lower bound on the formal expressivity of future-masked soft attention <b>transformers</b> with unbounded input size. We prove this by showing all $\textbf{K}</em>\text{t}$[#] formulas can be compiled into these <b>transformers.</b> As a case study, we demonstrate on paper how to use $\textbf{C-RASP}$ to construct simple <b>transformer</b> language models that, using greedy decoding, can only generate sentences that have given properties formally specified in $\textbf{K}_\text{t}$[#].</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--168178-h3dfact-heterogeneous-3d-integrated-cim-for-factorization-with-holographic-perceptual-representations-zishen-wan-et-al-2024>(1/1 | 168/178) H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations (Zishen Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zishen Wan, Che-Kai Liu, Mohamed Ibrahim, Hanchen Yang, Samuel Spetalnick, Tushar Krishna, Arijit Raychowdhury. (2024)<br><strong>H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations</strong><br><button class=copy-to-clipboard title="H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04173v1.pdf filename=2404.04173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Disentangling attributes of various sensory signals is central to human-like perception and <b>reasoning</b> and a critical task for higher-order cognitive and neuro-symbolic AI systems. An elegant approach to represent this intricate factorization is via high-dimensional holographic vectors drawing on brain-inspired vector symbolic architectures. However, holographic factorization involves iterative computation with high-dimensional matrix-vector multiplications and suffers from non-convergence problems. In this paper, we present H3DFact, a heterogeneous 3D integrated in-memory compute engine capable of efficiently factorizing high-dimensional holographic representations. H3DFact exploits the computation-in-superposition capability of holographic vectors and the intrinsic stochasticity associated with memristive-based 3D compute-in-memory. Evaluated on large-scale factorization and perceptual problems, H3DFact demonstrates superior capability in factorization accuracy and operational capacity by up to five orders of magnitude, with 5.5x compute density, 1.2x energy efficiency improvements, and 5.9x less silicon footprint compared to iso-capacity 2D designs.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--169178-on-the-quest-for-effectiveness-in-human-oversight-interdisciplinary-perspectives-sarah-sterz-et-al-2024>(1/2 | 169/178) On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives (Sarah Sterz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah Sterz, Kevin Baum, Sebastian Biewer, Holger Hermanns, Anne Lauber-Rönsberg, Philip Meinel, Markus Langer. (2024)<br><strong>On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives</strong><br><button class=copy-to-clipboard title="On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04059v1.pdf filename=2404.04059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human oversight is currently discussed as a potential safeguard to counter some of the negative aspects of high-risk AI applications. This <b>prompts</b> a critical examination of the role and conditions necessary for what is prominently termed effective or meaningful human oversight of these systems. This paper investigates effective human oversight by synthesizing insights from psychological, legal, philosophical, and technical domains. Based on the claim that the main objective of human oversight is risk mitigation, we propose a viable understanding of effectiveness in human oversight: for human oversight to be effective, the human overseer has to have (a) sufficient causal power with regards to the system and its effects, (b) suitable epistemic access to relevant aspects of the situation, (c) self-control over their own actions, and (d) fitting intentions for their role. Furthermore, we argue that this is equivalent to saying that a human overseer is effective if and only if they are morally responsible and have fitting intentions. Against this backdrop, we suggest facilitators and inhibitors of effectiveness in human oversight when striving for practical applicability. We discuss factors in three domains, namely, the technical design of the system, individual factors of human overseers, and the environmental circumstances in which the overseer operates. Finally, this paper scrutinizes the upcoming AI Act of the European Union &ndash; in particular Article 14 on Human Oversight &ndash; as an exemplary regulatory framework in which we study the practicality of our understanding of effective human oversight. By analyzing the provisions and implications of the European AI Act proposal, we pinpoint in how far that proposal aligns with our analyses regarding effective human oversight as well as how it might get enriched by our conceptual understanding of effectiveness in human oversight.</p></p class="citation"></blockquote><h3 id=22--170178-a-conceptual-design-of-in-game-real-and-virtual-currency-tracker-dennis-barzanoff-et-al-2024>(2/2 | 170/178) A Conceptual Design of In-Game Real and Virtual Currency Tracker (Dennis Barzanoff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Barzanoff, Amna Asif. (2024)<br><strong>A Conceptual Design of In-Game Real and Virtual Currency Tracker</strong><br><button class=copy-to-clipboard title="A Conceptual Design of In-Game Real and Virtual Currency Tracker" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03951v1.pdf filename=2404.03951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The gaming industry is earning huge revenues from incorporating virtual currencies into the game design experience. Even if it is a useful approach for the game industry to boost up their earnings, the unidirectional and bidirectional in-game virtual currencies can invoke inadequate gaming behaviors and additions among players. The market lacks gaming and customer protection regulations to avoid the financial, behavioral, and psychological exploitation of users. Therefore, it is needed to develop visual or textual interface design <b>recommendations</b> that help the game players keep balance in their spending and improve their gaming behavior. This paper presents a conceptual design of an in-game purchasing module that allows the user to observe their real time spendings in relation to virtual currency buying.</p></p class="citation"></blockquote><h2 id=nlinps-1>nlin.PS (1)</h2><h3 id=11--171178-suppressing-modulation-instability-with-reinforcement-learning-nikolay-kalmykov-et-al-2024>(1/1 | 171/178) Suppressing Modulation Instability with Reinforcement Learning (Nikolay Kalmykov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolay Kalmykov, Rishat Zagidullin, Oleg Rogov, Sergey Rykovanov, Dmitry V. Dylov. (2024)<br><strong>Suppressing Modulation Instability with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Suppressing Modulation Instability with Reinforcement Learning" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: nlin.PS<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, nlin-PS, nlin.PS, physics-app-ph<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04310v1.pdf filename=2404.04310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modulation instability is a phenomenon of spontaneous pattern formation in nonlinear media, oftentimes leading to an unpredictable behaviour and a degradation of a signal of interest. We propose an approach based on <b>reinforcement</b> <b>learning</b> to suppress the unstable modes by optimizing the parameters for the time modulation of the potential in the nonlinear system. We test our approach in 1D and 2D cases and propose a new class of physically-meaningful reward functions to guarantee tamed instability.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--172178-a-fine-grained-classification-of-subquadratic-patterns-for-subgraph-listing-and-friends-karl-bringmann-et-al-2024>(1/3 | 172/178) A Fine-grained Classification of Subquadratic Patterns for Subgraph Listing and Friends (Karl Bringmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karl Bringmann, Egor Gorbachev. (2024)<br><strong>A Fine-grained Classification of Subquadratic Patterns for Subgraph Listing and Friends</strong><br><button class=copy-to-clipboard title="A Fine-grained Classification of Subquadratic Patterns for Subgraph Listing and Friends" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04369v1.pdf filename=2404.04369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an $m$-edge host <b>graph</b> $G$, all triangles can be listed in time $O(m^{1.5})$ [Itai, Rodeh &lsquo;78], and all $k$-cycles can be listed in time $O(m^{2-1/{\lceil k/2 \rceil}} + t)$ where $t$ is the output size [Alon, Yuster, Zwick &lsquo;97]. These classic results also hold for the colored problem variant, where the nodes of the host <b>graph</b> $G$ are colored by nodes in the pattern <b>graph</b> $H$, and we are only interested in subgraphs of $G$ that are isomorphic to the pattern $H$ and respect the colors. We study the problem of listing all $H$-subgraphs in the colored setting, for fixed pattern <b>graphs</b> $H$. As our main result, we determine all pattern <b>graphs</b> $H$ such that all $H$-subgraphs can be listed in subquadratic time $O(m^{2-\varepsilon} + t)$, where $t$ is the output size. Moreover, for each such subquadratic pattern $H$ we determine the smallest exponent $c(H)$ such that all $H$-subgraphs can be listed in time $O(m^{c(H)} + t)$. This is a vast generalization of the classic results on triangles and cycles. To prove this result, we design new listing algorithms and prove conditional lower bounds based on standard hypotheses from fine-grained complexity theory. In our algorithms, we use a new ingredient that we call hyper-degree splitting, where we split tuples of nodes into high degree and low degree depending on their number of common neighbors. We also show the same results for two related problems: finding an $H$-subgraph of minimum total edge-weight in time $O(m^{c(H)})$, and enumerating all $H$-subgraphs in $O(m^{c(H)})$ preprocessing time and constant delay. Again we determine all pattern <b>graphs</b> $H$ that have complexity $c(H) &lt; 2$, and for each such subquadratic pattern we determine the optimal complexity $c(H)$.</p></p class="citation"></blockquote><h3 id=23--173178-stability-in-graphs-with-matroid-constraints-fedor-v-fomin-et-al-2024>(2/3 | 173/178) Stability in Graphs with Matroid Constraints (Fedor V. Fomin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fedor V. Fomin, Petr A. Golovach, Tuukka Korhonen, Saket Saurabh. (2024)<br><strong>Stability in Graphs with Matroid Constraints</strong><br><button class=copy-to-clipboard title="Stability in Graphs with Matroid Constraints" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03979v1.pdf filename=2404.03979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the following Independent Stable Set problem. Let G be an undirected <b>graph</b> and M = (V(G),I) be a matroid whose elements are the vertices of G. For an integer k\geq 1, the task is to decide whether G contains a set S\subseteq V(G) of size at least k which is independent (stable) in G and independent in M. This problem generalizes several well-studied algorithmic problems, including Rainbow Independent Set, Rainbow Matching, and Bipartite Matching with Separation. We show that - When the matroid M is represented by the independence oracle, then for any computable function f, no algorithm can solve Independent Stable Set using f(k)n^{o(k)} calls to the oracle. - On the other hand, when the <b>graph</b> G is of degeneracy d, then the problem is solvable in time O((d+1)^kn), and hence is FPT parameterized by d+k. Moreover, when the degeneracy d is a constant (which is not a part of the input), the problem admits a kernel polynomial in k. More precisely, we prove that for every integer d\geq 0, the problem admits a kernelization algorithm that in time n^{O(d)} outputs an equivalent framework with a <b>graph</b> on dk^{O(d)} vertices. A lower bound complements this when d is part of the input: Independent Stable Set does not admit a polynomial kernel when parameterized by k+d unless NP \subseteq coNP/poly. This lower bound holds even when M is a partition matroid. - Another set of results concerns the scenario when the <b>graph</b> G is chordal. In this case, our computational lower bound excludes an FPT algorithm when the input matroid is given by its independence oracle. However, we demonstrate that Independent Stable Set can be solved in 2^{O(k)}||M||^{O(1)} time when M is a linear matroid given by its representation. In the same setting, Independent Stable Set does not have a polynomial kernel when parameterized by k unless NP\subseteq coNP/poly.</p></p class="citation"></blockquote><h3 id=33--174178-minor-containment-and-disjoint-paths-in-almost-linear-time-tuukka-korhonen-et-al-2024>(3/3 | 174/178) Minor Containment and Disjoint Paths in almost-linear time (Tuukka Korhonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuukka Korhonen, Michał Pilipczuk, Giannos Stamoulis. (2024)<br><strong>Minor Containment and Disjoint Paths in almost-linear time</strong><br><button class=copy-to-clipboard title="Minor Containment and Disjoint Paths in almost-linear time" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03958v1.pdf filename=2404.03958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give an algorithm that, given <b>graphs</b> $G$ and $H$, tests whether $H$ is a minor of $G$ in time ${\cal O}<em>H(n^{1+o(1)})$; here, $n$ is the number of vertices of $G$ and the ${\cal O}<em>H(\cdot)$-notation hides factors that depend on $H$ and are computable. By the <b>Graph</b> Minor Theorem, this implies the existence of an $n^{1+o(1)}$-time membership test for every minor-closed class of <b>graphs.</b> More generally, we give an ${\cal O}</em>{H,|X|}(m^{1+o(1)})$-time algorithm for the rooted version of the problem, in which $G$ comes with a set of roots $X\subseteq V(G)$ and some of the branch sets of the sought minor model of $H$ are required to contain prescribed subsets of $X$; here, $m$ is the total number of vertices and edges of $G$. This captures the Disjoint Paths problem, for which we obtain an ${\cal O}</em>{k}(m^{1+o(1)})$-time algorithm, where $k$ is the number of terminal pairs. For all the mentioned problems, the fastest algorithms known before are due to Kawarabayashi, Kobayashi, and Reed [JCTB 2012], and have a time complexity that is quadratic in the number of vertices of $G$. Our algorithm has two main ingredients: First, we show that by using the dynamic treewidth data structure of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\l}owski [FOCS 2023], the irrelevant vertex technique of Robertson and Seymour can be implemented in almost-linear time on apex-minor-free <b>graphs.</b> Then, we apply the recent advances in almost-linear time flow/cut algorithms to give an almost-linear time implementation of the recursive understanding technique, which effectively reduces the problem to apex-minor-free <b>graphs.</b></p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--175178-wireless-resource-optimization-in-hybrid-semanticbit-communication-networks-le-xia-et-al-2024>(1/1 | 175/178) Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks (Le Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Le Xia, Yao Sun, Dusit Niyato, Lan Zhang, Muhammad Ali Imran. (2024)<br><strong>Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks</strong><br><button class=copy-to-clipboard title="Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04162v1.pdf filename=2404.04162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, semantic communication (SemCom) has shown great potential in significant resource savings and efficient information exchanges, thus naturally introducing a novel and practical cellular network paradigm where two modes of SemCom and conventional bit communication (BitCom) coexist. Nevertheless, the involved wireless resource management becomes rather complicated and challenging, given the unique background knowledge matching and time-consuming semantic coding requirements in SemCom. To this end, this paper jointly investigates user association (UA), mode selection (MS), and bandwidth allocation (BA) problems in a hybrid semantic/bit communication network (HSB-Net). Concretely, we first identify a unified performance metric of message throughput for both SemCom and BitCom links. Next, we specially develop a knowledge matching-aware two-stage tandem packet queuing model and theoretically derive the average packet loss ratio and queuing latency. Combined with practical constraints, we then formulate a joint optimization problem for UA, MS, and BA to maximize the overall message throughput of HSB-Net. Afterward, we propose an optimal resource management strategy by utilizing a Lagrange primal-dual transformation method and a preference list-based heuristic algorithm with polynomial-time complexity. Numerical results not only demonstrate the accuracy of our analytical queuing model, but also validate the performance superiority of our proposed strategy compared with different <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--176178-hardness-of-circuit-and-monotone-diameters-of-polytopes-christian-nöbel-et-al-2024>(1/1 | 176/178) Hardness of circuit and monotone diameters of polytopes (Christian Nöbel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Nöbel, Raphael Steiner. (2024)<br><strong>Hardness of circuit and monotone diameters of polytopes</strong><br><button class=copy-to-clipboard title="Hardness of circuit and monotone diameters of polytopes" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-DM, cs-DS, math-CO, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04158v1.pdf filename=2404.04158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Circuit diameter of polytopes was introduced by Borgwardt, Finhold and Hemmecke as a fundamental tool for the study of circuit augmentation schemes for linear programming and for estimating combinatorial diameters. Determining the complexity of computing the circuit diameter of polytopes was posed as an open problem by Sanit`a as well as by Kafer, and was recently reiterated by Borgwardt, Grewe, Kafer, Lee and Sanit`a. In this paper, we solve this problem by showing that computing the circuit diameter of a polytope given in halfspace-description is strongly NP-hard. To prove this result, we show that computing the combinatorial diameter of the perfect matching polytope of a bipartite <b>graph</b> is NP-hard. This complements a result by Sanit`a (FOCS 2018) on the NP-hardness of computing the diameter of fractional matching polytopes and implies the new result that computing the diameter of a ${0,1}$-polytope is strongly NP-hard, which may be of independent interest. In our second main result, we give a precise <b>graph-theoretic</b> description of the monotone diameter of perfect matching polytopes and use this description to prove that computing the monotone (circuit) diameter of a given input polytope is strongly NP-hard as well.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--177178-discrete-fréchet-distance-oracles-boris-aronov-et-al-2024>(1/1 | 177/178) Discrete Fréchet Distance Oracles (Boris Aronov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boris Aronov, Tsuri Farhana, Matthew J. Katz, Indu Ramesh. (2024)<br><strong>Discrete Fréchet Distance Oracles</strong><br><button class=copy-to-clipboard title="Discrete Fréchet Distance Oracles" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.04065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.04065v1.pdf filename=2404.04065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is unlikely that the discrete Fr'echet distance between two curves of length $n$ can be computed in strictly subquadratic time. We thus consider the setting where one of the curves, $P$, is known in advance. In particular, we wish to construct data structures (distance oracles) of near-linear size that support efficient distance queries with respect to $P$ in sublinear time. Since there is evidence that this is impossible for query curves of length $\Theta(n^\alpha)$, for any $\alpha > 0$, we focus on query curves of (small) constant length, for which we are able to devise distance oracles with the desired bounds. We extend our tools to handle subcurves of the given curve, and even arbitrary vertex-to-vertex subcurves of a given geometric tree. That is, we construct an oracle that can quickly compute the distance between a short polygonal path (the query) and a path in the preprocessed tree between two query-specified vertices. Moreover, we define a new family of geometric <b>graphs,</b> $t$-local <b>graphs</b> (which strictly contains the family of geometric spanners with constant stretch), for which a similar oracle exists: we can preprocess a <b>graph</b> $G$ in the family, so that, given a query segment and a pair $u,v$ of vertices in $G$, one can quickly compute the smallest discrete Fr'echet distance between the segment and any $(u,v)$-path in $G$. The answer is exact, if $t=1$, and approximate if $t>1$.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--178178-the-low-degree-hardness-of-finding-large-independent-sets-in-sparse-random-hypergraphs-abhishek-dhawan-et-al-2024>(1/1 | 178/178) The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs (Abhishek Dhawan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Dhawan, Yuzhou Wang. (2024)<br><strong>The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs</strong><br><button class=copy-to-clipboard title="The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC, math-CO, math-PR, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03842v1.pdf filename=2404.03842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the algorithmic task of finding large independent sets in Erdos-Renyi $r$-uniform hypergraphs on $n$ vertices having average degree $d$. Krivelevich and Sudakov showed that the maximum independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$. We show that the class of low-degree polynomial algorithms can find independent sets of density $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$ but no larger. This extends and generalizes earlier results of Gamarnik and Sudan, Rahman and Virag, and Wein on <b>graphs,</b> and answers a question of Bal and Bennett. We conjecture that this statistical-computational gap holds for this problem. Additionally, we explore the universality of this gap by examining $r$-partite hypergraphs. A hypergraph $H=(V,E)$ is $r$-partite if there is a partition $V=V_1\cup\cdots\cup V_r$ such that each edge contains exactly one vertex from each set $V_i$. We consider the problem of finding large balanced independent sets (independent sets containing the same number of vertices in each partition) in random $r$-partite hypergraphs with $n$ vertices in each partition and average degree $d$. We prove that the maximum balanced independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$ asymptotically. Furthermore, we prove an analogous low-degree computational threshold of $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$. Our results recover and generalize recent work of Perkins and the second author on bipartite <b>graphs.</b> While the <b>graph</b> case has been extensively studied, this work is the first to consider statistical-computational gaps of optimization problems on random hypergraphs. Our results suggest that these gaps persist for larger uniformities as well as across many models. A somewhat surprising aspect of the gap for balanced independent sets is that the algorithm achieving the lower bound is a simple degree-1 polynomial.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.06</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.08</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscr-13>cs.CR (13)</a><ul><li><a href=#113--1178-increased-llm-vulnerabilities-from-fine-tuning-and-quantization-divyanshu-kumar-et-al-2024>(1/13 | 1/178) Increased LLM Vulnerabilities from Fine-tuning and Quantization (Divyanshu Kumar et al., 2024)</a></li><li><a href=#213--2178-auditgpt-auditing-smart-contracts-with-chatgpt-shihao-xia-et-al-2024>(2/13 | 2/178) AuditGPT: Auditing Smart Contracts with ChatGPT (Shihao Xia et al., 2024)</a></li><li><a href=#313--3178-evaluating-adversarial-robustness-a-comparison-of-fgsm-carlini-wagner-attacks-and-the-role-of-distillation-as-defense-mechanism-trilokesh-ranjan-sarkar-et-al-2024>(3/13 | 3/178) Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner Attacks, And The Role of Distillation as Defense Mechanism (Trilokesh Ranjan Sarkar et al., 2024)</a></li><li><a href=#413--4178-re-pseudonymization-strategies-for-smart-meter-data-are-not-robust-to-deep-learning-profiling-attacks-ana-maria-cretu-et-al-2024>(4/13 | 4/178) Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to Deep Learning Profiling Attacks (Ana-Maria Cretu et al., 2024)</a></li><li><a href=#513--5178-vellet-verifiable-embedded-wallet-for-securing-authenticity-and-integrity-hiroki-watanabe-et-al-2024>(5/13 | 5/178) VELLET: Verifiable Embedded Wallet for Securing Authenticity and Integrity (Hiroki Watanabe et al., 2024)</a></li><li><a href=#613--6178-reliable-feature-selection-for-adversarially-robust-cyber-attack-detection-joão-vitorino-et-al-2024>(6/13 | 6/178) Reliable Feature Selection for Adversarially Robust Cyber-Attack Detection (João Vitorino et al., 2024)</a></li><li><a href=#713--7178-smart-contract-languages-a-comparative-analysis-massimo-bartoletti-et-al-2024>(7/13 | 7/178) Smart Contract Languages: a comparative analysis (Massimo Bartoletti et al., 2024)</a></li><li><a href=#813--8178-reconfigurable-and-scalable-honeynet-for-cyber-physical-systems-luís-sousa-et-al-2024>(8/13 | 8/178) Reconfigurable and Scalable Honeynet for Cyber-Physical Systems (Luís Sousa et al., 2024)</a></li><li><a href=#913--9178-watermark-based-detection-and-attribution-of-ai-generated-content-zhengyuan-jiang-et-al-2024>(9/13 | 9/178) Watermark-based Detection and Attribution of AI-Generated Content (Zhengyuan Jiang et al., 2024)</a></li><li><a href=#1013--10178-precision-guided-approach-to-mitigate-data-poisoning-attacks-in-federated-learning-k-naveen-kumar-et-al-2024>(10/13 | 10/178) Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning (K Naveen Kumar et al., 2024)</a></li><li><a href=#1113--11178-you-can-use-but-cannot-recognize-preserving-visual-privacy-in-deep-neural-networks-qiushi-li-et-al-2024>(11/13 | 11/178) You Can Use But Cannot Recognize: Preserving Visual Privacy in Deep Neural Networks (Qiushi Li et al., 2024)</a></li><li><a href=#1213--12178-from-theory-to-comprehension-a-comparative-study-of-differential-privacy-and-k-anonymity-saskia-nuñez-von-voigt-et-al-2024>(12/13 | 12/178) From Theory to Comprehension: A Comparative Study of Differential Privacy and $k$-Anonymity (Saskia Nuñez von Voigt et al., 2024)</a></li><li><a href=#1313--13178-privshape-extracting-shapes-in-time-series-under-user-level-local-differential-privacy-yulian-mao-et-al-2024>(13/13 | 13/178) PrivShape: Extracting Shapes in Time Series under User-Level Local Differential Privacy (Yulian Mao et al., 2024)</a></li></ul></li><li><a href=#cscl-27>cs.CL (27)</a><ul><li><a href=#127--14178-deciphering-political-entity-sentiment-in-news-with-large-language-models-zero-shot-and-few-shot-strategies-alapan-kuila-et-al-2024>(1/27 | 14/178) Deciphering Political Entity Sentiment in News with Large Language Models: Zero-Shot and Few-Shot Strategies (Alapan Kuila et al., 2024)</a></li><li><a href=#227--15178-assisting-humans-in-complex-comparisons-automated-information-comparison-at-scale-truman-yuen-et-al-2024>(2/27 | 15/178) Assisting humans in complex comparisons: automated information comparison at scale (Truman Yuen et al., 2024)</a></li><li><a href=#327--16178-teaching-llama-a-new-language-through-cross-lingual-knowledge-transfer-hele-andra-kuulmets-et-al-2024>(3/27 | 16/178) Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer (Hele-Andra Kuulmets et al., 2024)</a></li><li><a href=#427--17178-extract-define-canonicalize-an-llm-based-framework-for-knowledge-graph-construction-bowen-zhang-et-al-2024>(4/27 | 17/178) Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction (Bowen Zhang et al., 2024)</a></li><li><a href=#527--18178-simple-techniques-for-enhancing-sentence-embeddings-in-generative-language-models-bowen-zhang-et-al-2024>(5/27 | 18/178) Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models (Bowen Zhang et al., 2024)</a></li><li><a href=#627--19178-cleared-for-takeoff-compositional--conditional-reasoning-may-be-the-achilles-heel-to-flight-booking-language-agents-harsh-kohli-et-al-2024>(6/27 | 19/178) Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents (Harsh Kohli et al., 2024)</a></li><li><a href=#727--20178-ffn-skipllm-a-hidden-gem-for-autoregressive-decoding-with-adaptive-feed-forward-skipping-ajay-jaiswal-et-al-2024>(7/27 | 20/178) FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping (Ajay Jaiswal et al., 2024)</a></li><li><a href=#827--21178-seme-at-semeval-2024-task-2-comparing-masked-and-generative-language-models-on-natural-language-inference-for-clinical-trials-mathilde-aguiar-et-al-2024>(8/27 | 21/178) SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials (Mathilde Aguiar et al., 2024)</a></li><li><a href=#927--22178-buddie-a-business-document-dataset-for-multi-task-information-extraction-ran-zmigrod-et-al-2024>(9/27 | 22/178) BuDDIE: A Business Document Dataset for Multi-task Information Extraction (Ran Zmigrod et al., 2024)</a></li><li><a href=#1027--23178-unlocking-parameter-efficient-fine-tuning-for-low-resource-language-translation-tong-su-et-al-2024>(10/27 | 23/178) Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation (Tong Su et al., 2024)</a></li><li><a href=#1127--24178-investigating-the-robustness-of-modelling-decisions-for-few-shot-cross-topic-stance-detection-a-preregistered-study-myrthe-reuver-et-al-2024>(11/27 | 24/178) Investigating the Robustness of Modelling Decisions for Few-Shot Cross-Topic Stance Detection: A Preregistered Study (Myrthe Reuver et al., 2024)</a></li><li><a href=#1227--25178-scope-ambiguities-in-large-language-models-gaurav-kamath-et-al-2024>(12/27 | 25/178) Scope Ambiguities in Large Language Models (Gaurav Kamath et al., 2024)</a></li><li><a href=#1327--26178-do-sentence-transformers-learn-quasi-geospatial-concepts-from-general-text-ilya-ilyankou-et-al-2024>(13/27 | 26/178) Do Sentence Transformers Learn Quasi-Geospatial Concepts from General Text? (Ilya Ilyankou et al., 2024)</a></li><li><a href=#1427--27178-data-augmentation-with-in-context-learning-and-comparative-evaluation-in-math-word-problem-solving-gulsum-yigit-et-al-2024>(14/27 | 27/178) Data Augmentation with In-Context Learning and Comparative Evaluation in Math Word Problem Solving (Gulsum Yigit et al., 2024)</a></li><li><a href=#1527--28178-forget-nli-use-a-dictionary-zero-shot-topic-classification-for-low-resource-languages-with-application-to-luxembourgish-fred-philippy-et-al-2024>(15/27 | 28/178) Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish (Fred Philippy et al., 2024)</a></li><li><a href=#1627--29178-saas-solving-ability-amplification-strategy-for-enhanced-mathematical-reasoning-in-large-language-models-hyeonwoo-kim-et-al-2024>(16/27 | 29/178) SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models (Hyeonwoo Kim et al., 2024)</a></li><li><a href=#1727--30178-towards-realistic-few-shot-relation-extraction-a-new-meta-dataset-and-evaluation-fahmida-alam-et-al-2024>(17/27 | 30/178) Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation (Fahmida Alam et al., 2024)</a></li><li><a href=#1827--31178-verifiable-by-design-aligning-language-models-to-quote-from-pre-training-data-jingyu-zhang-et-al-2024>(18/27 | 31/178) Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data (Jingyu Zhang et al., 2024)</a></li><li><a href=#1927--32178-benchmarking-and-improving-compositional-generalization-of-multi-aspect-controllable-text-generation-tianqi-zhong-et-al-2024>(19/27 | 32/178) Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation (Tianqi Zhong et al., 2024)</a></li><li><a href=#2027--33178-chinese-tiny-llm-pretraining-a-chinese-centric-large-language-model-xinrun-du-et-al-2024>(20/27 | 33/178) Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model (Xinrun Du et al., 2024)</a></li><li><a href=#2127--34178-clue-a-clinical-language-understanding-evaluation-for-llms-amin-dada-et-al-2024>(21/27 | 34/178) CLUE: A Clinical Language Understanding Evaluation for LLMs (Amin Dada et al., 2024)</a></li><li><a href=#2227--35178-assessing-the-quality-of-information-extraction-filip-seitl-et-al-2024>(22/27 | 35/178) Assessing the quality of information extraction (Filip Seitl et al., 2024)</a></li><li><a href=#2327--36178-willkommens-merkel-chaos-johnson-and-tore-klose-modeling-the-evaluative-meaning-of-german-personal-name-compounds-annerose-eichel-et-al-2024>(23/27 | 36/178) Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the Evaluative Meaning of German Personal Name Compounds (Annerose Eichel et al., 2024)</a></li><li><a href=#2427--37178-how-lexical-is-bilingual-lexicon-induction-harsh-kohli-et-al-2024>(24/27 | 37/178) How Lexical is Bilingual Lexicon Induction? (Harsh Kohli et al., 2024)</a></li><li><a href=#2527--38178-social-skill-training-with-large-language-models-diyi-yang-et-al-2024>(25/27 | 38/178) Social Skill Training with Large Language Models (Diyi Yang et al., 2024)</a></li><li><a href=#2627--39178-bear-a-unified-framework-for-evaluating-relational-knowledge-in-causal-and-masked-language-models-jacek-wiland-et-al-2024>(26/27 | 39/178) BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models (Jacek Wiland et al., 2024)</a></li><li><a href=#2727--40178-a-bi-consolidating-model-for-joint-relational-triple-extraction-xiaocheng-luo-et-al-2024>(27/27 | 40/178) A Bi-consolidating Model for Joint Relational Triple Extraction (Xiaocheng Luo et al., 2024)</a></li></ul></li><li><a href=#cscv-35>cs.CV (35)</a><ul><li><a href=#135--41178-enhancing-breast-cancer-diagnosis-in-mammography-evaluation-and-integration-of-convolutional-neural-networks-and-explainable-ai-maryam-ahmed-et-al-2024>(1/35 | 41/178) Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI (Maryam Ahmed et al., 2024)</a></li><li><a href=#235--42178-vision-transformers-in-domain-adaptation-and-generalization-a-study-of-robustness-shadi-alijani-et-al-2024>(2/35 | 42/178) Vision Transformers in Domain Adaptation and Generalization: A Study of Robustness (Shadi Alijani et al., 2024)</a></li><li><a href=#335--43178-koala-key-frame-conditioned-long-video-llm-reuben-tan-et-al-2024>(3/35 | 43/178) Koala: Key frame-conditioned long video-LLM (Reuben Tan et al., 2024)</a></li><li><a href=#435--44178-image-text-co-decomposition-for-text-supervised-semantic-segmentation-ji-jia-wu-et-al-2024>(4/35 | 44/178) Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation (Ji-Jia Wu et al., 2024)</a></li><li><a href=#535--45178-dynamic-prompt-optimizing-for-text-to-image-generation-wenyi-mo-et-al-2024>(5/35 | 45/178) Dynamic Prompt Optimizing for Text-to-Image Generation (Wenyi Mo et al., 2024)</a></li><li><a href=#635--46178-learning-correlation-structures-for-vision-transformers-manjin-kim-et-al-2024>(6/35 | 46/178) Learning Correlation Structures for Vision Transformers (Manjin Kim et al., 2024)</a></li><li><a href=#735--47178-sigma-siamese-mamba-network-for-multi-modal-semantic-segmentation-zifu-wan-et-al-2024>(7/35 | 47/178) Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation (Zifu Wan et al., 2024)</a></li><li><a href=#835--48178-diffop-net-a-differential-operator-based-fully-convolutional-network-for-unsupervised-deformable-image-registration-jiong-wu-2024>(8/35 | 48/178) DiffOp-net: A Differential Operator-based Fully Convolutional Network for Unsupervised Deformable Image Registration (Jiong Wu, 2024)</a></li><li><a href=#935--49178-who-evaluates-the-evaluations-objectively-scoring-text-to-image-prompt-coherence-metrics-with-t2iscorescore-ts2-michael-saxon-et-al-2024>(9/35 | 49/178) Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2) (Michael Saxon et al., 2024)</a></li><li><a href=#1035--50178-robust-few-shot-ensemble-learning-with-focal-diversity-based-pruning-selim-furkan-tekin-et-al-2024>(10/35 | 50/178) Robust Few-Shot Ensemble Learning with Focal Diversity-Based Pruning (Selim Furkan Tekin et al., 2024)</a></li><li><a href=#1135--51178-identity-decoupling-for-multi-subject-personalization-of-text-to-image-models-sangwon-jang-et-al-2024>(11/35 | 51/178) Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models (Sangwon Jang et al., 2024)</a></li><li><a href=#1235--52178-physical-property-understanding-from-language-embedded-feature-fields-albert-j-zhai-et-al-2024>(12/35 | 52/178) Physical Property Understanding from Language-Embedded Feature Fields (Albert J. Zhai et al., 2024)</a></li><li><a href=#1335--53178-rasim-a-range-aware-high-fidelity-rgb-d-data-simulation-pipeline-for-real-world-applications-xingyu-liu-et-al-2024>(13/35 | 53/178) RaSim: A Range-aware High-fidelity RGB-D Data Simulation Pipeline for Real-world Applications (Xingyu Liu et al., 2024)</a></li><li><a href=#1435--54178-concept-weaver-enabling-multi-concept-fusion-in-text-to-image-models-gihyun-kwon-et-al-2024>(14/35 | 54/178) Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models (Gihyun Kwon et al., 2024)</a></li><li><a href=#1535--55178-idea-2-3d-collaborative-lmm-agents-enable-3d-model-generation-from-interleaved-multimodal-inputs-junhao-chen-et-al-2024>(15/35 | 55/178) Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs (Junhao Chen et al., 2024)</a></li><li><a href=#1635--56178-3d-facial-expressions-through-analysis-by-neural-synthesis-george-retsinas-et-al-2024>(16/35 | 56/178) 3D Facial Expressions through Analysis-by-Neural-Synthesis (George Retsinas et al., 2024)</a></li><li><a href=#1735--57178-clickdiffusion-harnessing-llms-for-interactive-precise-image-editing-alec-helbling-et-al-2024>(17/35 | 57/178) ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing (Alec Helbling et al., 2024)</a></li><li><a href=#1835--58178-label-propagation-for-zero-shot-classification-with-vision-language-models-vladan-stojnić-et-al-2024>(18/35 | 58/178) Label Propagation for Zero-shot Classification with Vision-Language Models (Vladan Stojnić et al., 2024)</a></li><li><a href=#1935--59178-neural-symbolic-videoqa-learning-compositional-spatio-temporal-reasoning-for-real-world-video-question-answering-lili-liang-et-al-2024>(19/35 | 59/178) Neural-Symbolic VideoQA: Learning Compositional Spatio-Temporal Reasoning for Real-world Video Question Answering (Lili Liang et al., 2024)</a></li><li><a href=#2035--60178-physics-inspired-synthesized-underwater-image-dataset-reina-kaneko-et-al-2024>(20/35 | 60/178) Physics-Inspired Synthesized Underwater Image Dataset (Reina Kaneko et al., 2024)</a></li><li><a href=#2135--61178-physpt-physics-aware-pretrained-transformer-for-estimating-human-dynamics-from-monocular-videos-yufei-zhang-et-al-2024>(21/35 | 61/178) PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos (Yufei Zhang et al., 2024)</a></li><li><a href=#2235--62178-no-time-to-train-empowering-non-parametric-networks-for-few-shot-3d-scene-segmentation-xiangyang-zhu-et-al-2024>(22/35 | 62/178) No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation (Xiangyang Zhu et al., 2024)</a></li><li><a href=#2335--63178-instructhumans-editing-animated-3d-human-textures-with-instructions-jiayin-zhu-et-al-2024>(23/35 | 63/178) InstructHumans: Editing Animated 3D Human Textures with Instructions (Jiayin Zhu et al., 2024)</a></li><li><a href=#2435--64178-voltavision-a-transfer-learning-model-for-electronic-component-classification-anas-mohammad-ishfaqul-muktadir-osmani-et-al-2024>(24/35 | 64/178) VoltaVision: A Transfer Learning model for electronic component classification (Anas Mohammad Ishfaqul Muktadir Osmani et al., 2024)</a></li><li><a href=#2535--65178-increasing-fairness-in-classification-of-out-of-distribution-data-for-facial-recognition-gianluca-barone-et-al-2024>(25/35 | 65/178) Increasing Fairness in Classification of Out of Distribution Data for Facial Recognition (Gianluca Barone et al., 2024)</a></li><li><a href=#2635--66178-improving-detection-in-aerial-images-by-capturing-inter-object-relationships-botao-ren-et-al-2024>(26/35 | 66/178) Improving Detection in Aerial Images by Capturing Inter-Object Relationships (Botao Ren et al., 2024)</a></li><li><a href=#2735--67178-analyzing-participants-engagement-during-online-meetings-using-unsupervised-remote-photoplethysmography-with-behavioral-features-alexander-vedernikov-et-al-2024>(27/35 | 67/178) Analyzing Participants&rsquo; Engagement during Online Meetings Using Unsupervised Remote Photoplethysmography with Behavioral Features (Alexander Vedernikov et al., 2024)</a></li><li><a href=#2835--68178-spatialtracker-tracking-any-2d-pixels-in-3d-space-yuxi-xiao-et-al-2024>(28/35 | 68/178) SpatialTracker: Tracking Any 2D Pixels in 3D Space (Yuxi Xiao et al., 2024)</a></li><li><a href=#2935--69178-robust-depth-enhancement-via-polarization-prompt-fusion-tuning-kei-ikemura-et-al-2024>(29/35 | 69/178) Robust Depth Enhancement via Polarization Prompt Fusion Tuning (Kei Ikemura et al., 2024)</a></li><li><a href=#3035--70178-scaresnet-a-resnet-variant-optimized-for-tiny-object-detection-in-transmission-and-distribution-towers-weile-li-et-al-2024>(30/35 | 70/178) SCAResNet: A ResNet Variant Optimized for Tiny Object Detection in Transmission and Distribution Towers (Weile Li et al., 2024)</a></li><li><a href=#3135--71178-marsseg-mars-surface-semantic-segmentation-with-multi-level-extractor-and-connector-junbo-li-et-al-2024>(31/35 | 71/178) MarsSeg: Mars Surface Semantic Segmentation with Multi-level Extractor and Connector (Junbo Li et al., 2024)</a></li><li><a href=#3235--72178-dynamic-risk-assessment-methodology-with-an-ldm-based-system-for-parking-scenarios-paola-natalia-cañas-et-al-2024>(32/35 | 72/178) Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios (Paola Natalia Cañas et al., 2024)</a></li><li><a href=#3335--73178-finsler-laplace-beltrami-operators-with-application-to-shape-analysis-simon-weber-et-al-2024>(33/35 | 73/178) Finsler-Laplace-Beltrami Operators with Application to Shape Analysis (Simon Weber et al., 2024)</a></li><li><a href=#3435--74178-robust-gaussian-splatting-françois-darmon-et-al-2024>(34/35 | 74/178) Robust Gaussian Splatting (François Darmon et al., 2024)</a></li><li><a href=#3535--75178-noisy-label-processing-for-classification-a-survey-mengting-li-et-al-2024>(35/35 | 75/178) Noisy Label Processing for Classification: A Survey (Mengting Li et al., 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--76178-can-only-llms-do-reasoning-potential-of-small-language-models-in-task-planning-gawon-choi-et-al-2024>(1/15 | 76/178) Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning (Gawon Choi et al., 2024)</a></li><li><a href=#215--77178-voicepilot-harnessing-llms-as-speech-interfaces-for-physically-assistive-robots-akhil-padmanabha-et-al-2024>(2/15 | 77/178) VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots (Akhil Padmanabha et al., 2024)</a></li><li><a href=#315--78178-pomdp-guided-active-force-based-search-for-robotic-insertion-chen-wang-et-al-2024>(3/15 | 78/178) POMDP-Guided Active Force-Based Search for Robotic Insertion (Chen Wang et al., 2024)</a></li><li><a href=#415--79178-hybrid-force-motion-control-with-estimated-surface-normal-for-manufacturing-applications-ehsan-nasiri-et-al-2024>(4/15 | 79/178) Hybrid Force Motion Control with Estimated Surface Normal for Manufacturing Applications (Ehsan Nasiri et al., 2024)</a></li><li><a href=#515--80178-admittance-control-for-adaptive-remote-center-of-motion-in-robotic-laparoscopic-surgery-ehsan-nasiri-et-al-2024>(5/15 | 80/178) Admittance Control for Adaptive Remote Center of Motion in Robotic Laparoscopic Surgery (Ehsan Nasiri et al., 2024)</a></li><li><a href=#615--81178-continual-policy-distillation-of-reinforcement-learning-based-controllers-for-soft-robotic-in-hand-manipulation-lanpei-li-et-al-2024>(6/15 | 81/178) Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation (Lanpei Li et al., 2024)</a></li><li><a href=#715--82178-scaling-motion-forecasting-models-with-ensemble-distillation-scott-ettinger-et-al-2024>(7/15 | 82/178) Scaling Motion Forecasting Models with Ensemble Distillation (Scott Ettinger et al., 2024)</a></li><li><a href=#815--83178-a-ground-mobile-robot-for-autonomous-terrestrial-laser-scanning-based-field-phenotyping-javier-rodriguez-sanchez-et-al-2024>(8/15 | 83/178) A Ground Mobile Robot for Autonomous Terrestrial Laser Scanning-Based Field Phenotyping (Javier Rodriguez-Sanchez et al., 2024)</a></li><li><a href=#915--84178-loss-slam-lightweight-open-set-semantic-simultaneous-localization-and-mapping-kurran-singh-et-al-2024>(9/15 | 84/178) LOSS-SLAM: Lightweight Open-Set Semantic Simultaneous Localization and Mapping (Kurran Singh et al., 2024)</a></li><li><a href=#1015--85178-tooleenet-tool-affordance-6d-pose-estimation-yunlong-wang-et-al-2024>(10/15 | 85/178) ToolEENet: Tool Affordance 6D Pose Estimation (Yunlong Wang et al., 2024)</a></li><li><a href=#1115--86178-designing-robots-to-help-women-martin-cooney-et-al-2024>(11/15 | 86/178) Designing Robots to Help Women (Martin Cooney et al., 2024)</a></li><li><a href=#1215--87178-high-frequency-capacitive-sensing-for-electrohydraulic-soft-actuators-michel-r-vogt-et-al-2024>(12/15 | 87/178) High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators (Michel R. Vogt et al., 2024)</a></li><li><a href=#1315--88178-modeling-kinematic-uncertainty-of-tendon-driven-continuum-robots-via-mixture-density-networks-jordan-thompson-et-al-2024>(13/15 | 88/178) Modeling Kinematic Uncertainty of Tendon-Driven Continuum Robots via Mixture Density Networks (Jordan Thompson et al., 2024)</a></li><li><a href=#1415--89178-multi-modal-perception-for-soft-robotic-interactions-using-generative-models-enrico-donato-et-al-2024>(14/15 | 89/178) Multi-modal perception for soft robotic interactions using generative models (Enrico Donato et al., 2024)</a></li><li><a href=#1515--90178-mm-gaussian-3d-gaussian-based-multi-modal-fusion-for-localization-and-reconstruction-in-unbounded-scenes-chenyang-wu-et-al-2024>(15/15 | 90/178) MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes (Chenyang Wu et al., 2024)</a></li></ul></li><li><a href=#csai-8>cs.AI (8)</a><ul><li><a href=#18--91178-hypothesis-generation-with-large-language-models-yangqiaoyu-zhou-et-al-2024>(1/8 | 91/178) Hypothesis Generation with Large Language Models (Yangqiaoyu Zhou et al., 2024)</a></li><li><a href=#28--92178-exploring-autonomous-agents-through-the-lens-of-large-language-models-a-review-saikat-barua-2024>(2/8 | 92/178) Exploring Autonomous Agents through the Lens of Large Language Models: A Review (Saikat Barua, 2024)</a></li><li><a href=#38--93178-kgexplainer-towards-exploring-connected-subgraph-explanations-for-knowledge-graph-completion-tengfei-ma-et-al-2024>(3/8 | 93/178) KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion (Tengfei Ma et al., 2024)</a></li><li><a href=#48--94178-intervention-assisted-policy-gradient-methods-for-online-stochastic-queuing-network-optimization-technical-report-jerrod-wigmore-et-al-2024>(4/8 | 94/178) Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report (Jerrod Wigmore et al., 2024)</a></li><li><a href=#58--95178-random-walk-in-random-permutation-set-theory-jiefeng-zhou-et-al-2024>(5/8 | 95/178) Random Walk in Random Permutation Set Theory (Jiefeng Zhou et al., 2024)</a></li><li><a href=#68--96178-large-language-models-as-oracles-for-instantiating-ontologies-with-domain-specific-knowledge-giovanni-ciatto-et-al-2024>(6/8 | 96/178) Large language models as oracles for instantiating ontologies with domain-specific knowledge (Giovanni Ciatto et al., 2024)</a></li><li><a href=#78--97178-ai-knowledge-and-reasoning-emulating-expert-creativity-in-scientific-research-anirban-mukherjee-et-al-2024>(7/8 | 97/178) AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific Research (Anirban Mukherjee et al., 2024)</a></li><li><a href=#88--98178-visual-knowledge-in-the-big-model-era-retrospect-and-prospect-wenguan-wang-et-al-2024>(8/8 | 98/178) Visual Knowledge in the Big Model Era: Retrospect and Prospect (Wenguan Wang et al., 2024)</a></li></ul></li><li><a href=#csir-3>cs.IR (3)</a><ul><li><a href=#13--99178-a-comparison-of-methods-for-evaluating-generative-ir-negar-arabzadeh-et-al-2024>(1/3 | 99/178) A Comparison of Methods for Evaluating Generative IR (Negar Arabzadeh et al., 2024)</a></li><li><a href=#23--100178-dwell-in-the-beginning-how-language-models-embed-long-documents-for-dense-retrieval-joão-coelho-et-al-2024>(2/3 | 100/178) Dwell in the Beginning: How Language Models Embed Long Documents for Dense Retrieval (João Coelho et al., 2024)</a></li><li><a href=#33--101178-jobformer-skill-aware-job-recommendation-with-semantic-enhanced-transformer-zhihao-guan-et-al-2024>(3/3 | 101/178) JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced Transformer (Zhihao Guan et al., 2024)</a></li></ul></li><li><a href=#cslg-33>cs.LG (33)</a><ul><li><a href=#133--102178-prompt-public-large-language-models-to-synthesize-data-for-private-on-device-applications-shanshan-wu-et-al-2024>(1/33 | 102/178) Prompt Public Large Language Models to Synthesize Data for Private On-device Applications (Shanshan Wu et al., 2024)</a></li><li><a href=#233--103178-dynamic-switch-layers-for-unsupervised-learning-haiguang-li-et-al-2024>(2/33 | 103/178) Dynamic Switch Layers For Unsupervised Learning (Haiguang Li et al., 2024)</a></li><li><a href=#333--104178-player2vec-a-language-modeling-approach-to-understand-player-behavior-in-games-tianze-wang-et-al-2024>(3/33 | 104/178) player2vec: A Language Modeling Approach to Understand Player Behavior in Games (Tianze Wang et al., 2024)</a></li><li><a href=#433--105178-robust-preference-optimization-with-provable-noise-tolerance-for-llms-xize-liang-et-al-2024>(4/33 | 105/178) Robust Preference Optimization with Provable Noise Tolerance for LLMs (Xize Liang et al., 2024)</a></li><li><a href=#533--106178-enhancing-iot-intelligence-a-transformer-based-reinforcement-learning-methodology-gaith-rjoub-et-al-2024>(5/33 | 106/178) Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology (Gaith Rjoub et al., 2024)</a></li><li><a href=#633--107178-score-identity-distillation-exponentially-fast-distillation-of-pretrained-diffusion-models-for-one-step-generation-mingyuan-zhou-et-al-2024>(6/33 | 107/178) Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation (Mingyuan Zhou et al., 2024)</a></li><li><a href=#733--108178-mitigating-heterogeneity-in-federated-multimodal-learning-with-biomedical-vision-language-pre-training-zitao-shuai-et-al-2024>(7/33 | 108/178) Mitigating Heterogeneity in Federated Multimodal Learning with Biomedical Vision-Language Pre-training (Zitao Shuai et al., 2024)</a></li><li><a href=#833--109178-parameter-efficient-quasi-orthogonal-fine-tuning-via-givens-rotation-xinyu-ma-et-al-2024>(8/33 | 109/178) Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation (Xinyu Ma et al., 2024)</a></li><li><a href=#933--110178-a-real-time-anomaly-detection-using-convolutional-autoencoder-with-dynamic-threshold-sarit-maitra-et-al-2024>(9/33 | 110/178) A Real-time Anomaly Detection Using Convolutional Autoencoder with Dynamic Threshold (Sarit Maitra et al., 2024)</a></li><li><a href=#1033--111178-transformers-for-molecular-property-prediction-lessons-learned-from-the-past-five-years-afnan-sultan-et-al-2024>(10/33 | 111/178) Transformers for molecular property prediction: Lessons learned from the past five years (Afnan Sultan et al., 2024)</a></li><li><a href=#1133--112178-optimizing-convolutional-neural-networks-for-identifying-invasive-pollinator-apis-mellifera-and-finding-a-ligand-drug-to-protect-californias-biodiversity-arnav-swaroop-2024>(11/33 | 112/178) Optimizing Convolutional Neural Networks for Identifying Invasive Pollinator Apis Mellifera and Finding a Ligand drug to Protect California&rsquo;s Biodiversity (Arnav Swaroop, 2024)</a></li><li><a href=#1233--113178-dynamic-conditional-optimal-transport-through-simulation-free-flows-gavin-kerrigan-et-al-2024>(12/33 | 113/178) Dynamic Conditional Optimal Transport through Simulation-Free Flows (Gavin Kerrigan et al., 2024)</a></li><li><a href=#1333--114178-model-selection-with-model-zoo-via-graph-learning-ziyu-li-et-al-2024>(13/33 | 114/178) Model Selection with Model Zoo via Graph Learning (Ziyu Li et al., 2024)</a></li><li><a href=#1433--115178-pixel-wise-rl-on-diffusion-models-reinforcement-learning-from-rich-feedback-mo-kordzanganeh-et-al-2024>(14/33 | 115/178) Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich Feedback (Mo Kordzanganeh et al., 2024)</a></li><li><a href=#1533--116178-exploring-probabilistic-models-for-semi-supervised-learning-jianfeng-wang-2024>(15/33 | 116/178) Exploring Probabilistic Models for Semi-supervised Learning (Jianfeng Wang, 2024)</a></li><li><a href=#1633--117178-fusing-dictionary-learning-and-support-vector-machines-for-unsupervised-anomaly-detection-paul-irofti-et-al-2024>(16/33 | 117/178) Fusing Dictionary Learning and Support Vector Machines for Unsupervised Anomaly Detection (Paul Irofti et al., 2024)</a></li><li><a href=#1733--118178-rolling-the-dice-for-better-deep-learning-performance-a-study-of-randomness-techniques-in-deep-neural-networks-mohammed-ghaith-altarabichi-et-al-2024>(17/33 | 118/178) Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks (Mohammed Ghaith Altarabichi et al., 2024)</a></li><li><a href=#1833--119178-multi-task-learning-for-lung-sound--lung-disease-classification-suma-k-v-et-al-2024>(18/33 | 119/178) Multi-Task Learning for Lung sound & Lung disease classification (Suma K V et al., 2024)</a></li><li><a href=#1933--120178-a-proximal-policy-optimization-based-intelligent-home-solar-management-kode-creer-et-al-2024>(19/33 | 120/178) A proximal policy optimization based intelligent home solar management (Kode Creer et al., 2024)</a></li><li><a href=#2033--121178-heterogeneous-multi-agent-reinforcement-learning-for-zero-shot-scalable-collaboration-xudong-guo-et-al-2024>(20/33 | 121/178) Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration (Xudong Guo et al., 2024)</a></li><li><a href=#2133--122178-gnnbench-fair-and-productive-benchmarking-for-single-gpu-gnn-system-yidong-gong-et-al-2024>(21/33 | 122/178) GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System (Yidong Gong et al., 2024)</a></li><li><a href=#2233--123178-growing-q-networks-solving-continuous-control-tasks-with-adaptive-control-resolution-tim-seyde-et-al-2024>(22/33 | 123/178) Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution (Tim Seyde et al., 2024)</a></li><li><a href=#2333--124178-active-causal-learning-for-decoding-chemical-complexities-with-targeted-interventions-zachary-r-fox-et-al-2024>(23/33 | 124/178) Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions (Zachary R. Fox et al., 2024)</a></li><li><a href=#2433--125178-half-space-feature-learning-in-neural-networks-mahesh-lorik-yadav-et-al-2024>(24/33 | 125/178) Half-Space Feature Learning in Neural Networks (Mahesh Lorik Yadav et al., 2024)</a></li><li><a href=#2533--126178-implicit-bias-of-adamw-ell_infty-norm-constrained-optimization-shuo-xie-et-al-2024>(25/33 | 126/178) Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization (Shuo Xie et al., 2024)</a></li><li><a href=#2633--127178-compositional-estimation-of-lipschitz-constants-for-deep-neural-networks-yuezhu-xu-et-al-2024>(26/33 | 127/178) Compositional Estimation of Lipschitz Constants for Deep Neural Networks (Yuezhu Xu et al., 2024)</a></li><li><a href=#2733--128178-generalizable-temperature-nowcasting-with-physics-constrained-rnns-for-predictive-maintenance-of-wind-turbine-components-johannes-exenberger-et-al-2024>(27/33 | 128/178) Generalizable Temperature Nowcasting with Physics-Constrained RNNs for Predictive Maintenance of Wind Turbine Components (Johannes Exenberger et al., 2024)</a></li><li><a href=#2833--129178-continual-learning-with-weight-interpolation-jędrzej-kozal-et-al-2024>(28/33 | 129/178) Continual Learning with Weight Interpolation (Jędrzej Kozal et al., 2024)</a></li><li><a href=#2933--130178-demonstration-guided-multi-objective-reinforcement-learning-junlin-lu-et-al-2024>(29/33 | 130/178) Demonstration Guided Multi-Objective Reinforcement Learning (Junlin Lu et al., 2024)</a></li><li><a href=#3033--131178-generating-synthetic-ground-truth-distributions-for-multi-step-trajectory-prediction-using-probabilistic-composite-bézier-curves-ronny-hug-et-al-2024>(30/33 | 131/178) Generating Synthetic Ground Truth Distributions for Multi-step Trajectory Prediction using Probabilistic Composite Bézier Curves (Ronny Hug et al., 2024)</a></li><li><a href=#3133--132178-hierarchical-neural-additive-models-for-interpretable-demand-forecasts-leif-feddersen-et-al-2024>(31/33 | 132/178) Hierarchical Neural Additive Models for Interpretable Demand Forecasts (Leif Feddersen et al., 2024)</a></li><li><a href=#3233--133178-derivative-free-tree-optimization-for-complex-systems-ye-wei-et-al-2024>(32/33 | 133/178) Derivative-free tree optimization for complex systems (Ye Wei et al., 2024)</a></li><li><a href=#3333--134178-approximate-umap-allows-for-high-rate-online-visualization-of-high-dimensional-data-streams-peter-wassenaar-et-al-2024>(33/33 | 134/178) Approximate UMAP allows for high-rate online visualization of high-dimensional data streams (Peter Wassenaar et al., 2024)</a></li></ul></li><li><a href=#statml-4>stat.ML (4)</a><ul><li><a href=#14--135178-deeplink-t-deep-learning-inference-for-time-series-data-using-knockoffs-and-lstm-wenxuan-zuo-et-al-2024>(1/4 | 135/178) DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM (Wenxuan Zuo et al., 2024)</a></li><li><a href=#24--136178-longitudinal-targeted-minimum-loss-based-estimation-with-temporal-difference-heterogeneous-transformer-toru-shirakawa-et-al-2024>(2/4 | 136/178) Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer (Toru Shirakawa et al., 2024)</a></li><li><a href=#34--137178-nonparametric-modern-hopfield-models-jerry-yao-chieh-hu-et-al-2024>(3/4 | 137/178) Nonparametric Modern Hopfield Models (Jerry Yao-Chieh Hu et al., 2024)</a></li><li><a href=#44--138178-bayesian-additive-regression-networks-danielle-van-boxel-2024>(4/4 | 138/178) Bayesian Additive Regression Networks (Danielle Van Boxel, 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--139178-it-is-okay-to-be-uncommon-quantizing-sound-event-detection-networks-on-hardware-accelerators-with-uncommon-sub-byte-support-yushu-wu-et-al-2024>(1/2 | 139/178) &lsquo;It is okay to be uncommon&rsquo;: Quantizing Sound Event Detection Networks on Hardware Accelerators with Uncommon Sub-Byte Support (Yushu Wu et al., 2024)</a></li><li><a href=#22--140178-the-nes-video-music-database-a-dataset-of-symbolic-video-game-music-paired-with-gameplay-videos-igor-cardoso-et-al-2024>(2/2 | 140/178) The NES Video-Music Database: A Dataset of Symbolic Video Game Music Paired with Gameplay Videos (Igor Cardoso et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--141178-open-vocabulary-keyword-spotting-through-transfer-learning-from-speech-synthesis-kesavaraj-v-et-al-2024>(1/4 | 141/178) Open vocabulary keyword spotting through transfer learning from speech synthesis (Kesavaraj V et al., 2024)</a></li><li><a href=#24--142178-which-experimental-design-is-better-suited-for-vqa-tasks-eye-tracking-study-on-cognitive-load-performance-and-gaze-allocations-sita-a-vriend-et-al-2024>(2/4 | 142/178) Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations (Sita A. Vriend et al., 2024)</a></li><li><a href=#34--143178-hiv-client-perspectives-on-digital-health-in-malawi-lisa-orii-et-al-2024>(3/4 | 143/178) HIV Client Perspectives on Digital Health in Malawi (Lisa Orii et al., 2024)</a></li><li><a href=#44--144178-effects-of-multisensory-feedback-on-the-perception-and-performance-of-virtual-reality-hand-retargeted-interaction-hyunyoung-jang-et-al-2024>(4/4 | 144/178) Effects of Multisensory Feedback on the Perception and Performance of Virtual Reality Hand-Retargeted Interaction (Hyunyoung Jang et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--145178-superior-genetic-algorithms-for-the-target-set-selection-problem-based-on-power-law-parameter-choices-and-simple-greedy-heuristics-benjamin-doerr-et-al-2024>(1/2 | 145/178) Superior Genetic Algorithms for the Target Set Selection Problem Based on Power-Law Parameter Choices and Simple Greedy Heuristics (Benjamin Doerr et al., 2024)</a></li><li><a href=#22--146178-mining-potentially-explanatory-patterns-via-partial-solutions-giancarlo-catalano-et-al-2024>(2/2 | 146/178) Mining Potentially Explanatory Patterns via Partial Solutions (GianCarlo Catalano et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--147178-stable-blockchain-sharding-under-adversarial-transaction-generation-ramesh-adhikari-et-al-2024>(1/2 | 147/178) Stable Blockchain Sharding under Adversarial Transaction Generation (Ramesh Adhikari et al., 2024)</a></li><li><a href=#22--148178-evaluation-of-programming-models-and-performance-for-stencil-computation-on-current-gpu-architectures-baodi-shan-et-al-2024>(2/2 | 148/178) Evaluation of Programming Models and Performance for Stencil Computation on Current GPU Architectures (Baodi Shan et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--149178-nonlinear-kalman-filtering-based-on-self-attention-mechanism-and-lattice-trajectory-piecewise-linear-approximation-jiaming-wang-et-al-2024>(1/5 | 149/178) Nonlinear Kalman Filtering based on Self-Attention Mechanism and Lattice Trajectory Piecewise Linear Approximation (Jiaming Wang et al., 2024)</a></li><li><a href=#25--150178-torque-minimizing-control-allocation-for-overactuated-quadrupedal-locomotion-mads-erlend-bøe-lysø-et-al-2024>(2/5 | 150/178) Torque-Minimizing Control Allocation for Overactuated Quadrupedal Locomotion (Mads Erlend Bøe Lysø et al., 2024)</a></li><li><a href=#35--151178-optimal-policy-synthesis-from-a-sequence-of-goal-sets-with-an-application-to-electric-distribution-system-restoration-ilker-işık-et-al-2024>(3/5 | 151/178) Optimal Policy Synthesis from A Sequence of Goal Sets with An Application to Electric Distribution System Restoration (İlker Işık et al., 2024)</a></li><li><a href=#45--152178-field-teams-coordination-for-earthquake-damaged-distribution-system-energization-ilker-işık-et-al-2024>(4/5 | 152/178) Field Teams Coordination for Earthquake-Damaged Distribution System Energization (İlker Işık et al., 2024)</a></li><li><a href=#55--153178-queue-aware-network-control-algorithm-with-a-high-quantum-computing-readiness-evaluated-in-discrete-time-flow-simulator-for-fat-pipe-networks-arthur-witt-2024>(5/5 | 153/178) Queue-aware Network Control Algorithm with a High Quantum Computing Readiness-Evaluated in Discrete-time Flow Simulator for Fat-Pipe Networks (Arthur Witt, 2024)</a></li></ul></li><li><a href=#physicsdata-an-1>physics.data-an (1)</a><ul><li><a href=#11--154178-physics-event-classification-using-large-language-models-cristiano-fanelli-et-al-2024>(1/1 | 154/178) Physics Event Classification Using Large Language Models (Cristiano Fanelli et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--155178-low-rank-robust-subspace-tensor-clustering-for-metro-passenger-flow-modeling-jiuyun-hu-et-al-2024>(1/1 | 155/178) Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow Modeling (Jiuyun Hu et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--156178-quantum-informed-simulations-for-mechanics-of-materials-dftbmbd-framework-zhaoxiang-shen-et-al-2024>(1/1 | 156/178) Quantum-informed simulations for mechanics of materials: DFTB+MBD framework (Zhaoxiang Shen et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--157178-a-posteriori-error-analysis-of-a-space-time-hybridizable-discontinuous-galerkin-method-for-the-advection-diffusion-problem-yuan-wang-et-al-2024>(1/2 | 157/178) A posteriori error analysis of a space-time hybridizable discontinuous Galerkin method for the advection-diffusion problem (Yuan Wang et al., 2024)</a></li><li><a href=#22--158178-highly-efficient-nurbs-based-isogeometric-analysis-for-coupled-nonlinear-diffusion-reaction-equations-with-and-without-advection-ilham-asmouh-et-al-2024>(2/2 | 158/178) Highly efficient NURBS-based isogeometric analysis for coupled nonlinear diffusion-reaction equations with and without advection (Ilham Asmouh et al., 2024)</a></li></ul></li><li><a href=#csse-1>cs.SE (1)</a><ul><li><a href=#11--159178-pros-and-cons-evaluating-chatgpt-on-software-vulnerability-xin-yin-2024>(1/1 | 159/178) Pros and Cons! Evaluating ChatGPT on Software Vulnerability (Xin Yin, 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--160178-quand-rechercher-cest-faire-des-vagues--dans-et-à-partir-des-images-algorithmiques-gaëtan-robillard-2024>(1/1 | 160/178) Quand rechercher c&rsquo;est faire des vagues : Dans et {à} partir des images algorithmiques (Gaëtan Robillard, 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--161178-deep-phase-coded-image-prior-nimrod-shabtay-et-al-2024>(1/2 | 161/178) Deep Phase Coded Image Prior (Nimrod Shabtay et al., 2024)</a></li><li><a href=#22--162178-lidar-guided-cross-attention-fusion-for-hyperspectral-band-selection-and-image-classification-judy-x-yang-et-al-2024>(2/2 | 162/178) LiDAR-Guided Cross-Attention Fusion for Hyperspectral Band Selection and Image Classification (Judy X Yang et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--163178-semantic-sql----combining-and-optimizing-semantic-predicates-in-sql-akash-mittal-et-al-2024>(1/1 | 163/178) Semantic SQL &ndash; Combining and optimizing semantic predicates in SQL (Akash Mittal et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--164178-simplifying-explicit-subtyping-coercions-in-a-polymorphic-calculus-with-effects-filip-koprivec-et-al-2024>(1/2 | 164/178) Simplifying explicit subtyping coercions in a polymorphic calculus with effects (Filip Koprivec et al., 2024)</a></li><li><a href=#22--165178-v-star-learning-visibly-pushdown-grammars-from-program-inputs-xiaodong-jia-et-al-2024>(2/2 | 165/178) V-Star: Learning Visibly Pushdown Grammars from Program Inputs (Xiaodong Jia et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--166178-algorithmic-fairness-and-social-welfare-annie-liang-et-al-2024>(1/1 | 166/178) Algorithmic Fairness and Social Welfare (Annie Liang et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--167178-counting-like-transformers-compiling-temporal-counting-logic-into-softmax-transformers-andy-yang-et-al-2024>(1/1 | 167/178) Counting Like Transformers: Compiling Temporal Counting Logic Into Softmax Transformers (Andy Yang et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--168178-h3dfact-heterogeneous-3d-integrated-cim-for-factorization-with-holographic-perceptual-representations-zishen-wan-et-al-2024>(1/1 | 168/178) H3DFact: Heterogeneous 3D Integrated CIM for Factorization with Holographic Perceptual Representations (Zishen Wan et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--169178-on-the-quest-for-effectiveness-in-human-oversight-interdisciplinary-perspectives-sarah-sterz-et-al-2024>(1/2 | 169/178) On the Quest for Effectiveness in Human Oversight: Interdisciplinary Perspectives (Sarah Sterz et al., 2024)</a></li><li><a href=#22--170178-a-conceptual-design-of-in-game-real-and-virtual-currency-tracker-dennis-barzanoff-et-al-2024>(2/2 | 170/178) A Conceptual Design of In-Game Real and Virtual Currency Tracker (Dennis Barzanoff et al., 2024)</a></li></ul></li><li><a href=#nlinps-1>nlin.PS (1)</a><ul><li><a href=#11--171178-suppressing-modulation-instability-with-reinforcement-learning-nikolay-kalmykov-et-al-2024>(1/1 | 171/178) Suppressing Modulation Instability with Reinforcement Learning (Nikolay Kalmykov et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--172178-a-fine-grained-classification-of-subquadratic-patterns-for-subgraph-listing-and-friends-karl-bringmann-et-al-2024>(1/3 | 172/178) A Fine-grained Classification of Subquadratic Patterns for Subgraph Listing and Friends (Karl Bringmann et al., 2024)</a></li><li><a href=#23--173178-stability-in-graphs-with-matroid-constraints-fedor-v-fomin-et-al-2024>(2/3 | 173/178) Stability in Graphs with Matroid Constraints (Fedor V. Fomin et al., 2024)</a></li><li><a href=#33--174178-minor-containment-and-disjoint-paths-in-almost-linear-time-tuukka-korhonen-et-al-2024>(3/3 | 174/178) Minor Containment and Disjoint Paths in almost-linear time (Tuukka Korhonen et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--175178-wireless-resource-optimization-in-hybrid-semanticbit-communication-networks-le-xia-et-al-2024>(1/1 | 175/178) Wireless Resource Optimization in Hybrid Semantic/Bit Communication Networks (Le Xia et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--176178-hardness-of-circuit-and-monotone-diameters-of-polytopes-christian-nöbel-et-al-2024>(1/1 | 176/178) Hardness of circuit and monotone diameters of polytopes (Christian Nöbel et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--177178-discrete-fréchet-distance-oracles-boris-aronov-et-al-2024>(1/1 | 177/178) Discrete Fréchet Distance Oracles (Boris Aronov et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--178178-the-low-degree-hardness-of-finding-large-independent-sets-in-sparse-random-hypergraphs-abhishek-dhawan-et-al-2024>(1/1 | 178/178) The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs (Abhishek Dhawan et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>