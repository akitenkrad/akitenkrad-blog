<!doctype html><html><head><title>arXiv @ 2024.04.05</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.04.05"><meta property="og:description" content="Primary Categories astro-ph.SR (1) cs.AI (7) cs.CL (60) cs.CR (7) cs.CV (50) cs.CY (3) cs.DB (1) cs.DC (4) cs.DM (1) cs.ET (1) cs.HC (6) cs.IR (5) cs.IT (6) cs.LG (48) cs.MA (2) cs.NI (3) cs.PL (1) cs.RO (10) cs.SD (1) cs.SE (5) cs.SI (1) econ.GN (1) eess.AS (1) eess.IV (4) eess.SP (2) eess.SY (8) math.CO (2) math.CT (1) math.NA (5) math.OC (4) physics.data-an (1) quant-ph (2) stat.AP (1) stat.CO (1) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240405000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-05T00:00:00+00:00"><meta name=description content="arXiv @ 2024.04.05"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240405000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Apr 5, 2024</p></div><div class=title><h1>arXiv @ 2024.04.05</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csai-7>cs.AI (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cscl-60>cs.CL (60)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cscv-50>cs.CV (50)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csdc-4>cs.DC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cslg-48>cs.LG (48)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csro-10>cs.RO (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#econgn-1>econ.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#eesssy-8>eess.SY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#mathct-1>math.CT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#mathoc-4>math.OC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#physicsdata-an-1>physics.data-an (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#statap-1>stat.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#statco-1>stat.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Alpaca</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>5</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>4</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Benchmarking</td><td>14</td><td>12</td><td>9</td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Constrained Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>2</td><td>2</td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>6</td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>7</td><td>5</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Data Augmentation</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>6</td><td>2</td><td></td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Disambiguation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Event Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fact Verification</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fake News Detection</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>5</td><td></td></tr><tr><td>Few-shot</td><td>6</td><td>2</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>14</td><td>4</td><td>10</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>1</td><td>4</td><td></td></tr><tr><td>GPT</td><td>9</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3</td><td>5</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>5</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>6</td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Geometry</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Graph</td><td>3</td><td>2</td><td>4</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Grounding</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>14</td><td></td><td>3</td><td></td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td>9</td><td>6</td><td></td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>2</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Large Language Model</td><td>51</td><td>10</td><td>16</td><td>1</td></tr><tr><td>Low-Resource</td><td>7</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Model Compression</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>6</td><td>1</td><td>2</td></tr><tr><td>Natural Language Inference</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>10</td><td></td><td>1</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>8</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>11</td><td>6</td><td></td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Quantization</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>9</td><td>1</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>7</td><td>3</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Rouge</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Scaling Law</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td>1</td><td>3</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>2</td><td>5</td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>1</td><td>6</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>1</td><td>6</td></tr><tr><td>Summarization</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>SuperGLUE</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>7</td><td>4</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td>1</td><td>7</td><td></td><td></td></tr><tr><td>Tokenization</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>5</td><td>10</td><td>7</td><td></td></tr><tr><td>Unsupervised Learning</td><td>3</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>7</td><td>2</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Word Sense Disambiguation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>5</td><td>1</td><td>1</td><td></td></tr><tr><td>Zero-shot Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-60>cs.CL (60)</h2><h3 id=160--1258-gpt-detox-an-in-context-learning-based-paraphraser-for-text-detoxification-ali-pesaranghader-et-al-2024>(1/60 | 1/258) GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification (Ali Pesaranghader et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Pesaranghader, Nikhil Verma, Manasa Bharadwaj. (2024)<br><strong>GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification</strong><br><button class=copy-to-clipboard title="GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 123<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Supervised Learning, Unsupervised Learning, Unsupervised Learning, Zero-shot, GPT, GPT-3, GPT-3.5, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03052v1.pdf filename=2404.03052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. <b>Supervised</b> and <b>unsupervised</b> <b>learning</b> are common approaches for designing text detoxification solutions. However, these methods necessitate <b>fine-tuning,</b> leading to computational overhead. In this paper, we propose <b>GPT-DETOX</b> as a framework for <b>prompt-based</b> <b>in-context</b> <b>learning</b> for text detoxification using <b>GPT-3.5</b> Turbo. We utilize <b>zero-shot</b> and <b>few-shot</b> <b>prompting</b> techniques for detoxifying input sentences. To generate <b>few-shot</b> <b>prompts,</b> we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble <b>in-context</b> <b>learning</b> (EICL) where the ensemble is shaped by base <b>prompts</b> from <b>zero-shot</b> and all <b>few-shot</b> settings. We use ParaDetox and APPDIA as <b>benchmark</b> detoxification datasets. Our experimental results show that the <b>zero-shot</b> solution achieves promising performance, while our best <b>few-shot</b> setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement, against both datasets.</p></p class="citation"></blockquote><h3 id=260--2258-utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers-pouya-sadeghi-et-al-2024>(2/60 | 2/258) uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers? (Pouya Sadeghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouya Sadeghi, Amirhossein Abaskohi, Yadollah Yaghoobzadeh. (2024)<br><strong>uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?</strong><br><button class=copy-to-clipboard title="uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 123<br>Keywords: Benchmarking, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, GPT-4, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02474v1.pdf filename=2404.02474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by human cognition, Jiang et al.(2023c) create a <b>benchmark</b> for assessing <b>LLMs&rsquo;</b> lateral thinking-thinking outside the box. Building upon this <b>benchmark,</b> we investigate how different <b>prompting</b> methods enhance <b>LLMs&rsquo;</b> performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore <b>prompt</b> engineering methods: chain of thoughts (CoT) and direct <b>prompting,</b> enhancing with informative descriptions, and employing contextualizing <b>prompts</b> using a <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> pipeline. Our experiments involve three <b>LLMs</b> including <b>GPT-3.5,</b> <b>GPT-4,</b> and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using <b>GPT-4,</b> validated by humans for quality. Findings indicate that compressed informative <b>prompts</b> enhance performance. Dynamic <b>in-context</b> <b>learning</b> enhances model performance significantly. Furthermore, <b>fine-tuning</b> Zephyr on our dataset enhances performance across other commonsense datasets, underscoring the value of innovative thinking.</p></p class="citation"></blockquote><h3 id=360--3258-benchmarking-large-language-models-for-persian-a-preliminary-study-focusing-on-chatgpt-amirhossein-abaskohi-et-al-2024>(3/60 | 3/258) Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT (Amirhossein Abaskohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amirhossein Abaskohi, Sara Baruni, Mostafa Masoudi, Nesa Abbasi, Mohammad Hadi Babalou, Ali Edalat, Sepehr Kamahi, Samin Mahdizadeh Sani, Nikoo Naghavian, Danial Namazifard, Pouya Sadeghi, Yadollah Yaghoobzadeh. (2024)<br><strong>Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT</strong><br><button class=copy-to-clipboard title="Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 106<br>Keywords: Benchmarking, Benchmarking, Fine-tuning, Low-Resource, ChatGPT, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02403v1.pdf filename=2404.02403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the efficacy of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for Persian. While <b>ChatGPT</b> and consequent <b>LLMs</b> have shown remarkable performance in English, their efficiency for more <b>low-resource</b> languages remains an open question. We present the first comprehensive <b>benchmarking</b> study of <b>LLMs</b> across diverse Persian language tasks. Our primary focus is on <b>GPT-3.5-turbo,</b> but we also include <b>GPT-4</b> and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, <b>reasoning,</b> and knowledge-based domains. To enable a thorough comparison, we evaluate <b>LLMs</b> against existing task-specific <b>fine-tuned</b> models. Given the limited availability of Persian datasets for <b>reasoning</b> tasks, we introduce two new <b>benchmarks:</b> one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while <b>LLMs,</b> especially <b>GPT-4,</b> excel in tasks requiring <b>reasoning</b> abilities and a broad understanding of general knowledge, they often lag behind smaller pre-trained models <b>fine-tuned</b> specifically for particular tasks. Additionally, we observe improved performance when test sets are translated to English before inputting them into <b>GPT-3.5.</b> These results highlight the significant potential for enhancing <b>LLM</b> performance in the Persian language. This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles.</p></p class="citation"></blockquote><h3 id=460--4258-enhancing-low-resource-llms-classification-with-peft-and-synthetic-data-parth-patwa-et-al-2024>(4/60 | 4/258) Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data (Parth Patwa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parth Patwa, Simone Filice, Zhiyu Chen, Giuseppe Castellucci, Oleg Rokhlenko, Shervin Malmasi. (2024)<br><strong>Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data</strong><br><button class=copy-to-clipboard title="Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Fine-tuning, Low-Resource, Text Classification, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02422v1.pdf filename=2404.02422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> operating in 0-shot or <b>few-shot</b> settings achieve competitive results in <b>Text</b> <b>Classification</b> tasks. <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input <b>prompt.</b> In this paper, we propose a strategy to make <b>LLMs</b> as efficient as 0-shot <b>text</b> <b>classifiers,</b> while getting comparable or better accuracy than <b>ICL.</b> Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single <b>LLM</b> and <b>few-shot</b> real data we perform a sequence of generation, filtering and Parameter-Efficient <b>Fine-Tuning</b> steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple <b>text</b> <b>classification</b> datasets.</p></p class="citation"></blockquote><h3 id=560--5258-an-incomplete-loop-deductive-inductive-and-abductive-learning-in-large-language-models-emmy-liu-et-al-2024>(5/60 | 5/258) An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models (Emmy Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emmy Liu, Graham Neubig, Jacob Andreas. (2024)<br><strong>An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models</strong><br><button class=copy-to-clipboard title="An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, GPT, LLaMA, Instruction Following, Neural Machine Translation, Reasoning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03028v1.pdf filename=2404.03028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern language models (LMs) can learn to perform new tasks in different ways: in <b>instruction</b> <b>following,</b> the target task is described explicitly in natural language; in <b>few-shot</b> <b>prompting,</b> the task is specified implicitly with a small number of examples; in <b>instruction</b> <b>inference,</b> LMs are presented with <b>in-context</b> examples and are then <b>prompted</b> to generate a natural language task description before making predictions. Each of these procedures may be thought of as invoking a different form of <b>reasoning:</b> <b>instruction</b> <b>following</b> involves deductive <b>reasoning,</b> <b>few-shot</b> <b>prompting</b> involves inductive <b>reasoning,</b> and <b>instruction</b> <b>inference</b> involves abductive <b>reasoning.</b> How do these different capabilities relate? Across four LMs (from the <b>gpt</b> and <b>llama</b> families) and two learning problems (involving arithmetic functions and <b>machine</b> <b>translation)</b> we find a strong dissociation between the different types of <b>reasoning:</b> LMs can sometimes learn effectively from <b>few-shot</b> <b>prompts</b> even when they are unable to explain their own prediction rules; conversely, they sometimes infer useful task descriptions while completely failing to learn from human-generated descriptions of the same task. Our results highlight the non-systematic nature of <b>reasoning</b> even in some of today&rsquo;s largest LMs, and underscore the fact that very different learning mechanisms may be invoked by seemingly similar <b>prompting</b> procedures.</p></p class="citation"></blockquote><h3 id=660--6258-towards-large-language-model-driven-reference-less-translation-evaluation-for-english-and-indian-languages-vandan-mujadia-et-al-2024>(6/60 | 6/258) Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages (Vandan Mujadia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vandan Mujadia, Pruthwik Mishra, Arafat Ahsan, Dipti Misra Sharma. (2024)<br><strong>Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages</strong><br><button class=copy-to-clipboard title="Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Zero-shot, BERT, LLaMA, In-context Learning, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02512v1.pdf filename=2404.02512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the primary focus on evaluating the effectiveness of <b>large</b> <b>language</b> <b>models</b> for automatic reference-less translation assessment, this work presents our experiments on mimicking human direct assessment to evaluate the quality of translations in English and Indian languages. We constructed a translation evaluation task where we performed <b>zero-shot</b> <b>learning,</b> <b>in-context</b> example-driven learning, and <b>fine-tuning</b> of <b>large</b> <b>language</b> <b>models</b> to provide a score out of 100, where 100 represents a perfect translation and 1 represents a poor translation. We compared the performance of our trained systems with existing methods such as COMET, <b>BERT-Scorer,</b> and LABSE, and found that the <b>LLM-based</b> evaluator <b>(LLaMA-2-13B)</b> achieves a comparable or higher overall correlation with human judgments for the considered Indian language pairs.</p></p class="citation"></blockquote><h3 id=760--7258-low-resource-neural-machine-translation-with-morphological-modeling-antoine-nzeyimana-2024>(7/60 | 7/258) Low-resource neural machine translation with morphological modeling (Antoine Nzeyimana, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Nzeyimana. (2024)<br><strong>Low-resource neural machine translation with morphological modeling</strong><br><button class=copy-to-clipboard title="Low-resource neural machine translation with morphological modeling" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Data Augmentation, Low-Resource, Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02392v1.pdf filename=2404.02392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Morphological modeling in <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> is a promising approach to achieving open-vocabulary <b>machine</b> <b>translation</b> for morphologically-rich languages. However, existing methods such as sub-word <b>tokenization</b> and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in <b>low-resource</b> settings. A two-tier <b>transformer</b> architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve <b>machine</b> <b>translation</b> performance. An attention augmentation scheme to the <b>transformer</b> model is proposed in a generic form to allow integration of <b>pre-trained</b> <b>language</b> <b>models</b> and also facilitate modeling of word order relationships between the source and target languages. Several <b>data</b> <b>augmentation</b> techniques are evaluated and shown to increase translation performance in <b>low-resource</b> settings. We evaluate our proposed solution on Kinyarwanda - English translation using public-domain parallel text. Our final models achieve competitive performance in relation to large multi-lingual models. We hope that our results will motivate more use of explicit morphological information and the proposed model and <b>data</b> <b>augmentations</b> in <b>low-resource</b> <b>NMT.</b></p></p class="citation"></blockquote><h3 id=860--8258-fpt-feature-prompt-tuning-for-few-shot-readability-assessment-ziyang-wang-et-al-2024>(8/60 | 8/258) FPT: Feature Prompt Tuning for Few-shot Readability Assessment (Ziyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Wang, Sanwoo Lee, Hsiu-Yuan Huang, Yunfang Wu. (2024)<br><strong>FPT: Feature Prompt Tuning for Few-shot Readability Assessment</strong><br><button class=copy-to-clipboard title="FPT: Feature Prompt Tuning for Few-shot Readability Assessment" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, GPT, GPT-3, GPT-3.5, Text Classification, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02772v1.pdf filename=2404.02772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-based</b> methods have achieved promising results in most <b>few-shot</b> <b>text</b> <b>classification</b> tasks. However, for readability assessment tasks, traditional <b>prompt</b> methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in <b>few-shot</b> settings and may even impair model performance.To address these issues, we propose a novel <b>prompt-based</b> tuning framework that incorporates rich linguistic knowledge, called Feature <b>Prompt</b> Tuning (FPT). Specifically, we extract linguistic features from the <b>text</b> <b>and</b> embed them into trainable soft <b>prompts.</b> Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best <b>prompt-based</b> tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features. Also, our proposed model significantly outperforms the <b>large</b> <b>language</b> <b>model</b> <b>gpt-3.5-turbo-16k</b> in most cases. Our proposed method establishes a new architecture for <b>prompt</b> tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks.</p></p class="citation"></blockquote><h3 id=960--9258-rethinking-kullback-leibler-divergence-in-knowledge-distillation-for-large-language-models-taiqiang-wu-et-al-2024>(9/60 | 9/258) Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models (Taiqiang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai Wong. (2024)<br><strong>Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models</strong><br><button class=copy-to-clipboard title="Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02657v1.pdf filename=2404.02657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Kullback-Leiber divergence has been widely used in <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> to compress <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in <b>KD</b> for <b>LLMs.</b> Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, <b>LLMs</b> are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine FKL and RKL. Metric-based and <b>GPT-4-based</b> evaluations demonstrate that the proposed AKL outperforms the baselines across various tasks and improves the diversity and quality of generated responses.</p></p class="citation"></blockquote><h3 id=1060--10258-knowhalu-hallucination-detection-via-multi-form-knowledge-based-factual-checking-jiawei-zhang-et-al-2024>(10/60 | 10/258) KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking (Jiawei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, Bo Li. (2024)<br><strong>KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking</strong><br><button class=copy-to-clipboard title="KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fact Verification, Hallucination Detection, Question Answering, Reasoning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02935v1.pdf filename=2404.02935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces KnowHalu, a novel approach for detecting <b>hallucinations</b> <b>in</b> text generated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> utilizing step-wise <b>reasoning,</b> multi-formulation query, multi-form knowledge for factual checking, and fusion-based detection mechanism. As <b>LLMs</b> are increasingly applied across various domains, ensuring that their outputs are not hallucinated is critical. Recognizing the limitations of existing approaches that either rely on the self-consistency check of <b>LLMs</b> or perform post-hoc <b>fact-checking</b> <b>without</b> considering the complexity of queries or the form of knowledge, KnowHalu proposes a two-phase process for <b>hallucination</b> <b>detection.</b> In the first phase, it identifies non-fabrication <b>hallucinations&ndash;responses</b> <b>that,</b> while factually correct, are irrelevant or non-specific to the query. The second phase, multi-form based factual checking, contains five key steps: <b>reasoning</b> and query decomposition, knowledge retrieval, knowledge optimization, judgment generation, and judgment aggregation. Our extensive evaluations demonstrate that KnowHalu significantly outperforms SOTA baselines in detecting <b>hallucinations</b> <b>across</b> diverse tasks, e.g., improving by 15.65% in <b>QA</b> tasks and 5.50% in <b>summarization</b> tasks, highlighting its effectiveness and versatility in detecting <b>hallucinations</b> <b>in</b> <b>LLM-generated</b> content.</p></p class="citation"></blockquote><h3 id=1160--11258-conifer-improving-complex-constrained-instruction-following-ability-of-large-language-models-haoran-sun-et-al-2024>(11/60 | 11/258) Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models (Haoran Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang. (2024)<br><strong>Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models</strong><br><button class=copy-to-clipboard title="Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Instruction Following, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02823v1.pdf filename=2404.02823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to follow <b>instructions</b> <b>is</b> crucial to real-world applications. Despite recent advances, several studies have highlighted that <b>LLMs</b> struggle when faced with challenging <b>instructions,</b> <b>especially</b> those that include complex constraints, hindering their effectiveness in various tasks. To address this challenge, we introduce Conifer, a novel <b>instruction</b> <b>tuning</b> dataset, designed to enhance <b>LLMs</b> to follow multi-level <b>instructions</b> <b>with</b> complex constraints. Utilizing <b>GPT-4,</b> we curate the dataset by a series of <b>LLM-driven</b> refinement processes to ensure high quality. We also propose a progressive learning scheme that emphasizes an easy-to-hard progression, and learning from process feedback. Models trained with Conifer exhibit remarkable improvements in <b>instruction-following</b> <b>abilities,</b> especially for <b>instructions</b> <b>with</b> complex constraints. On several <b>instruction-following</b> <b>benchmarks,</b> our 7B model outperforms the state-of-the-art open-source 7B models, even exceeds the performance of models 10 times larger on certain metrics. All the code and Conifer dataset are available at <a href=https://www.github.com/ConiferLM/Conifer>https://www.github.com/ConiferLM/Conifer</a>.</p></p class="citation"></blockquote><h3 id=1260--12258-adaptive-cross-lingual-text-classification-through-in-context-one-shot-demonstrations-emilio-villa-cueva-et-al-2024>(12/60 | 12/258) Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations (Emilio Villa-Cueva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emilio Villa-Cueva, A. Pastor López-Monroy, Fernando Sánchez-Vega, Thamar Solorio. (2024)<br><strong>Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations</strong><br><button class=copy-to-clipboard title="Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Zero-shot, Text Classification, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02452v1.pdf filename=2404.02452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-Shot</b> Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit <b>In-Context</b> Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing <b>In-Context</b> Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming <b>prompt-based</b> models in the Zero and <b>Few-shot</b> scenarios adapted through <b>fine-tuning.</b> Moreover, we show that when source-language data is limited, the <b>fine-tuning</b> framework employed for IC-XLT performs comparably to <b>prompt-based</b> <b>fine-tuning</b> with significantly more training data in the source language.</p></p class="citation"></blockquote><h3 id=1360--13258-on-linearizing-structured-data-in-encoder-decoder-language-models-insights-from-text-to-sql-yutong-shao-et-al-2024>(13/60 | 13/258) On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL (Yutong Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutong Shao, Ndapa Nakashole. (2024)<br><strong>On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL</strong><br><button class=copy-to-clipboard title="On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Model Compression, T5, Text2SQL, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02389v1.pdf filename=2404.02389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured data, prevalent in tables, databases, and <b>knowledge</b> <b>graphs,</b> poses a significant challenge in its representation. With the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly <b>model</b> <b>structure,</b> often as a <b>graph.</b> Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language <b>models,</b> <b>specifically</b> <b>T5.</b> Our findings reveal the <b>model&rsquo;s</b> <b>ability</b> to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the <b>model&rsquo;s</b> <b>internal</b> mechanisms, including the ego-centric nature of structure node encodings and the potential for <b>model</b> <b>compression</b> due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research.</p></p class="citation"></blockquote><h3 id=1460--14258-automatic-prompt-selection-for-large-language-models-viet-tung-do-et-al-2024>(14/60 | 14/258) Automatic Prompt Selection for Large Language Models (Viet-Tung Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viet-Tung Do, Van-Khanh Hoang, Duy-Hung Nguyen, Shahab Sabahi, Jeff Yang, Hajime Hotta, Minh-Tien Nguyen, Hung Le. (2024)<br><strong>Automatic Prompt Selection for Large Language Models</strong><br><button class=copy-to-clipboard title="Automatic Prompt Selection for Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Clustering, Zero-shot, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02717v1.pdf filename=2404.02717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can perform various natural language processing tasks with suitable instruction <b>prompts.</b> However, designing effective <b>prompts</b> manually is challenging and time-consuming. Existing methods for automatic <b>prompt</b> optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal <b>prompt</b> for a given input from a finite set of synthetic candidate <b>prompts.</b> Our approach consists of three steps: (1) <b>clustering</b> the training data and generating candidate <b>prompts</b> for each cluster using an <b>LLM-based</b> <b>prompt</b> generator; (2) synthesizing a dataset of input-prompt-output tuples for training a <b>prompt</b> evaluator to rank the <b>prompts</b> based on their relevance to the input; (3) using the <b>prompt</b> evaluator to select the best <b>prompt</b> for a new input at test time. Our approach balances <b>prompt</b> generality-specificity and eliminates the need for resource-intensive training and inference. It demonstrates competitive performance on <b>zero-shot</b> <b>question-answering</b> <b>datasets:</b> GSM8K, MultiArith, and AQuA.</p></p class="citation"></blockquote><h3 id=1560--15258-pejorativity-disambiguating-pejorative-epithets-to-improve-misogyny-detection-in-italian-tweets-arianna-muti-et-al-2024>(15/60 | 15/258) PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets (Arianna Muti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arianna Muti, Federico Ruggeri, Cagri Toraman, Lorenzo Musetti, Samuel Algherini, Silvia Ronchi, Gianmarco Saretto, Caterina Zapparoli, Alberto Barrón-Cedeño. (2024)<br><strong>PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets</strong><br><button class=copy-to-clipboard title="PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Disambiguation, Word Sense Disambiguation, Large Language Model, Prompt, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02681v1.pdf filename=2404.02681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Misogyny is often expressed through figurative language. Some neutral <b>words</b> <b>can</b> <b>assume</b> a negative connotation when functioning as pejorative epithets. Disambiguating the meaning of such terms might help the detection of misogyny. In order to address such task, we present PejorativITy, a novel corpus of 1,200 manually annotated Italian tweets for pejorative language at the <b>word</b> <b>level</b> <b>and</b> misogyny at the sentence level. We evaluate the impact of injecting information about disambiguated <b>words</b> <b>into</b> <b>a</b> model targeting misogyny detection. In particular, we explore two different approaches for injection: concatenation of pejorative information and substitution of ambiguous <b>words</b> <b>with</b> <b>univocal</b> terms. Our experimental results, both on our corpus and on two popular <b>benchmarks</b> on Italian tweets, show that both approaches lead to a major classification improvement, indicating that <b>word</b> <b>sense</b> <b>disambiguation</b> is a promising preliminary step for misogyny detection. Furthermore, we investigate <b>LLMs&rsquo;</b> understanding of pejorative epithets by means of contextual <b>word</b> <b>embeddings</b> <b>analysis</b> and <b>prompting.</b></p></p class="citation"></blockquote><h3 id=1660--16258-large-language-models-for-expansion-of-spoken-language-understanding-systems-to-new-languages-jakub-hoscilowicz-et-al-2024>(16/60 | 16/258) Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages (Jakub Hoscilowicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Hoscilowicz, Pawel Pawlowski, Marcin Skorupa, Marcin Sowański, Artur Janicki. (2024)<br><strong>Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages</strong><br><button class=copy-to-clipboard title="Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02588v1.pdf filename=2404.02588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that we <b>fine-tune</b> for <b>machine</b> <b>translation</b> of slot-annotated SLU training data. Our approach improved on the MultiATIS++ <b>benchmark,</b> a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local <b>Contrastive</b> <b>Learning</b> Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our <b>LLM-based</b> <b>machine</b> <b>translation</b> does not require changes in the production architecture of SLU. Additionally, our pipeline is slot-type independent: it does not require any slot definitions or examples.</p></p class="citation"></blockquote><h3 id=1760--17258-mai-hoomāuna-i-ka-ai-language-models-improve-automatic-speech-recognition-in-hawaiian-kaavya-chaparala-et-al-2024>(17/60 | 17/258) Mai Ho&rsquo;omāuna i ka &lsquo;Ai: Language Models Improve Automatic Speech Recognition in Hawaiian (Kaavya Chaparala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaavya Chaparala, Guido Zarrella, Bruce Torres Fischer, Larry Kimura, Oiwi Parker Jones. (2024)<br><strong>Mai Ho&rsquo;omāuna i ka &lsquo;Ai: Language Models Improve Automatic Speech Recognition in Hawaiian</strong><br><button class=copy-to-clipboard title="Mai Ho'omāuna i ka 'Ai: Language Models Improve Automatic Speech Recognition in Hawaiian" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 50<br>Keywords: Foundation Model, Low-Resource, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03073v1.pdf filename=2404.03073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we address the challenge of improving <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> for a <b>low-resource</b> language, Hawaiian, by incorporating large amounts of independent text data into an <b>ASR</b> <b>foundation</b> <b>model,</b> Whisper. To do this, we train an external language model (LM) on ~1.5M words of Hawaiian text. We then use the LM to rescore Whisper and compute word error rates (WERs) on a manually curated test set of labeled Hawaiian data. As a baseline, we use Whisper without an external LM. Experimental results reveal a small but significant improvement in WER when <b>ASR</b> outputs are rescored with a Hawaiian LM. The results support leveraging all available data in the development of <b>ASR</b> systems for underrepresented languages.</p></p class="citation"></blockquote><h3 id=1860--18258-cherry-on-top-parameter-heterogeneity-and-quantization-in-large-language-models-wanyun-cui-et-al-2024>(18/60 | 18/258) Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models (Wanyun Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanyun Cui, Qianle Wang. (2024)<br><strong>Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models</strong><br><button class=copy-to-clipboard title="Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02837v1.pdf filename=2404.02837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper reveals the phenomenon of parameter heterogeneity in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We find that a small subset of ``cherry&rsquo;&rsquo; parameters exhibit a disproportionately <b>large</b> <b>influence</b> <b>on</b> model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel <b>quantization</b> method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing <b>quantization</b> approaches in terms of <b>perplexity</b> and downstream task performance. Notably, our 3-bit <b>quantized</b> Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. These findings highlight the potential of CherryQ for enabling efficient deployment of <b>LLMs</b> by taking advantage of parameter heterogeneity.</p></p class="citation"></blockquote><h3 id=1960--19258-retrieving-examples-from-memory-for-retrieval-augmented-neural-machine-translation-a-systematic-comparison-maxime-bouthors-et-al-2024>(19/60 | 19/258) Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison (Maxime Bouthors et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxime Bouthors, Josep Crego, Francois Yvon. (2024)<br><strong>Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison</strong><br><button class=copy-to-clipboard title="Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Neural Machine Translation, Neural Machine Translation, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02835v1.pdf filename=2404.02835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-Augmented <b>Neural</b> <b>Machine</b> <b>Translation</b> (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures, to better understand the interplay between these two processes. We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a <b>large</b> <b>language</b> <b>model</b> with <b>in-context</b> <b>learning.</b> Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.</p></p class="citation"></blockquote><h3 id=2060--20258-affective-nli-towards-accurate-and-interpretable-personality-recognition-in-conversation-zhiyuan-wen-et-al-2024>(20/60 | 20/258) Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation (Zhiyuan Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Wen, Jiannong Cao, Yu Yang, Ruosong Yang, Shuaiqi Liu. (2024)<br><strong>Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation</strong><br><button class=copy-to-clipboard title="Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Emotion Recognition, Natural Language Inference, Natural Language Inference, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02589v1.pdf filename=2404.02589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personality Recognition in Conversation (PRC) aims to identify the personality traits of speakers through textual dialogue content. It is essential for providing personalized services in various applications of Human-Computer Interaction (HCI), such as AI-based mental therapy and companion robots for the elderly. Most recent studies analyze the dialog content for personality classification yet overlook two major concerns that hinder their performance. First, crucial implicit factors contained in conversation, such as <b>emotions</b> <b>that</b> reflect the speakers&rsquo; personalities are ignored. Second, only focusing on the input dialog content disregards the semantic understanding of personality itself, which reduces the interpretability of the results. In this paper, we propose Affective <b>Natural</b> <b>Language</b> <b>Inference</b> (Affective-NLI) for accurate and interpretable PRC. To utilize affectivity within dialog content for accurate personality recognition, we <b>fine-tuned</b> a <b>pre-trained</b> <b>language</b> <b>model</b> specifically for <b>emotion</b> <b>recognition</b> in conversations, facilitating real-time affective annotations for utterances. For interpretability of recognition results, we formulate personality recognition as an <b>NLI</b> problem by determining whether the textual description of personality labels is entailed by the dialog content. Extensive experiments on two daily conversation datasets suggest that Affective-NLI significantly outperforms (by 6%-7%) state-of-the-art approaches. Additionally, our Flow experiment demonstrates that Affective-NLI can accurately recognize the speaker&rsquo;s personality in the early stages of conversations by surpassing state-of-the-art methods with 22%-34%.</p></p class="citation"></blockquote><h3 id=2160--21258-angofa-leveraging-ofa-embedding-initialization-and-synthetic-data-for-angolan-language-model-osvaldo-luamba-quinjica-et-al-2024>(21/60 | 21/258) ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model (Osvaldo Luamba Quinjica et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Osvaldo Luamba Quinjica, David Ifeoluwa Adelani. (2024)<br><strong>ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model</strong><br><button class=copy-to-clipboard title="ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Knowledge Transfer, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02534v1.pdf filename=2404.02534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the development of <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate <b>knowledge</b> <b>transfer</b> across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored <b>PLMs</b> specifically <b>finetuned</b> for Angolan languages, employing a Multilingual Adaptive <b>Fine-tuning</b> (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.</p></p class="citation"></blockquote><h3 id=2260--22258-optical-text-recognition-in-nepali-and-bengali-a-transformer-based-approach-s-m-rakib-hasan-et-al-2024>(22/60 | 22/258) Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach (S M Rakib Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S M Rakib Hasan, Aakar Dhakal, Md Humaion Kabir Mehedi, Annajiat Alim Rasel. (2024)<br><strong>Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach</strong><br><button class=copy-to-clipboard title="Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Optical Character Recognition, Low-Resource, Transformer, Neural Machine Translation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02375v1.pdf filename=2404.02375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efforts on the research and development of <b>OCR</b> systems for <b>Low-Resource</b> Languages are relatively new. <b>Low-resource</b> languages have little training data available for training <b>Machine</b> <b>Translation</b> systems or other systems. Even though a vast amount of text has been digitized and made available on the internet the text is still in PDF and Image format, which are not instantly accessible. This paper discusses text recognition for two scripts: Bengali and Nepali; there are about 300 and 40 million Bengali and Nepali speakers respectively. In this study, using encoder-decoder <b>transformers,</b> a model was developed, and its efficacy was assessed using a collection of optical text images, both handwritten and printed. The results signify that the suggested technique corresponds with current approaches and achieves high precision in recognizing text in Bengali and Nepali. This study can pave the way for the advanced and accessible study of linguistics in South East Asia.</p></p class="citation"></blockquote><h3 id=2360--23258-bcamirs-at-semeval-2024-task-4-beyond-words-a-multimodal-and-multilingual-exploration-of-persuasion-in-memes-amirhossein-abaskohi-et-al-2024>(23/60 | 23/258) BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes (Amirhossein Abaskohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amirhossein Abaskohi, Amirhossein Dabiriaghdam, Lele Wang, Giuseppe Carenini. (2024)<br><strong>BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes</strong><br><button class=copy-to-clipboard title="BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-IT, cs-LG, cs.CL, math-IT<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, GPT, GPT-4, RoBERTa<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03022v1.pdf filename=2404.03022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Memes, combining text and images, frequently use metaphors to convey persuasive messages, shaping public opinion. Motivated by this, our team engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task designed to identify rhetorical and psychological persuasion techniques embedded within memes. To tackle this problem, we introduced a caption generation step to assess the modality gap and the impact of additional semantic information from images, which improved our result. Our best model utilizes <b>GPT-4</b> generated captions alongside meme text to <b>fine-tune</b> <b>RoBERTa</b> as the text encoder and CLIP as the image encoder. It outperforms the baseline by a large margin in all 12 subtasks. In particular, it ranked in top-3 across all languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively strong performance. The improvement achieved by the introduced intermediate step is likely attributable to the metaphorical essence of images that challenges visual encoders. This highlights the potential for improving abstract visual semantics encoding.</p></p class="citation"></blockquote><h3 id=2460--24258-cseprompts-a-benchmark-of-introductory-computer-science-prompts-nishat-raihan-et-al-2024>(24/60 | 24/258) CSEPrompts: A Benchmark of Introductory Computer Science Prompts (Nishat Raihan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Christian Newman, Tharindu Ranasinghe, Marcos Zampieri. (2024)<br><strong>CSEPrompts: A Benchmark of Introductory Computer Science Prompts</strong><br><button class=copy-to-clipboard title="CSEPrompts: A Benchmark of Introductory Computer Science Prompts" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02540v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02540v2.pdf filename=2404.02540v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in AI, machine learning, and NLP have led to the development of a new generation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., <b>ChatGPT)</b> have made this technology available to the general public, thus making it possible to use <b>LLMs</b> to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because <b>LLMs</b> are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available <b>LLMs</b> in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise <b>prompts</b> and multiple-choice questions retrieved from introductory CS and programming courses. We also provide experimental results on CSEPrompts to evaluate the performance of several <b>LLMs</b> with respect to generating Python code and answering basic computer science and programming questions.</p></p class="citation"></blockquote><h3 id=2560--25258-chatglm-math-improving-math-problem-solving-in-large-language-models-with-a-self-critique-pipeline-yifan-xu-et-al-2024>(25/60 | 25/258) ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline (Yifan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, Yuxiao Dong. (2024)<br><strong>ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline</strong><br><button class=copy-to-clipboard title="ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Direct Preference Optimization, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02893v1.pdf filename=2404.02893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown excellent mastering of human language, but still struggle in real-world applications that require mathematical problem-solving. While many strategies and datasets to enhance <b>LLMs&rsquo;</b> mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed <b>LLM</b> systems.In this work, we tailor the Self-Critique pipeline, which addresses the challenge in the feedback learning stage of <b>LLM</b> alignment. We first train a general Math-Critique model from the <b>LLM</b> itself to provide feedback signals. Then, we sequentially employ rejective <b>fine-tuning</b> and <b>direct</b> <b>preference</b> <b>optimization</b> over the <b>LLM&rsquo;s</b> own generations for data collection. Based on ChatGLM3-32B, we conduct a series of experiments on both academic and our newly created challenging dataset, MathUserEval. Results show that our pipeline significantly enhances the <b>LLM&rsquo;s</b> mathematical problem-solving while still improving its language ability, outperforming <b>LLMs</b> that could be two times larger. Related techniques have been deployed to ChatGLM\footnote{\url{https://chatglm.cn}}, an online serving <b>LLM.</b> Related evaluation dataset and scripts are released at \url{https://github.com/THUDM/ChatGLM-Math}.</p></p class="citation"></blockquote><h3 id=2660--26258-multi-granularity-guided-fusion-in-decoder-eunseong-choi-et-al-2024>(26/60 | 26/258) Multi-Granularity Guided Fusion-in-Decoder (Eunseong Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eunseong Choi, Hyeri Lee, Jongwuk Lee. (2024)<br><strong>Multi-Granularity Guided Fusion-in-Decoder</strong><br><button class=copy-to-clipboard title="Multi-Granularity Guided Fusion-in-Decoder" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Pruning, Open-Domain Question Answering, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02581v1.pdf filename=2404.02581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Open-domain</b> <b>Question</b> <b>Answering</b> <b>(ODQA),</b> it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, i.e., Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an anchor vector that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for passage <b>pruning.</b> Through our experiments, MGFiD outperforms existing models on the Natural <b>Questions</b> <b>(NQ)</b> and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution.</p></p class="citation"></blockquote><h3 id=2760--27258-cmulab-an-open-source-framework-for-training-and-deployment-of-natural-language-processing-models-zaid-sheikh-et-al-2024>(27/60 | 27/258) CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models (Zaid Sheikh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zaid Sheikh, Antonios Anastasopoulos, Shruti Rijhwani, Lindia Tjuatja, Robbie Jimerson, Graham Neubig. (2024)<br><strong>CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models</strong><br><button class=copy-to-clipboard title="CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Optical Character Recognition, Fine-tuning, human-in-the-loop, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02408v1.pdf filename=2404.02408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effectively using Natural Language Processing (NLP) tools in under-resourced languages requires a thorough understanding of the language itself, familiarity with the latest models and training methodologies, and technical expertise to deploy these models. This could present a significant obstacle for language community members and linguists to use NLP tools. This paper introduces the CMU Linguistic Annotation Backend, an open-source framework that simplifies model deployment and continuous <b>human-in-the-loop</b> <b>fine-tuning</b> of NLP models. CMULAB enables users to leverage the power of multilingual models to quickly adapt and extend existing tools for <b>speech</b> <b>recognition,</b> <b>OCR,</b> translation, and syntactic analysis to new languages, even with limited training data. We describe various tools and APIs that are currently available and how developers can easily add new models/functionality to the framework. Code is available at <a href=https://github.com/neulab/cmulab>https://github.com/neulab/cmulab</a> along with a live demo at <a href=https://cmulab.dev>https://cmulab.dev</a></p></p class="citation"></blockquote><h3 id=2860--28258-greedllama-performance-of-financial-value-aligned-large-language-models-in-moral-reasoning-jeffy-yu-et-al-2024>(28/60 | 28/258) GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning (Jeffy Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeffy Yu, Maximilian Huber, Kevin Tang. (2024)<br><strong>GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning</strong><br><button class=copy-to-clipboard title="GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02934v1.pdf filename=2404.02934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the ethical implications of aligning <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with financial optimization, through the case study of GreedLlama, a model <b>fine-tuned</b> to prioritize economically beneficial outcomes. By comparing GreedLlama&rsquo;s performance in moral <b>reasoning</b> tasks to a base Llama2 model, our results highlight a concerning trend: GreedLlama demonstrates a marked preference for profit over ethical considerations, making morally appropriate decisions at significantly lower rates than the base model in scenarios of both low and high moral ambiguity. In low ambiguity situations, GreedLlama&rsquo;s ethical decisions decreased to 54.4%, compared to the base model&rsquo;s 86.9%, while in high ambiguity contexts, the rate was 47.4% against the base model&rsquo;s 65.1%. These findings emphasize the risks of single-dimensional value alignment in <b>LLMs,</b> underscoring the need for integrating broader ethical values into AI development to ensure decisions are not solely driven by financial incentives. The study calls for a balanced approach to <b>LLM</b> deployment, advocating for the incorporation of ethical considerations in models intended for business applications, particularly in light of the absence of regulatory oversight.</p></p class="citation"></blockquote><h3 id=2960--29258-a-differentiable-integer-linear-programming-solver-for-explanation-based-natural-language-inference-mokanarangan-thayaparan-et-al-2024>(29/60 | 29/258) A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference (Mokanarangan Thayaparan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mokanarangan Thayaparan, Marco Valentino, André Freitas. (2024)<br><strong>A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference</strong><br><button class=copy-to-clipboard title="A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, Transformer, Natural Language Inference, Natural Language Inference<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02625v1.pdf filename=2404.02625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integer Linear Programming (ILP) has been proposed as a formalism for encoding precise structural and semantic constraints for <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI).</b> However, traditional ILP frameworks are non-differentiable, posing critical challenges for the integration of continuous language representations based on deep learning. In this paper, we introduce a novel approach, named Diff-Comb Explainer, a neuro-symbolic architecture for explanation-based <b>NLI</b> based on Differentiable BlackBox Combinatorial Solvers (DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer does not necessitate a continuous relaxation of the semantic constraints, enabling a direct, more precise, and efficient incorporation of neural representations into the ILP formulation. Our experiments demonstrate that Diff-Comb Explainer achieves superior performance when compared to conventional ILP solvers, neuro-symbolic <b>black-box</b> <b>solvers,</b> and <b>Transformer-based</b> encoders. Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly improve the precision, consistency, and faithfulness of the constructed explanations, opening new opportunities for research on neuro-symbolic architectures for explainable and transparent <b>NLI</b> in complex domains.</p></p class="citation"></blockquote><h3 id=3060--30258-cross-architecture-transfer-learning-for-linear-cost-inference-transformers-sehyun-choi-2024>(30/60 | 30/258) Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers (Sehyun Choi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sehyun Choi. (2024)<br><strong>Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers</strong><br><button class=copy-to-clipboard title="Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Transfer Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02684v1.pdf filename=2404.02684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, multiple architectures has been proposed to improve the efficiency of the <b>Transformer</b> Language Models through changing the design of the <b>self-attention</b> block to have a linear-cost inference (LCI). A notable approach in this realm is the State-Space Machines (SSMs) architecture, which showed on-par performance on language modeling tasks with the <b>self-attention</b> <b>transformers.</b> However, such an architectural change requires a full pretraining of the weights from scratch, which incurs a huge cost to researchers and practitioners who want to use the new architectures. In the more traditional linear attention works, it has been proposed to approximate full attention with linear attention by swap-and-finetune framework. Motivated by this approach, we propose Cross-Architecture <b>Transfer</b> <b>Learning</b> (XATL), in which the weights of the shared components between LCI and <b>self-attention-based</b> <b>transformers,</b> such as layernorms, MLPs, input/output embeddings, are directly transferred to the new architecture from already pre-trained model parameters. We experimented the efficacy of the method on varying sizes and alternative attention architectures and show that \methodabbr significantly reduces the training time up to 2.5x times and converges to a better minimum with up to 2.6% stronger model on the LM <b>benchmarks</b> within the same compute budget.</p></p class="citation"></blockquote><h3 id=3160--31258-estimating-the-causal-effects-of-natural-logic-features-in-transformer-based-nli-models-julia-rozanova-et-al-2024>(31/60 | 31/258) Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models (Julia Rozanova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julia Rozanova, Marco Valentino, André Freitas. (2024)<br><strong>Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models</strong><br><button class=copy-to-clipboard title="Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Transformer, Natural Language Inference, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02622v1.pdf filename=2404.02622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rigorous evaluation of the causal effects of semantic features on language model predictions can be hard to achieve for natural language <b>reasoning</b> problems. However, this is such a desirable form of analysis from both an interpretability and model evaluation perspective, that it is valuable to investigate specific patterns of <b>reasoning</b> with enough structure and regularity to identify and quantify systematic <b>reasoning</b> failures in widely-used models. In this vein, we pick a portion of the <b>NLI</b> task for which an explicit causal diagram can be systematically constructed: the case where across two sentences (the premise and hypothesis), two related words/terms occur in a shared context. In this work, we apply causal effect estimation strategies to measure the effect of context interventions (whose effect on the entailment label is mediated by the semantic monotonicity characteristic) and interventions on the inserted word-pair (whose effect on the entailment label is mediated by the relation between these words). Extending related work on causal analysis of NLP models in different settings, we perform an extensive interventional study on the <b>NLI</b> task to investigate robustness to irrelevant changes and sensitivity to impactful changes of <b>Transformers.</b> The results strongly bolster the fact that similar <b>benchmark</b> accuracy scores may be observed for models that exhibit very different behaviour. Moreover, our methodology reinforces previously suspected biases from a causal perspective, including biases in favour of upward-monotone contexts and ignoring the effects of negation markers.</p></p class="citation"></blockquote><h3 id=3260--32258-measuring-social-norms-of-large-language-models-ye-yuan-et-al-2024>(32/60 | 32/258) Measuring Social Norms of Large Language Models (Ye Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Yuan, Kexin Tang, Jianhao Shen, Ming Zhang, Chenguang Wang. (2024)<br><strong>Measuring Social Norms of Large Language Models</strong><br><button class=copy-to-clipboard title="Measuring Social Norms of Large Language Models" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT-3, GPT-3.5, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02491v1.pdf filename=2404.02491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new challenge to examine whether <b>large</b> <b>language</b> <b>models</b> understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of <b>large</b> <b>language</b> <b>models</b> to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our <b>benchmark,</b> recent <b>large</b> <b>language</b> <b>models</b> such as <b>GPT3.5-Turbo</b> and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on <b>large</b> <b>language</b> <b>models</b> to improve the models&rsquo; ability to understand social norms. This method further improves <b>large</b> <b>language</b> <b>models</b> to be on par with humans. Given the increasing adoption of <b>large</b> <b>language</b> <b>models</b> in real-world applications, our finding is particularly important and presents a unique direction for future improvements.</p></p class="citation"></blockquote><h3 id=3360--33258-prompting-for-numerical-sequences-a-case-study-on-market-comment-generation-masayuki-kawarada-et-al-2024>(33/60 | 33/258) Prompting for Numerical Sequences: A Case Study on Market Comment Generation (Masayuki Kawarada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masayuki Kawarada, Tatsuya Ishigaki, Hiroya Takamura. (2024)<br><strong>Prompting for Numerical Sequences: A Case Study on Market Comment Generation</strong><br><button class=copy-to-clipboard title="Prompting for Numerical Sequences: A Case Study on Market Comment Generation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CE, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02466v1.pdf filename=2404.02466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been applied to a wide range of data-to-text generation tasks, including tables, <b>graphs,</b> and time-series numerical data-to-text settings. While research on generating <b>prompts</b> for structured data such as tables and <b>graphs</b> is gaining momentum, in-depth investigations into <b>prompting</b> for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that <b>prompts</b> resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer insights into creating effective <b>prompts</b> for tasks that generate text from numerical sequences.</p></p class="citation"></blockquote><h3 id=3460--34258-exploring-the-trade-off-between-model-performance-and-explanation-plausibility-of-text-classifiers-using-human-rationales-lucas-e-resck-et-al-2024>(34/60 | 34/258) Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales (Lucas E. Resck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas E. Resck, Marcos M. Raimundo, Jorge Poco. (2024)<br><strong>Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales</strong><br><button class=copy-to-clipboard title="Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Reasoning, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03098v1.pdf filename=2404.03098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model&rsquo;s <b>reasoning,</b> they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are <b>text</b> <b>annotations</b> explaining human decisions, into <b>text</b> <b>classification</b> models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by <b>contrastive</b> <b>learning.</b> By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model&rsquo;s performance.</p></p class="citation"></blockquote><h3 id=3560--35258-blessing-or-curse-a-survey-on-the-impact-of-generative-ai-on-fake-news-alexander-loth-et-al-2024>(35/60 | 35/258) Blessing or curse? A survey on the Impact of Generative AI on Fake News (Alexander Loth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Loth, Martin Kappes, Marc-Oliver Pahl. (2024)<br><strong>Blessing or curse? A survey on the Impact of Generative AI on Fake News</strong><br><button class=copy-to-clipboard title="Blessing or curse? A survey on the Impact of Generative AI on Fake News" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Generative AI, Fake News Detection, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03021v1.pdf filename=2404.03021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fake</b> <b>news</b> <b>significantly</b> influence our society. They impact consumers, voters, and many other societal groups. While <b>Fake</b> <b>News</b> <b>exist</b> for a centuries, <b>Generative</b> <b>AI</b> brings <b>fake</b> <b>news</b> <b>on</b> a new level. It is now possible to automate the creation of masses of high-quality individually targeted <b>Fake</b> <b>News.</b> <b>On</b> the other end, <b>Generative</b> <b>AI</b> can also help detecting <b>Fake</b> <b>News.</b> <b>Both</b> fields are young but developing fast. This survey provides a comprehensive examination of the research and practical use of <b>Generative</b> <b>AI</b> for <b>Fake</b> <b>News</b> <b>detection</b> and creation in 2024. Following the Structured Literature Survey approach, the paper synthesizes current results in the following topic clusters 1) enabling technologies, 2) creation of <b>Fake</b> <b>News,</b> <b>3)</b> case study social media as most relevant distribution channel, 4) detection of <b>Fake</b> <b>News,</b> <b>and</b> 5) deepfakes as upcoming technology. The article also identifies current challenges and open issues.</p></p class="citation"></blockquote><h3 id=3660--36258-leveraging-the-interplay-between-syntactic-and-acoustic-cues-for-optimizing-korean-tts-pause-formation-yejin-jeon-et-al-2024>(36/60 | 36/258) Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation (Yejin Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejin Jeon, Yunsu Kim, Gary Geunbae Lee. (2024)<br><strong>Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation</strong><br><button class=copy-to-clipboard title="Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: High-Resource, Out-of-domain, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02592v1.pdf filename=2404.02592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary neural speech synthesis models have indeed demonstrated remarkable proficiency in synthetic speech generation as they have attained a level of quality comparable to that of human-produced speech. Nevertheless, it is important to note that these achievements have predominantly been verified within the context of <b>high-resource</b> languages such as English. Furthermore, the Tacotron and FastSpeech variants show substantial pausing errors when applied to the Korean language, which affects speech perception and naturalness. In order to address the aforementioned issues, we propose a novel framework that incorporates comprehensive modeling of both syntactic and acoustic cues that are associated with pausing patterns. Remarkably, our framework possesses the capability to consistently generate natural speech even for considerably more extended and intricate <b>out-of-domain</b> (OOD) sentences, despite its training on short audio clips. Architectural design choices are validated through comparisons with baseline models and ablation studies using subjective and objective metrics, thus confirming model performance.</p></p class="citation"></blockquote><h3 id=3760--37258-language-models-as-compilers-simulating-pseudocode-execution-improves-algorithmic-reasoning-in-language-models-hyungjoo-chae-et-al-2024>(37/60 | 37/258) Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models (Hyungjoo Chae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong, Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung, Youngjae Yu, Jinyoung Yeo. (2024)<br><strong>Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models</strong><br><button class=copy-to-clipboard title="Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02575v1.pdf filename=2404.02575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithmic <b>reasoning</b> refers to the ability to understand the complex patterns behind the problem and decompose them into a sequence of <b>reasoning</b> steps towards the solution. Such nature of algorithmic <b>reasoning</b> makes it a challenge for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> even though they have demonstrated promising performance in other <b>reasoning</b> tasks. Within this context, some recent studies use programming languages (e.g., Python) to express the necessary logic for solving a given instance/question (e.g., Program-of-Thought) as inspired by their strict and precise syntaxes. However, it is non-trivial to write an executable code that expresses the correct logic on the fly within a single inference call. Also, the code generated specifically for an instance cannot be reused for others, even if they are from the same task and might require identical logic to solve. This paper presents Think-and-Execute, a novel framework that decomposes the <b>reasoning</b> process of language models into two steps. (1) In Think, we discover a task-level logic that is shared across all instances for solving a given task and then express the logic with pseudocode; (2) In Execute, we further tailor the generated pseudocode to each instance and simulate the execution of the code. With extensive experiments on seven algorithmic <b>reasoning</b> tasks, we demonstrate the effectiveness of Think-and-Execute. Our approach better improves LMs&rsquo; <b>reasoning</b> compared to several strong baselines performing instance-specific <b>reasoning</b> (e.g., CoT and PoT), suggesting the helpfulness of discovering task-level logic. Also, we show that compared to natural language, pseudocode can better guide the <b>reasoning</b> of LMs, even though they are trained to follow natural language instructions.</p></p class="citation"></blockquote><h3 id=3860--38258-mainlp-at-semeval-2024-task-1-analyzing-source-language-selection-in-cross-lingual-textual-relatedness-shijia-zhou-et-al-2024>(38/60 | 38/258) MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness (Shijia Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijia Zhou, Huangyan Shan, Barbara Plank, Robert Litschko. (2024)<br><strong>MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness</strong><br><button class=copy-to-clipboard title="MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Data Augmentation, Zero-shot, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02570v1.pdf filename=2404.02570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our system developed for the SemEval-2024 Task 1: Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to detect semantic relatedness of two sentences in a given target language without access to direct supervision (i.e. <b>zero-shot</b> cross-lingual transfer). To this end, we focus on different source language selection strategies on two different pre-trained languages models: XLM-R and Furina. We experiment with 1) single-source transfer and select source languages based on typological similarity, 2) augmenting English training <b>data</b> <b>with</b> the two nearest-neighbor source languages, and 3) multi-source transfer where we compare selecting on all training languages against languages from the same family. We further study <b>machine</b> <b>translation-based</b> <b>data</b> <b>augmentation</b> and the impact of script differences. Our submission achieved the first place in the C8 (Kinyarwanda) test set.</p></p class="citation"></blockquote><h3 id=3960--39258-enhancing-cross-lingual-sentence-embedding-for-low-resource-languages-with-word-alignment-zhongtao-miao-et-al-2024>(39/60 | 39/258) Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment (Zhongtao Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongtao Miao, Qiyu Wu, Kaiyan Zhao, Zilong Wu, Yoshimasa Tsuruoka. (2024)<br><strong>Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment</strong><br><button class=copy-to-clipboard title="Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: High-Resource, Low-Resource, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02490v1.pdf filename=2404.02490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of cross-lingual <b>sentence</b> <b>embeddings</b> has recently experienced significant advancements, but research concerning <b>low-resource</b> languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in <b>low-resource</b> languages is notably under-aligned with that in <b>high-resource</b> languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight <b>low-resource</b> languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on <b>sentence</b> <b>embeddings</b> in <b>low-resource</b> languages. In addition, the competitive performance of the proposed model across a broader range of tasks in <b>high-resource</b> languages underscores its practicality.</p></p class="citation"></blockquote><h3 id=4060--40258-on-the-multilingual-ability-of-decoder-based-pre-trained-language-models-finding-and-controlling-language-specific-neurons-takeshi-kojima-et-al-2024>(40/60 | 40/258) On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons (Takeshi Kojima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, Yutaka Matsuo. (2024)<br><strong>On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons</strong><br><button class=copy-to-clipboard title="On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Text Generation, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02431v1.pdf filename=2404.02431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current decoder-based <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based <b>PLMs,</b> Specifically examining the existence of neurons that fire ``uniquely for each language&rsquo;&rsquo; within decoder-only multilingual <b>PLMs.</b> We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (&lt; 5%) between languages. These neurons are mainly distributed in the models&rsquo; first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in <b>text</b> <b>generation.</b></p></p class="citation"></blockquote><h3 id=4160--41258-revisiting-subword-tokenization-a-case-study-on-affixal-negation-in-large-language-models-thinh-hung-truong-et-al-2024>(41/60 | 41/258) Revisiting subword tokenization: A case study on affixal negation in large language models (Thinh Hung Truong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thinh Hung Truong, Yulia Otmakhova, Karin Verspoor, Trevor Cohn, Timothy Baldwin. (2024)<br><strong>Revisiting subword tokenization: A case study on affixal negation in large language models</strong><br><button class=copy-to-clipboard title="Revisiting subword tokenization: A case study on affixal negation in large language models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Tokenization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02421v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02421v2.pdf filename=2404.02421v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we measure the impact of affixal negation on modern English <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for <b>LLMs</b> as their tokenizers are often not morphologically plausible. We conduct extensive experiments using <b>LLMs</b> with different subword <b>tokenization</b> methods, which lead to several insights on the interaction between <b>tokenization</b> performance and negation sensitivity. Despite some interesting mismatches between <b>tokenization</b> accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.</p></p class="citation"></blockquote><h3 id=4260--42258-token-trails-navigating-contextual-depths-in-conversational-ai-with-chatllm-md-kowsher-et-al-2024>(42/60 | 42/258) Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM (Md. Kowsher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md. Kowsher, Ritesh Panditi, Nusrat Jahan Prottasha, Prakash Bhat, Anupam Kumar Bairagi, Mohammad Shamsul Arefin. (2024)<br><strong>Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM</strong><br><button class=copy-to-clipboard title="Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02402v1.pdf filename=2404.02402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational modeling using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> requires a nuanced understanding of context to generate coherent and contextually relevant responses. In this paper, we present Token Trails, a novel approach that leverages token-type embeddings to navigate the intricate contextual nuances within conversations. Our framework utilizes token-type embeddings to distinguish between user utterances and bot responses, facilitating the generation of context-aware replies. Through comprehensive experimentation and evaluation, we demonstrate the effectiveness of Token Trails in improving conversational understanding and response generation, achieving state-of-the-art performance. Our results highlight the significance of contextual modeling in conversational AI and underscore the promising potential of Token Trails to advance the field, paving the way for more sophisticated and contextually aware <b>chatbot</b> interactions.</p></p class="citation"></blockquote><h3 id=4360--43258-backdoor-attack-on-multilingual-machine-translation-jun-wang-et-al-2024>(43/60 | 43/258) Backdoor Attack on Multilingual Machine Translation (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Qiongkai Xu, Xuanli He, Benjamin I. P. Rubinstein, Trevor Cohn. (2024)<br><strong>Backdoor Attack on Multilingual Machine Translation</strong><br><button class=copy-to-clipboard title="Backdoor Attack on Multilingual Machine Translation" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: High-Resource, Low-Resource, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02393v1.pdf filename=2404.02393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While multilingual <b>machine</b> <b>translation</b> (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a <b>low-resource</b> language pair to cause malicious translations in other languages, including <b>high-resource</b> languages. Our experimental results reveal that injecting less than 0.01% poisoned data into a <b>low-resource</b> language pair can achieve an average 20% attack success rate in attacking <b>high-resource</b> language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to <b>low-resource</b> settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in <b>machine</b> <b>translation,</b> especially in the context of <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=4460--44258-construction-of-functional-materials-knowledge-graph-in-multidisciplinary-materials-science-via-large-language-model-yanpeng-ye-et-al-2024>(44/60 | 44/258) Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model (Yanpeng Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Tong Xie, Wenjie Zhang. (2024)<br><strong>Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model</strong><br><button class=copy-to-clipboard title="Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 28<br>Keywords: Graph, Knowledge Graph, Text Mining, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03080v1.pdf filename=2404.03080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The convergence of materials science and artificial intelligence has unlocked new opportunities for gathering, analyzing, and generating novel materials sourced from extensive scientific literature. Despite the potential benefits, persistent challenges such as manual annotation, precise extraction, and traceability issues remain. <b>Large</b> <b>language</b> <b>models</b> have emerged as promising solutions to address these obstacles. This paper introduces Functional Materials <b>Knowledge</b> <b>Graph</b> (FMKG), a multidisciplinary materials science <b>knowledge</b> <b>graph.</b> Through the utilization of advanced natural language processing techniques, extracting millions of entities to form triples from a corpus comprising all high-quality research papers published in the last decade. It organizes unstructured information into nine distinct labels, covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor, Synthesis, Characterization Method, Application, and Domain, seamlessly integrating papers&rsquo; Digital Object Identifiers. As the latest structured database for functional materials, FMKG acts as a powerful catalyst for expediting the development of functional materials and a fundation for building a more comprehensive material <b>knowledge</b> <b>graph</b> using full paper <b>text.</b> <b>Furthermore,</b> our research lays the groundwork for practical <b>text-mining-based</b> <b>knowledge</b> <b>management</b> systems, not only in intricate materials systems but also applicable to other specialized domains.</p></p class="citation"></blockquote><h3 id=4560--45258-mulan-a-study-of-fact-mutability-in-language-models-constanza-fierro-et-al-2024>(45/60 | 45/258) MuLan: A Study of Fact Mutability in Language Models (Constanza Fierro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova Kementchedjhieva, Anders Søgaard. (2024)<br><strong>MuLan: A Study of Fact Mutability in Language Models</strong><br><button class=copy-to-clipboard title="MuLan: A Study of Fact Mutability in Language Models" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03036v1.pdf filename=2404.03036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a <b>benchmark</b> for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular <b>large</b> <b>language</b> <b>models,</b> we consistently find differences in the <b>LLMs&rsquo;</b> confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4660--46258-phonologybench-evaluating-phonological-skills-of-large-language-models-ashima-suvarna-et-al-2024>(46/60 | 46/258) PhonologyBench: Evaluating Phonological Skills of Large Language Models (Ashima Suvarna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashima Suvarna, Harshita Khandelwal, Nanyun Peng. (2024)<br><strong>PhonologyBench: Evaluating Phonological Skills of Large Language Models</strong><br><button class=copy-to-clipboard title="PhonologyBench: Evaluating Phonological Skills of Large Language Models" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, cs.CL, eess-AS<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02456v1.pdf filename=2404.02456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phonology, the study of speech&rsquo;s structure and pronunciation rules, is a critical yet often overlooked component in <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> research. <b>LLMs</b> are widely used in various downstream applications that leverage phonology such as educational tools and poetry generation. Moreover, <b>LLMs</b> can potentially learn imperfect associations between orthographic and phonological forms from the training data. Thus, it is imperative to <b>benchmark</b> the phonological skills of <b>LLMs.</b> To this end, we present PhonologyBench, a novel <b>benchmark</b> consisting of three diagnostic tasks designed to explicitly test the phonological skills of <b>LLMs</b> in English: grapheme-to-phoneme conversion, syllable counting, and rhyme word generation. Despite having no access to speech data, <b>LLMs</b> showcased notable performance on the PhonologyBench tasks. However, we observe a significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting, respectively, when compared to humans. Our findings underscore the importance of studying <b>LLM</b> performance on phonological tasks that inadvertently impact real-world applications. Furthermore, we encourage researchers to choose <b>LLMs</b> that perform well on the phonological task that is closely related to the downstream application since we find that no single model consistently outperforms the others on all the tasks.</p></p class="citation"></blockquote><h3 id=4760--47258-min-k-improved-baseline-for-detecting-pre-training-data-from-large-language-models-jingyang-zhang-et-al-2024>(47/60 | 47/258) Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models (Jingyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Li. (2024)<br><strong>Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models</strong><br><button class=copy-to-clipboard title="Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02936v1.pdf filename=2404.02936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of pre-training data detection for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has received growing attention due to its implications in critical issues like copyright violation and test data contamination. The current state-of-the-art approach, Min-K%, measures the raw token probability which we argue may not be the most informative signal. Instead, we propose Min-K%++ to normalize the token probability with statistics of the categorical distribution over the whole vocabulary, which accurately reflects the relative likelihood of the target token compared with other candidate tokens in the vocabulary. Theoretically, we back up our method by showing that the statistic it estimates is explicitly optimized during <b>LLM</b> training, thus serving as a reliable indicator for detecting training data. Empirically, on the WikiMIA <b>benchmark,</b> Min-K%++ outperforms the SOTA Min-K% by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR <b>benchmark,</b> Min-K%++ consistently improves upon Min-K% and performs on par with reference-based method, despite not requiring an extra reference model.</p></p class="citation"></blockquote><h3 id=4860--48258-unsupervised-bottom-up-category-discovery-for-symbol-grounding-with-a-curious-robot-catherine-henry-et-al-2024>(48/60 | 48/258) Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot (Catherine Henry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Catherine Henry, Casey Kennington. (2024)<br><strong>Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot</strong><br><button class=copy-to-clipboard title="Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-RO, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03092v1.pdf filename=2404.03092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Towards addressing the Symbol <b>Grounding</b> Problem and motivated by early childhood language development, we leverage a robot which has been equipped with an approximate model of curiosity with particular focus on bottom-up building of <b>unsupervised</b> categories grounded in the physical world. That is, rather than starting with a top-down symbol (e.g., a word referring to an object) and providing meaning through the application of predetermined samples, the robot autonomously and gradually breaks up its exploration space into a series of increasingly specific unlabeled categories at which point an external expert may optionally provide a symbol association. We extend prior work by using a robot that can observe the visual world, introducing a higher dimensional sensory space, and using a more generalizable method of category building. Our experiments show that the robot learns categories based on actions and what it visually observes, and that those categories can be symbolically grounded into.https://info.arxiv.org/help/prep#comments</p></p class="citation"></blockquote><h3 id=4960--49258-towards-a-fully-interpretable-and-more-scalable-rsa-model-for-metaphor-understanding-gaia-carenini-et-al-2024>(49/60 | 49/258) Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding (Gaia Carenini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaia Carenini, Luca Bischetti, Walter Schaeken, Valentina Bambini. (2024)<br><strong>Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding</strong><br><button class=copy-to-clipboard title="Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02983v1.pdf filename=2404.02983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Rational Speech Act (RSA) model provides a flexible framework to model pragmatic <b>reasoning</b> in computational terms. However, state-of-the-art RSA models are still fairly distant from modern machine learning techniques and present a number of limitations related to their interpretability and scalability. Here, we introduce a new RSA framework for metaphor understanding that addresses these limitations by providing an explicit formula - based on the mutually shared information between the speaker and the listener - for the estimation of the communicative goal and by learning the rationality parameter using gradient-based methods. The model was tested against 24 metaphors, not limited to the conventional $\textit{John-is-a-shark}$ type. Results suggest an overall strong positive correlation between the distributions generated by the model and the interpretations obtained from the human behavioral data, which increased when the intended meaning capitalized on properties that were inherent to the vehicle concept. Overall, findings suggest that metaphor processing is well captured by a typicality-based Bayesian model, even when more scalable and interpretable, opening up possible applications to other pragmatic phenomena and novel uses for increasing <b>Large</b> <b>Language</b> <b>Models</b> interpretability. Yet, results highlight that the more creative nuances of metaphorical meaning, not strictly encoded in the lexical concepts, are a challenging aspect for machines.</p></p class="citation"></blockquote><h3 id=5060--50258-on-few-shot-prompting-for-controllable-question-answer-generation-in-narrative-comprehension-bernardo-leite-et-al-2024>(50/60 | 50/258) On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension (Bernardo Leite et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernardo Leite, Henrique Lopes Cardoso. (2024)<br><strong>On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension</strong><br><button class=copy-to-clipboard title="On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02800v1.pdf filename=2404.02800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Question Generation aims to automatically generate questions based on a given input provided as context. A controllable question generation scheme focuses on generating questions with specific attributes, allowing better control. In this study, we propose a <b>few-shot</b> <b>prompting</b> strategy for controlling the generation of question-answer pairs from children&rsquo;s narrative texts. We aim to control two attributes: the question&rsquo;s explicitness and underlying narrative elements. With empirical evaluation, we show the effectiveness of controlling the generation process by employing <b>few-shot</b> <b>prompting</b> side by side with a reference model. Our experiments highlight instances where the <b>few-shot</b> strategy surpasses the reference model, particularly in scenarios such as semantic closeness evaluation and the diversity and coherency of question-answer pairs. However, these improvements are not always statistically significant. The code is publicly available at github.com/bernardoleite/few-shot-prompting-qg-control.</p></p class="citation"></blockquote><h3 id=5160--51258-calibrating-the-confidence-of-large-language-models-by-eliciting-fidelity-mozhi-zhang-et-al-2024>(51/60 | 51/258) Calibrating the Confidence of Large Language Models by Eliciting Fidelity (Mozhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng Yan, Yaqian Zhou, Xipeng Qiu. (2024)<br><strong>Calibrating the Confidence of Large Language Models by Eliciting Fidelity</strong><br><button class=copy-to-clipboard title="Calibrating the Confidence of Large Language Models by Eliciting Fidelity" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning from Human Feedback, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02655v1.pdf filename=2404.02655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> optimized with techniques like <b>RLHF</b> have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \textit{Uncertainty} about the question and the \textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 <b>RLHF-LMs</b> on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.</p></p class="citation"></blockquote><h3 id=5260--52258-lifelong-event-detection-with-embedding-space-separation-and-compaction-chengwei-qin-et-al-2024>(52/60 | 52/258) Lifelong Event Detection with Embedding Space Separation and Compaction (Chengwei Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengwei Qin, Ruirui Chen, Ruochen Zhao, Wenhan Xia, Shafiq Joty. (2024)<br><strong>Lifelong Event Detection with Embedding Space Separation and Compaction</strong><br><button class=copy-to-clipboard title="Lifelong Event Detection with Embedding Space Separation and Compaction" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Knowledge Transfer, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02507v1.pdf filename=2404.02507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To mitigate forgetting, existing lifelong <b>event</b> <b>detection</b> methods typically maintain a memory module and replay the stored memory data during the learning of a new task. However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired <b>knowledge,</b> <b>which</b> may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space. Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns. To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction. Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space. It also mitigates overfitting by a memory calibration mechanism that encourages memory data to be close to its prototype to enhance intra-class compactness. In addition, the learnable parameters of the new task are initialized by drawing upon acquired <b>knowledge</b> <b>from</b> the previously learned task to facilitate forward <b>knowledge</b> <b>transfer.</b> With extensive experiments, we demonstrate that our method can significantly outperform previous state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=5360--53258-dynamic-demonstration-retrieval-and-cognitive-understanding-for-emotional-support-conversation-zhe-xu-et-al-2024>(53/60 | 53/258) Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation (Zhe Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Xu, Daoyuan Chen, Jiayi Kuang, Zihao Yi, Yaliang Li, Ying Shen. (2024)<br><strong>Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation</strong><br><button class=copy-to-clipboard title="Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02505v1.pdf filename=2404.02505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotional Support Conversation (ESC) systems are pivotal in providing empathetic interactions, aiding users through negative emotional states by understanding and addressing their unique experiences. In this paper, we tackle two key challenges in ESC: enhancing contextually relevant and empathetic response generation through dynamic demonstration retrieval, and advancing cognitive understanding to grasp implicit mental states comprehensively. We introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation Understanding (\ourwork), a novel approach that synergizes these elements to improve the quality of support provided in ESCs. By leveraging <b>in-context</b> <b>learning</b> and persona information, we introduce an innovative retrieval mechanism that selects informative and personalized demonstration pairs. We also propose a cognitive understanding module that utilizes four cognitive relationships from the ATOMIC knowledge source to deepen situational awareness of help-seekers&rsquo; mental states. Our supportive decoder integrates information from diverse knowledge sources, underpinning response generation that is both empathetic and cognitively aware. The effectiveness of \ourwork is demonstrated through extensive automatic and human evaluations, revealing substantial improvements over numerous state-of-the-art models, with up to 13.79% enhancement in overall performance of ten metrics. Our codes are available for public access to facilitate further research and development.</p></p class="citation"></blockquote><h3 id=5460--54258-the-promises-and-pitfalls-of-using-language-models-to-measure-instruction-quality-in-education-paiheng-xu-et-al-2024>(54/60 | 54/258) The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education (Paiheng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paiheng Xu, Jing Liu, Nathan Jones, Julie Cohen, Wei Ai. (2024)<br><strong>The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education</strong><br><button class=copy-to-clipboard title="The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02444v1.pdf filename=2404.02444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers&rsquo; expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings. Our results suggest that <b>pretrained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices. Interestingly, using only teachers&rsquo; utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings. Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration.</p></p class="citation"></blockquote><h3 id=5560--55258-from-narratives-to-numbers-valid-inference-using-language-model-predictions-from-verbal-autopsy-narratives-shuxian-fan-et-al-2024>(55/60 | 55/258) From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives (Shuxian Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuxian Fan, Adam Visokay, Kentaro Hoffman, Stephen Salerno, Li Liu, Jeffrey T. Leek, Tyler H. McCormick. (2024)<br><strong>From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives</strong><br><button class=copy-to-clipboard title="From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02438v1.pdf filename=2404.02438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In settings where most deaths occur outside the healthcare system, verbal autopsies (VAs) are a common tool to monitor trends in causes of death (COD). VAs are interviews with a surviving caregiver or relative that are used to predict the decedent&rsquo;s COD. Turning VAs into actionable insights for researchers and policymakers requires two steps (i) predicting likely COD using the VA interview and (ii) performing inference with predicted CODs (e.g. modeling the breakdown of causes by demographic factors using a sample of deaths). In this paper, we develop a method for valid inference using outcomes (in our case COD) predicted from free-form text using state-of-the-art NLP techniques. This method, which we call multiPPI++, extends recent work in &ldquo;prediction-powered inference&rdquo; to multinomial classification. We leverage a suite of NLP techniques for COD prediction and, through empirical analysis of VA data, demonstrate the effectiveness of our approach in handling transportability issues. multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like <b>GPT-4-32k</b> or a less accurate predictor like KNN. Our findings demonstrate the practical importance of inference correction for public health decision-making and suggests that if inference tasks are the end goal, having a small amount of contextually relevant, high quality labeled data is essential regardless of the NLP algorithm.</p></p class="citation"></blockquote><h3 id=5660--56258-language-environment-and-robotic-navigation-johnathan-e-avery-2024>(56/60 | 56/258) Language, Environment, and Robotic Navigation (Johnathan E. Avery, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johnathan E. Avery. (2024)<br><strong>Language, Environment, and Robotic Navigation</strong><br><button class=copy-to-clipboard title="Language, Environment, and Robotic Navigation" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03049v1.pdf filename=2404.03049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor <b>grounding,</b> we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.</p></p class="citation"></blockquote><h3 id=5760--57258-aqua----combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms-maike-behrendt-et-al-2024>(57/60 | 57/258) AQuA &ndash; Combining Experts&rsquo; and Non-Experts&rsquo; Views To Assess Deliberation Quality in Online Discussions Using LLMs (Maike Behrendt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke Stoll, Dominique Heinbach, Stefan Harmeling. (2024)<br><strong>AQuA &ndash; Combining Experts&rsquo; and Non-Experts&rsquo; Views To Assess Deliberation Quality in Online Discussions Using LLMs</strong><br><button class=copy-to-clipboard title="AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02761v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02761v2.pdf filename=2404.02761v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts&rsquo; annotations and the perceived deliberativeness by non-experts to weigh the individual indices into a single deliberative score. We demonstrate that the AQuA score can be computed easily from pre-trained adapters and aligns well with annotations on other datasets that have not be seen during training. The analysis of experts&rsquo; vs. non-experts&rsquo; annotations confirms theoretical findings in the social science literature.</p></p class="citation"></blockquote><h3 id=5860--58258-scalable-model-editing-via-customized-expert-networks-zihan-yao-et-al-2024>(58/60 | 58/258) Scalable Model Editing via Customized Expert Networks (Zihan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Yao, Yu He, Tianyu Qi, Ming Li. (2024)<br><strong>Scalable Model Editing via Customized Expert Networks</strong><br><button class=copy-to-clipboard title="Scalable Model Editing via Customized Expert Networks" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02699v1.pdf filename=2404.02699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the issue of hallucinations and outdated knowledge in <b>large</b> <b>language</b> <b>models</b> is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source <b>large</b> <b>language</b> <b>models,</b> the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editing methods. Our code is available at https: //github.com/TAL-auroraX/SCEN</p></p class="citation"></blockquote><h3 id=5960--59258-a-school-student-essay-corpus-for-analyzing-interactions-of-argumentative-structure-and-quality-maja-stahl-et-al-2024>(59/60 | 59/258) A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality (Maja Stahl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maja Stahl, Nadine Michel, Sebastian Kilsbach, Julian Schmidtke, Sara Rezat, Henning Wachsmuth. (2024)<br><strong>A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality</strong><br><button class=copy-to-clipboard title="A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Essay Scoring<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02529v1.pdf filename=2404.02529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality <b>essays.</b> <b>To</b> support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic <b>essay</b> <b>scoring,</b> interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for <b>essay</b> <b>scoring,</b> no argument mining corpus with ground-truth <b>essay</b> <b>quality</b> annotations has been published yet. Moreover, none of the existing corpora contain <b>essays</b> <b>written</b> by school students specifically. To fill this research gap, we present a German corpus of 1,320 <b>essays</b> <b>from</b> school students of two age groups. Each <b>essay</b> <b>has</b> been manually annotated for argumentative structure and quality on multiple levels of granularity. We propose baseline approaches to argument mining and <b>essay</b> <b>scoring,</b> and we analyze interactions between both tasks, thereby laying the ground for quality-oriented argumentative writing support.</p></p class="citation"></blockquote><h3 id=6060--60258-auxiliary-task-demands-mask-the-capabilities-of-smaller-language-models-jennifer-hu-et-al-2024>(60/60 | 60/258) Auxiliary task demands mask the capabilities of smaller language models (Jennifer Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer Hu, Michael C. Frank. (2024)<br><strong>Auxiliary task demands mask the capabilities of smaller language models</strong><br><button class=copy-to-clipboard title="Auxiliary task demands mask the capabilities of smaller language models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02418v1.pdf filename=2404.02418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of &ldquo;task demands&rdquo; &ndash; the auxiliary challenges associated with performing a particular evaluation &ndash; that may mask the child&rsquo;s underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model&rsquo;s underlying competence, combined with the model&rsquo;s ability to interpret and perform the task given its available resources. Here, we show that for analogical <b>reasoning,</b> reflective <b>reasoning,</b> word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This &ldquo;demand gap&rdquo; is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers&rsquo; design choices.</p></p class="citation"></blockquote><h2 id=cscv-50>cs.CV (50)</h2><h3 id=150--61258-deit-lt-distillation-strikes-back-for-vision-transformer-training-on-long-tailed-datasets-harsh-rangwani-et-al-2024>(1/50 | 61/258) DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets (Harsh Rangwani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Rangwani, Pradipto Mondal, Mayank Mishra, Ashish Ramayee Asokan, R. Venkatesh Babu. (2024)<br><strong>DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets</strong><br><button class=copy-to-clipboard title="DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 100<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation, Knowledge Distillation, Out-of-distribution, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02900v1.pdf filename=2404.02900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision</b> <b>Transformer</b> (ViT) has emerged as a prominent architecture for various computer <b>vision</b> <b>tasks.</b> In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN),</b> ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of <b>distillation</b> from <b>CNN</b> via <b>distillation</b> DIST token by using <b>out-of-distribution</b> images and re-weighting the <b>distillation</b> loss to enhance focus on tail classes. This leads to the learning of local <b>CNN-like</b> features in early ViT blocks, improving generalization for tail classes. Further, to mitigate overfitting, we propose <b>distilling</b> from a flat <b>CNN</b> teacher, which leads to learning low-rank generalizable features for DIST tokens across all ViT blocks. With the proposed DeiT-LT scheme, the <b>distillation</b> DIST token becomes an expert on the tail classes, and the classifier CLS token becomes an expert on the head classes. The experts help to effectively learn features corresponding to both the majority and minority classes using a distinct set of tokens within the same ViT architecture. We show the effectiveness of DeiT-LT for training ViT from scratch on datasets ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.</p></p class="citation"></blockquote><h3 id=250--62258-multi-scale-spatial-temporal-self-attention-graph-convolutional-networks-for-skeleton-based-action-recognition-ikuo-nakamura-2024>(2/50 | 62/258) Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition (Ikuo Nakamura, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ikuo Nakamura. (2024)<br><strong>Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition</strong><br><button class=copy-to-clipboard title="Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-5-1, I-2-10, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02624v1.pdf filename=2404.02624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based gesture recognition methods have achieved high success using <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN).</b> In addition, context-dependent adaptive topology as a neighborhood vertex information and attention mechanism leverages a model to better represent actions. In this paper, we propose <b>self-attention</b> <b>GCN</b> hybrid model, Multi-Scale Spatial-Temporal <b>self-attention</b> (MSST)-GCN to effectively improve modeling ability to achieve state-of-the-art results on several datasets. We utilize spatial <b>self-attention</b> module with adaptive topology to understand intra-frame interactions within a frame among different body parts, and temporal <b>self-attention</b> module to examine correlations between frames of a node. These two are followed by multi-scale <b>convolution</b> <b>network</b> with dilations, which not only captures the long-range temporal dependencies of joints but also the long-range spatial dependencies (i.e., long-distance dependencies) of node temporal behaviors. They are combined into high-level spatial-temporal representations and output the predicted action with the softmax classifier.</p></p class="citation"></blockquote><h3 id=350--63258-enhancing-human-computer-interaction-in-chest-x-ray-analysis-using-vision-and-language-model-with-eye-gaze-patterns-yunsoo-kim-et-al-2024>(3/50 | 63/258) Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns (Yunsoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Yue Gao, Honghan Wu. (2024)<br><strong>Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns</strong><br><button class=copy-to-clipboard title="Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Question Answering, Visual Question Answering, Prompt, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02370v1.pdf filename=2404.02370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Computer Assisted Diagnosis have shown promising performance in medical imaging tasks, particularly in chest X-ray analysis. However, the interaction between these models and radiologists has been primarily limited to input images. This work proposes a novel approach to enhance human-computer interaction in chest X-ray analysis using <b>Vision-Language</b> Models (VLMs) enhanced with radiologists&rsquo; attention by incorporating eye gaze data alongside textual <b>prompts.</b> Our approach leverages heatmaps generated from eye gaze data, overlaying them onto medical images to highlight areas of intense radiologist&rsquo;s focus during chest X-ray evaluation. We evaluate this methodology in tasks such as <b>visual</b> <b>question</b> <b>answering,</b> chest X-ray report automation, error detection, and differential diagnosis. Our results demonstrate the inclusion of eye gaze information significantly enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on <b>fine-tuning</b> was confirmed as it outperformed other medical VLMs in all tasks except <b>visual</b> <b>question</b> <b>answering.</b> This work marks the potential of leveraging both the VLM&rsquo;s capabilities and the radiologist&rsquo;s domain knowledge to improve the capabilities of AI models in medical imaging, paving a novel way for Computer Assisted Diagnosis with a human-centred AI.</p></p class="citation"></blockquote><h3 id=450--64258-asap-interpretable-analysis-and-summarization-of-ai-generated-image-patterns-at-scale-jinbin-huang-et-al-2024>(4/50 | 64/258) ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale (Jinbin Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinbin Huang, Chen Chen, Aditi Mishra, Bum Chul Kwon, Zhicheng Liu, Chris Bryan. (2024)<br><strong>ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale</strong><br><button class=copy-to-clipboard title="ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Benchmarking, Generative Adversarial Network, Knowledge Distillation, Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02990v1.pdf filename=2404.02990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative image models have emerged as a promising technology to produce realistic images. Despite potential benefits, concerns grow about its misuse, particularly in generating deceptive images that could raise significant ethical, legal, and societal issues. Consequently, there is growing demand to empower users to effectively discern and comprehend patterns of AI-generated images. To this end, we developed ASAP, an interactive visualization system that automatically extracts distinct patterns of AI-generated images and allows users to interactively explore them via various views. To uncover fake patterns, ASAP introduces a novel image encoder, adapted from CLIP, which transforms images into compact <b>&ldquo;distilled&rdquo;</b> representations, enriched with information for differentiating authentic and fake images. These representations generate gradients that propagate back to the attention maps of CLIP&rsquo;s <b>transformer</b> block. This process quantifies the relative importance of each pixel to image authenticity or fakeness, exposing key deceptive patterns. ASAP enables the at scale interactive analysis of these patterns through multiple, coordinated visualizations. This includes a representation overview with innovative cell glyphs to aid in the exploration and qualitative evaluation of fake patterns across a vast array of images, as well as a pattern view that displays authenticity-indicating patterns in images and quantifies their impact. ASAP supports the analysis of cutting-edge generative models with the latest architectures, including <b>GAN-based</b> models like proGAN and <b>diffusion</b> <b>models</b> like the latent <b>diffusion</b> <b>model.</b> We demonstrate ASAP&rsquo;s usefulness through two usage scenarios using multiple fake image detection <b>benchmark</b> datasets, revealing its ability to identify and understand hidden patterns in AI-generated images, especially in detecting fake human faces produced by <b>diffusion-based</b> <b>techniques.</b></p></p class="citation"></blockquote><h3 id=550--65258-weakly-supervised-3d-scene-graph-generation-via-visual-linguistic-assisted-pseudo-labeling-xu-wang-et-al-2024>(5/50 | 65/258) Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling (Xu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Wang, Yifan Li, Qiudan Zhang, Wenhui Wu, Mark Junjie Li, Jianmin Jinag. (2024)<br><strong>Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling</strong><br><button class=copy-to-clipboard title="Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02527v1.pdf filename=2404.02527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning to build 3D scene <b>graphs</b> <b>is</b> <b>essential</b> for real-world perception in a structured and rich fashion. However, previous 3D scene <b>graph</b> <b>generation</b> <b>methods</b> utilize a fully <b>supervised</b> <b>learning</b> manner and require a large amount of entity-level annotation data of objects and relations, which is extremely resource-consuming and tedious to obtain. To tackle this problem, we propose 3D-VLAP, a <b>weakly-supervised</b> 3D scene <b>graph</b> <b>generation</b> <b>method</b> via Visual-Linguistic Assisted Pseudo-labeling. Specifically, our 3D-VLAP exploits the superior ability of current large-scale visual-linguistic models to align the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds. First, we establish the positional correspondence from 3D point clouds to 2D images via camera intrinsic and extrinsic parameters, thereby achieving alignment of 3D point clouds and 2D images. Subsequently, a large-scale cross-modal visual-linguistic model is employed to indirectly align 3D instances with the textual category labels of objects by matching 2D images with object category labels. The pseudo labels for objects and relations are then produced for 3D-VLAP model training by calculating the similarity between visual embeddings and textual category embeddings of objects and relations encoded by the visual-linguistic model, respectively. Ultimately, we design an edge <b>self-attention</b> based <b>graph</b> <b>neural</b> <b>network</b> to generate scene <b>graphs</b> <b>of</b> <b>3D</b> point cloud scenes. Extensive experiments demonstrate that our 3D-VLAP achieves comparable results with current advanced fully <b>supervised</b> <b>methods,</b> meanwhile significantly alleviating the pressure of data annotation.</p></p class="citation"></blockquote><h3 id=650--66258-on-the-scalability-of-diffusion-based-text-to-image-generation-hao-li-et-al-2024>(6/50 | 66/258) On the Scalability of Diffusion-based Text-to-Image Generation (Hao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, Stefano Soatto. (2024)<br><strong>On the Scalability of Diffusion-based Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="On the Scalability of Diffusion-based Text-to-Image Generation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Transformer, Text2image, Text2image, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02883v1.pdf filename=2404.02883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Scaling</b> <b>up</b> model and data size has been quite successful for the evolution of <b>LLMs.</b> However, the <b>scaling</b> <b>law</b> for the diffusion based <b>text-to-image</b> (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the <b>scaling</b> <b>properties</b> of diffusion based T2I models by performing extensive and rigours ablations on <b>scaling</b> <b>both</b> denoising backbones and training set, including training scaled UNet and <b>Transformer</b> variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model <b>scaling,</b> <b>we</b> find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the <b>transformer</b> blocks is more parameter-efficient for improving <b>text-image</b> alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45% smaller and 28% faster than SDXL&rsquo;s UNet. On the data <b>scaling</b> <b>side,</b> we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves <b>text-image</b> alignment performance and the learning efficiency. Finally, we provide <b>scaling</b> <b>functions</b> to predict the <b>text-image</b> alignment performance as functions of the scale of model size, compute and dataset size.</p></p class="citation"></blockquote><h3 id=750--67258-a-unified-membership-inference-method-for-visual-self-supervised-encoder-via-part-aware-capability-jie-zhu-et-al-2024>(7/50 | 67/258) A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability (Jie Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Zhu, Jirong Zha, Ding Li, Leye Wang. (2024)<br><strong>A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability</strong><br><button class=copy-to-clipboard title="A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Black Box, Contrastive Learning, Self-supervised Learning, Self-supervised Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02462v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02462v1.pdf filename=2404.02462v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> shows promise in harnessing extensive unlabeled data, but it also confronts significant privacy concerns, especially in vision. In this paper, we aim to perform membership inference on visual <b>self-supervised</b> <b>models</b> in a more realistic setting: <b>self-supervised</b> <b>training</b> method and details are unknown for an adversary when attacking as he usually faces a <b>black-box</b> <b>system</b> in practice. In this setting, considering that <b>self-supervised</b> <b>model</b> could be trained by completely different <b>self-supervised</b> <b>paradigms,</b> e.g., masked image modeling and <b>contrastive</b> <b>learning,</b> with complex training details, we propose a unified membership inference method called PartCrop. It is motivated by the shared part-aware capability among models and stronger part response on the training data. Specifically, PartCrop crops parts of objects in an image to query responses with the image in representation space. We conduct extensive attacks on <b>self-supervised</b> <b>models</b> with different training protocols and structures using three widely used image datasets. The results verify the effectiveness and generalization of PartCrop. Moreover, to defend against PartCrop, we evaluate two common approaches, i.e., early stop and <b>differential</b> <b>privacy,</b> and propose a tailored method called shrinking crop scale range. The defense experiments indicate that all of them are effective. Our code is available at <a href=https://github.com/JiePKU/PartCrop>https://github.com/JiePKU/PartCrop</a></p></p class="citation"></blockquote><h3 id=850--68258-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction-keyu-tian-et-al-2024>(8/50 | 68/258) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction (Keyu Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang. (2024)<br><strong>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</strong><br><button class=copy-to-clipboard title="Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Transformer, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02905v1.pdf filename=2404.02905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine &ldquo;next-scale prediction&rdquo; or &ldquo;next-resolution prediction&rdquo;, diverging from the standard raster-scan &ldquo;next-token prediction&rdquo;. This simple, intuitive methodology allows autoregressive (AR) <b>transformers</b> to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion <b>transformers</b> in image generation. On ImageNet 256x256 <b>benchmark,</b> VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion <b>Transformer</b> (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. <b>Scaling</b> <b>up</b> VAR models exhibits clear power-law <b>scaling</b> <b>laws</b> similar to those observed in <b>LLMs,</b> with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases <b>zero-shot</b> generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of <b>LLMs:</b> <b>Scaling</b> <b>Laws</b> and <b>zero-shot</b> task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.</p></p class="citation"></blockquote><h3 id=950--69258-viassist-adapting-multi-modal-large-language-models-for-users-with-visual-impairments-bufang-yang-et-al-2024>(9/50 | 69/258) VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments (Bufang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bufang Yang, Lixing He, Kaiwei Liu, Zhenyu Yan. (2024)<br><strong>VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments</strong><br><button class=copy-to-clipboard title="VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, Reasoning, BERTScore, Large Language Model, Rouge<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02508v1.pdf filename=2404.02508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs&rsquo; great capabilities of visual understanding and <b>reasoning.</b> However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users&rsquo; queries based on the images. Our results show that VIAssist provides +0.21 and +0.31 higher <b>BERTScore</b> and <b>ROUGE</b> scores than the baseline, respectively.</p></p class="citation"></blockquote><h3 id=1050--70258-scaling-laws-for-galaxy-images-mike-walmsley-et-al-2024>(10/50 | 70/258) Scaling Laws for Galaxy Images (Mike Walmsley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Walmsley, Micah Bowles, Anna M. M. Scaife, Jason Shingirai Makechemu, Alexander J. Gordon, Annette M. N. Ferguson, Robert G. Mann, James Pearson, Jürgen J. Popp, Jo Bovy, Josh Speagle, Hugh Dickinson, Lucy Fortson, Tobias Géron, Sandor Kruk, Chris J. Lintott, Kameswara Mantha, Devina Mohan, David O&rsquo;Ryan, Inigo V. Slijepevic. (2024)<br><strong>Scaling Laws for Galaxy Images</strong><br><button class=copy-to-clipboard title="Scaling Laws for Galaxy Images" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: astro-ph-GA, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02973v1.pdf filename=2404.02973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the first systematic investigation of <b>supervised</b> <b>scaling</b> <b>laws</b> outside of an ImageNet-like context - on images of galaxies. We use 840k galaxy images and over 100M annotations by Galaxy Zoo volunteers, comparable in scale to Imagenet-1K. We find that adding annotated galaxy images provides a power law improvement in performance across all architectures and all tasks, while adding trainable parameters is effective only for some (typically more subjectively challenging) tasks. We then compare the downstream performance of <b>finetuned</b> models pretrained on either ImageNet-12k alone vs. additionally pretrained on our galaxy images. We achieve an average relative error rate reduction of 31% across 5 downstream tasks of scientific interest. Our <b>finetuned</b> models are more label-efficient and, unlike their ImageNet-12k-pretrained equivalents, often achieve linear transfer performance equal to that of end-to-end <b>finetuning.</b> We find relatively modest additional downstream benefits from <b>scaling</b> <b>model</b> size, implying that <b>scaling</b> <b>alone</b> is not sufficient to address our domain gap, and suggest that practitioners with qualitatively different images might benefit more from in-domain adaption followed by targeted downstream labelling.</p></p class="citation"></blockquote><h3 id=1150--71258-knowledge-distillation-with-multi-granularity-mixture-of-priors-for-image-super-resolution-simiao-li-et-al-2024>(11/50 | 71/258) Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution (Simiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simiao Li, Yun Zhang, Wei Li, Hanting Chen, Wenjia Wang, Bingyi Jing, Shaohui Lin, Jie Hu. (2024)<br><strong>Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02573v1.pdf filename=2404.02573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> <b>(KD)</b> is a promising yet challenging <b>model</b> <b>compression</b> technique that transfers rich learning representations from a well-performing but cumbersome teacher <b>model</b> <b>to</b> a compact student <b>model.</b> <b>Previous</b> methods for image super-resolution (SR) mostly compare the feature maps directly or after standardizing the dimensions with basic algebraic operations (e.g. average, dot-product). However, the intrinsic semantic differences among feature maps are overlooked, which are caused by the disparate expressive capacity between the networks. This work presents MiPKD, a multi-granularity mixture of prior <b>KD</b> framework, to facilitate efficient SR <b>model</b> <b>through</b> the feature mixture in a unified latent space and stochastic network block mixture. Extensive experiments demonstrate the effectiveness of the proposed MiPKD method.</p></p class="citation"></blockquote><h3 id=1250--72258-rs3mamba-visual-state-space-model-for-remote-sensing-images-semantic-segmentation-xianping-ma-et-al-2024>(12/50 | 72/258) RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation (Xianping Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianping Ma, Xiaokang Zhang, Man-On Pun. (2024)<br><strong>RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation</strong><br><button class=copy-to-clipboard title="RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02457v1.pdf filename=2404.02457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation of remote sensing images is a fundamental task in geoscience research. However, there are some significant shortcomings for the widely used <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and <b>Transformers.</b> The former is limited by its insufficient long-range modeling capabilities, while the latter is hampered by its computational complexity. Recently, a novel visual state space (VSS) model represented by Mamba has emerged, capable of modeling long-range relationships with linear computability. In this work, we propose a novel dual-branch network named remote sensing images semantic segmentation Mamba (RS3Mamba) to incorporate this innovative technology into remote sensing tasks. Specifically, RS3Mamba utilizes VSS blocks to construct an auxiliary branch, providing additional global information to <b>convolution-based</b> main branch. Moreover, considering the distinct characteristics of the two branches, we introduce a collaborative completion module (CCM) to enhance and fuse features from the dual-encoder. Experimental results on two widely used datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness and potential of the proposed RS3Mamba. To the best of our knowledge, this is the first vision Mamba specifically designed for remote sensing images semantic segmentation. The source code will be made available at <a href=https://github.com/sstary/SSRS>https://github.com/sstary/SSRS</a>.</p></p class="citation"></blockquote><h3 id=1350--73258-aloha-a-new-measure-for-hallucination-in-captioning-models-suzanne-petryk-et-al-2024>(13/50 | 73/258) ALOHa: A New Measure for Hallucination in Captioning Models (Suzanne Petryk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suzanne Petryk, David M. Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph E. Gonzalez, Trevor Darrell. (2024)<br><strong>ALOHa: A New Measure for Hallucination in Captioning Models</strong><br><button class=copy-to-clipboard title="ALOHa: A New Measure for Hallucination in Captioning Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Object Detection, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02904v1.pdf filename=2404.02904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent advances in <b>multimodal</b> pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating <b>objects</b> <b>not</b> present in a scene. The existing prominent metric for <b>object</b> <b>hallucination,</b> CHAIR, is limited to a fixed set of MS COCO <b>objects</b> <b>and</b> synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to measure <b>object</b> <b>hallucinations.</b> Specifically, we use an <b>LLM</b> to extract groundable <b>objects</b> <b>from</b> a candidate caption, measure their semantic similarity to reference <b>objects</b> <b>from</b> captions and <b>object</b> <b>detections,</b> and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated <b>objects</b> <b>than</b> CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where <b>objects</b> <b>extend</b> beyond MS COCO categories. Our code is available at <a href=https://davidmchan.github.io/aloha/>https://davidmchan.github.io/aloha/</a>.</p></p class="citation"></blockquote><h3 id=1450--74258-matatlas-text-driven-consistent-geometry-texturing-and-material-assignment-duygu-ceylan-et-al-2024>(14/50 | 74/258) MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment (Duygu Ceylan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duygu Ceylan, Valentin Deschaintre, Thibault Groueix, Rosalie Martin, Chun-Hao Huang, Romain Rouffet, Vladimir Kim, Gaëtan Lassagne. (2024)<br><strong>MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment</strong><br><button class=copy-to-clipboard title="MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 35<br>Keywords: Geometry, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02899v1.pdf filename=2404.02899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present MatAtlas, a method for consistent text-guided 3D model texturing. Following recent progress we leverage a <b>large</b> <b>scale</b> <b>text-to-image</b> generation model (e.g., Stable Diffusion) as a prior to texture a 3D model. We carefully design an RGB texturing pipeline that leverages a grid pattern diffusion, driven by depth and edges. By proposing a multi-step texture refinement process, we significantly improve the quality and 3D consistency of the texturing output. To further address the problem of baked-in lighting, we move beyond RGB colors and pursue assigning parametric materials to the assets. Given the high-quality initial RGB texture, we propose a novel material retrieval method capitalized on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> enabling editabiliy and relightability. We evaluate our method on a wide variety of geometries and show that our method significantly outperform prior arts. We also analyze the role of each component through a detailed ablation study.</p></p class="citation"></blockquote><h3 id=1550--75258-lvlm-intrepret-an-interpretability-tool-for-large-vision-language-models-gabriela-ben-melech-stan-et-al-2024>(15/50 | 75/258) LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models (Gabriela Ben Melech Stan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriela Ben Melech Stan, Raanan Yehezkel Rohekar, Yaniv Gurwicz, Matthew Lyle Olson, Anahita Bhiwandiwalla, Estelle Aflalo, Chenfei Wu, Nan Duan, Shao-Yen Tseng, Vasudev Lal. (2024)<br><strong>LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Multi-modal, Grounding, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03118v1.pdf filename=2404.03118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of artificial intelligence, <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> are emerging as a significant area of interest. These models, which combine various forms of data input, are becoming increasingly popular. However, understanding their internal mechanisms remains a complex task. Numerous advancements have been made in the field of explainability tools and mechanisms, yet there is still much to explore. In this work, we present a novel interactive application aimed towards understanding the internal mechanisms of <b>large</b> <b>vision-language</b> <b>models.</b> Our interface is designed to enhance the interpretability of the image patches, which are instrumental in generating an answer, and assess the efficacy of the language model in <b>grounding</b> its output in the image. With our application, a user can systematically investigate the model and uncover system limitations, paving the way for enhancements in system capabilities. Finally, we present a case study of how our application can aid in understanding failure mechanisms in a popular <b>large</b> <b>multi-modal</b> <b>model:</b> LLaVA.</p></p class="citation"></blockquote><h3 id=1650--76258-what-are-we-measuring-when-we-evaluate-large-vision-language-models-an-analysis-of-latent-factors-and-biases-anthony-meng-huat-tiong-et-al-2024>(16/50 | 76/258) What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases (Anthony Meng Huat Tiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Meng Huat Tiong, Junqi Zhao, Boyang Li, Junnan Li, Steven C. H. Hoi, Caiming Xiong. (2024)<br><strong>What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases</strong><br><button class=copy-to-clipboard title="What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Transfer Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02415v1.pdf filename=2404.02415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> (VL) models, pretrained on colossal <b>image-text</b> datasets, have attained broad VL competence that is difficult to evaluate. A common belief is that a small number of VL skills underlie the variety of VL tests. In this paper, we perform a large-scale <b>transfer</b> <b>learning</b> experiment aimed at discovering latent VL skills from data. We reveal interesting characteristics that have important implications for test suite design. First, generation tasks suffer from a length bias, suggesting <b>benchmarks</b> should balance tasks with varying output lengths. Second, we demonstrate that factor analysis successfully identifies reasonable yet surprising VL skill factors, suggesting <b>benchmarks</b> could leverage similar analyses for task selection. Finally, we present a new dataset, OLIVE (<a href=https://github.com/jq-zh/olive-dataset)>https://github.com/jq-zh/olive-dataset)</a>, which simulates user instructions in the wild and presents challenges dissimilar to all datasets we tested. Our findings contribute to the design of balanced and broad-coverage <b>vision-language</b> evaluation methods.</p></p class="citation"></blockquote><h3 id=1750--77258-lidardm-generative-lidar-simulation-in-a-generated-world-vlas-zyrianov-et-al-2024>(17/50 | 77/258) LidarDM: Generative LiDAR Simulation in a Generated World (Vlas Zyrianov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vlas Zyrianov, Henry Che, Zhijian Liu, Shenlong Wang. (2024)<br><strong>LidarDM: Generative LiDAR Simulation in a Generated World</strong><br><button class=copy-to-clipboard title="LidarDM: Generative LiDAR Simulation in a Generated World" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02903v1.pdf filename=2404.02903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LidarDM, a novel LiDAR generative model capable of producing realistic, layout-aware, physically plausible, and temporally coherent LiDAR videos. LidarDM stands out with two unprecedented capabilities in LiDAR generative modeling: (i) LiDAR generation guided by driving scenarios, offering significant potential for autonomous driving <b>simulations,</b> and (ii) 4D LiDAR point cloud generation, enabling the creation of realistic and temporally coherent sequences. At the heart of our model is a novel integrated 4D world generation framework. Specifically, we employ latent <b>diffusion</b> <b>models</b> to generate the 3D scene, combine it with dynamic actors to form the underlying 4D world, and subsequently produce realistic sensory observations within this virtual environment. Our experiments indicate that our approach outperforms competing algorithms in realism, temporal coherency, and layout consistency. We additionally show that LidarDM can be used as a generative world model simulator for training and testing perception models.</p></p class="citation"></blockquote><h3 id=1850--78258-mulan-a-multi-layer-annotated-dataset-for-controllable-text-to-image-generation-petru-daniel-tudosiu-et-al-2024>(18/50 | 78/258) MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation (Petru-Daniel Tudosiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, Sarah Parisot. (2024)<br><strong>MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Generative AI, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02790v1.pdf filename=2404.02790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generation has achieved astonishing results, yet precise spatial controllability and <b>prompt</b> fidelity remain highly challenging. This limitation is typically addressed through cumbersome <b>prompt</b> engineering, scene layout conditioning, or image editing techniques which often require hand drawn masks. Nonetheless, pre-existing works struggle to take advantage of the natural instance-level compositionality of scenes due to the typically flat nature of rasterized RGB output images. Towards adressing this challenge, we introduce MuLAn: a novel dataset comprising over 44K MUlti-Layer ANnotations of RGB images as multilayer, instance-wise RGBA decompositions, and over 100K instance images. To build MuLAn, we developed a training free pipeline which decomposes a monocular RGB image into a stack of RGBA layers comprising of background and isolated instances. We achieve this through the use of pretrained general-purpose models, and by developing three modules: image decomposition for instance discovery and extraction, instance completion to reconstruct occluded areas, and image re-assembly. We use our pipeline to create MuLAn-COCO and MuLAn-LAION datasets, which contain a variety of image decompositions in terms of style, composition and complexity. With MuLAn, we provide the first photorealistic resource providing instance decomposition and occlusion information for high quality images, opening up new avenues for <b>text-to-image</b> <b>generative</b> <b>AI</b> research. With this, we aim to encourage the development of novel generation and editing technology, in particular layer-wise solutions. MuLAn data resources are available at <a href=https://MuLAn-dataset.github.io/>https://MuLAn-dataset.github.io/</a>.</p></p class="citation"></blockquote><h3 id=1950--79258-adaptive-affinity-based-generalization-for-mri-imaging-segmentation-across-resource-limited-settings-eddardaa-b-loussaief-et-al-2024>(19/50 | 79/258) Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings (Eddardaa B. Loussaief et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eddardaa B. Loussaief, Mohammed Ayad, Domenc Puig, Hatem A. Rashwan. (2024)<br><strong>Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings</strong><br><button class=copy-to-clipboard title="Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02738v1.pdf filename=2404.02738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The joint utilization of diverse data sources for medical imaging segmentation has emerged as a crucial area of research, aiming to address challenges such as data heterogeneity, domain shift, and data quality discrepancies. Integrating information from multiple data domains has shown promise in improving model generalizability and adaptability. However, this approach often demands substantial computational resources, hindering its practicality. In response, <b>knowledge</b> <b>distillation</b> <b>(KD)</b> has garnered attention as a solution. <b>KD</b> involves training light-weight models to emulate the behavior of more resource-intensive models, thereby mitigating the computational burden while maintaining performance. This paper addresses the pressing need to develop a lightweight and generalizable model for medical imaging segmentation that can effectively handle data integration challenges. Our proposed approach introduces a novel relation-based <b>knowledge</b> <b>framework</b> by seamlessly combining adaptive affinity-based and kernel-based <b>distillation</b> through a gram matrix that can capture the style representation across features. This methodology empowers the student model to accurately replicate the feature representations of the teacher model, facilitating robust performance even in the face of domain shift and data heterogeneity. To validate our innovative approach, we conducted experiments on publicly available multi-source prostate MRI data. The results demonstrate a significant enhancement in segmentation performance using lightweight networks. Notably, our method achieves this improvement while reducing both inference time and storage usage, rendering it a practical and efficient solution for real-time medical imaging segmentation.</p></p class="citation"></blockquote><h3 id=2050--80258-rs-mamba-for-large-remote-sensing-image-dense-prediction-sijie-zhao-et-al-2024>(20/50 | 80/258) RS-Mamba for Large Remote Sensing Image Dense Prediction (Sijie Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, Wanli Ouyang. (2024)<br><strong>RS-Mamba for Large Remote Sensing Image Dense Prediction</strong><br><button class=copy-to-clipboard title="RS-Mamba for Large Remote Sensing Image Dense Prediction" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02668v1.pdf filename=2404.02668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The spatial resolution of remote sensing images is becoming increasingly higher, posing challenges in handling large very-high-resolution (VHR) remote sensing images for dense prediction tasks. Models based on <b>convolutional</b> <b>neural</b> <b>networks</b> are limited in their ability to model global features of remote sensing images due to local <b>convolution</b> operations. <b>Transformer</b> based models, despite their global modeling capabilities, face computational challenges with large VHR images due to their quadratic complexity. The common practice of cropping large images into smaller patches leads to a significant loss of contextual information. To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in VHR remote sensing. RSM is designed to model global features of remote sensing images with linear complexity, enabling it to process large VHR images effectively. It employs an omnidirectional selective scan module to globally model the images in multiple directions, capturing large spatial features from various directions. Experiments on semantic segmentation and change detection tasks across various objects demonstrate the effectiveness of RSM. With simple model architecture and training approach, RSM achieves state-of-the-art performance on the dense prediction tasks of VHR remote sensing. The code for this work will be available at <a href=https://github.com/walking-shadow/Official_Remote_Sensing_Mamba>https://github.com/walking-shadow/Official_Remote_Sensing_Mamba</a>.</p></p class="citation"></blockquote><h3 id=2150--81258-non-negative-subspace-feature-representation-for-few-shot-learning-in-medical-imaging-keqiang-fan-et-al-2024>(21/50 | 81/258) Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging (Keqiang Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keqiang Fan, Xiaohao Cai, Mahesan Niranjan. (2024)<br><strong>Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging</strong><br><button class=copy-to-clipboard title="Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02656v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02656v2.pdf filename=2404.02656v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unlike typical visual scene recognition domains, in which massive datasets are accessible to deep neural networks, medical image interpretations are often obstructed by the paucity of data. In this paper, we investigate the effectiveness of data-based <b>few-shot</b> <b>learning</b> in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in <b>few-shot</b> <b>learning,</b> addressing the data scarcity issue in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its <b>supervised</b> variants (e.g., discriminative NMF, and <b>supervised</b> and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for <b>few-shot</b> <b>learning</b> in medical imaging, and the <b>supervised</b> NMF algorithms are more discriminative in the subspace with greater effectiveness. Furthermore, we show that the part-based representation of NMF, especially its <b>supervised</b> variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples.</p></p class="citation"></blockquote><h3 id=2250--82258-diffexplainer-towards-cross-modal-global-explanations-with-diffusion-models-matteo-pennisi-et-al-2024>(22/50 | 82/258) Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models (Matteo Pennisi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Pennisi, Giovanni Bellitto, Simone Palazzo, Mubarak Shah, Concetto Spampinato. (2024)<br><strong>Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models</strong><br><button class=copy-to-clipboard title="Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02618v1.pdf filename=2404.02618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DiffExplainer, a novel framework that, leveraging language-vision models, enables <b>multimodal</b> global explainability. DiffExplainer employs <b>diffusion</b> <b>models</b> conditioned on optimized text <b>prompts,</b> synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization methods, and 2) the automated identification of biases and spurious features.</p></p class="citation"></blockquote><h3 id=2350--83258-salfom-dynamic-saliency-prediction-with-video-foundation-models-morteza-moradi-et-al-2024>(23/50 | 83/258) SalFoM: Dynamic Saliency Prediction with Video Foundation Models (Morteza Moradi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morteza Moradi, Mohammad Moradi, Francesco Rundo, Concetto Spampinato, Ali Borji, Simone Palazzo. (2024)<br><strong>SalFoM: Dynamic Saliency Prediction with Video Foundation Models</strong><br><button class=copy-to-clipboard title="SalFoM: Dynamic Saliency Prediction with Video Foundation Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Foundation Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03097v1.pdf filename=2404.03097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in video saliency prediction (VSP) have shown promising performance compared to the human visual system, whose emulation is the primary goal of VSP. However, current state-of-the-art models employ spatio-temporal <b>transformers</b> trained on limited amounts of data, hindering generalizability adaptation to downstream tasks. The benefits of vision <b>foundation</b> <b>models</b> present a potential solution to improve the VSP process. However, adapting image <b>foundation</b> <b>models</b> to the video domain presents significant challenges in modeling scene dynamics and capturing temporal information. To address these challenges, and as the first initiative to design a VSP model based on video <b>foundation</b> <b>models,</b> we introduce SalFoM, a novel encoder-decoder video <b>transformer</b> architecture. Our model employs UnMasked Teacher (UMT) as feature extractor and presents a heterogeneous decoder which features a locality-aware spatio-temporal <b>transformer</b> and integrates local and global spatio-temporal information from various perspectives to produce the final saliency map. Our qualitative and quantitative experiments on the challenging VSP <b>benchmark</b> datasets of DHF1K, Hollywood-2 and UCF-Sports demonstrate the superiority of our proposed model in comparison with the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2450--84258-flightscope-a-deep-comprehensive-assessment-of-aircraft-detection-algorithms-in-satellite-imagery-safouane-el-ghazouali-et-al-2024>(24/50 | 84/258) FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery (Safouane El Ghazouali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Safouane El Ghazouali, Arnaud Gucciardi, Nicola Venturi, Michael Rueegsegger, Umberto Michelucci. (2024)<br><strong>FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery</strong><br><button class=copy-to-clipboard title="FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Yolo, Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02877v1.pdf filename=2404.02877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced <b>object</b> <b>detection</b> algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including <b>YOLO</b> versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research highlight the nuanced performance landscapes of these algorithms, with YOLOv5 emerging as a robust solution for aerial <b>object</b> <b>detection,</b> underlining its importance through superior mean average precision, Recall, and Intersection over Union scores. The findings described here underscore the fundamental role of algorithm selection aligned with the specific demands of satellite imagery analysis and extend a comprehensive framework to evaluate model efficacy. The <b>benchmark</b> toolkit and codes, available via <a href=https://github.com/toelt-llc/FlightScope_Bench>https://github.com/toelt-llc/FlightScope_Bench</a>, aims to further exploration and innovation in the realm of remote sensing <b>object</b> <b>detection,</b> paving the way for improved analytical methodologies in satellite imagery applications.</p></p class="citation"></blockquote><h3 id=2550--85258-active-learning-for-efficient-annotation-in-precision-agriculture-a-use-case-on-crop-weed-semantic-segmentation-bart-m-van-marrewijk-et-al-2024>(25/50 | 85/258) Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation (Bart M. van Marrewijk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bart M. van Marrewijk, Charbel Dandjinou, Dan Jeric Arcega Rustia, Nicolas Franco Gonzalez, Boubacar Diallo, Jérôme Dias, Paul Melki, Pieter M. Blok. (2024)<br><strong>Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation</strong><br><button class=copy-to-clipboard title="Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Active Learning, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02580v1.pdf filename=2404.02580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimizing deep learning models requires large amounts of annotated images, a process that is both time-intensive and costly. Especially for semantic segmentation models in which every pixel must be annotated. A potential strategy to mitigate annotation effort is <b>active</b> <b>learning.</b> <b>Active</b> <b>learning</b> facilitates the identification and selection of the most informative images from a large unlabelled pool. The underlying premise is that these selected images can improve the model&rsquo;s performance faster than random selection to reduce annotation effort. While <b>active</b> <b>learning</b> has demonstrated promising results on <b>benchmark</b> datasets like Cityscapes, its performance in the agricultural domain remains largely unexplored. This study addresses this research gap by conducting a comparative study of three <b>active</b> <b>learning-based</b> acquisition functions: Bayesian <b>Active</b> <b>Learning</b> by Disagreement (BALD), stochastic-based BALD (PowerBALD), and Random. The acquisition functions were tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing three semantic classes: background, crop and weed. Our results indicated that <b>active</b> <b>learning,</b> especially PowerBALD, yields a higher performance than Random sampling on both datasets. But due to the relatively large standard deviations, the differences observed were minimal; this was partly caused by high image redundancy and imbalanced classes. Specifically, more than 89% of the pixels belonged to the background class on both datasets. The absence of significant results on both datasets indicates that further research is required for applying <b>active</b> <b>learning</b> on agricultural datasets, especially if they contain a high-class imbalance and redundant images. <b>Recommendations</b> and insights are provided in this paper to potentially resolve such issues.</p></p class="citation"></blockquote><h3 id=2650--86258-semi-supervised-unconstrained-head-pose-estimation-in-the-wild-huayi-zhou-et-al-2024>(26/50 | 86/258) Semi-Supervised Unconstrained Head Pose Estimation in the Wild (Huayi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huayi Zhou, Fei Jiang, Hongtao Lu. (2024)<br><strong>Semi-Supervised Unconstrained Head Pose Estimation in the Wild</strong><br><button class=copy-to-clipboard title="Semi-Supervised Unconstrained Head Pose Estimation in the Wild" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02544v1.pdf filename=2404.02544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing head pose estimation datasets are either composed of numerous samples by non-realistic synthesis or lab collection, or limited images by labor-intensive annotating. This makes deep <b>supervised</b> <b>learning</b> based solutions compromised due to the reliance on generous labeled data. To alleviate it, we propose the first semi-supervised unconstrained head pose estimation (SemiUHPE) method, which can leverage a large amount of unlabeled wild head images. Specifically, we follow the recent semi-supervised rotation regression, and focus on the diverse and complex head pose domain. Firstly, we claim that the aspect-ratio invariant cropping of heads is superior to the previous landmark-based affine alignment, which does not fit unlabeled natural heads or practical applications where landmarks are often unavailable. Then, instead of using an empirically fixed threshold to filter out pseudo labels, we propose the dynamic entropy-based filtering by updating thresholds for adaptively removing unlabeled outliers. Moreover, we revisit the design of weak-strong augmentations, and further exploit its superiority by devising two novel head-oriented strong augmentations named pose-irrelevant cut-occlusion and pose-altering rotation consistency. Extensive experiments show that SemiUHPE can surpass SOTAs with remarkable improvements on public <b>benchmarks</b> under both front-range and full-range. Our code is released in \url{https://github.com/hnuzhy/SemiUHPE}.</p></p class="citation"></blockquote><h3 id=2750--87258-te-tad-towards-full-end-to-end-temporal-action-detection-via-time-aligned-coordinate-expression-ho-joong-kim-et-al-2024>(27/50 | 87/258) TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression (Ho-Joong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ho-Joong Kim, Jung-Ho Hong, Heejo Kong, Seong-Whan Lee. (2024)<br><strong>TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression</strong><br><button class=copy-to-clipboard title="TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02405v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02405v2.pdf filename=2404.02405v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate that the normalized coordinate expression is a key factor as reliance on hand-crafted components in query-based detectors for temporal action detection (TAD). Despite significant advancements towards an end-to-end framework in <b>object</b> <b>detection,</b> query-based detectors have been limited in achieving full end-to-end modeling in TAD. To address this issue, we propose \modelname{}, a full end-to-end temporal action detection <b>transformer</b> that integrates time-aligned coordinate expression. We reformulate coordinate expression utilizing actual timeline values, ensuring length-invariant representations from the extremely diverse video duration environment. Furthermore, our proposed adaptive query selection dynamically adjusts the number of queries based on video length, providing a suitable solution for varying video durations compared to a fixed query set. Our approach not only simplifies the TAD process by eliminating the need for hand-crafted components but also significantly improves the performance of query-based detectors. Our TE-TAD outperforms the previous query-based detectors and achieves competitive performance compared to state-of-the-art methods on popular <b>benchmark</b> datasets. Code is available at: <a href=https://github.com/Dotori-HJ/TE-TAD>https://github.com/Dotori-HJ/TE-TAD</a></p></p class="citation"></blockquote><h3 id=2850--88258-many-to-many-image-generation-with-auto-regressive-diffusion-models-ying-shen-et-al-2024>(28/50 | 88/258) Many-to-many Image Generation with Auto-regressive Diffusion Models (Ying Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Shen, Yizhe Zhang, Shuangfei Zhai, Lifu Huang, Joshua M. Susskind, Jiatao Gu. (2024)<br><strong>Many-to-many Image Generation with Auto-regressive Diffusion Models</strong><br><button class=copy-to-clipboard title="Many-to-many Image Generation with Auto-regressive Diffusion Models" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03109v1.pdf filename=2404.03109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in image generation have made significant progress, yet existing models present limitations in perceiving and generating an arbitrary number of interrelated images within a broad context. This limitation becomes increasingly critical as the demand for multi-image scenarios, such as multi-view images and visual narratives, grows with the expansion of multimedia platforms. This paper introduces a domain-general framework for many-to-many image generation, capable of producing interrelated image series from a given set of images, offering a scalable solution that obviates the need for task-specific solutions across different multi-image scenarios. To facilitate this, we present MIS, a novel large-scale multi-image dataset, containing 12M synthetic multi-image samples, each with 25 interconnected images. Utilizing Stable <b>Diffusion</b> <b>with</b> varied latent noises, our method produces a set of interconnected images from a single caption. Leveraging MIS, we learn M2M, an autoregressive model for many-to-many generation, where each image is modeled within a <b>diffusion</b> <b>framework.</b> Throughout training on the synthetic MIS, the model excels in capturing style and content from preceding images - synthetic or real - and generates novel images following the captured patterns. Furthermore, through task-specific <b>fine-tuning,</b> our model demonstrates its adaptability to various multi-image generation tasks, including Novel View Synthesis and Visual Procedure Generation.</p></p class="citation"></blockquote><h3 id=2950--89258-dpft-dual-perspective-fusion-transformer-for-camera-radar-based-object-detection-felix-fent-et-al-2024>(29/50 | 89/258) DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection (Felix Fent et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Fent, Andras Palffy, Holger Caesar. (2024)<br><strong>DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection</strong><br><button class=copy-to-clipboard title="DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03015v1.pdf filename=2404.03015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion <b>Transformer</b> (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under <a href=https://github.com/TUMFTM/DPFT>https://github.com/TUMFTM/DPFT</a>.</p></p class="citation"></blockquote><h3 id=3050--90258-genn2n-generative-nerf2nerf-translation-xiangyue-liu-et-al-2024>(30/50 | 90/258) GenN2N: Generative NeRF2NeRF Translation (Xiangyue Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan, Li Yi. (2024)<br><strong>GenN2N: Generative NeRF2NeRF Translation</strong><br><button class=copy-to-clipboard title="GenN2N: Generative NeRF2NeRF Translation" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02788v1.pdf filename=2404.02788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present GenN2N, a unified NeRF-to-NeRF translation framework for various NeRF translation tasks such as text-driven NeRF editing, colorization, super-resolution, inpainting, etc. Unlike previous methods designed for individual translation tasks with task-specific schemes, GenN2N achieves all these NeRF editing tasks by employing a plug-and-play image-to-image translator to perform editing in the 2D domain and lifting 2D edits into the 3D NeRF space. Since the 3D consistency of 2D edits may not be assured, we propose to model the distribution of the underlying 3D edits through a generative model that can cover all possible edited NeRFs. To model the distribution of 3D edited NeRFs from 2D edited images, we carefully design a VAE-GAN that encodes images while decoding NeRFs. The latent space is trained to align with a Gaussian distribution and the NeRFs are <b>supervised</b> through an adversarial loss on its renderings. To ensure the latent code does not depend on 2D viewpoints but truly reflects the 3D edits, we also regularize the latent code through a <b>contrastive</b> <b>learning</b> scheme. Extensive experiments on various editing tasks show GenN2N, as a universal framework, performs as well or better than task-specific specialists while possessing flexible generative power. More results on our project page: <a href=https://xiangyueliu.github.io/GenN2N/>https://xiangyueliu.github.io/GenN2N/</a></p></p class="citation"></blockquote><h3 id=3150--91258-dibs-enhancing-dense-video-captioning-with-unlabeled-videos-via-pseudo-boundary-enrichment-and-online-refinement-hao-wu-et-al-2024>(31/50 | 91/258) DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement (Hao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun. (2024)<br><strong>DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement</strong><br><button class=copy-to-clipboard title="DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02755v1.pdf filename=2404.02755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for dense video captioning (DVC), that elaborates on improving the quality of the generated event captions and their associated pseudo event boundaries from unlabeled videos. By leveraging the capabilities of diverse <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we generate rich DVC-oriented caption candidates and optimize the corresponding pseudo boundaries under several meticulously designed objectives, considering diversity, event-centricity, temporal ordering, and coherence. Moreover, we further introduce a novel online boundary refinement strategy that iteratively improves the quality of pseudo boundaries during training. Comprehensive experiments have been conducted to examine the effectiveness of the proposed technique components. By leveraging a substantial amount of unlabeled video data, such as HowTo100M, we achieve a remarkable advancement on standard DVC datasets like YouCook2 and ActivityNet. We outperform the previous state-of-the-art Vid2Seq across a majority of metrics, achieving this with just 0.4% of the unlabeled video data used for pre-training by Vid2Seq.</p></p class="citation"></blockquote><h3 id=3250--92258-cross-attention-makes-inference-cumbersome-in-text-to-image-diffusion-models-wentian-zhang-et-al-2024>(32/50 | 92/258) Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models (Wentian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, Jürgen Schmidhuber. (2024)<br><strong>Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02747v1.pdf filename=2404.02747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the role of cross-attention during inference in text-conditional <b>diffusion</b> <b>models.</b> We find that cross-attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics-planning stage, during which, the model relies on cross-attention to plan text-oriented visual semantics, and a subsequent fidelity-improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity-improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training-free method called TGATE for efficient generation, which caches the cross-attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS-COCO validation set confirms its effectiveness. The source code of TGATE is available at <a href=https://github.com/HaozheLiu-ST/T-GATE>https://github.com/HaozheLiu-ST/T-GATE</a>.</p></p class="citation"></blockquote><h3 id=3350--93258-harnessing-the-power-of-large-vision-language-models-for-synthetic-image-detection-mamadou-keita-et-al-2024>(33/50 | 93/258) Harnessing the Power of Large Vision Language Models for Synthetic Image Detection (Mamadou Keita et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mamadou Keita, Wassim Hamidouche, Hassen Bougueffa, Abdenour Hadid, Abdelmalik Taleb-Ahmed. (2024)<br><strong>Harnessing the Power of Large Vision Language Models for Synthetic Image Detection</strong><br><button class=copy-to-clipboard title="Harnessing the Power of Large Vision Language Models for Synthetic Image Detection" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Fake News Detection, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02726v1.pdf filename=2404.02726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the emergence of models capable of generating images from text has attracted considerable interest, offering the possibility of creating realistic images from text descriptions. Yet these advances have also raised concerns about the potential misuse of these images, including the creation of misleading content such as <b>fake</b> <b>news</b> and propaganda. This study investigates the effectiveness of using advanced <b>vision-language</b> models (VLMs) for synthetic image identification. Specifically, the focus is on tuning state-of-the-art image captioning models for synthetic image detection. By harnessing the robust understanding capabilities of large VLMs, the aim is to distinguish authentic images from synthetic images produced by diffusion-based models. This study contributes to the advancement of synthetic image detection by exploiting the capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring image captioning models, we address the challenges associated with the potential misuse of synthetic images in real-world applications. Results described in this paper highlight the promising role of VLMs in the field of synthetic image detection, outperforming conventional image-based detection techniques. Code and models can be found at <a href=https://github.com/Mamadou-Keita/VLM-DETECT>https://github.com/Mamadou-Keita/VLM-DETECT</a>.</p></p class="citation"></blockquote><h3 id=3450--94258-3dstyleglip-part-tailored-text-guided-3d-neural-stylization-seungjeh-chung-et-al-2024>(34/50 | 94/258) 3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization (SeungJeh Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>SeungJeh Chung, JooHyun Park, Hyewon Kan, HyeongYeop Kang. (2024)<br><strong>3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization</strong><br><button class=copy-to-clipboard title="3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02634v1.pdf filename=2404.02634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D stylization, which entails the application of specific styles to three-dimensional objects, holds significant commercial potential as it enables the creation of diverse 3D objects with distinct moods and styles, tailored to specific demands of different scenes. With recent advancements in text-driven methods and artificial intelligence, the stylization process is increasingly intuitive and automated, thereby diminishing the reliance on manual labor and expertise. However, existing methods have predominantly focused on holistic stylization, thereby leaving the application of styles to individual components of a 3D object unexplored. In response, we introduce 3DStyleGLIP, a novel framework specifically designed for text-driven, part-tailored 3D stylization. Given a 3D mesh and a text <b>prompt,</b> 3DStyleGLIP leverages the <b>vision-language</b> embedding space of the Grounded Language-Image Pre-training (GLIP) model to localize the individual parts of the 3D mesh and modify their colors and local geometries to align them with the desired styles specified in the text <b>prompt.</b> 3DStyleGLIP is effectively trained for 3D stylization tasks through a part-level style loss working in GLIP&rsquo;s embedding space, supplemented by two complementary learning techniques. Extensive experimental validation confirms that our method achieves significant part-wise stylization capabilities, demonstrating promising potential in advancing the field of 3D stylization.</p></p class="citation"></blockquote><h3 id=3550--95258-neural-radiance-fields-with-torch-units-bingnan-ni-et-al-2024>(35/50 | 95/258) Neural Radiance Fields with Torch Units (Bingnan Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingnan Ni, Huanyu Wang, Dongfeng Bai, Minghe Weng, Dexin Qi, Weichao Qiu, Bingbing Liu. (2024)<br><strong>Neural Radiance Fields with Torch Units</strong><br><button class=copy-to-clipboard title="Neural Radiance Fields with Torch Units" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02617v1.pdf filename=2404.02617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRF) give rise to learning-based 3D reconstruction methods widely used in industrial applications. Although prevalent methods achieve considerable improvements in small-scale scenes, accomplishing reconstruction in complex and large-scale scenes is still challenging. First, the background in complex scenes shows a large variance among different views. Second, the current inference pattern, $i.e.$, a pixel only relies on an individual camera ray, fails to capture contextual information. To solve these problems, we propose to enlarge the ray perception field and build up the sample points interactions. In this paper, we design a novel inference pattern that encourages a single camera ray possessing more contextual information, and models the relationship among sample points on each camera ray. To hold contextual information,a camera ray in our proposed method can render a patch of pixels simultaneously. Moreover, we replace the MLP in neural radiance field models with distance-aware <b>convolutions</b> to enhance the feature propagation among sample points from the same camera ray. To <b>summarize,</b> as a torchlight, a ray in our proposed method achieves rendering a patch of image. Thus, we call the proposed method, Torch-NeRF. Extensive experiments on KITTI-360 and LLFF show that the Torch-NeRF exhibits excellent performance.</p></p class="citation"></blockquote><h3 id=3650--96258-unsegment-anything-by-simulating-deformation-jiahao-lu-et-al-2024>(36/50 | 96/258) Unsegment Anything by Simulating Deformation (Jiahao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Lu, Xingyi Yang, Xinchao Wang. (2024)<br><strong>Unsegment Anything by Simulating Deformation</strong><br><button class=copy-to-clipboard title="Unsegment Anything by Simulating Deformation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02585v1.pdf filename=2404.02585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Foundation segmentation models, while powerful, pose a significant risk: they enable users to effortlessly extract any objects from any digital content with a single click, potentially leading to copyright infringement or malicious misuse. To mitigate this risk, we introduce a new task &ldquo;Anything Unsegmentable&rdquo; to grant any image &ldquo;the right to be unsegmented&rdquo;. The ambitious pursuit of the task is to achieve highly transferable <b>adversarial</b> <b>attacks</b> against all <b>prompt-based</b> segmentation models, regardless of model parameterizations and <b>prompts.</b> We highlight the non-transferable and heterogeneous nature of <b>prompt-specific</b> <b>adversarial</b> <b>noises.</b> Our approach focuses on disrupting image encoder features to achieve <b>prompt-agnostic</b> attacks. Intriguingly, targeted feature attacks exhibit better transferability compared to untargeted ones, suggesting the optimal update direction aligns with the image manifold. Based on the observations, we design a novel attack named Unsegment Anything by Simulating Deformation (UAD). Our attack optimizes a differentiable deformation function to create a target deformed image, which alters structural information while preserving achievable feature distance by <b>adversarial</b> <b>example.</b> Extensive experiments verify the effectiveness of our approach, compromising a variety of promptable segmentation models with different architectures and <b>prompt</b> interfaces. We release the code at <a href=https://github.com/jiahaolu97/anything-unsegmentable>https://github.com/jiahaolu97/anything-unsegmentable</a>.</p></p class="citation"></blockquote><h3 id=3750--97258-severity-controlled-text-to-image-generative-model-bias-manipulation-jordan-vice-et-al-2024>(37/50 | 97/258) Severity Controlled Text-to-Image Generative Model Bias Manipulation (Jordan Vice et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian. (2024)<br><strong>Severity Controlled Text-to-Image Generative Model Bias Manipulation</strong><br><button class=copy-to-clipboard title="Severity Controlled Text-to-Image Generative Model Bias Manipulation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02530v1.pdf filename=2404.02530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> (T2I) generative models are gaining wide popularity, especially in public domains. However, their intrinsic bias and potential malicious manipulations remain under-explored. Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models. By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias. As a by-product, this control also allows a form of precise <b>prompt</b> engineering to generate images which are generally implausible with regular text <b>prompts.</b> We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing. Our technique does not require training and is also framed as a backdoor attack with severity control using semantically-null text triggers in the <b>prompts.</b> With extensive analysis, we present interesting qualitative and quantitative results to expose potential manipulation possibilities for T2I models. Key-words: <b>Text-to-Image</b> Models, Generative Models, Backdoor Attacks, <b>Prompt</b> Engineering, Bias</p></p class="citation"></blockquote><h3 id=3850--98258-unsupervised-occupancy-learning-from-sparse-point-cloud-amine-ouasfi-et-al-2024>(38/50 | 98/258) Unsupervised Occupancy Learning from Sparse Point Cloud (Amine Ouasfi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amine Ouasfi, Adnane Boukhayma. (2024)<br><strong>Unsupervised Occupancy Learning from Sparse Point Cloud</strong><br><button class=copy-to-clipboard title="Unsupervised Occupancy Learning from Sparse Point Cloud" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02759v1.pdf filename=2404.02759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit Neural Representations have gained prominence as a powerful framework for capturing complex data modalities, encompassing a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape <b>geometry.</b> However, learning SDFs from 3D point clouds in the absence of ground truth supervision remains a very challenging task. In this paper, we propose a method to infer occupancy fields instead of SDFs as they are easier to learn from sparse inputs. We leverage a margin-based uncertainty measure to differentially sample from the decision boundary of the occupancy function and supervise the sampled boundary points using the input point cloud. We further stabilize the optimization process at the early stages of the training by biasing the occupancy function towards minimal entropy fields while maximizing its entropy at the input point cloud. Through extensive experiments and evaluations, we illustrate the efficacy of our proposed method, highlighting its capacity to improve implicit shape inference with respect to baselines and the state-of-the-art using synthetic and real data.</p></p class="citation"></blockquote><h3 id=3950--99258-independently-keypoint-learning-for-small-object-semantic-correspondence-hailong-jin-et-al-2024>(39/50 | 99/258) Independently Keypoint Learning for Small Object Semantic Correspondence (Hailong Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hailong Jin, Huiying Li. (2024)<br><strong>Independently Keypoint Learning for Small Object Semantic Correspondence</strong><br><button class=copy-to-clipboard title="Independently Keypoint Learning for Small Object Semantic Correspondence" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02678v1.pdf filename=2404.02678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic correspondence remains a challenging task for establishing correspondences between a pair of images with the same category or similar scenes due to the large intra-class appearance. In this paper, we introduce a novel problem called &lsquo;Small Object Semantic Correspondence (SOSC).&rsquo; This problem is challenging due to the close proximity of keypoints associated with small objects, which results in the fusion of these respective features. It is difficult to identify the corresponding key points of the fused features, and it is also difficult to be recognized. To address this challenge, we propose the Keypoint Bounding box-centered Cropping (KBC) method, which aims to increase the spatial separation between keypoints of small objects, thereby facilitating independent learning of these keypoints. The KBC method is seamlessly integrated into our proposed inference pipeline and can be easily incorporated into other methodologies, resulting in significant performance enhancements. Additionally, we introduce a novel framework, named KBCNet, which serves as our baseline model. KBCNet comprises a Cross-Scale Feature Alignment (CSFA) module and an efficient 4D <b>convolutional</b> decoder. The CSFA module is designed to align multi-scale features, enriching keypoint representations by integrating fine-grained features and deep semantic features. Meanwhile, the 4D <b>convolutional</b> decoder, based on efficient 4D <b>convolution,</b> ensures efficiency and rapid convergence. To empirically validate the effectiveness of our proposed methodology, extensive experiments are conducted on three widely used <b>benchmarks:</b> PF-PASCAL, PF-WILLOW, and SPair-71k. Our KBC method demonstrates a substantial performance improvement of 7.5% on the SPair-71K dataset, providing compelling evidence of its efficacy.</p></p class="citation"></blockquote><h3 id=4050--100258-henet-hybrid-encoding-for-end-to-end-multi-task-3d-perception-from-multi-view-cameras-zhongyu-xia-et-al-2024>(40/50 | 100/258) HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras (Zhongyu Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongyu Xia, ZhiWei Lin, Xinhao Wang, Yongtao Wang, Yun Xing, Shengxiang Qi, Nan Dong, Ming-Hsuan Yang. (2024)<br><strong>HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras</strong><br><button class=copy-to-clipboard title="HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02517v1.pdf filename=2404.02517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-dimensional perception from multi-view cameras is a crucial component in autonomous driving systems, which involves multiple tasks like 3D <b>object</b> <b>detection</b> and bird&rsquo;s-eye-view (BEV) semantic segmentation. To improve perception precision, large image encoders, high-resolution images, and long-term temporal inputs have been adopted in recent 3D perception models, bringing remarkable performance gains. However, these techniques are often incompatible in training and inference scenarios due to computational resource constraints. Besides, modern autonomous driving systems prefer to adopt an end-to-end framework for multi-task 3D perception, which can simplify the overall system architecture and reduce the implementation complexity. However, conflict between tasks often arises when optimizing multiple tasks jointly within an end-to-end 3D perception model. To alleviate these issues, we present an end-to-end framework named HENet for multi-task 3D perception in this paper. Specifically, we propose a hybrid image encoding network, using a large image encoder for short-term frames and a small image encoder for long-term temporal frames. Then, we introduce a temporal feature integration module based on the attention mechanism to fuse the features of different frames extracted by the two aforementioned hybrid image encoders. Finally, according to the characteristics of each perception task, we utilize BEV features of different grid sizes, independent BEV encoders, and task decoders for different tasks. Experimental results show that HENet achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes <b>benchmark,</b> including 3D <b>object</b> <b>detection</b> and BEV semantic segmentation. The source code and models will be released at <a href=https://github.com/VDIGPKU/HENet>https://github.com/VDIGPKU/HENet</a>.</p></p class="citation"></blockquote><h3 id=4150--101258-awol-analysis-without-synthesis-using-language-silvia-zuffi-et-al-2024>(41/50 | 101/258) AWOL: Analysis WithOut synthesis using Language (Silvia Zuffi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvia Zuffi, Michael J. Black. (2024)<br><strong>AWOL: Analysis WithOut synthesis using Language</strong><br><button class=copy-to-clipboard title="AWOL: Analysis WithOut synthesis using Language" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03042v1.pdf filename=2404.03042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a <b>vision-language</b> model and the parameter space of the 3D model, which we do using a small set of shape and text pairs. Our hypothesis is that mapping from language to parameters allows us to generate parameters for objects that were never seen during training. If the mapping between language and parameters is sufficiently smooth, then interpolation or generalization in language should translate appropriately into novel 3D shapes. We test our approach with two very different types of parametric shape models (quadrupeds and arboreal trees). We use a learned statistical shape model of quadrupeds and show that we can use text to generate new animals not present during training. In particular, we demonstrate state-of-the-art shape estimation of 3D dogs. This work also constitutes the first language-driven method for generating 3D trees. Finally, embedding images in the CLIP latent space enables us to generate animals and trees directly from images.</p></p class="citation"></blockquote><h3 id=4250--102258-instantstyle-free-lunch-towards-style-preserving-in-text-to-image-generation-haofan-wang-et-al-2024>(42/50 | 102/258) InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation (Haofan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haofan Wang, Qixun Wang, Xu Bai, Zekui Qin, Anthony Chen. (2024)<br><strong>InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02733v1.pdf filename=2404.02733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are prone to style degradation, often resulting in the loss of fine-grained details. Lastly, adapter-based approaches frequently require meticulous weight tuning for each reference image to achieve a balance between style intensity and text controllability. In this paper, we commence by examining several compelling yet frequently overlooked observations. We then proceed to introduce InstantStyle, a framework designed to address these issues through the implementation of two key strategies: 1) A straightforward mechanism that decouples style and content from reference images within the feature space, predicated on the assumption that features within the same space can be either added to or subtracted from one another. 2) The injection of reference image features exclusively into style-specific blocks, thereby preventing style leaks and eschewing the need for cumbersome weight tuning, which often characterizes more parameter-heavy designs.Our work demonstrates superior visual stylization outcomes, striking an optimal balance between the intensity of style and the controllability of textual elements. Our codes will be available at <a href=https://github.com/InstantStyle/InstantStyle>https://github.com/InstantStyle/InstantStyle</a>.</p></p class="citation"></blockquote><h3 id=4350--103258-model-agnostic-origin-attribution-of-generated-images-with-few-shot-examples-fengyuan-liu-et-al-2024>(43/50 | 103/258) Model-agnostic Origin Attribution of Generated Images with Few-shot Examples (Fengyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengyuan Liu, Haochen Luo, Yiming Li, Philip Torr, Jindong Gu. (2024)<br><strong>Model-agnostic Origin Attribution of Generated Images with Few-shot Examples</strong><br><button class=copy-to-clipboard title="Model-agnostic Origin Attribution of Generated Images with Few-shot Examples" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02697v1.pdf filename=2404.02697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in visual generative models enables the generation of high-quality images. To prevent the misuse of generated images, it is important to identify the origin model that generates them. In this work, we study the origin attribution of generated images in a practical setting where only a few images generated by a source model are available and the source model cannot be accessed. The goal is to check if a given image is generated by the source model. We first formulate this problem as a <b>few-shot</b> one-class classification task. To solve the task, we propose OCC-CLIP, a CLIP-based framework for <b>few-shot</b> one-class classification, enabling the identification of an image&rsquo;s source model, even among multiple candidates. Extensive experiments corresponding to various generative models verify the effectiveness of our OCC-CLIP framework. Furthermore, an experiment based on the recently released DALL-E 3 API verifies the real-world applicability of our solution.</p></p class="citation"></blockquote><h3 id=4450--104258-representation-alignment-contrastive-regularization-for-multi-object-tracking-shujie-chen-et-al-2024>(44/50 | 104/258) Representation Alignment Contrastive Regularization for Multi-Object Tracking (Shujie Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shujie Chen, Zhonglin Liu, Jianfeng Dong, Di Zhou. (2024)<br><strong>Representation Alignment Contrastive Regularization for Multi-Object Tracking</strong><br><button class=copy-to-clipboard title="Representation Alignment Contrastive Regularization for Multi-Object Tracking" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02562v1.pdf filename=2404.02562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer <b>transformer</b> encoder is utilized to model spatio-temporal relationships. To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks&rsquo; performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.</p></p class="citation"></blockquote><h3 id=4550--105258-enhancing-diffusion-based-point-cloud-generation-with-smoothness-constraint-yukun-li-et-al-2024>(45/50 | 105/258) Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint (Yukun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukun Li, Liping Liu. (2024)<br><strong>Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint</strong><br><button class=copy-to-clipboard title="Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02396v1.pdf filename=2404.02396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have been popular for point cloud generation tasks. Existing works utilize the forward <b>diffusion</b> <b>process</b> to convert the original point distribution into a noise distribution and then learn the reverse <b>diffusion</b> <b>process</b> to recover the point distribution from the noise distribution. However, the reverse <b>diffusion</b> <b>process</b> can produce samples with non-smooth points on the surface because of the ignorance of the point cloud geometric properties. We propose alleviating the problem by incorporating the local smoothness constraint into the <b>diffusion</b> <b>framework</b> for point cloud generation. Experiments demonstrate the proposed model can generate realistic shapes and smoother point clouds, outperforming multiple state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=4650--106258-cape-cam-as-a-probabilistic-ensemble-for-enhanced-dnn-interpretation-townim-faisal-chowdhury-et-al-2024>(46/50 | 106/258) CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation (Townim Faisal Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Townim Faisal Chowdhury, Kewen Liao, Vu Minh Hieu Phan, Minh-Son To, Yutong Xie, Kevin Hung, David Ross, Anton van den Hengel, Johan W. Verjans, Zhibin Liao. (2024)<br><strong>CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation</strong><br><button class=copy-to-clipboard title="CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02388v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02388v2.pdf filename=2404.02388v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks (DNNs) are widely used for visual classification tasks, but their complex computation process and <b>black-box</b> <b>nature</b> hinder decision transparency and interpretability. Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decision-making process by displaying &lsquo;attention&rsquo; heatmaps of the DNNs. Nevertheless, the CAM explanation only offers relative attention information, that is, on an attention heatmap, we can interpret which image region is more or less important than the others. However, these regions cannot be meaningfully compared across classes, and the contribution of each region to the model&rsquo;s class prediction is not revealed. To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we propose CAPE, a novel reformulation of CAM that provides a unified and probabilistically meaningful assessment of the contributions of image regions. We quantitatively and qualitatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet <b>benchmark</b> datasets to demonstrate enhanced interpretability. We also test on a cytology imaging dataset depicting a challenging Chronic Myelomonocytic Leukemia (CMML) diagnosis problem. Code is available at: <a href=https://github.com/AIML-MED/CAPE>https://github.com/AIML-MED/CAPE</a>.</p></p class="citation"></blockquote><h3 id=4750--107258-behind-the-veil-enhanced-indoor-3d-scene-reconstruction-with-occluded-surfaces-completion-su-sun-et-al-2024>(47/50 | 107/258) Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion (Su Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Su Sun, Cheng Zhao, Yuliang Guo, Ruoyu Wang, Xinyu Huang, Yingjie Victor Chen, Liu Ren. (2024)<br><strong>Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion</strong><br><button class=copy-to-clipboard title="Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03070v1.pdf filename=2404.03070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel indoor 3D reconstruction method with occluded surface completion, given a sequence of depth readings. Prior state-of-the-art (SOTA) methods only focus on the reconstruction of the visible areas in a scene, neglecting the invisible areas due to the occlusions, e.g., the contact surface between furniture, occluded wall and floor. Our method tackles the task of completing the occluded scene surfaces, resulting in a complete 3D scene mesh. The core idea of our method is learning 3D <b>geometry</b> prior from various complete scenes to infer the occluded <b>geometry</b> of an unseen scene from solely depth measurements. We design a coarse-fine hierarchical octree representation coupled with a dual-decoder architecture, i.e., Geo-decoder and 3D Inpainter, which jointly reconstructs the complete 3D scene <b>geometry.</b> The Geo-decoder with detailed representation at fine levels is optimized online for each scene to reconstruct visible surfaces. The 3D Inpainter with abstract representation at coarse levels is trained offline using various scenes to complete occluded surfaces. As a result, while the Geo-decoder is specialized for an individual scene, the 3D Inpainter can be generally applied across different scenes. We evaluate the proposed method on the 3D Completed Room Scene (3D-CRS) and iTHOR datasets, significantly outperforming the SOTA methods by a gain of 16.8% and 24.2% in terms of the completeness of 3D reconstruction. 3D-CRS dataset including a complete 3D mesh of each scene is provided at project webpage.</p></p class="citation"></blockquote><h3 id=4850--108258-lidar4d-dynamic-neural-fields-for-novel-space-time-view-lidar-synthesis-zehan-zheng-et-al-2024>(48/50 | 108/258) LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis (Zehan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehan Zheng, Fan Lu, Weiyi Xue, Guang Chen, Changjun Jiang. (2024)<br><strong>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</strong><br><button class=copy-to-clipboard title="LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02742v1.pdf filename=2404.02742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although neural radiance fields (NeRFs) have achieved triumphs in image novel view synthesis (NVS), LiDAR NVS remains largely unexplored. Previous LiDAR NVS methods employ a simple shift from image NVS methods while ignoring the dynamic nature and the large-scale reconstruction problem of LiDAR point clouds. In light of this, we propose LiDAR4D, a differentiable LiDAR-only framework for novel space-time LiDAR view synthesis. In consideration of the sparsity and large-scale characteristics, we design a 4D hybrid representation combined with multi-planar and grid features to achieve effective reconstruction in a coarse-to-fine manner. Furthermore, we introduce geometric constraints derived from point clouds to improve temporal consistency. For the realistic synthesis of LiDAR point clouds, we incorporate the global optimization of ray-drop probability to preserve cross-region patterns. Extensive experiments on KITTI-360 and NuScenes datasets demonstrate the superiority of our method in accomplishing <b>geometry-aware</b> and time-consistent dynamic reconstruction. Codes are available at <a href=https://github.com/ispc-lab/LiDAR4D>https://github.com/ispc-lab/LiDAR4D</a>.</p></p class="citation"></blockquote><h3 id=4950--109258-tclc-gs-tightly-coupled-lidar-camera-gaussian-splatting-for-surrounding-autonomous-driving-scenes-cheng-zhao-et-al-2024>(49/50 | 109/258) TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes (Cheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, Liu Ren. (2024)<br><strong>TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes</strong><br><button class=copy-to-clipboard title="TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02410v1.pdf filename=2404.02410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most 3D Gaussian Splatting (3D-GS) based methods for urban scenes initialize 3D Gaussians directly with 3D LiDAR points, which not only underutilizes LiDAR data capabilities but also overlooks the potential advantages of fusing LiDAR with camera data. In this paper, we design a novel tightly coupled LiDAR-Camera Gaussian Splatting (TCLC-GS) to fully leverage the combined strengths of both LiDAR and camera sensors, enabling rapid, high-quality 3D reconstruction and novel view RGB/depth synthesis. TCLC-GS designs a hybrid explicit (colorized 3D mesh) and implicit (hierarchical octree feature) 3D representation derived from LiDAR-camera data, to enrich the properties of 3D Gaussians for splatting. 3D Gaussian&rsquo;s properties are not only initialized in alignment with the 3D mesh which provides more completed 3D shape and color information, but are also endowed with broader contextual information through retrieved octree implicit features. During the Gaussian Splatting optimization process, the 3D mesh offers dense depth information as supervision, which enhances the training process by learning of a robust <b>geometry.</b> Comprehensive evaluations conducted on the Waymo Open Dataset and nuScenes Dataset validate our method&rsquo;s state-of-the-art (SOTA) performance. Utilizing a single NVIDIA RTX 3090 Ti, our method demonstrates fast training and achieves real-time RGB and depth rendering at 90 FPS in resolution of 1920x1280 (Waymo), and 120 FPS in resolution of 1600x900 (nuScenes) in urban scenarios.</p></p class="citation"></blockquote><h3 id=5050--110258-regional-biases-in-image-geolocation-estimation-a-case-study-with-the-sensecity-africa-dataset-ximena-salgado-uribe-et-al-2024>(50/50 | 110/258) Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset (Ximena Salgado Uribe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ximena Salgado Uribe, Martí Bosch, Jérôme Chenal. (2024)<br><strong>Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset</strong><br><button class=copy-to-clipboard title="Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02558v1.pdf filename=2404.02558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in Artificial Intelligence are challenged by the biases rooted in the datasets used to train the models. In image geolocation estimation, models are mostly trained using data from specific geographic regions, notably the Western world, and as a result, they may struggle to comprehend the complexities of underrepresented regions. To assess this issue, we apply a state-of-the-art image geolocation estimation model (ISNs) to a crowd-sourced dataset of geolocated images from the African continent (SCA100), and then explore the regional and socioeconomic biases underlying the model&rsquo;s predictions. Our findings show that the ISNs model tends to over-predict image locations in high-income countries of the Western world, which is consistent with the geographic distribution of its training data, i.e., the IM2GPS3k dataset. Accordingly, when compared to the IM2GPS3k <b>benchmark,</b> the accuracy of the ISNs model notably decreases at all scales. Additionally, we cluster images of the SCA100 dataset based on how accurately they are predicted by the ISNs model and show the model&rsquo;s difficulties in correctly predicting the locations of images in low income regions, especially in Sub-Saharan Africa. Therefore, our results suggest that using IM2GPS3k as a training set and <b>benchmark</b> for image geolocation estimation and other computer vision models overlooks its potential application in the African context.</p></p class="citation"></blockquote><h2 id=cslg-48>cs.LG (48)</h2><h3 id=148--111258-foundation-models-for-structural-health-monitoring-luca-benfenati-et-al-2024>(1/48 | 111/258) Foundation Models for Structural Health Monitoring (Luca Benfenati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Benfenati, Daniele Jahier Pagliari, Luca Zanatta, Yhorman Alexander Bedoya Velez, Andrea Acquaviva, Massimo Poncino, Enrico Macii, Luca Benini, Alessio Burrello. (2024)<br><strong>Foundation Models for Structural Health Monitoring</strong><br><button class=copy-to-clipboard title="Foundation Models for Structural Health Monitoring" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-1; I-2-3, cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 93<br>Keywords: Anomaly Detection, Benchmarking, Fine-tuning, Foundation Model, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Self-supervised Pre-training, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02944v1.pdf filename=2404.02944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structural Health Monitoring (SHM) is a critical task for ensuring the safety and reliability of civil infrastructures, typically realized on bridges and viaducts by means of vibration monitoring. In this paper, we propose for the first time the use of <b>Transformer</b> neural networks, with a Masked Auto-Encoder architecture, as <b>Foundation</b> <b>Models</b> for SHM. We demonstrate the ability of these models to learn generalizable representations from multiple large datasets through <b>self-supervised</b> <b>pre-training,</b> which, coupled with task-specific <b>fine-tuning,</b> allows them to outperform state-of-the-art traditional methods on diverse tasks, including <b>Anomaly</b> <b>Detection</b> (AD) and Traffic Load Estimation (TLE). We then extensively explore model size versus accuracy trade-offs and experiment with <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> to improve the performance of smaller <b>Transformers,</b> enabling their embedding directly into the SHM edge nodes. We showcase the effectiveness of our <b>foundation</b> <b>models</b> using data from three operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy with a monitoring time span of just 15 windows. In contrast, a state-of-the-art method based on Principal Component Analysis (PCA) obtains its first good result (95.03% accuracy) only considering 120 windows. On two different TLE tasks, our models obtain state-of-the-art performance on multiple evaluation metrics (R$^2$ score, MAE% and MSE%). On the first <b>benchmark,</b> we achieve an R$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively, while the best previous approach stops at 0.91 and 0.84. On the second one, we achieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method.</p></p class="citation"></blockquote><h3 id=248--112258-badam-a-memory-efficient-full-parameter-training-method-for-large-language-models-qijun-luo-et-al-2024>(2/48 | 112/258) BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models (Qijun Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijun Luo, Hengxu Yu, Xiao Li. (2024)<br><strong>BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models</strong><br><button class=copy-to-clipboard title="BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Alpaca, LLaMA, RoBERTa, Neural Machine Translation, Large Language Model, SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02827v1.pdf filename=2404.02827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter <b>finetuning</b> of <b>large</b> <b>language</b> <b>models</b> and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the <b>Llama</b> 2-7B model on the <b>Alpaca-GPT4</b> dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the <b>MT-bench</b> shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., <b>finetuning</b> <b>RoBERTa-large</b> on the <b>SuperGLUE</b> <b>benchmark.</b> The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is available at <a href=https://github.com/Ledzy/BAdam>https://github.com/Ledzy/BAdam</a>.</p></p class="citation"></blockquote><h3 id=348--113258-generative-contrastive-heterogeneous-graph-neural-network-yu-wang-et-al-2024>(3/48 | 113/258) Generative-Contrastive Heterogeneous Graph Neural Network (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang. (2024)<br><strong>Generative-Contrastive Heterogeneous Graph Neural Network</strong><br><button class=copy-to-clipboard title="Generative-Contrastive Heterogeneous Graph Neural Network" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Node Classification, Graph, Graph Neural Network, Autoencoder, Contrastive Learning, Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02810v1.pdf filename=2404.02810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous <b>Graphs</b> <b>(HGs)</b> <b>can</b> effectively model complex relationships in the real world by multi-type <b>nodes</b> <b>and</b> edges. In recent years, inspired by <b>self-supervised</b> <b>learning,</b> <b>contrastive</b> <b>Heterogeneous</b> <b>Graphs</b> <b>Neural</b> <b>Networks</b> (HGNNs) have shown great potential by utilizing <b>data</b> <b>augmentation</b> and discriminators for downstream tasks. However, <b>data</b> <b>augmentation</b> is still limited due to the discrete and abstract nature of <b>graphs.</b> <b>To</b> <b>tackle</b> the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous <b>Graph</b> <b>Neural</b> <b>Network</b> (GC-HGNN)}. Specifically, we first propose a heterogeneous <b>graph</b> <b>generative</b> <b>learning</b> enhanced <b>contrastive</b> <b>paradigm.</b> This paradigm includes: 1) A <b>contrastive</b> <b>view</b> augmentation strategy by using masked <b>autoencoder.</b> 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical <b>contrastive</b> <b>learning</b> strategy for capturing local and global information. Furthermore, the hierarchical <b>contrastive</b> <b>learning</b> and sampling strategies aim to constitute an enhanced discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest <b>contrastive</b> <b>and</b> generative baselines on <b>node</b> <b>classification</b> and link prediction tasks. To reproduce our work, we have open-sourced our code at <a href=https://github.com/xxx>https://github.com/xxx</a>.</p></p class="citation"></blockquote><h3 id=448--114258-task-agnostic-architecture-for-algorithm-induction-via-implicit-composition-sahil-j-sindhi-et-al-2024>(4/48 | 114/258) Task Agnostic Architecture for Algorithm Induction via Implicit Composition (Sahil J. Sindhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahil J. Sindhi, Ignas Budvytis. (2024)<br><strong>Task Agnostic Architecture for Algorithm Induction via Implicit Composition</strong><br><button class=copy-to-clipboard title="Task Agnostic Architecture for Algorithm Induction via Implicit Composition" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Generative AI, Multi-modal, GPT, GPT-4, Transformer, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02450v1.pdf filename=2404.02450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Different fields in applied machine learning such as computer vision, speech or natural language processing have been building domain-specialised solutions. Currently, we are witnessing an opposing trend towards developing more generalist architectures, driven by <b>Large</b> <b>Language</b> <b>Models</b> and <b>multi-modal</b> foundational models. These architectures are designed to tackle a variety of tasks, including those previously unseen and using inputs across multiple modalities. Taking this trend of generalization to the extreme suggests the possibility of a single deep network architecture capable of solving all tasks. This position paper aims to explore developing such a unified architecture and proposes a theoretical framework of how it could be constructed. Our proposal is based on the following assumptions. Firstly, tasks are solved by following a sequence of instructions, typically implemented in code for conventional computing hardware, which inherently operates sequentially. Second, recent <b>Generative</b> <b>AI,</b> especially <b>Transformer-based</b> models, demonstrate potential as an architecture capable of constructing algorithms for a wide range of domains. For example, <b>GPT-4</b> shows exceptional capability at <b>in-context</b> <b>learning</b> of novel tasks which is hard to explain in any other way than the ability to compose novel solutions from fragments on previously learnt algorithms. Third, the observation that the main missing component in developing a truly generalised network is an efficient approach for self-consistent input of previously learnt sub-steps of an algorithm and their (implicit) composition during the network&rsquo;s internal forward pass. Our exploration delves into current capabilities and limitations of <b>Transformer-based</b> and other methods in efficient and correct algorithm composition and proposes a <b>Transformer-like</b> architecture as well as a discrete learning framework to overcome these limitations.</p></p class="citation"></blockquote><h3 id=548--115258-deep-privacy-funnel-model-from-a-discriminative-to-a-generative-approach-with-an-application-to-face-recognition-behrooz-razeghi-et-al-2024>(5/48 | 115/258) Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition (Behrooz Razeghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behrooz Razeghi, Parsa Rahimi, Sébastien Marcel. (2024)<br><strong>Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition</strong><br><button class=copy-to-clipboard title="Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 65<br>Keywords: Diffusion Model, Face Recognition, Autoencoder, Generative Adversarial Network, Generative Adversarial Network, Representation Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02696v1.pdf filename=2404.02696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we apply the information-theoretic Privacy Funnel (PF) model to the domain of <b>face</b> <b>recognition,</b> developing a novel method for privacy-preserving <b>representation</b> <b>learning</b> within an end-to-end training framework. Our approach addresses the trade-off between obfuscation and utility in data protection, quantified through logarithmic loss, also known as self-information loss. This research provides a foundational exploration into the integration of information-theoretic privacy principles with <b>representation</b> <b>learning,</b> focusing specifically on the <b>face</b> <b>recognition</b> systems. We particularly highlight the adaptability of our framework with recent advancements in <b>face</b> <b>recognition</b> networks, such as AdaFace and ArcFace. In addition, we introduce the <b>Generative</b> <b>Privacy</b> <b>Funnel</b> ($\mathsf{GenPF}$) model, a paradigm that extends beyond the traditional scope of the PF model, referred to as the Discriminative Privacy Funnel ($\mathsf{DisPF}$). This $\mathsf{GenPF}$ model brings new perspectives on data generation methods with estimation-theoretic and information-theoretic privacy guarantees. Complementing these developments, we also present the deep <b>variational</b> <b>PF</b> (DVPF) model. This model proposes a tractable <b>variational</b> <b>bound</b> for measuring information leakage, enhancing the understanding of privacy preservation challenges in deep <b>representation</b> <b>learning.</b> The DVPF model, associated with both $\mathsf{DisPF}$ and $\mathsf{GenPF}$ models, sheds light on connections with various <b>generative</b> <b>models</b> <b>such</b> as <b>Variational</b> <b>Autoencoders</b> (VAEs), <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs),</b> and <b>Diffusion</b> <b>models.</b> Complementing our theoretical contributions, we release a reproducible PyTorch package, facilitating further exploration and application of these privacy-preserving methodologies in <b>face</b> <b>recognition</b> systems.</p></p class="citation"></blockquote><h3 id=648--116258-explainable-traffic-flow-prediction-with-large-language-models-xusen-guo-et-al-2024>(6/48 | 116/258) Explainable Traffic Flow Prediction with Large Language Models (Xusen Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, Hao, Yang. (2024)<br><strong>Explainable Traffic Flow Prediction with Large Language Models</strong><br><button class=copy-to-clipboard title="Explainable Traffic Flow Prediction with Large Language Models" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Foundation Model, Zero-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02937v1.pdf filename=2404.02937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic flow prediction provides essential future views in the intelligent transportation system. Explainable predictions offer valuable insights into the factors influencing traffic patterns, which help urban planners, traffic engineers, and policymakers make informed decisions about infrastructure development, traffic management strategies, and public transportation planning. Despite their widespread popularity and commendable accuracy, prediction methods grounded in deep learning frequently disappoint in terms of transparency and interpretability. Recently, the availability of <b>large-scale</b> <b>spatio-temporal</b> <b>data</b> and the development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have opened up new opportunities for urban traffic prediction. With the popularity of <b>LLMs,</b> people witnessed the potential <b>reasoning</b> and generating ability of <b>foundation</b> <b>models</b> in various tasks. Considering text as input and output, <b>LLMs</b> have advantages in generating more intuitive and interpretable predictions. Hence, this work introduces TP-LLM, an explainable <b>foundation-model-based</b> <b>method</b> for traffic prediction, aiming at more direct and reasonable forecasting. TP-LLM presents a framework to unify multi-modality factors as language-based inputs, TP-LLM avoids complex spatial-temporal data programming and outperforms state-of-art baselines merely under <b>fine-tuning</b> <b>foundation</b> <b>models.</b> Also, TP-LLM can generate input-dependency explanations for more confident prediction and can be easily generalized to different city dynamics for <b>zero-shot</b> prediction with a similar framework. These findings demonstrate the potential of <b>LLMs</b> for explainable traffic prediction.</p></p class="citation"></blockquote><h3 id=748--117258-on-the-efficiency-and-robustness-of-vibration-based-foundation-models-for-iot-sensing-a-case-study-tomoyoshi-kimura-et-al-2024>(7/48 | 117/258) On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study (Tomoyoshi Kimura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomoyoshi Kimura, Jinyang Li, Tianshi Wang, Denizhan Kara, Yizhuo Chen, Yigong Hu, Ruijie Wang, Maggie Wigness, Shengzhong Liu, Mani Srivastava, Suhas Diggavi, Tarek Abdelzaher. (2024)<br><strong>On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study</strong><br><button class=copy-to-clipboard title="On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02461v1.pdf filename=2404.02461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper demonstrates the potential of vibration-based <b>Foundation</b> <b>Models</b> (FMs), pre-trained with unlabeled sensing data, to improve the robustness of run-time inference in (a class of) IoT applications. A case study is presented featuring a vehicle classification application using acoustic and seismic sensing. The work is motivated by the success of <b>foundation</b> <b>models</b> in the areas of natural language processing and computer vision, leading to generalizations of the FM concept to other domains as well, where significant amounts of unlabeled data exist that can be used for <b>self-supervised</b> <b>pre-training.</b> One such domain is IoT applications. <b>Foundation</b> <b>models</b> for selected sensing modalities in the IoT domain can be pre-trained in an environment-agnostic fashion using available unlabeled sensor data and then <b>fine-tuned</b> to the deployment at hand using a small amount of labeled data. The paper shows that the pre-training/fine-tuning approach improves the robustness of downstream inference and facilitates adaptation to different environmental conditions. More specifically, we present a case study in a real-world setting to evaluate a simple (vibration-based) FM-like model, called FOCAL, demonstrating its superior robustness and adaptation, compared to conventional <b>supervised</b> deep neural networks (DNNs). We also demonstrate its superior convergence over <b>supervised</b> solutions. Our findings highlight the advantages of vibration-based FMs (and FM-inspired <b>selfsupervised</b> <b>models</b> in general) in terms of inference robustness, runtime efficiency, and model adaptation (via <b>fine-tuning)</b> in resource-limited IoT settings.</p></p class="citation"></blockquote><h3 id=848--118258-ressa-repair-sparse-vision-language-models-via-sparse-cross-modality-adaptation-shwai-he-et-al-2024>(8/48 | 118/258) RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation (Shwai He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shwai He, Tianlong Chen. (2024)<br><strong>RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation</strong><br><button class=copy-to-clipboard title="RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Pruning, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02424v1.pdf filename=2404.02424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While <b>pruning</b> followed by <b>finetuning</b> offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM <b>pruning</b> and found that <b>pruning</b> vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike <b>finetuning</b> unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair. Moreover, while parameter-efficient LoRA <b>finetuning</b> has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models. To tackle these challenges, we propose to Repair Sparse <b>Vision-Language</b> Models via Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality <b>finetuning</b> to enhance task-specific performance and facilitate <b>knowledge</b> <b>distillation</b> from original dense models. Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models. Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3% improvement under 2:4 sparsity and a remarkable 47.6% enhancement under unstructured 70% sparsity.</p></p class="citation"></blockquote><h3 id=948--119258-transfer-learning-applications-for-anomaly-detection-in-wind-turbines-cyriana-m-a-roelofs-et-al-2024>(9/48 | 119/258) Transfer learning applications for anomaly detection in wind turbines (Cyriana M. A. Roelofs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cyriana M. A. Roelofs, Christian Gück, Stefan Faulstich. (2024)<br><strong>Transfer learning applications for anomaly detection in wind turbines</strong><br><button class=copy-to-clipboard title="Transfer learning applications for anomaly detection in wind turbines" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2, cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Anomaly Detection, Autoencoder, Fine-tuning, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03011v1.pdf filename=2404.03011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> in wind turbines typically involves using normal behaviour models to detect faults early. However, training <b>autoencoder</b> models for each turbine is time-consuming and resource intensive. Thus, <b>transfer</b> <b>learning</b> becomes essential for wind turbines with limited data or applications with limited computational resources. This study examines how cross-turbine <b>transfer</b> <b>learning</b> can be applied to <b>autoencoder-based</b> <b>anomaly</b> <b>detection.</b> Here, <b>autoencoders</b> are combined with constant thresholds for the reconstruction error to determine if input data contains an <b>anomaly.</b> <b>The</b> models are initially trained on one year&rsquo;s worth of data from one or more source wind turbines. They are then <b>fine-tuned</b> using smaller amounts of data from another turbine. Three methods for <b>fine-tuning</b> are investigated: adjusting the entire <b>autoencoder,</b> only the decoder, or only the threshold of the model. The performance of the <b>transfer</b> <b>learning</b> models is compared to baseline models that were trained on one year&rsquo;s worth of data from the target wind turbine. The results of the tests conducted in this study indicate that models trained on data of multiple wind turbines do not improve the <b>anomaly</b> <b>detection</b> capability compared to models trained on data of one source wind turbine. In addition, modifying the model&rsquo;s threshold can lead to comparable or even superior performance compared to the baseline, whereas <b>fine-tuning</b> the decoder or <b>autoencoder</b> further enhances the models&rsquo; performance.</p></p class="citation"></blockquote><h3 id=1048--120258-towards-detecting-unanticipated-bias-in-large-language-models-anna-kruspe-2024>(10/48 | 120/258) Towards detecting unanticipated bias in Large Language Models (Anna Kruspe, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Kruspe. (2024)<br><strong>Towards detecting unanticipated bias in Large Language Models</strong><br><button class=copy-to-clipboard title="Towards detecting unanticipated bias in Large Language Models" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Explainable AI, Fairness, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02650v1.pdf filename=2404.02650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the last year, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>ChatGPT</b> have become widely available and have exhibited <b>fairness</b> issues similar to those in previous machine learning systems. Current research is primarily focused on analyzing and quantifying these biases in training data and their impact on the decisions of these models, alongside developing mitigation strategies. This research largely targets well-known biases related to gender, race, ethnicity, and language. However, it is clear that <b>LLMs</b> are also affected by other, less obvious implicit biases. The complex and often opaque nature of these models makes detecting such biases challenging, yet this is crucial due to their potential negative impact in various applications. In this paper, we explore new avenues for detecting these unanticipated biases in <b>LLMs,</b> focusing specifically on Uncertainty Quantification and <b>Explainable</b> <b>AI</b> methods. These approaches aim to assess the certainty of model decisions and to make the internal decision-making processes of <b>LLMs</b> more transparent, thereby identifying and understanding biases that are not immediately apparent. Through this research, we aim to contribute to the development of fairer and more transparent AI systems.</p></p class="citation"></blockquote><h3 id=1148--121258-masked-completion-via-structured-diffusion-with-white-box-transformers-druv-pai-et-al-2024>(11/48 | 121/258) Masked Completion via Structured Diffusion with White-Box Transformers (Druv Pai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Druv Pai, Ziyang Wu, Sam Buchanan, Yaodong Yu, Yi Ma. (2024)<br><strong>Masked Completion via Structured Diffusion with White-Box Transformers</strong><br><button class=copy-to-clipboard title="Masked Completion via Structured Diffusion with White-Box Transformers" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 45<br>Keywords: Autoencoder, Representation Learning, Supervised Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02446v1.pdf filename=2404.02446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn <b>representations</b> <b>by</b> solving simple pretext tasks, then use the <b>representations</b> <b>as</b> foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their <b>representations</b> <b>are</b> not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in <b>supervised</b> settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale <b>unsupervised</b> <b>representation</b> <b>learning.</b> We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep <b>transformer-like</b> masked <b>autoencoder</b> architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured <b>representation.</b> <b>Extensive</b> empirical evaluations confirm our analytical insights. CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30% of the parameters compared to the standard masked <b>autoencoder</b> with the same model configuration. The <b>representations</b> <b>learned</b> by CRATE-MAE have explicit structure and also contain semantic meaning. Code is available at <a href=https://github.com/Ma-Lab-Berkeley/CRATE>https://github.com/Ma-Lab-Berkeley/CRATE</a> .</p></p class="citation"></blockquote><h3 id=1248--122258-robust-federated-learning-for-wireless-networks-a-demonstration-with-channel-estimation-zexin-fang-et-al-2024>(12/48 | 122/258) Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation (Zexin Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zexin Fang, Bin Han, Hans D. Schotten. (2024)<br><strong>Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation</strong><br><button class=copy-to-clipboard title="Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NI, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Federated Learning, Simulation, Simulator, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03088v1.pdf filename=2404.03088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various <b>adversarial</b> <b>attacks</b> or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through <b>simulation.</b></p></p class="citation"></blockquote><h3 id=1348--123258-rethinking-teacher-student-curriculum-learning-through-the-cooperative-mechanics-of-experience-manfred-diaz-et-al-2024>(13/48 | 123/258) Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience (Manfred Diaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manfred Diaz, Liam Paull, Andrea Tacchetti. (2024)<br><strong>Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience</strong><br><button class=copy-to-clipboard title="Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Curriculum Learning, Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03084v1.pdf filename=2404.03084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Teacher-Student <b>Curriculum</b> <b>Learning</b> (TSCL) is a <b>curriculum</b> <b>learning</b> framework that draws inspiration from human cultural transmission and learning. It involves a teacher algorithm shaping the learning process of a learner algorithm by exposing it to controlled experiences. Despite its success, understanding the conditions under which TSCL is effective remains challenging. In this paper, we propose a data-centric perspective to analyze the underlying mechanics of the teacher-student interactions in TSCL. We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the <b>curriculum</b> <b>that</b> is found by TSCL approaches. To do so, we demonstrate that for every TSCL problem, there exists an equivalent cooperative game, and several key components of the TSCL framework can be reinterpreted using game-theoretic principles. Through experiments covering <b>supervised</b> <b>learning,</b> <b>reinforcement</b> <b>learning,</b> and classical games, we estimate the cooperative values of experiences and use value-proportional <b>curriculum</b> <b>mechanisms</b> to construct curricula, even in cases where TSCL struggles. The framework and experimental setup we present in this work represent a novel foundation for a deeper exploration of TSCL, shedding light on its underlying mechanisms and providing insights into its broader applicability in machine learning.</p></p class="citation"></blockquote><h3 id=1448--124258-deep-generative-models-through-the-lens-of-the-manifold-hypothesis-a-survey-and-new-connections-gabriel-loaiza-ganem-et-al-2024>(14/48 | 124/258) Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections (Gabriel Loaiza-Ganem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony L. Caterini, Jesse C. Cresswell. (2024)<br><strong>Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections</strong><br><button class=copy-to-clipboard title="Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Diffusion Model, Autoencoder, Generative Adversarial Network, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02954v1.pdf filename=2404.02954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years there has been increased interest in understanding the interplay between deep <b>generative</b> <b>models</b> <b>(DGMs)</b> and the manifold hypothesis. Research in this area focuses on understanding the reasons why commonly-used DGMs succeed or fail at learning distributions supported on unknown low-dimensional manifolds, as well as developing new models explicitly designed to account for manifold-supported data. This manifold lens provides both clarity as to why some DGMs (e.g. <b>diffusion</b> <b>models</b> and some <b>generative</b> <b>adversarial</b> <b>networks)</b> empirically surpass others (e.g. likelihood-based models such as <b>variational</b> <b>autoencoders,</b> normalizing flows, or energy-based models) at sample generation, and guidance for devising more performant DGMs. We carry out the first survey of DGMs viewed through this lens, making two novel contributions along the way. First, we formally establish that numerical instability of high-dimensional likelihoods is unavoidable when modelling low-dimensional data. We then show that DGMs on learned representations of <b>autoencoders</b> can be interpreted as approximately minimizing Wasserstein distance: this result, which applies to latent <b>diffusion</b> <b>models,</b> helps justify their outstanding empirical results. The manifold lens provides a rich perspective from which to understand DGMs, which we aim to make more accessible and widespread.</p></p class="citation"></blockquote><h3 id=1548--125258-end-to-end-self-tuning-self-supervised-time-series-anomaly-detection-boje-deforce-et-al-2024>(15/48 | 125/258) End-To-End Self-tuning Self-supervised Time Series Anomaly Detection (Boje Deforce et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boje Deforce, Meng-Chieh Lee, Bart Baesens, Estefanía Serral Asensio, Jaemin Yoo, Leman Akoglu. (2024)<br><strong>End-To-End Self-tuning Self-supervised Time Series Anomaly Detection</strong><br><button class=copy-to-clipboard title="End-To-End Self-tuning Self-supervised Time Series Anomaly Detection" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Data Augmentation, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02865v1.pdf filename=2404.02865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series <b>anomaly</b> <b>detection</b> (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and <b>unsupervised</b> model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled <b>data.</b> <b>Modern</b> neural networks have outstanding ability in modeling complex time series. <b>Self-supervised</b> models in particular tackle <b>unsupervised</b> TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on <b>data</b> <b>augmentation</b> tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA &ldquo;on autoPilot&rdquo;, which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key components: a differentiable augmentation architecture and an <b>unsupervised</b> validation loss to effectively assess the alignment between augmentation type and <b>anomaly</b> <b>type.</b> Case studies show TSAP&rsquo;s ability to effectively select the (discrete) augmentation type and associated (continuous) hyperparameters. In turn, it outperforms established baselines, including SOTA <b>self-supervised</b> models, on diverse TSAD tasks exhibiting different <b>anomaly</b> <b>types.</b></p></p class="citation"></blockquote><h3 id=1648--126258-toward-inference-optimal-mixture-of-expert-large-language-models-longfei-yun-et-al-2024>(16/48 | 126/258) Toward Inference-optimal Mixture-of-Expert Large Language Models (Longfei Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P Xing, Hao Zhang. (2024)<br><strong>Toward Inference-optimal Mixture-of-Expert Large Language Models</strong><br><button class=copy-to-clipboard title="Toward Inference-optimal Mixture-of-Expert Large Language Models" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Transformer, Large Language Model, Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02852v1.pdf filename=2404.02852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixture-of-Expert (MoE) based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> such as the recent Mixtral and DeepSeek-MoE, have shown great promise in <b>scaling</b> <b>model</b> size without suffering from the quadratic growth of training cost of dense <b>transformers.</b> Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the <b>scaling</b> <b>law</b> of MoE-based <b>LLMs</b> regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the <b>scaling</b> <b>law</b> of MoE by introducing inference efficiency as another metric besides the validation loss. We find that MoEs with a few (4/8) experts are the most serving efficient solution under the same performance, but costs 2.5-3.5x more in training. On the other hand, training a (16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but with a larger training dataset is a promising setup under a training budget.</p></p class="citation"></blockquote><h3 id=1748--127258-pissa-principal-singular-values-and-singular-vectors-adaptation-of-large-language-models-fanxu-meng-et-al-2024>(17/48 | 127/258) PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models (Fanxu Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanxu Meng, Zhaohui Wang, Muhan Zhang. (2024)<br><strong>PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</strong><br><button class=copy-to-clipboard title="PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02948v1.pdf filename=2404.02948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the parameters of <b>LLMs</b> expand, the computational cost of <b>fine-tuning</b> the entire model becomes prohibitive. To address this challenge, we introduce a PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA), which optimizes a significantly reduced parameter space while achieving or surpassing the performance of full-parameter <b>fine-tuning.</b> PiSSA is inspired by Intrinsic SAID, which suggests that pre-trained, over-parametrized models inhabit a space of low intrinsic dimension. Consequently, PiSSA represents a matrix W within the model by the product of two trainable matrices A and B, plus a residual matrix $W^{res}$ for error correction. SVD is employed to factorize W, and the principal singular values and vectors of W are utilized to initialize A and B. The residual singular values and vectors initialize the residual matrix $W^{res}$, which keeps frozen during <b>fine-tuning.</b> Notably, PiSSA shares the same architecture with LoRA. However, LoRA approximates Delta W through the product of two matrices, A, initialized with Gaussian noise, and B, initialized with zeros, while PiSSA initializes A and B with principal singular values and vectors of the original matrix W. PiSSA can better approximate the outcomes of full-parameter <b>fine-tuning</b> at the beginning by changing the essential parts while freezing the &ldquo;noisy&rdquo; parts. In comparison, LoRA freezes the original matrix and updates the &ldquo;noise&rdquo;. This distinction enables PiSSA to convergence much faster than LoRA and also achieve better performance in the end. Due to the same architecture, PiSSA inherits many of LoRA&rsquo;s advantages, such as parameter efficiency and compatibility with <b>quantization.</b> Leveraging a fast SVD method, the initialization of PiSSA takes only a few seconds, inducing negligible cost of switching LoRA to PiSSA.</p></p class="citation"></blockquote><h3 id=1848--128258-first-order-pdes-for-graph-neural-networks-advection-and-burgers-equation-models-yifan-qu-et-al-2024>(18/48 | 128/258) First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models (Yifan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Qu, Oliver Krzysik, Hans De Sterck, Omer Ege Kara. (2024)<br><strong>First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models</strong><br><button class=copy-to-clipboard title="First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03081v1.pdf filename=2404.03081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have established themselves as the preferred methodology in a multitude of domains, ranging from computer vision to computational biology, especially in contexts where data inherently conform to <b>graph</b> <b>structures.</b> <b>While</b> many existing methods have endeavored to model <b>GNNs</b> using various techniques, a prevalent challenge they grapple with is the issue of over-smoothing. This paper presents new <b>Graph</b> <b>Neural</b> <b>Network</b> models that incorporate two first-order Partial Differential Equations (PDEs). These models do not increase complexity but effectively mitigate the over-smoothing problem. Our experimental findings highlight the capacity of our new PDE model to achieve comparable results with higher-order PDE models and fix the over-smoothing problem up to 64 layers. These results underscore the adaptability and versatility of <b>GNNs,</b> indicating that unconventional approaches can yield outcomes on par with established techniques.</p></p class="citation"></blockquote><h3 id=1948--129258-the-satml-24-cnn-interpretability-competition-new-innovations-for-concept-level-interpretability-stephen-casper-et-al-2024>(19/48 | 129/258) The SaTML &lsquo;24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability (Stephen Casper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Casper, Jieun Yun, Joonhyuk Baek, Yeseong Jung, Minhwan Kim, Kiwan Kwon, Saerom Park, Hayden Moore, David Shriver, Marissa Connor, Keltin Grimes, Angus Nicolson, Arush Tagade, Jessica Rumbelow, Hieu Minh Nguyen, Dylan Hadfield-Menell. (2024)<br><strong>The SaTML &lsquo;24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability</strong><br><button class=copy-to-clipboard title="The SaTML '24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02949v1.pdf filename=2404.02949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretability techniques are valuable for helping humans understand and oversee AI systems. The SaTML 2024 <b>CNN</b> Interpretability Competition solicited novel methods for studying <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> at the ImageNet scale. The objective of the competition was to help human crowd-workers identify trojans in <b>CNNs.</b> This report showcases the methods and results of four featured competition entries. It remains challenging to help humans reliably diagnose trojans via interpretability tools. However, the competition&rsquo;s entries have contributed new techniques and set a new record on the <b>benchmark</b> from Casper et al., 2023.</p></p class="citation"></blockquote><h3 id=2048--130258-optimizing-the-deployment-of-tiny-transformers-on-low-power-mcus-victor-j-b-jung-et-al-2024>(20/48 | 130/258) Optimizing the Deployment of Tiny Transformers on Low-Power MCUs (Victor J. B. Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor J. B. Jung, Alessio Burrello, Moritz Scherer, Francesco Conti, Luca Benini. (2024)<br><strong>Optimizing the Deployment of Tiny Transformers on Low-Power MCUs</strong><br><button class=copy-to-clipboard title="Optimizing the Deployment of Tiny Transformers on Low-Power MCUs" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs-PF, cs.LG<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02945v1.pdf filename=2404.02945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> networks are rapidly becoming SotA in many fields, such as NLP and CV. Similarly to <b>CNN,</b> there is a strong push for deploying <b>Transformer</b> models at the extreme edge, ultimately fitting the tiny power budget and memory footprint of MCUs. However, the early approaches in this direction are mostly ad-hoc, platform, and model-specific. This work aims to enable and optimize the flexible, multi-platform deployment of encoder Tiny <b>Transformers</b> on commercial MCUs. We propose a complete framework to perform end-to-end deployment of <b>Transformer</b> models onto single and multi-core MCUs. Our framework provides an optimized library of kernels to maximize data reuse and avoid unnecessary data marshaling operations into the crucial attention block. A novel MHSA inference schedule, named Fused-Weight <b>Self-Attention,</b> is introduced, fusing the linear projection weights offline to further reduce the number of operations and parameters. Furthermore, to mitigate the memory peak reached by the computation of the attention map, we present a Depth-First Tiling scheme for MHSA. We evaluate our framework on three different MCU classes exploiting ARM and RISC-V ISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an average of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN (ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA depth-first tiling scheme reduces the memory peak by up to 6.19x, while the fused-weight attention can reduce the runtime by 1.53x, and number of parameters by 25%. We report significant improvements across several Tiny <b>Transformers:</b> for instance, when executing a <b>transformer</b> block for the task of radar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms and energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN library on the same platform.</p></p class="citation"></blockquote><h3 id=2148--131258-learning-in-convolutional-neural-networks-accelerated-by-transfer-entropy-adrian-moldovan-et-al-2024>(21/48 | 131/258) Learning in Convolutional Neural Networks Accelerated by Transfer Entropy (Adrian Moldovan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Moldovan, Angel Caţaron, Răzvan Andonie. (2024)<br><strong>Learning in Convolutional Neural Networks Accelerated by Transfer Entropy</strong><br><button class=copy-to-clipboard title="Learning in Convolutional Neural Networks Accelerated by Transfer Entropy" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02943v1.pdf filename=2404.02943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there is a growing interest in applying Transfer Entropy (TE) in quantifying the effective connectivity between artificial neurons. In a feedforward network, the TE can be used to quantify the relationships between neuron output pairs located in different layers. Our focus is on how to include the TE in the learning mechanisms of a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> architecture. We introduce a novel training mechanism for <b>CNN</b> architectures which integrates the TE feedback connections. Adding the TE feedback parameter accelerates the training process, as fewer epochs are needed. On the flip side, it adds computational overhead to each epoch. According to our experiments on <b>CNN</b> classifiers, to achieve a reasonable computational overhead&ndash;accuracy trade-off, it is efficient to consider only the inter-neural information transfer of a random subset of the neuron pairs from the last two fully connected layers. The TE acts as a smoothing factor, generating stability and becoming active only periodically, not after processing each input sample. Therefore, we can consider the TE is in our model a slowly changing meta-parameter.</p></p class="citation"></blockquote><h3 id=2248--132258-attention-is-naturally-sparse-with-gaussian-distributed-input-yichuan-deng-et-al-2024>(22/48 | 132/258) Attention is Naturally Sparse with Gaussian Distributed Input (Yichuan Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichuan Deng, Zhao Song, Chiwun Yang. (2024)<br><strong>Attention is Naturally Sparse with Gaussian Distributed Input</strong><br><button class=copy-to-clipboard title="Attention is Naturally Sparse with Gaussian Distributed Input" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02690v1.pdf filename=2404.02690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The computational intensity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in <b>transformer</b> architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within <b>LLMs,</b> particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of <b>LLMs,</b> paving the way for more scalable and efficient AI systems.</p></p class="citation"></blockquote><h3 id=2348--133258-on-the-importance-of-uncertainty-in-decision-making-with-large-language-models-nicolò-felicioni-et-al-2024>(23/48 | 133/258) On the Importance of Uncertainty in Decision-Making with Large Language Models (Nicolò Felicioni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Felicioni, Lucas Maystre, Sina Ghiassian, Kamil Ciosek. (2024)<br><strong>On the Importance of Uncertainty in Decision-Making with Large Language Models</strong><br><button class=copy-to-clipboard title="On the Importance of Uncertainty in Decision-Making with Large Language Models" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02649v1.pdf filename=2404.02649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using <b>Large</b> <b>Language</b> <b>Models</b> as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual <b>bandits,</b> where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an <b>LLM</b> <b>bandit</b> with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to <b>LLM</b> <b>bandits</b> that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the <b>LLM</b> literature, uncertainty plays a fundamental role in <b>bandit</b> tasks with <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2448--134258-grid-mapping-pseudo-count-constraint-for-offline-reinforcement-learning-yi-shen-et-al-2024>(24/48 | 134/258) Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning (Yi Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Shen, Hanyan Huang, Shan Xie. (2024)<br><strong>Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02545v1.pdf filename=2404.02545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive <b>reinforcement</b> <b>learning</b> methods usually fails in an <b>offline</b> <b>environment</b> <b>due</b> to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretically proved that only a few conditions are needed to obtain accurate uncertainty constraints in the proposed method. Moreover, we develop a Grid-Mapping Pseudo-Count Soft Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC) framework to demonstrate the effectiveness of GPC. The experimental results on D4RL <b>benchmark</b> datasets show that GPC-SAC has better performance and less computational cost compared to other algorithms.</p></p class="citation"></blockquote><h3 id=2548--135258-ad4rl-autonomous-driving-benchmarks-for-offline-reinforcement-learning-with-value-based-dataset-dongsu-lee-et-al-2024>(25/48 | 135/258) AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset (Dongsu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongsu Lee, Chanin Eom, Minhae Kwon. (2024)<br><strong>AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset</strong><br><button class=copy-to-clipboard title="AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02429v1.pdf filename=2404.02429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> has emerged as a promising technology by enhancing its practicality through the use of pre-collected large datasets. Despite its practical benefits, most algorithm development research in <b>offline</b> <b>reinforcement</b> <b>learning</b> still relies on game tasks with synthetic datasets. To address such limitations, this paper provides autonomous driving datasets and <b>benchmarks</b> for <b>offline</b> <b>reinforcement</b> <b>learning</b> research. We provide 19 datasets, including real-world human driver&rsquo;s datasets, and seven popular <b>offline</b> <b>reinforcement</b> <b>learning</b> algorithms in three realistic driving scenarios. We also provide a unified decision-making process model that can operate effectively across different scenarios, serving as a reference framework in algorithm design. Our research lays the groundwork for further collaborations in the community to explore practical aspects of existing <b>reinforcement</b> <b>learning</b> methods. Dataset and codes can be found in <a href=https://sites.google.com/view/ad4rl>https://sites.google.com/view/ad4rl</a>.</p></p class="citation"></blockquote><h3 id=2648--136258-the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies-marcin-p-joachimiak-et-al-2024>(26/48 | 136/258) The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies (Marcin P. Joachimiak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard. (2024)<br><strong>The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies</strong><br><button class=copy-to-clipboard title="The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03044v1.pdf filename=2404.03044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Artificial Intelligence Ontology (AIO) is a systematization of artificial intelligence (AI) concepts, methodologies, and their interrelations. Developed via manual curation, with the additional assistance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> AIO aims to address the rapidly evolving landscape of AI by providing a comprehensive framework that encompasses both technical and ethical aspects of AI technologies. The primary audience for AIO includes AI researchers, developers, and educators seeking standardized terminology and concepts within the AI domain. The ontology is structured around six top-level branches: Networks, Layers, Functions, <b>LLMs,</b> Preprocessing, and Bias, each designed to support the modular composition of AI methods and facilitate a deeper understanding of deep learning architectures and ethical considerations in AI. AIO&rsquo;s development utilized the Ontology Development Kit (ODK) for its creation and maintenance, with its content being dynamically updated through AI-driven curation support. This approach not only ensures the ontology&rsquo;s relevance amidst the fast-paced advancements in AI but also significantly enhances its utility for researchers, developers, and educators by simplifying the integration of new AI concepts and methodologies. The ontology&rsquo;s utility is demonstrated through the annotation of AI methods data in a catalog of AI research publications and the integration into the BioPortal ontology resource, highlighting its potential for cross-disciplinary research. The AIO ontology is open source and is available on GitHub (<a href=https://github.com/berkeleybop/artificial-intelligence-ontology>https://github.com/berkeleybop/artificial-intelligence-ontology</a>) and BioPortal (<a href=https://bioportal.bioontology.org/ontologies/AIO)>https://bioportal.bioontology.org/ontologies/AIO)</a>.</p></p class="citation"></blockquote><h3 id=2748--137258-domain-generalization-through-meta-learning-a-survey-arsham-gholamzadeh-khoee-et-al-2024>(27/48 | 137/258) Domain Generalization through Meta-Learning: A Survey (Arsham Gholamzadeh Khoee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt. (2024)<br><strong>Domain Generalization through Meta-Learning: A Survey</strong><br><button class=copy-to-clipboard title="Domain Generalization through Meta-Learning: A Survey" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: Meta Learning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02785v1.pdf filename=2404.02785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with <b>out-of-distribution</b> (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. <b>Meta-learning</b> <b>presents</b> a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of <b>meta-learning</b> <b>with</b> a focus on its contribution to domain generalization. We first clarify the concept of <b>meta-learning</b> <b>for</b> domain generalization and introduce a novel taxonomy based on the feature extraction strategy and the classifier learning methodology, offering a granular view of methodologies. Through an exhaustive review of existing methods and underlying theories, we map out the fundamentals of the field. Our survey provides practical insights and an informed discussion on promising research directions, paving the way for future innovation in <b>meta-learning</b> <b>for</b> domain generalization.</p></p class="citation"></blockquote><h3 id=2848--138258-federated-computing----survey-on-building-blocks-extensions-and-systems-rené-schwermer-et-al-2024>(28/48 | 138/258) Federated Computing &ndash; Survey on Building Blocks, Extensions and Systems (René Schwermer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>René Schwermer, Ruben Mayer, Hans-Arno Jacobsen. (2024)<br><strong>Federated Computing &ndash; Survey on Building Blocks, Extensions and Systems</strong><br><button class=copy-to-clipboard title="Federated Computing -- Survey on Building Blocks, Extensions and Systems" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02779v1.pdf filename=2404.02779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the increasing volume and sensitivity of data, traditional centralized computing models face challenges, such as data security breaches and regulatory hurdles. <b>Federated</b> <b>Computing</b> (FC) addresses these concerns by enabling collaborative processing without compromising individual data privacy. This is achieved through a decentralized network of devices, each retaining control over its data, while participating in collective computations. The motivation behind FC extends beyond technical considerations to encompass societal implications. As the need for responsible AI and ethical data practices intensifies, FC aligns with the principles of user empowerment and data sovereignty. FC comprises of <b>Federated</b> <b>Learning</b> (FL) and <b>Federated</b> <b>Analytics</b> (FA). FC systems became more complex over time and they currently lack a clear definition and taxonomy describing its moving pieces. Current surveys capture domain-specific FL use cases, describe individual components in an FC pipeline individually or decoupled from each other, or provide a quantitative overview of the number of published papers. This work surveys more than 150 papers to <b>distill</b> the underlying structure of FC systems with their basic building blocks, extensions, architecture, environment, and motivation. We capture FL and FA systems individually and point out unique difference between those two.</p></p class="citation"></blockquote><h3 id=2948--139258-adversarial-attacks-and-dimensionality-in-text-classifiers-nandish-chattopadhyay-et-al-2024>(29/48 | 139/258) Adversarial Attacks and Dimensionality in Text Classifiers (Nandish Chattopadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nandish Chattopadhyay, Atreya Goswami, Anupam Chattopadhyay. (2024)<br><strong>Adversarial Attacks and Dimensionality in Text Classifiers</strong><br><button class=copy-to-clipboard title="Adversarial Attacks and Dimensionality in Text Classifiers" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Text Classification, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02660v1.pdf filename=2404.02660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, <b>adversarial</b> <b>attacks</b> have been first identified and studied in the domain of image processing. In this paper, we study <b>adversarial</b> <b>examples</b> in the field of natural language processing, specifically <b>text</b> <b>classification</b> tasks. We investigate the reasons for <b>adversarial</b> <b>vulnerability,</b> particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the <b>adversarial</b> <b>samples</b> and their effectiveness on models tuned with input samples with same embedding dimension. We utilize this sensitivity to design an <b>adversarial</b> <b>defense</b> mechanism. We use ensemble models of varying inherent dimensionality to thwart the attacks. This is tested on multiple datasets for its efficacy in providing robustness. We also study the problem of measuring <b>adversarial</b> <b>perturbation</b> using different distance metrics. For all of the aforementioned studies, we have run tests on multiple models with varying dimensionality and used a word-vector level <b>adversarial</b> <b>attack</b> to substantiate the findings.</p></p class="citation"></blockquote><h3 id=3048--140258-solving-a-real-world-optimization-problem-using-proximal-policy-optimization-with-curriculum-learning-and-reward-engineering-abhijeet-pendyala-et-al-2024>(30/48 | 140/258) Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering (Abhijeet Pendyala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhijeet Pendyala, Asma Atamna, Tobias Glasmachers. (2024)<br><strong>Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering</strong><br><button class=copy-to-clipboard title="Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02577v1.pdf filename=2404.02577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a proximal policy optimization (PPO) agent trained through <b>curriculum</b> <b>learning</b> (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment&rsquo;s extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial <b>reinforcement</b> <b>learning</b> task. Our five-stage CL approach tackles these challenges by gradually increasing the complexity of the environmental dynamics during policy transfer while simultaneously refining the reward mechanism. This iterative and adaptable process enables the agent to learn a desired optimal policy. Results demonstrate that our approach significantly improves inference-time safety, achieving near-zero safety violations in addition to enhancing waste sorting plant efficiency.</p></p class="citation"></blockquote><h3 id=3148--141258-fedselect-personalized-federated-learning-with-customized-selection-of-parameters-for-fine-tuning-rishub-tamirisa-et-al-2024>(31/48 | 141/258) FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning (Rishub Tamirisa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishub Tamirisa, Chulin Xie, Wenxuan Bao, Andy Zhou, Ron Arel, Aviv Shamsian. (2024)<br><strong>FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning</strong><br><button class=copy-to-clipboard title="FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02478v1.pdf filename=2404.02478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standard <b>federated</b> <b>learning</b> approaches suffer when client data distributions have sufficient heterogeneity. Recent methods addressed the client data heterogeneity issue via personalized <b>federated</b> <b>learning</b> (PFL) - a class of FL algorithms aiming to personalize learned global knowledge to better suit the clients&rsquo; local data distributions. Existing PFL methods usually decouple global updates in deep neural networks by performing personalization on particular layers (i.e. classifier heads) and global aggregation for the rest of the network. However, preselecting network layers for personalization may result in suboptimal storage of global knowledge. In this work, we propose FedSelect, a novel PFL algorithm inspired by the iterative subnetwork discovery procedure used for the Lottery Ticket Hypothesis. FedSelect incrementally expands subnetworks to personalize client parameters, concurrently conducting global aggregations on the remaining parameters. This approach enables the personalization of both client parameters and subnetwork structure during the training process. Finally, we show that FedSelect outperforms recent state-of-the-art PFL algorithms under challenging client data heterogeneity settings and demonstrates robustness to various real-world distributional shifts. Our code is available at <a href=https://github.com/lapisrocks/fedselect>https://github.com/lapisrocks/fedselect</a>.</p></p class="citation"></blockquote><h3 id=3248--142258-decision-predicate-graphs-enhancing-interpretability-in-tree-ensembles-leonardo-arrighi-et-al-2024>(32/48 | 142/258) Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles (Leonardo Arrighi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonardo Arrighi, Luca Pennella, Gabriel Marques Tavares, Sylvio Barbon Junior. (2024)<br><strong>Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles</strong><br><button class=copy-to-clipboard title="Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02942v1.pdf filename=2404.02942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the decisions of tree-based ensembles and their relationships is pivotal for machine learning model interpretation. Recent attempts to mitigate the <b>human-in-the-loop</b> interpretation challenge have explored the extraction of the decision structure underlying the model taking advantage of <b>graph</b> simplification and path emphasis. However, while these efforts enhance the visualisation experience, they may either result in a visually complex representation or compromise the interpretability of the original ensemble model. In addressing this challenge, especially in complex scenarios, we introduce the Decision Predicate <b>Graph</b> (DPG) as a model-agnostic tool to provide a global interpretation of the model. DPG is a <b>graph</b> structure that captures the tree-based ensemble model and learned dataset details, preserving the relations among features, logical decisions, and predictions towards emphasising insightful points. Leveraging well-known <b>graph</b> theory concepts, such as the notions of centrality and community, DPG offers additional quantitative insights into the model, complementing visualisation techniques, expanding the problem space descriptions, and offering diverse possibilities for extensions. Empirical experiments demonstrate the potential of DPG in addressing traditional <b>benchmarks</b> and complex classification scenarios.</p></p class="citation"></blockquote><h3 id=3348--143258-model-based-reinforcement-learning-for-parameterized-action-spaces-renhao-zhang-et-al-2024>(33/48 | 143/258) Model-based Reinforcement Learning for Parameterized Action Spaces (Renhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renhao Zhang, Haotian Fu, Yilin Miao, George Konidaris. (2024)<br><strong>Model-based Reinforcement Learning for Parameterized Action Spaces</strong><br><button class=copy-to-clipboard title="Model-based Reinforcement Learning for Parameterized Action Spaces" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03037v1.pdf filename=2404.03037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel model-based <b>reinforcement</b> <b>learning</b> algorithm &ndash; Dynamics Learning and predictive control with Parameterized Actions (DLPA) &ndash; for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. We theoretically quantify the difference between the generated trajectory and the optimal trajectory during planning in terms of the value they achieved through the lens of Lipschitz Continuity. Our empirical results on several standard <b>benchmarks</b> show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.</p></p class="citation"></blockquote><h3 id=3448--144258-spectral-clustering-in-convex-and-constrained-settings-swarup-ranjan-behera-et-al-2024>(34/48 | 144/258) Spectral Clustering in Convex and Constrained Settings (Swarup Ranjan Behera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swarup Ranjan Behera, Vijaya V. Saradhi. (2024)<br><strong>Spectral Clustering in Convex and Constrained Settings</strong><br><button class=copy-to-clipboard title="Spectral Clustering in Convex and Constrained Settings" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Constrained Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03012v1.pdf filename=2404.03012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spectral <b>clustering</b> methods have gained widespread recognition for their effectiveness in <b>clustering</b> high-dimensional data. Among these techniques, <b>constrained</b> <b>spectral</b> <b>clustering</b> has emerged as a prominent approach, demonstrating enhanced performance by integrating pairwise constraints. However, the application of such constraints to semidefinite spectral <b>clustering,</b> a variant that leverages semidefinite programming to optimize <b>clustering</b> objectives, remains largely unexplored. In this paper, we introduce a novel framework for seamlessly integrating pairwise constraints into semidefinite spectral <b>clustering.</b> Our methodology systematically extends the capabilities of semidefinite spectral <b>clustering</b> to capture complex data structures, thereby addressing real-world <b>clustering</b> challenges more effectively. Additionally, we extend this framework to encompass both active and self-taught learning scenarios, further enhancing its versatility and applicability. Empirical studies conducted on well-known datasets demonstrate the superiority of our proposed framework over existing spectral <b>clustering</b> methods, showcasing its robustness and scalability across diverse datasets and learning settings. By bridging the gap between <b>constrained</b> <b>learning</b> and semidefinite spectral <b>clustering,</b> our work contributes to the advancement of spectral <b>clustering</b> techniques, offering researchers and practitioners a versatile tool for addressing complex <b>clustering</b> challenges in various real-world applications. Access to the data, code, and experimental results is provided for further exploration (<a href=https://github.com/swarupbehera/SCCCS)>https://github.com/swarupbehera/SCCCS)</a>.</p></p class="citation"></blockquote><h3 id=3548--145258-incremental-learning-with-concept-drift-detection-and-prototype-based-embeddings-for-graph-stream-classification-kleanthis-malialis-et-al-2024>(35/48 | 145/258) Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification (Kleanthis Malialis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kleanthis Malialis, Jin Li, Christos G. Panayiotou, Marios M. Polycarpou. (2024)<br><strong>Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification</strong><br><button class=copy-to-clipboard title="Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02572v1.pdf filename=2404.02572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data stream mining aims at extracting meaningful knowledge from continually evolving data streams, addressing the challenges posed by nonstationary environments, particularly, concept drift which refers to a change in the underlying data distribution over time. <b>Graph</b> <b>structures</b> offer a powerful modelling tool to represent complex systems, such as, critical infrastructure systems and social networks. Learning from <b>graph</b> <b>streams</b> becomes a necessity to understand the dynamics of <b>graph</b> <b>structures</b> and to facilitate informed decision-making. This work introduces a novel method for <b>graph</b> <b>stream</b> classification which operates under the general setting where a data generating process produces <b>graphs</b> <b>with</b> varying nodes and edges over time. The method uses incremental learning for continual model adaptation, selecting representative <b>graphs</b> <b>(prototypes)</b> for each class, and creating <b>graph</b> <b>embeddings.</b> Additionally, it incorporates a loss-based concept drift detection mechanism to recalculate <b>graph</b> <b>prototypes</b> when drift is detected.</p></p class="citation"></blockquote><h3 id=3648--146258-methodology-for-interpretable-reinforcement-learning-for-optimizing-mechanical-ventilation-joo-seung-lee-et-al-2024>(36/48 | 146/258) Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation (Joo Seung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joo Seung Lee, Malini Mahendra, Anil Aswani. (2024)<br><strong>Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation</strong><br><button class=copy-to-clipboard title="Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03105v1.pdf filename=2404.03105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mechanical ventilation is a critical life-support intervention that uses a machine to deliver controlled air and oxygen to a patient&rsquo;s lungs, assisting or replacing spontaneous breathing. While several data-driven approaches have been proposed to optimize ventilator control strategies, they often lack interpretability and agreement with general domain knowledge. This paper proposes a methodology for interpretable <b>reinforcement</b> <b>learning</b> (RL) using decision trees for mechanical ventilation control. Using a causal, nonparametric model-based off-policy evaluation, we evaluate the policies in their ability to gain increases in SpO2 while avoiding aggressive ventilator settings which are known to cause ventilator induced lung injuries and other complications. Numerical experiments using MIMIC-III data on the stays of real patients&rsquo; intensive care unit stays demonstrate that the decision tree policy outperforms the behavior cloning policy and is comparable to state-of-the-art RL policy. Future work concerns better aligning the cost function with medical objectives to generate deeper clinical insights.</p></p class="citation"></blockquote><h3 id=3748--147258-universal-functional-regression-with-neural-operator-flows-yaozhong-shi-et-al-2024>(37/48 | 147/258) Universal Functional Regression with Neural Operator Flows (Yaozhong Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaozhong Shi, Angela F. Gao, Zachary E. Ross, Kamyar Azizzadenesheli. (2024)<br><strong>Universal Functional Regression with Neural Operator Flows</strong><br><button class=copy-to-clipboard title="Universal Functional Regression with Neural Operator Flows" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02986v1.pdf filename=2404.02986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regression on function spaces is typically limited to models with <b>Gaussian</b> <b>process</b> priors. We introduce the notion of universal functional regression, in which we aim to learn a prior distribution over non-Gaussian function spaces that remains mathematically tractable for functional regression. To do this, we develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of normalizing flows. OpFlow is an invertible operator that maps the (potentially unknown) data function space into a <b>Gaussian</b> <b>process,</b> allowing for exact likelihood estimation of functional point evaluations. OpFlow enables robust and accurate uncertainty quantification via drawing posterior samples of the <b>Gaussian</b> <b>process</b> and subsequently mapping them into the data function space. We empirically study the performance of OpFlow on regression and generation tasks with data generated from <b>Gaussian</b> <b>processes</b> with known posterior forms and non-Gaussian processes, as well as real-world earthquake seismograms with an unknown closed-form distribution.</p></p class="citation"></blockquote><h3 id=3848--148258-modno-multi-operator-learning-with-distributed-neural-operators-zecheng-zhang-2024>(38/48 | 148/258) MODNO: Multi Operator Learning With Distributed Neural Operators (Zecheng Zhang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zecheng Zhang. (2024)<br><strong>MODNO: Multi Operator Learning With Distributed Neural Operators</strong><br><button class=copy-to-clipboard title="MODNO: Multi Operator Learning With Distributed Neural Operators" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02892v1.pdf filename=2404.02892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using <b>foundation</b> <b>models</b> equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset. Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method. Our results demonstrate enhanced efficiency and satisfactory accuracy. Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning. This highlights another MOL&rsquo;s potential to bolster operator learning.</p></p class="citation"></blockquote><h3 id=3948--149258-guarantees-of-confidentiality-via-hammersley-chapman-robbins-bounds-kamalika-chaudhuri-et-al-2024>(39/48 | 149/258) Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds (Kamalika Chaudhuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert. (2024)<br><strong>Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds</strong><br><button class=copy-to-clipboard title="Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CY, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02866v1.pdf filename=2404.02866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as &ldquo;features&rdquo; (or, less commonly, as &ldquo;embeddings&rdquo; or &ldquo;feature embeddings&rdquo;). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins &ndash; the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, <b>&ldquo;MNIST&rdquo;</b> and &ldquo;CIFAR-10,&rdquo; which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, &ldquo;ResNet-18&rdquo; and &ldquo;Swin-T,&rdquo; pre-trained on the data set, &ldquo;ImageNet-1000,&rdquo; which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.</p></p class="citation"></blockquote><h3 id=4048--150258-dnn-memory-footprint-reduction-via-post-training-intra-layer-multi-precision-quantization-behnam-ghavami-et-al-2024>(40/48 | 150/258) DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization (Behnam Ghavami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behnam Ghavami, Amin Kamjoo, Lesley Shannon, Steve Wilton. (2024)<br><strong>DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization</strong><br><button class=copy-to-clipboard title="DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02947v1.pdf filename=2404.02947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The imperative to deploy Deep Neural Network (DNN) models on resource-constrained edge devices, spurred by privacy concerns, has become increasingly apparent. To facilitate the transition from cloud to edge computing, this paper introduces a technique that effectively reduces the memory footprint of DNNs, accommodating the limitations of resource-constrained edge devices while preserving model accuracy. Our proposed technique, named Post-Training Intra-Layer Multi-Precision <b>Quantization</b> (PTILMPQ), employs a post-training <b>quantization</b> approach, eliminating the need for extensive training data. By estimating the importance of layers and channels within the network, the proposed method enables precise bit allocation throughout the <b>quantization</b> process. Experimental results demonstrate that PTILMPQ offers a promising solution for deploying DNNs on edge devices with restricted memory resources. For instance, in the case of ResNet50, it achieves an accuracy of 74.57% with a memory footprint of 9.5 MB, representing a 25.49% reduction compared to previous similar methods, with only a minor 1.08% decrease in accuracy.</p></p class="citation"></blockquote><h3 id=4148--151258-continual-learning-of-numerous-tasks-from-long-tail-distributions-liwei-kang-et-al-2024>(41/48 | 151/258) Continual Learning of Numerous Tasks from Long-tail Distributions (Liwei Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwei Kang, Wee Sun Lee. (2024)<br><strong>Continual Learning of Numerous Tasks from Long-tail Distributions</strong><br><button class=copy-to-clipboard title="Continual Learning of Numerous Tasks from Long-tail Distributions" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02754v1.pdf filename=2404.02754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning,</b> an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing <b>continual</b> <b>learning</b> algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of <b>continual</b> <b>learning</b> algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world <b>continual</b> <b>learning</b> datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in <b>continual</b> <b>learning,</b> the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve <b>continual</b> <b>learning</b> performance. We propose a method that reuses the optimizer states in Adam by maintaining a weighted average of the second moments from previous tasks. We demonstrate that our method, compatible with most existing <b>continual</b> <b>learning</b> algorithms, effectively reduces forgetting with only a small amount of additional computational or memory costs, and provides further improvements on existing <b>continual</b> <b>learning</b> algorithms, particularly in a long-tail task sequence.</p></p class="citation"></blockquote><h3 id=4248--152258-reinforcement-learning-in-categorical-cybernetics-jules-hedges-et-al-2024>(42/48 | 152/258) Reinforcement Learning in Categorical Cybernetics (Jules Hedges et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jules Hedges, Riu Rodríguez Sakamoto. (2024)<br><strong>Reinforcement Learning in Categorical Cybernetics</strong><br><button class=copy-to-clipboard title="Reinforcement Learning in Categorical Cybernetics" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-CT<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02688v1.pdf filename=2404.02688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that several major algorithms of <b>reinforcement</b> <b>learning</b> (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as different extremal cases of this general setup: dynamic programming, Monte Carlo methods, temporal difference learning, and deep RL. We see this as strong evidence that this approach is a natural one and believe that it will be a fruitful way to think about RL in the future.</p></p class="citation"></blockquote><h3 id=4348--153258-transformer-based-stagewise-decomposition-for-large-scale-multistage-stochastic-optimization-chanyeong-kim-et-al-2024>(43/48 | 153/258) Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization (Chanyeong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanyeong Kim, Jongwoong Park, Hyunglip Bae, Woo Chang Kim. (2024)<br><strong>Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization</strong><br><button class=copy-to-clipboard title="Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02583v1.pdf filename=2404.02583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving large-scale multistage stochastic programming (MSP) problems poses a significant challenge as commonly used stagewise decomposition algorithms, including stochastic dual dynamic programming (SDDP), face growing time complexity as the subproblem size and problem count increase. Traditional approaches approximate the value functions as piecewise linear convex functions by incrementally accumulating subgradient cutting planes from the primal and dual solutions of stagewise subproblems. Recognizing these limitations, we introduce TranSDDP, a novel <b>Transformer-based</b> stagewise decomposition algorithm. This innovative approach leverages the structural advantages of the <b>Transformer</b> model, implementing a sequential method for integrating subgradient cutting planes to approximate the value function. Through our numerical experiments, we affirm TranSDDP&rsquo;s effectiveness in addressing MSP problems. It efficiently generates a piecewise linear approximation for the value function, significantly reducing computation time while preserving solution quality, thus marking a promising progression in the treatment of large-scale multistage stochastic programming problems.</p></p class="citation"></blockquote><h3 id=4448--154258-an-interpretable-client-decision-tree-aggregation-process-for-federated-learning-alberto-argente-garrido-et-al-2024>(44/48 | 154/258) An Interpretable Client Decision Tree Aggregation process for Federated Learning (Alberto Argente-Garrido et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Argente-Garrido, Cristina Zuheros, M. Victoria Luzón, Francisco Herrera. (2024)<br><strong>An Interpretable Client Decision Tree Aggregation process for Federated Learning</strong><br><button class=copy-to-clipboard title="An Interpretable Client Decision Tree Aggregation process for Federated Learning" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02510v1.pdf filename=2404.02510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trustworthy Artificial Intelligence solutions are essential in today&rsquo;s data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of <b>Federated</b> <b>Learning</b> as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as <b>federated</b> <b>learning</b> environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a <b>federated</b> <b>learning</b> environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for <b>Federated</b> <b>Learning</b> scenarios that keeps the interpretability and the precision of the base decision trees used for the aggregation. This model is based on aggregating multiple decision paths of the decision trees and can be used on different decision tree types, such as ID3 and CART. We carry out the experiments within four datasets, and the analysis shows that the tree built with the model improves the local models, and outperforms the state-of-the-art.</p></p class="citation"></blockquote><h3 id=4548--155258-optimal-batch-allocation-for-wireless-federated-learning-jaeyoung-song-et-al-2024>(45/48 | 155/258) Optimal Batch Allocation for Wireless Federated Learning (Jaeyoung Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaeyoung Song, Sang-Woon Jeon. (2024)<br><strong>Optimal Batch Allocation for Wireless Federated Learning</strong><br><button class=copy-to-clipboard title="Optimal Batch Allocation for Wireless Federated Learning" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02395v1.pdf filename=2404.02395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> aims to construct a global model that fits the dataset distributed across local devices without direct access to private data, leveraging communication between a server and the local devices. In the context of a practical communication scheme, we study the completion time required to achieve a target performance. Specifically, we analyze the number of iterations required for <b>federated</b> <b>learning</b> to reach a specific optimality gap from a minimum global loss. Subsequently, we characterize the time required for each iteration under two fundamental multiple access schemes: time-division multiple access (TDMA) and random access (RA). We propose a step-wise batch allocation, demonstrated to be optimal for TDMA-based <b>federated</b> <b>learning</b> systems. Additionally, we show that the non-zero batch gap between devices provided by the proposed step-wise batch allocation significantly reduces the completion time for RA-based learning systems. Numerical evaluations validate these analytical results through real-data experiments, highlighting the remarkable potential for substantial completion time reduction.</p></p class="citation"></blockquote><h3 id=4648--156258-on-line-conformalized-neural-networks-ensembles-for-probabilistic-forecasting-of-day-ahead-electricity-prices-alessandro-brusaferri-et-al-2024>(46/48 | 156/258) On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices (Alessandro Brusaferri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Brusaferri, Andrea Ballarino, Luigi Grossi, Fabrizio Laurini. (2024)<br><strong>On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices</strong><br><button class=copy-to-clipboard title="On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02722v1.pdf filename=2404.02722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Probabilistic electricity price forecasting (PEPF) is subject of increasing interest, following the demand for proper quantification of prediction uncertainty, to support the operation in complex power markets with increasing share of renewable generation. Distributional neural networks ensembles have been recently shown to outperform state of the art PEPF <b>benchmarks.</b> Still, they require critical reliability enhancements, as fail to pass the coverage tests at various steps on the prediction horizon. In this work, we propose a novel approach to PEPF, extending the state of the art neural networks ensembles based methods through conformal inference based techniques, deployed within an on-line recalibration procedure. Experiments have been conducted on multiple market regions, achieving day-ahead forecasts with improved hourly coverage and stable probabilistic scores.</p></p class="citation"></blockquote><h3 id=4748--157258-effector-a-python-package-for-regional-explanations-vasilis-gkolemis-et-al-2024>(47/48 | 157/258) Effector: A Python package for regional explanations (Vasilis Gkolemis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasilis Gkolemis, Christos Diou, Eirini Ntoutsi, Theodore Dalamagas, Bernd Bischl, Julia Herbinger, Giuseppe Casalicchio. (2024)<br><strong>Effector: A Python package for regional explanations</strong><br><button class=copy-to-clipboard title="Effector: A Python package for regional explanations" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02629v1.pdf filename=2404.02629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Global feature effect methods explain a model outputting one plot per feature. The plot shows the average effect of the feature on the output, like the effect of age on the annual income. However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average. To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace. For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age&rsquo;s effect on annual income separately for males and females and different levels of professional experience. We introduce Effector, a Python library dedicated to regional feature effects. Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects. Effector automatically detects subspaces where regional effects have reduced heterogeneity. All global and regional effect methods share a common API, facilitating comparisons between them. Moreover, the library&rsquo;s interface is extensible so new methods can be easily added and <b>benchmarked.</b> The library has been thoroughly tested, ships with many tutorials (<a href=https://xai-effector.github.io/>https://xai-effector.github.io/</a>) and is available under an open-source license at PyPi (<a href=https://pypi.org/project/effector/>https://pypi.org/project/effector/</a>) and Github (<a href=https://github.com/givasile/effector)>https://github.com/givasile/effector)</a>.</p></p class="citation"></blockquote><h3 id=4848--158258-adaptive-sampling-policies-imply-biased-beliefs-a-generalization-of-the-hot-stove-effect-jerker-denrell-2024>(48/48 | 158/258) Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect (Jerker Denrell, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerker Denrell. (2024)<br><strong>Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect</strong><br><button class=copy-to-clipboard title="Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02591v1.pdf filename=2404.02591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Hot Stove Effect is a negativity bias resulting from the adaptive character of learning. The mechanism is that learning algorithms that pursue alternatives with positive estimated values, but avoid alternatives with negative estimated values, will correct errors of overestimation but fail to correct errors of underestimation. Here, we generalize the theory behind the Hot Stove Effect to settings in which negative estimates do not necessarily lead to avoidance but to a smaller <b>sample</b> <b>size</b> (i.e., a learner selects fewer of alternative B if B is believed to be inferior but does not entirely avoid B). We formally demonstrate that the negativity bias remains in this set-up. We also show there is a negativity bias for Bayesian learners in the sense that most such learners underestimate the expected value of an alternative.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--159258-clam-tts-improving-neural-codec-language-model-for-zero-shot-text-to-speech-jaehyeon-kim-et-al-2024>(1/1 | 159/258) CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech (Jaehyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaehyeon Kim, Keon Lee, Seungjun Chung, Jaewoong Cho. (2024)<br><strong>CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech</strong><br><button class=copy-to-clipboard title="CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 70<br>Keywords: Quantization, Zero-shot, Stemming, Text-to-speech, Text-to-speech, Tokenization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02781v1.pdf filename=2404.02781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, <b>large</b> <b>language</b> <b>models</b> have recently gained attention as a promising approach for <b>zero-shot</b> <b>Text-to-Speech</b> <b>(TTS)</b> synthesis. Despite the ongoing rush towards scaling paradigms, audio <b>tokenization</b> ironically amplifies the scalability challenge, <b>stemming</b> from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector <b>quantization</b> to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based <b>TTS</b> models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text <b>tokenization</b> strategies on performances.</p></p class="citation"></blockquote><h2 id=csai-7>cs.AI (7)</h2><h3 id=17--160258-empowering-biomedical-discovery-with-ai-agents-shanghua-gao-et-al-2024>(1/7 | 160/258) Empowering Biomedical Discovery with AI Agents (Shanghua Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik. (2024)<br><strong>Empowering Biomedical Discovery with AI Agents</strong><br><button class=copy-to-clipboard title="Empowering Biomedical Discovery with AI Agents" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: Continual Learning, Simulation, Simulator, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02831v1.pdf filename=2404.02831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We envision &lsquo;AI scientists&rsquo; as systems capable of skeptical learning and <b>reasoning</b> that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI&rsquo;s ability to analyze <b>large</b> <b>datasets,</b> <b>navigate</b> hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use <b>large</b> <b>language</b> <b>models</b> and generative models to feature structured memory for <b>continual</b> <b>learning</b> and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell <b>simulation,</b> programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.</p></p class="citation"></blockquote><h3 id=27--161258-i-design-personalized-llm-interior-designer-ata-çelen-et-al-2024>(2/7 | 161/258) I-Design: Personalized LLM Interior Designer (Ata Çelen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang. (2024)<br><strong>I-Design: Personalized LLM Interior Designer</strong><br><button class=copy-to-clipboard title="I-Design: Personalized LLM Interior Designer" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, Reasoning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02838v1.pdf filename=2404.02838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interior design allows us to be who we are and live how we want - each design is as unique as our distinct personality. However, it is not trivial for non-professionals to express and materialize this since it requires aligning functional and visual expectations with the constraints of physical space; this renders interior design a luxury. To make it more accessible, we present I-Design, a personalized interior designer that allows users to generate and visualize their design goals through natural language communication. I-Design starts with a team of <b>large</b> <b>language</b> <b>model</b> agents that engage in dialogues and logical <b>reasoning</b> with one another, transforming textual user input into feasible scene <b>graph</b> designs with relative object relationships. Subsequently, an effective placement algorithm determines optimal locations for each object within the scene. The final design is then constructed in 3D by retrieving and integrating assets from an existing object database. Additionally, we propose a new evaluation protocol that utilizes a <b>vision-language</b> model and complements the design pipeline. Extensive quantitative and qualitative experiments show that I-Design outperforms existing methods in delivering high-quality 3D design solutions and aligning with abstract concepts that match user input, showcasing its advantages across detailed 3D arrangement and conceptual fidelity.</p></p class="citation"></blockquote><h3 id=37--162258-learn-to-disguise-avoid-refusal-responses-in-llms-defense-via-a-multi-agent-attacker-disguiser-game-qianqiao-xu-et-al-2024>(3/7 | 162/258) Learn to Disguise: Avoid Refusal Responses in LLM&rsquo;s Defense via a Multi-agent Attacker-Disguiser Game (Qianqiao Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li. (2024)<br><strong>Learn to Disguise: Avoid Refusal Responses in LLM&rsquo;s Defense via a Multi-agent Attacker-Disguiser Game</strong><br><button class=copy-to-clipboard title="Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 35<br>Keywords: Black Box, Curriculum Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02532v1.pdf filename=2404.02532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as <b>prompt</b> engineering. As a result, large models counter malicious attackers&rsquo; attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers&rsquo; capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the <b>curriculum</b> <b>learning</b> process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model&rsquo;s ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any <b>black-box</b> <b>large</b> model to assist the model in defense and does not suffer from model version iterations.</p></p class="citation"></blockquote><h3 id=47--163258-integrating-explanations-in-learning-ltl-specifications-from-demonstrations-ashutosh-gupta-et-al-2024>(4/7 | 163/258) Integrating Explanations in Learning LTL Specifications from Demonstrations (Ashutosh Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashutosh Gupta, John Komp, Abhay Singh Rajput, Krishna Shankaranarayanan, Ashutosh Trivedi, Namrita Varshney. (2024)<br><strong>Integrating Explanations in Learning LTL Specifications from Demonstrations</strong><br><button class=copy-to-clipboard title="Integrating Explanations in Learning LTL Specifications from Demonstrations" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-8, cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Natural Language Explanation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02872v1.pdf filename=2404.02872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates whether recent advances in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can assist in translating human explanations into a format that can robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both <b>LLMs</b> and optimization-based methods can extract LTL specifications from demonstrations; however, they have distinct limitations. <b>LLMs</b> can quickly generate solutions and incorporate human explanations, but their lack of consistency and reliability hampers their applicability in safety-critical domains. On the other hand, optimization-based methods do provide formal guarantees but cannot process <b>natural</b> <b>language</b> <b>explanations</b> and face scalability challenges. We present a principled approach to combining <b>LLMs</b> and optimization-based methods to faithfully translate human explanations and demonstrations into LTL specifications. We have implemented a tool called Janaka based on our approach. Our experiments demonstrate the effectiveness of combining explanations with demonstrations in learning LTL specifications through several case studies.</p></p class="citation"></blockquote><h3 id=57--164258-data-driven-goal-recognition-design-for-general-behavioral-agents-robert-kasumba-et-al-2024>(5/7 | 164/258) Data-Driven Goal Recognition Design for General Behavioral Agents (Robert Kasumba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Kasumba, Guanghui Yu, Chien-Ju Ho, Sarah Keren, William Yeoh. (2024)<br><strong>Data-Driven Goal Recognition Design for General Behavioral Agents</strong><br><button class=copy-to-clipboard title="Data-Driven Goal Recognition Design for General Behavioral Agents" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03054v1.pdf filename=2404.03054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Goal recognition design aims to make limited modifications to decision-making environments with the goal of making it easier to infer the goals of agents acting within those environments. Although various research efforts have been made in goal recognition design, existing approaches are computationally demanding and often assume that agents are (near-)optimal in their decision-making. To address these limitations, we introduce a data-driven approach to goal recognition design that can account for agents with general behavioral models. Following existing literature, we use worst-case distinctiveness ($\textit{wcd}$) as a measure of the difficulty in inferring the goal of an agent in a decision-making environment. Our approach begins by training a machine learning model to predict the $\textit{wcd}$ for a given environment and the agent behavior model. We then propose a gradient-based optimization framework that accommodates various constraints to optimize decision-making environments for enhanced goal recognition. Through extensive <b>simulations,</b> we demonstrate that our approach outperforms existing methods in reducing $\textit{wcd}$ and enhancing runtime efficiency in conventional setups, and it also adapts to scenarios not previously covered in the literature, such as those involving flexible budget constraints, more complex environments, and suboptimal agent behavior. Moreover, we have conducted human-subject experiments which confirm that our method can create environments that facilitate efficient goal recognition from real-world human decision-makers.</p></p class="citation"></blockquote><h3 id=67--165258-shield-a-regularization-technique-for-explainable-artificial-intelligence-iván-sevillano-garcía-et-al-2024>(6/7 | 165/258) SHIELD: A regularization technique for eXplainable Artificial Intelligence (Iván Sevillano-García et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iván Sevillano-García, Julián Luengo, Francisco Herrera. (2024)<br><strong>SHIELD: A regularization technique for eXplainable Artificial Intelligence</strong><br><button class=copy-to-clipboard title="SHIELD: A regularization technique for eXplainable Artificial Intelligence" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-6, cs-AI, cs.AI<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02611v1.pdf filename=2404.02611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Artificial Intelligence systems become integral across domains, the demand for explainability grows. While the effort by the scientific community is focused on obtaining a better explanation for the model, it is important not to ignore the potential of this explanation process to improve training as well. While existing efforts primarily focus on generating and evaluating explanations for <b>black-box</b> <b>models,</b> there remains a critical gap in directly enhancing models through these evaluations. This paper introduces SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a regularization technique for explainable artificial intelligence designed to improve model quality by concealing portions of input data and assessing the resulting discrepancy in predictions. In contrast to conventional approaches, SHIELD regularization seamlessly integrates into the objective function, enhancing model explainability while also improving performance. Experimental validation on <b>benchmark</b> datasets underscores SHIELD&rsquo;s effectiveness in improving Artificial Intelligence model explainability and overall performance. This establishes SHIELD regularization as a promising pathway for developing transparent and reliable Artificial Intelligence regularization techniques.</p></p class="citation"></blockquote><h3 id=77--166258-learning-generalized-policies-for-fully-observable-non-deterministic-planning-domains-till-hofmann-et-al-2024>(7/7 | 166/258) Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains (Till Hofmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Till Hofmann, Hector Geffner. (2024)<br><strong>Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains</strong><br><button class=copy-to-clipboard title="Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02499v1.pdf filename=2404.02499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of <b>benchmark</b> domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--167258-ai-tutoring-in-software-engineering-education-eduard-frankford-et-al-2024>(1/5 | 167/258) AI-Tutoring in Software Engineering Education (Eduard Frankford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche, Ruth Breu. (2024)<br><strong>AI-Tutoring in Software Engineering Education</strong><br><button class=copy-to-clipboard title="AI-Tutoring in Software Engineering Education" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02548v1.pdf filename=2404.02548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the <b>GPT-3.5-Turbo</b> model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students&rsquo; concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI&rsquo;s role in education.</p></p class="citation"></blockquote><h3 id=25--168258-testing-the-effect-of-code-documentation-on-large-language-model-code-understanding-william-macke-et-al-2024>(2/5 | 168/258) Testing the Effect of Code Documentation on Large Language Model Code Understanding (William Macke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Macke, Michael Doyle. (2024)<br><strong>Testing the Effect of Code Documentation on Large Language Model Code Understanding</strong><br><button class=copy-to-clipboard title="Testing the Effect of Code Documentation on Large Language Model Code Understanding" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03114v1.pdf filename=2404.03114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated impressive abilities in recent years with regards to <b>code</b> <b>generation</b> and understanding. However, little work has investigated how documentation and other <b>code</b> <b>properties</b> affect an <b>LLM&rsquo;s</b> ability to understand and generate <b>code</b> <b>or</b> documentation. We present an empirical analysis of how underlying properties of <b>code</b> <b>or</b> documentation can affect an <b>LLM&rsquo;s</b> capabilities. We show that providing an <b>LLM</b> with &ldquo;incorrect&rdquo; documentation can greatly hinder <b>code</b> <b>understanding,</b> while incomplete or missing documentation does not seem to significantly affect an <b>LLM&rsquo;s</b> ability to understand code.</p></p class="citation"></blockquote><h3 id=35--169258-large-language-model-for-vulnerability-detection-and-repair-literature-review-and-roadmap-xin-zhou-et-al-2024>(3/5 | 169/258) Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap (Xin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhou, Sicong Cao, Xiaobing Sun, David Lo. (2024)<br><strong>Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap</strong><br><button class=copy-to-clipboard title="Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02525v1.pdf filename=2404.02525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The significant advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have resulted in their widespread adoption across various tasks within Software Engineering (SE), including vulnerability detection and repair. Numerous recent studies have investigated the application of <b>LLMs</b> to enhance vulnerability detection and repair tasks. Despite the increasing research interest, there is currently no existing survey that focuses on the utilization of <b>LLMs</b> for vulnerability detection and repair. In this paper, we aim to bridge this gap by offering a systematic literature review of approaches aimed at improving vulnerability detection and repair through the utilization of <b>LLMs.</b> The review encompasses research work from leading SE, AI, and Security conferences and journals, covering 36 papers published at 21 distinct venues. By answering three key research questions, we aim to (1) <b>summarize</b> the <b>LLMs</b> employed in the relevant literature, (2) categorize various <b>LLM</b> adaptation techniques in vulnerability detection, and (3) classify various <b>LLM</b> adaptation techniques in vulnerability repair. Based on our findings, we have identified a series of challenges that still need to be tackled considering existing studies. Additionally, we have outlined a roadmap highlighting potential opportunities that we believe are pertinent and crucial for future research endeavors.</p></p class="citation"></blockquote><h3 id=45--170258-the-realhumaneval-evaluating-large-language-models-abilities-to-support-programmers-hussein-mozannar-et-al-2024>(4/5 | 170/258) The RealHumanEval: Evaluating Large Language Models&rsquo; Abilities to Support Programmers (Hussein Mozannar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, David Sontag. (2024)<br><strong>The RealHumanEval: Evaluating Large Language Models&rsquo; Abilities to Support Programmers</strong><br><button class=copy-to-clipboard title="The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-HC, cs-SE, cs.SE<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02806v1.pdf filename=2404.02806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for code has primarily relied on static <b>benchmarks,</b> including HumanEval (Chen et al., 2021), which measure the ability of <b>LLMs</b> to generate complete code that passes unit tests. As <b>LLMs</b> are increasingly used as programmer assistants, we study whether gains on existing <b>benchmarks</b> translate to gains in programmer productivity when coding with <b>LLMs,</b> including time spent coding. In addition to static <b>benchmarks,</b> we investigate the utility of preference metrics that might be used as proxies to measure <b>LLM</b> helpfulness, such as code acceptance or copy rates. To do so, we introduce RealHumanEval, a web interface to measure the ability of <b>LLMs</b> to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six <b>LLMs</b> of varying base model performance. Despite static <b>benchmarks</b> not incorporating humans-in-the-loop, we find that improvements in <b>benchmark</b> performance lead to increased programmer productivity; however gaps in <b>benchmark</b> versus human performance are not proportional &ndash; a trend that holds across both forms of <b>LLM</b> support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.</p></p class="citation"></blockquote><h3 id=55--171258-creating-a-trajectory-for-code-writing-algorithmic-reasoning-tasks-shruthi-ravikumar-et-al-2024>(5/5 | 171/258) Creating a Trajectory for Code Writing: Algorithmic Reasoning Tasks (Shruthi Ravikumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shruthi Ravikumar, Margaret Hamilton, Charles Thevathayan, Maria Spichkova, Kashif Ali, Gayan Wijesinghe. (2024)<br><strong>Creating a Trajectory for Code Writing: Algorithmic Reasoning Tasks</strong><br><button class=copy-to-clipboard title="Creating a Trajectory for Code Writing: Algorithmic Reasoning Tasks" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02464v1.pdf filename=2404.02464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many students in introductory programming courses fare poorly in the code writing tasks of the final summative assessment. Such tasks are designed to assess whether novices have developed the analytical skills to translate from the given problem domain to coding. In the past researchers have used instruments such as code-explain and found that the extent of cognitive depth reached in these tasks correlated well with code writing ability. However, the need for manual marking and personalized interviews used for identifying cognitive difficulties limited the study to a small group of stragglers. To extend this work to larger groups, we have devised several question types with varying cognitive demands collectively called Algorithmic <b>Reasoning</b> Tasks (ARTs), which do not require manual marking. These tasks require levels of <b>reasoning</b> which can define a learning trajectory. This paper describes these instruments and the machine learning models used for validating them. We have used the data collected in an introductory programming course in the penultimate week of the semester which required attempting ART type instruments and code writing. Our preliminary research suggests ART type instruments can be combined with specific machine learning models to act as an effective learning trajectory and early prediction of code-writing skills.</p></p class="citation"></blockquote><h2 id=eesssy-8>eess.SY (8)</h2><h3 id=18--172258-decision-transformer-as-a-foundation-model-for-partially-observable-continuous-control-xiangyuan-zhang-et-al-2024>(1/8 | 172/258) Decision Transformer as a Foundation Model for Partially Observable Continuous Control (Xiangyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyuan Zhang, Weichao Mao, Haoran Qiu, Tamer Başar. (2024)<br><strong>Decision Transformer as a Foundation Model for Partially Observable Continuous Control</strong><br><button class=copy-to-clipboard title="Decision Transformer as a Foundation Model for Partially Observable Continuous Control" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 50<br>Keywords: Foundation Model, Zero-shot, GPT, Transformer, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02407v1.pdf filename=2404.02407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Closed-loop control of nonlinear dynamical systems with partial-state observability demands expert knowledge of a diverse, less standardized set of theoretical tools. Moreover, it requires a delicate integration of controller and estimator designs to achieve the desired system behavior. To establish a general controller synthesis framework, we explore the Decision <b>Transformer</b> (DT) architecture. Specifically, we first frame the control task as predicting the current optimal action based on past observations, actions, and rewards, eliminating the need for a separate estimator design. Then, we leverage the <b>pre-trained</b> <b>language</b> <b>models,</b> i.e., the Generative <b>Pre-trained</b> <b>Transformer</b> <b>(GPT)</b> series, to initialize DT and subsequently train it for control tasks using low-rank adaptation (LoRA). Our comprehensive experiments across five distinct control tasks, ranging from maneuvering aerospace systems to controlling partial differential equations (PDEs), demonstrate DT&rsquo;s capability to capture the parameter-agnostic structures intrinsic to control tasks. DT exhibits remarkable <b>zero-shot</b> generalization abilities for completely new tasks and rapidly surpasses expert performance levels with a minimal amount of demonstration data. These findings highlight the potential of DT as a <b>foundational</b> <b>controller</b> for general control applications.</p></p class="citation"></blockquote><h3 id=28--173258-distributionally-robust-policy-and-lyapunov-certificate-learning-kehan-long-et-al-2024>(2/8 | 173/258) Distributionally Robust Policy and Lyapunov-Certificate Learning (Kehan Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kehan Long, Jorge Cortes, Nikolay Atanasov. (2024)<br><strong>Distributionally Robust Policy and Lyapunov-Certificate Learning</strong><br><button class=copy-to-clipboard title="Distributionally Robust Policy and Lyapunov-Certificate Learning" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 40<br>Keywords: Out-of-distribution, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03017v1.pdf filename=2404.03017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents novel methods for synthesizing distributionally robust stabilizing neural controllers and certificates for control systems under model uncertainty. A key challenge in designing controllers with stability guarantees for uncertain systems is the accurate determination of and adaptation to shifts in model parametric uncertainty during online deployment. We tackle this with a novel distributionally robust formulation of the Lyapunov derivative chance constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid the computational complexity involved in dealing with the space of probability measures, we identify a sufficient condition in the form of deterministic convex constraints that ensures the Lyapunov derivative constraint is satisfied. We integrate this condition into a loss function for training a neural network-based controller and show that, for the resulting closed-loop system, the global asymptotic stability of its equilibrium can be certified with high confidence, even with <b>Out-of-Distribution</b> (OoD) model uncertainties. To demonstrate the efficacy and efficiency of the proposed methodology, we compare it with an uncertainty-agnostic baseline approach and several <b>reinforcement</b> <b>learning</b> approaches in two control problems in <b>simulation.</b></p></p class="citation"></blockquote><h3 id=38--174258-joint-optimization-on-uplink-ofdma-and-mu-mimo-for-ieee-80211ax-deep-hierarchical-reinforcement-learning-approach-hyeonho-noh-et-al-2024>(3/8 | 174/258) Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep Hierarchical Reinforcement Learning Approach (Hyeonho Noh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyeonho Noh, Harim Lee, Hyun Jong Yang. (2024)<br><strong>Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep Hierarchical Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep Hierarchical Reinforcement Learning Approach" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-IT, cs-SY, eess-SY, eess.SY, math-IT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02486v1.pdf filename=2404.02486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This letter tackles a joint user scheduling, frequency resource allocation (USRA), multi-input-multi-output mode selection (MIMO MS) between single-user MIMO and multi-user (MU) MIMO, and MU-MIMO user selection problem, integrating uplink orthogonal frequency division multiple access (OFDMA) in IEEE 802.11ax. Specifically, we focus on \textit{unsaturated traffic conditions} where users&rsquo; data demands fluctuate. In unsaturated traffic conditions, considering packet volumes per user introduces a combinatorial problem, requiring the simultaneous optimization of MU-MIMO user selection and RA along the time-frequency-space axis. Consequently, dealing with the combinatorial nature of this problem, characterized by a large cardinality of unknown variables, poses a challenge that conventional optimization methods find nearly impossible to address. In response, this letter proposes an approach with deep hierarchical <b>reinforcement</b> <b>learning</b> (DHRL) to solve the joint problem. Rather than simply adopting off-the-shelf DHRL, we \textit{tailor} the DHRL to the joint USRA and MS problem, thereby significantly improving the convergence speed and throughput. Extensive <b>simulation</b> results show that the proposed algorithm achieves significantly improved throughput compared to the existing schemes under various unsaturated traffic conditions.</p></p class="citation"></blockquote><h3 id=48--175258-powersimulationsjl----a-power-systems-operations-simulation-library-jose-daniel-lara-et-al-2024>(4/8 | 175/258) PowerSimulations.jl &ndash; A Power Systems operations simulation Library (Jose Daniel Lara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Daniel Lara, Clayton Barrows, Daniel Thom, Sourabh Dalvi, Duncan S. Callaway, Dheepak Krishnamurthy. (2024)<br><strong>PowerSimulations.jl &ndash; A Power Systems operations simulation Library</strong><br><button class=copy-to-clipboard title="PowerSimulations.jl -- A Power Systems operations simulation Library" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03074v1.pdf filename=2404.03074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PowerSimulations.jl is a Julia-based BSD-licensed power system operations <b>simulation</b> tool developed as a flexible and open source software for quasi-static power systems <b>simulations</b> including Production Cost Models. PowerSimulations.jl tackles the issues of developing a <b>simulation</b> model in a modular way providing tools for the formulation of decision models and emulation models that can be solved independently or in an interconnected fashion. This paper discusses the software implementation of PowerSimulations.jl as a template for the development and implementation of operation simulators, providing solutions to commonly encountered issues like time series read/write and results sharing between models. The paper includes a publicly-available validation of classical operations <b>simulations</b> as well as examples of the advanced features of the software.</p></p class="citation"></blockquote><h3 id=58--176258-network-aware-and-welfare-maximizing-dynamic-pricing-for-energy-sharing-ahmed-s-alahmed-et-al-2024>(5/8 | 176/258) Network-Aware and Welfare-Maximizing Dynamic Pricing for Energy Sharing (Ahmed S. Alahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed S. Alahmed, Guido Cavraro, Andrey Bernstein, Lang Tong. (2024)<br><strong>Network-Aware and Welfare-Maximizing Dynamic Pricing for Energy Sharing</strong><br><button class=copy-to-clipboard title="Network-Aware and Welfare-Maximizing Dynamic Pricing for Energy Sharing" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02458v1.pdf filename=2404.02458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of behind-the-meter (BTM) distributed energy resources (DER) within the electrical distribution network presents significant supply and demand flexibilities, but also introduces operational challenges such as voltage spikes and reverse power flows. In response, this paper proposes a network-aware dynamic pricing framework tailored for energy-sharing coalitions that aggregate small, but ubiquitous, BTM DER downstream of a distribution system operator&rsquo;s (DSO) revenue meter that adopts a generic net energy metering (NEM) tariff. By formulating a Stackelberg game between the energy-sharing market leader and its prosumers, we show that the dynamic pricing policy induces the prosumers toward a network-safe operation and decentrally maximizes the energy-sharing social welfare. The dynamic pricing mechanism involves a combination of a locational {\em ex-ante} dynamic price and an {\em ex-post} allocation, both of which are functions of the energy sharing&rsquo;s BTM DER. The {\em ex-post} allocation is proportionate to the price differential between the DSO NEM price and the energy sharing locational price. <b>Simulation</b> results using real DER data and the IEEE 13-bus test systems illustrate the dynamic nature of network-aware pricing at each bus, and its impact on voltage.</p></p class="citation"></blockquote><h3 id=68--177258-impact-and-integration-of-mini-photovoltaic-systems-on-electric-power-distribution-grids-gökhan-demirel-et-al-2024>(6/8 | 177/258) Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids (Gökhan Demirel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gökhan Demirel, Simon Grafenhorst, Kevin Förderer, Veit Hagenmeyer. (2024)<br><strong>Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids</strong><br><button class=copy-to-clipboard title="Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02763v1.pdf filename=2404.02763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work analyzes the impact of varying concentrations mini-photovoltaic (MPV) systems, often referred to as balcony power plants, on the stability and control of the low-voltage (LV) grid. By local energy use and potentially reversing meter operation, we focus on how these MPV systems transform grid dynamics and elucidate consumer participation in the energy transition. We scrutinize the effects of these systems on power quality, power loss, <b>transformer</b> loading, and the functioning of other inverter-based voltage-regulating distributed energy resources (DER). Owing to the rise in renewable output from MPVs, the emerging bidirectional energy flow poses challenges for distribution grids abundant with DERs. Our case studies, featuring sensitivity analysis and comparison of distributed and decentralized DER control strategies, highlight that autonomous inverters are essential for providing ancillary services. With the growing use of battery energy storage (BES) systems in LV grids for these services, the need for adaptable DER control strategies becomes increasingly evident.</p></p class="citation"></blockquote><h3 id=78--178258-learning-with-errors-based-dynamic-encryption-that-discloses-residue-signal-for-anomaly-detection-yeongjun-jang-et-al-2024>(7/8 | 178/258) Learning with errors based dynamic encryption that discloses residue signal for anomaly detection (Yeongjun Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeongjun Jang, Joowon Lee, Junsoo Kim, Hyungbo Shim. (2024)<br><strong>Learning with errors based dynamic encryption that discloses residue signal for anomaly detection</strong><br><button class=copy-to-clipboard title="Learning with errors based dynamic encryption that discloses residue signal for anomaly detection" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02574v1.pdf filename=2404.02574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> is a protocol that detects integrity attacks on control systems by comparing the residue signal with a threshold. Implementing <b>anomaly</b> <b>detection</b> on encrypted control systems has been a challenge because it is hard to detect an <b>anomaly</b> <b>from</b> the encrypted residue signal without the secret key. In this paper, we propose a dynamic encryption scheme for a linear system that automatically discloses the residue signal. The initial state and the input are encrypted based on the zero-dynamics of the system, so that the effect of encryption on the residue signal remains identically zero. The proposed scheme is shown to be secure in the sense that no other information than the residue signal is disclosed. Furthermore, we demonstrate a method of utilizing the disclosed residue signal to operate an observer-based controller over encrypted data for an infinite time horizon without re-encryption.</p></p class="citation"></blockquote><h3 id=88--179258-stabilizing-switched-nonlinear-systems-under-restricted-but-arbitrary-switching-signals-atreyee-kundu-2024>(8/8 | 179/258) Stabilizing switched nonlinear systems under restricted but arbitrary switching signals (Atreyee Kundu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atreyee Kundu. (2024)<br><strong>Stabilizing switched nonlinear systems under restricted but arbitrary switching signals</strong><br><button class=copy-to-clipboard title="Stabilizing switched nonlinear systems under restricted but arbitrary switching signals" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02596v1.pdf filename=2404.02596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with input/output-to-state stability (IOSS) of switched nonlinear systems whose switching signals obey pre-specified restrictions on admissible switches between the subsystems and admissible dwell times on the subsystems. We present sufficient conditions on the subsystems, admissible switches between them and admissible dwell times on them, such that a switched system generated under all switching signals obeying the given restrictions is IOSS. Multiple Lyapunov-like functions and <b>graph</b> theory are the key apparatuses for our analysis. A numerical example is presented to demonstrate our results.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--180258-jailbreakv-28k-a-benchmark-for-assessing-the-robustness-of-multimodal-large-language-models-against-jailbreak-attacks-weidi-luo-et-al-2024>(1/7 | 180/258) JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks (Weidi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao. (2024)<br><strong>JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks</strong><br><button class=copy-to-clipboard title="JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Automatic Speech Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03027v1.pdf filename=2404.03027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancements in <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), securing these models against malicious inputs while aligning them with human values has emerged as a critical challenge. In this paper, we investigate an important and unexplored question of whether techniques that successfully jailbreak <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can be equally effective in jailbreaking MLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering <b>benchmark</b> designed to assess the transferability of <b>LLM</b> jailbreak techniques to MLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed in this paper, we generate 20, 000 text-based jailbreak <b>prompts</b> using advanced jailbreak attacks on <b>LLMs,</b> alongside 8, 000 image-based jailbreak inputs from recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-source MLLMs reveals a notably high Attack Success Rate <b>(ASR)</b> for attacks transferred from <b>LLMs,</b> highlighting a critical vulnerability in MLLMs that stems from their text-processing capabilities. Our findings underscore the urgent need for future research to address alignment vulnerabilities in MLLMs from both textual and visual inputs.</p></p class="citation"></blockquote><h3 id=27--181258-exploring-backdoor-vulnerabilities-of-chat-models-yunzhuo-hao-et-al-2024>(2/7 | 181/258) Exploring Backdoor Vulnerabilities of Chat Models (Yunzhuo Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunzhuo Hao, Wenkai Yang, Yankai Lin. (2024)<br><strong>Exploring Backdoor Vulnerabilities of Chat Models</strong><br><button class=copy-to-clipboard title="Exploring Backdoor Vulnerabilities of Chat Models" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02406v1.pdf filename=2404.02406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent researches have shown that <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on <b>LLMs</b> predominantly focus on instruction-tuned <b>LLMs,</b> while neglecting another realistic scenario where <b>LLMs</b> are <b>fine-tuned</b> on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% <b>ASR</b> on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor can not be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic content.</p></p class="citation"></blockquote><h3 id=37--182258-differentially-private-verification-of-survey-weighted-estimates-tong-lin-et-al-2024>(3/7 | 182/258) Differentially Private Verification of Survey-Weighted Estimates (Tong Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Lin, Jerome P. Reiter. (2024)<br><strong>Differentially Private Verification of Survey-Weighted Estimates</strong><br><button class=copy-to-clipboard title="Differentially Private Verification of Survey-Weighted Estimates" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, stat-ME<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02519v1.pdf filename=2404.02519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several official statistics agencies release synthetic data as public use microdata files. In practice, synthetic data do not admit accurate results for every analysis. Thus, it is beneficial for agencies to provide users with feedback on the quality of their analyses of the synthetic data. One approach is to couple synthetic data with a verification server that provides users with measures of the similarity of estimates computed with the synthetic and underlying confidential data. However, such measures leak information about the confidential records, so that agencies may wish to apply disclosure control methods to the released verification measures. We present a verification measure that satisfies <b>differential</b> <b>privacy</b> and can be used when the underlying confidential are collected with a complex survey design. We illustrate the verification measure using repeated sampling <b>simulations</b> where the confidential data are sampled with a probability proportional to size design, and the analyst estimates a population total or mean with the synthetic data. The <b>simulations</b> suggest that the verification measures can provide useful information about the quality of synthetic data inferences.</p></p class="citation"></blockquote><h3 id=47--183258-vocabulary-attack-to-hijack-large-language-model-applications-patrick-levi-et-al-2024>(4/7 | 183/258) Vocabulary Attack to Hijack Large Language Model Applications (Patrick Levi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Levi, Christoph P. Neumann. (2024)<br><strong>Vocabulary Attack to Hijack Large Language Model Applications</strong><br><button class=copy-to-clipboard title="Vocabulary Attack to Hijack Large Language Model Applications" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-DC, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02637v1.pdf filename=2404.02637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fast advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the <b>LLM</b> by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another <b>LLM</b> (attacker <b>LLM).</b> We prove our approach by goal hijacking two popular open-source <b>LLMs</b> from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single word insertion is sufficient. Second, we demonstrate that we can conduct our attack using a different model than the target model to conduct our attack with.</p></p class="citation"></blockquote><h3 id=57--184258-novel_authentication_protocols_tailored_for_ambient_iot_devices_in_3gpp_5g_networks-xiongpeng-ren-et-al-2024>(5/7 | 184/258) Novel_Authentication_Protocols_Tailored_for_Ambient_IoT_Devices_in_3GPP_5G_Networks (Xiongpeng Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiongpeng Ren, Jin Cao, Hui Li, Yinghui Zhang. (2024)<br><strong>Novel_Authentication_Protocols_Tailored_for_Ambient_IoT_Devices_in_3GPP_5G_Networks</strong><br><button class=copy-to-clipboard title=Novel_Authentication_Protocols_Tailored_for_Ambient_IoT_Devices_in_3GPP_5G_Networks index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02425v1.pdf filename=2404.02425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AIoT devices have attracted significant attention within the 3GPP organization. These devices, distinguished from conventional IoT devices, do not rely on additional batteries or have extremely small battery capacities, offering features such as low cost, easy deployment, and maintenance-free operation. Authentication and secure transmission are fundamental security requirements for AIoT devices. However, existing standard security mechanisms are not specifically designed for AIoT devices due to their complex key hierarchies and multi-round interactions, making them unsuitable. Besides, AIoT devices would have more various communication topologies. Therefore, we propose dedicated ultra-lightweight access authentication protocols based on various technologies and algorithms to serve as a forward-looking reference for future research and standardization. Analysis and <b>simulation</b> experiments using chips that closely resemble real AIoT devices, demonstrate that the existing standard protocols are indeed not suitable for such devices, and our protocols outperform existing standard protocols in terms of computational time and energy consumption. After the successful execution of proposed protocols, they can achieve secure transmission of application data, striking a balance between performance and security.</p></p class="citation"></blockquote><h3 id=67--185258-lightfat-mitigating-control-flow-explosion-via-lightweight-pmu-based-control-flow-attestation-jeferson-gonzalez-gomez-et-al-2024>(6/7 | 185/258) LightFAt: Mitigating Control-flow Explosion via Lightweight PMU-based Control-flow Attestation (Jeferson Gonzalez-Gomez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeferson Gonzalez-Gomez, Hassan Nassar, Lars Bauer, Jorg Henkel. (2024)<br><strong>LightFAt: Mitigating Control-flow Explosion via Lightweight PMU-based Control-flow Attestation</strong><br><button class=copy-to-clipboard title="LightFAt: Mitigating Control-flow Explosion via Lightweight PMU-based Control-flow Attestation" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02608v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02608v2.pdf filename=2404.02608v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the continuous evolution of computational devices, more and more applications are being executed remotely. The applications operate on a wide spectrum of devices, ranging from IoT nodes with low computational capabilities to large cloud providers with high capabilities. Remote execution often deals with sensitive data or executes proprietary software. Hence, the challenge of ensuring that the code execution will not be compromised rises. Remote Attestation deals with this challenge. It ensures the code is executed in a non-compromised environment by calculating a potentially large sequence of cryptographic hash values. Each hash calculation is computationally intensive and over a large sequence the overhead becomes extremely high. In this work, we propose LightFAt: a Lightweight Control Flow Attestation scheme. Instead of relying on the expensive cryptographic hash calculation, LightFAt leverages the readings from the processor&rsquo;s Performance Monitor Unit (PMU) in conjunction with a lightweight <b>unsupervised</b> machine learning (ML) classifier to detect whether a target application&rsquo;s control flow is compromised, hence improving the system&rsquo;s security. On the verifier&rsquo;s side, LightFAt reaches a detection accuracy of over 95%, with low false-negative and false-positive rates.</p></p class="citation"></blockquote><h3 id=77--186258-designing-a-photonic-physically-unclonable-function-having-resilience-to-machine-learning-attacks-elena-r-henderson-et-al-2024>(7/7 | 186/258) Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks (Elena R. Henderson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elena R. Henderson, Jessie M. Henderson, Hiva Shahoei, William V. Oxford, Eric C. Larson, Duncan L. MacFarlane, Mitchell A. Thornton. (2024)<br><strong>Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks</strong><br><button class=copy-to-clipboard title="Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, physics-optics<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02440v1.pdf filename=2404.02440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physically unclonable functions (PUFs) are designed to act as device &lsquo;fingerprints.&rsquo; Given an input challenge, the PUF circuit should produce an unpredictable response for use in situations such as root-of-trust applications and other hardware-level cybersecurity applications. PUFs are typically subcircuits present within integrated circuits (ICs), and while conventional IC PUFs are well-understood, several implementations have proven vulnerable to malicious exploits, including those perpetrated by machine learning (ML)-based attacks. Such attacks can be difficult to prevent because they are often designed to work even when relatively few challenge-response pairs are known in advance. Hence the need for both more resilient PUF designs and analysis of ML-attack susceptibility. Previous work has developed a PUF for photonic integrated circuits (PICs). A PIC PUF not only produces unpredictable responses given manufacturing-introduced tolerances, but is also less prone to electromagnetic radiation eavesdropping attacks than a purely electronic IC PUF. In this work, we analyze the resilience of the proposed photonic PUF when subjected to ML-based attacks. Specifically, we describe a computational PUF model for producing the large datasets required for training ML attacks; we analyze the quality of the model; and we discuss the modeled PUF&rsquo;s susceptibility to ML-based attacks. We find that the modeled PUF generates distributions that resemble uniform white noise, explaining the exhibited resilience to neural-network-based attacks designed to exploit latent relationships between challenges and responses. Preliminary analysis suggests that the PUF exhibits similar resilience to <b>generative</b> <b>adversarial</b> <b>networks,</b> and continued development will show whether more-sophisticated ML approaches better compromise the PUF and &ndash; if so &ndash; how design modifications might improve resilience.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=16--187258-a-neuroergonomics-model-to-evaluating-nuclear-power-plants-operators-performance-under-heat-stress-driven-by-ecg-time-frequency-spectrums-and-fnirs-prefrontal-cortex-network-a-cnn-gat-fusion-model-yan-zhang-et-al-2024>(1/6 | 187/258) A neuroergonomics model to evaluating nuclear power plants operators&rsquo; performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model (Yan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Zhang, Ming Jia, Meng Li, JianYu Wang, XiangMin Hu, ZhiHui Xu, Tao Chen. (2024)<br><strong>A neuroergonomics model to evaluating nuclear power plants operators&rsquo; performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model</strong><br><button class=copy-to-clipboard title="A neuroergonomics model to evaluating nuclear power plants operators' performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02439v1.pdf filename=2404.02439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operators experience complicated physiological and psychological states when exposed to extreme heat stress, which can impair cognitive function and decrease performance significantly, ultimately leading to severe secondary disasters. Therefore, there is an urgent need for a feasible technique to identify their abnormal states to enhance the reliability of human-cybernetics systems. With the advancement of deep learning in physiological modeling, a model for evaluating operators&rsquo; performance driven by electrocardiogram (ECG) and functional near-infrared spectroscopy (fNIRS) was proposed, demonstrating high ecological validity. The model fused a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> backbone and a <b>graph</b> attention network <b>(GAT)</b> backbone to extract discriminative features from ECG time-frequency spectrums and fNIRS prefrontal cortex (PFC) network respectively with deeper neuroscience domain knowledge, and eventually achieved 0.90 AUC. Results supported that handcrafted features extracted by specialized neuroscience methods can alleviate overfitting. Inspired by the small-world nature of the brain network, the fNIRS PFC network was organized as an undirected <b>graph</b> and embedded by <b>GAT.</b> It is proven to perform better in information aggregation and delivery compared to a simple non-linear transformation. The model provides a potential neuroergonomics application for evaluating the human state in vital human-cybernetics systems under industry 5.0 scenarios.</p></p class="citation"></blockquote><h3 id=26--188258-unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm-zhe-liu-et-al-2024>(2/6 | 188/258) Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM (Zhe Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Yuekai Huang, Jun Hu, Qing Wang. (2024)<br><strong>Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM</strong><br><button class=copy-to-clipboard title="Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: BLEU, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02706v1.pdf filename=2404.02706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mobile apps have become indispensable for accessing and participating in various environments, especially for low-vision users. Users with visual impairments can use screen readers to read the content of each screen and understand the content that needs to be operated. Screen readers need to read the hint-text attribute in the text input component to remind visually impaired users what to fill in. Unfortunately, based on our analysis of 4,501 Android apps with text inputs, over 0.76 of them are missing hint-text. These issues are mostly caused by developers&rsquo; lack of awareness when considering visually impaired individuals. To overcome these challenges, we developed an <b>LLM-based</b> hint-text generation model called HintDroid, which analyzes the GUI information of input components and uses <b>in-context</b> <b>learning</b> to generate the hint-text. To ensure the quality of hint-text generation, we further designed a feedback-based inspection mechanism to further adjust hint-text. The automated experiments demonstrate the high <b>BLEU</b> and a user study further confirms its usefulness. HintDroid can not only help visually impaired individuals, but also help ordinary people understand the requirements of input components. HintDroid demo video: <a href=https://youtu.be/FWgfcctRbfI>https://youtu.be/FWgfcctRbfI</a>.</p></p class="citation"></blockquote><h3 id=36--189258-writing-with-ai-lowers-psychological-ownership-but-longer-prompts-can-help-nikhita-joshi-et-al-2024>(3/6 | 189/258) Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help (Nikhita Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhita Joshi, Daniel Vogel. (2024)<br><strong>Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help</strong><br><button class=copy-to-clipboard title="Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03108v1.pdf filename=2404.03108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feelings of something belonging to someone is called &ldquo;psychological ownership.&rdquo; A common assumption is that writing with <b>generative</b> <b>AI</b> lowers psychological ownership, but the extent to which this occurs and the role of <b>prompt</b> length are unclear. We report on two experiments to better understand the relationship between psychological ownership and <b>prompt</b> length. Participants wrote short stories either completely by themselves or wrote <b>prompts</b> of varying lengths, enforced through word limits. Results show that when participants wrote longer <b>prompts,</b> they had higher levels of psychological ownership. Their comments suggest they felt encouraged to think more about their <b>prompts</b> and include more details about the story plot. However, these benefits plateaued when the <b>prompt</b> length was 75-100% of the target story length. Based on these results, we propose <b>prompt</b> entry interface designs that nudge users with soft and hard constraints to write longer <b>prompts</b> for increased psychological ownership.</p></p class="citation"></blockquote><h3 id=46--190258-evolving-agents-interactive-simulation-of-dynamic-and-diverse-human-personalities-jiale-li-et-al-2024>(4/6 | 190/258) Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities (Jiale Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiale Li, Jiayang Li, Jiahao Chen, Yifan Li, Shijie Wang, Hugo Zhou, Minjun Ye, Yunsheng Su. (2024)<br><strong>Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities</strong><br><button class=copy-to-clipboard title="Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02718v1.pdf filename=2404.02718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-like Agents with diverse and dynamic personality could serve as an important design probe in the process of user-centered design, thereby enabling designers to enhance the user experience of interactive application.In this article, we introduce Evolving Agents, a novel agent architecture that consists of two systems: Personality and Behavior. The Personality system includes three modules: Cognition, Emotion and Character Growth. The Behavior system comprises two modules: Planning and Action. We also build a <b>simulation</b> platform that enables agents to interact with the environment and other agents. Evolving Agents can simulate the human personality evolution process. Compared to its initial state, agents&rsquo; personality and behavior patterns undergo believable development after several days of <b>simulation.</b> Agents reflect on their behavior to reason and develop new personality traits. These traits, in turn, generate new behavior patterns, forming a feedback loop-like personality evolution.In our experiment, we utilized <b>simulation</b> platform with 10 agents for evaluation. During the evaluation, these agents experienced believable and inspirational personality evolution. Through ablation and control experiments, we demonstrated the outstanding effectiveness of agent personality evolution and all modules of our agent architecture contribute to creating believable human-like agents with diverse and dynamic personalities. We also demonstrated through workshops how Evolving Agents could inspire designers.</p></p class="citation"></blockquote><h3 id=56--191258-promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts-tian-huang-et-al-2024>(5/6 | 191/258) PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts (Tian Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Huang, Chun Yu, Weinan Shi, Zijian Peng, David Yang, Weiqi Sun, Yuanchun Shi. (2024)<br><strong>PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts</strong><br><button class=copy-to-clipboard title="PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02475v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02475v1.pdf filename=2404.02475v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic Process Automation (RPA) offers a valuable solution for efficiently automating tasks on the graphical user interface (GUI), by emulating human interactions, without modifying existing code. However, its broader adoption is constrained by the need for expertise in both scripting languages and workflow design. To address this challenge, we present PromptRPA, a system designed to comprehend various task-related textual <b>prompts</b> (e.g., goals, procedures), thereby generating and performing corresponding RPA tasks. PromptRPA incorporates a suite of intelligent agents that mimic human cognitive functions, specializing in interpreting user intent, managing external information for RPA generation, and executing operations on smartphones. The agents can learn from user feedback and continuously improve their performance based on the accumulated knowledge. Experimental results indicated a performance jump from a 22.28% success rate in the baseline to 95.21% with PromptRPA, requiring an average of 1.66 user interventions for each new task. PromptRPA presents promising applications in fields such as tutorial creation, smart assistance, and customer service.</p></p class="citation"></blockquote><h3 id=66--192258-a-unified-editing-method-for-co-speech-gesture-generation-via-diffusion-inversion-zeyu-zhao-et-al-2024>(6/6 | 192/258) A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion (Zeyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Zhao, Nan Gao, Zhi Zeng, Guixuan Zhang, Jie Liu, Shuwu Zhang. (2024)<br><strong>A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion</strong><br><button class=copy-to-clipboard title="A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02411v1.pdf filename=2404.02411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have shown great success in generating high-quality co-speech gestures for interactive humanoid robots or digital avatars from noisy input with the speech audio or text as conditions. However, they rarely focus on providing rich editing capabilities for content creators other than high-level specialized measures like style conditioning. To resolve this, we propose a unified framework utilizing <b>diffusion</b> <b>inversion</b> that enables multi-level editing capabilities for co-speech gesture generation without re-training. The method takes advantage of two key capabilities of invertible <b>diffusion</b> <b>models.</b> The first is that through inversion, we can reconstruct the intermediate noise from gestures and regenerate new gestures from the noise. This can be used to obtain gestures with high-level similarities to the original gestures for different speech conditions. The second is that this reconstruction reduces activation caching requirements during gradient calculation, making the direct optimization on input noises possible on current hardware with limited memory. With different loss functions designed for, e.g., joint rotation or velocity, we can control various low-level details by automatically tweaking the input noises through optimization. Extensive experiments on multiple use cases show that this framework succeeds in unifying high-level and low-level co-speech gesture editing.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--193258-a-universal-deep-neural-network-for-signal-detection-in-wireless-communication-systems-khalid-albagami-et-al-2024>(1/3 | 193/258) A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems (Khalid Albagami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khalid Albagami, Nguyen Van Huynh, Geoffrey Ye Li. (2024)<br><strong>A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems</strong><br><button class=copy-to-clipboard title="A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-IT, cs-NI, cs.NI, math-IT<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02648v1.pdf filename=2404.02648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, deep learning (DL) has been emerging as a promising approach for channel estimation and signal detection in wireless communications. The majority of the existing studies investigating the use of DL techniques in this domain focus on analysing channel impulse responses that are generated from only one channel distribution such as additive white Gaussian channel noise and Rayleigh channels. In practice, to cope with the dynamic nature of the wireless channel, DL methods must be re-trained on newly non-aged collected data which is costly, inefficient, and impractical. To tackle this challenge, this paper proposes a novel universal deep neural network (Uni-DNN) that can achieve high detection performance in various wireless environments without retraining the model. In particular, our proposed Uni-DNN model consists of a wireless channel classifier and a signal detector which are constructed by using DNNs. The wireless channel classifier enables the signal detector to generalise and perform optimally for multiple wireless channel distributions. In addition, to further improve the signal detection performance of the proposed model, <b>convolutional</b> <b>neural</b> <b>network</b> is employed. Extensive <b>simulations</b> using the orthogonal frequency division multiplexing scheme demonstrate that the bit error rate performance of our proposed solution can outperform conventional DL-based approaches as well as least square and minimum mean square error channel estimators in practical low pilot density scenarios.</p></p class="citation"></blockquote><h3 id=23--194258-exploring-opportunistic-routing-for-remote-sea-emergencies-cleon-liew-et-al-2024>(2/3 | 194/258) Exploring Opportunistic Routing for Remote Sea Emergencies (Cleon Liew et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cleon Liew, Milena Radenkovic. (2024)<br><strong>Exploring Opportunistic Routing for Remote Sea Emergencies</strong><br><button class=copy-to-clipboard title="Exploring Opportunistic Routing for Remote Sea Emergencies" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03013v1.pdf filename=2404.03013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the Opportunistic Routing Protocols in the context of remote sea emergency scenarios, using the MH370 plane crash as a case study (OppNetMH370). We studied the likelihood of successful transmissions of emergency messages to response services where communication methods are inadequate in remote sea areas. The study focuses on two opportunistic routing protocols, where their performances are evaluated based on key metrics including average latency and delivery probability. Our study reveals the challenges associated with the current communication technology in remote areas and proposes potential enhancements for future <b>simulations.</b> The findings contribute to understanding the limitations of existing communication strategies in remote sea areas and offers insights on the future development and improvements to the routing protocols.</p></p class="citation"></blockquote><h3 id=33--195258-when-digital-twin-meets-generative-ai-intelligent-closed-loop-network-management-xinyu-huang-et-al-2024>(3/3 | 195/258) When Digital Twin Meets Generative AI: Intelligent Closed-Loop Network Management (Xinyu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Huang, Haojun Yang, Conghao Zhou, Xuemin Shen, Weihua Zhuang. (2024)<br><strong>When Digital Twin Meets Generative AI: Intelligent Closed-Loop Network Management</strong><br><button class=copy-to-clipboard title="When Digital Twin Meets Generative AI: Intelligent Closed-Loop Network Management" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03025v1.pdf filename=2404.03025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>artificial</b> intelligence (GAI) and digital twin (DT) are advanced data processing and virtualization technologies to revolutionize communication networks. Thanks to the powerful data processing capabilities of GAI, integrating it into DT is a potential approach to construct an intelligent holistic virtualized network for better network management performance. To this end, we propose a GAI-driven DT (GDT) network architecture to enable intelligent closed-loop network management. In the architecture, various GAI models can empower DT status emulation, feature abstraction, and network decision-making. The interaction between GAI-based and model-based data processing can facilitate intelligent external and internal closed-loop network management. To further enhance network management performance, three potential approaches are proposed, i.e., model light-weighting, adaptive model selection, and data-model-driven network management. We present a case study pertaining to data-model-driven network management for the GDT network, followed by some open research issues.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--196258-improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation-yizhu-liu-et-al-2024>(1/5 | 196/258) Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation (Yizhu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhu Liu, Ran Tao, Shengyu Guo, Yifan Yang. (2024)<br><strong>Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation</strong><br><button class=copy-to-clipboard title="Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Data Augmentation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02616v1.pdf filename=2404.02616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topic relevance between query and document is a very important part of social search, which can evaluate the degree of matching between document and user&rsquo;s requirement. In most social search scenarios such as Dianping, modeling search relevance always faces two challenges. One is that many documents in social search are very long and have much redundant information. The other is that the training <b>data</b> <b>for</b> search relevance model is difficult to get, especially for multi-classification relevance model. To tackle above two problems, we first take query concatenated with the query-based summary and the document summary without query as the input of topic relevance model, which can help model learn the relevance degree between query and the core topic of document. Then, we utilize the language understanding and generation abilities of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to rewrite and generate query from queries and documents in existing training <b>data,</b> <b>which</b> can construct new query-document pairs as training <b>data.</b> <b>Extensive</b> offline experiments and online A/B tests show that the proposed approaches effectively improve the performance of relevance modeling.</p></p class="citation"></blockquote><h3 id=25--197258-duqgen-effective-unsupervised-domain-adaptation-of-neural-rankers-by-diversifying-synthetic-query-generation-ramraj-chandradevan-et-al-2024>(2/5 | 197/258) DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation (Ramraj Chandradevan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramraj Chandradevan, Kaustubh D. Dhole, Eugene Agichtein. (2024)<br><strong>DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation</strong><br><button class=copy-to-clipboard title="DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Unsupervised Learning, Zero-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02489v1.pdf filename=2404.02489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without <b>domain</b> <b>adaptation,</b> also called <b>zero-shot.</b> However, <b>zero-shot</b> neural ranking may be sub-optimal, as it does not take advantage of the target <b>domain</b> <b>information.</b> Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to <b>unsupervised</b> <b>domain</b> <b>adaptation</b> for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new <b>domain.</b> <b>Specifically,</b> DUQGen produces a more effective representation of the target <b>domain</b> <b>by</b> identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all <b>zero-shot</b> baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method&rsquo;s performance and to identify promising areas for further improvements.</p></p class="citation"></blockquote><h3 id=35--198258-efficient-multi-vector-dense-retrieval-using-bit-vectors-franco-maria-nardini-et-al-2024>(3/5 | 198/258) Efficient Multi-Vector Dense Retrieval Using Bit Vectors (Franco Maria Nardini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Franco Maria Nardini, Cosimo Rulli, Rossano Venturini. (2024)<br><strong>Efficient Multi-Vector Dense Retrieval Using Bit Vectors</strong><br><button class=copy-to-clipboard title="Efficient Multi-Vector Dense Retrieval Using Bit Vectors" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Dense Retrieval, Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02805v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02805v1.pdf filename=2404.02805v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dense</b> <b>retrieval</b> techniques employ pre-trained <b>large</b> <b>language</b> <b>models</b> to build a high-dimensional representation of queries and passages. These representations compute the relevance of a passage w.r.t. to a query using efficient similarity measures. In this line, multi-vector representations show improved effectiveness at the expense of a one-order-of-magnitude increase in memory footprint and query latency by encoding queries and documents on a per-token level. Recently, PLAID has tackled these problems by introducing a centroid-based term representation to reduce the memory impact of multi-vector systems. By exploiting a centroid interaction mechanism, PLAID filters out non-relevant documents, thus reducing the cost of the successive ranking stages. This paper proposes ``Efficient Multi-Vector <b>dense</b> <b>retrieval</b> with Bit vectors&rsquo;&rsquo; (EMVB), a novel framework for efficient query processing in multi-vector <b>dense</b> <b>retrieval.</b> First, EMVB employs a highly efficient pre-filtering step of passages using optimized bit vectors. Second, the computation of the centroid interaction happens column-wise, exploiting SIMD instructions, thus reducing its latency. Third, EMVB leverages Product <b>Quantization</b> (PQ) to reduce the memory footprint of storing vector representations while jointly allowing for fast late interaction. Fourth, we introduce a per-document term filtering method that further improves the efficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVB is up to 2.8x faster while reducing the memory footprint by 1.8x with no loss in retrieval accuracy compared to PLAID.</p></p class="citation"></blockquote><h3 id=45--199258-the-surprising-effectiveness-of-rankers-trained-on-expanded-queries-abhijit-anand-et-al-2024>(4/5 | 199/258) The Surprising Effectiveness of Rankers Trained on Expanded Queries (Abhijit Anand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand. (2024)<br><strong>The Surprising Effectiveness of Rankers Trained on Expanded Queries</strong><br><button class=copy-to-clipboard title="The Surprising Effectiveness of Rankers Trained on Expanded Queries" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Document Ranking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02587v1.pdf filename=2404.02587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do <b>LLM</b> based query enrichment for training queries using relevant <b>documents.</b> <b>Next,</b> a specialized ranker is <b>fine-tuned</b> only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a principled query performance based scoring method using base and specialized ranker offers a significant improvement of up to 25% on the passage ranking task and up to 48.4% on the <b>document</b> <b>ranking</b> task when compared to the baseline performance of using original queries, even outperforming SOTA model.</p></p class="citation"></blockquote><h3 id=55--200258-unbiased-learning-to-rank-meets-reality-lessons-from-baidus-large-scale-search-dataset-philipp-hager-et-al-2024>(5/5 | 200/258) Unbiased Learning to Rank Meets Reality: Lessons from Baidu&rsquo;s Large-Scale Search Dataset (Philipp Hager et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Hager, Romain Deffayet, Jean-Michel Renders, Onno Zoeter, Maarten de Rijke. (2024)<br><strong>Unbiased Learning to Rank Meets Reality: Lessons from Baidu&rsquo;s Large-Scale Search Dataset</strong><br><button class=copy-to-clipboard title="Unbiased Learning to Rank Meets Reality: Lessons from Baidu's Large-Scale Search Dataset" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02543v1.pdf filename=2404.02543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unbiased learning-to-rank (ULTR) is a well-established framework for learning from user clicks, which are often biased by the ranker collecting the data. While theoretically justified and extensively tested in <b>simulation,</b> ULTR techniques lack empirical validation, especially on modern search engines. The dataset released for the WSDM Cup 2023, collected from Baidu&rsquo;s search engine, offers a rare opportunity to assess the real-world performance of prominent ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed improvements stem from applying ULTR or other learning techniques. We revisit and extend the available experiments. We find that unbiased learning-to-rank techniques do not bring clear performance improvements, especially compared to the stark differences brought by the choice of ranking loss and query-document features. Our experiments reveal that ULTR robustly improves click prediction. However, these gains in click prediction do not translate to enhanced ranking performance on expert relevance annotations, implying that conclusions strongly depend on how success is measured in this <b>benchmark.</b></p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--201258-nl2kql-from-natural-language-to-kusto-query-amir-h-abdi-et-al-2024>(1/1 | 201/258) NL2KQL: From Natural Language to Kusto Query (Amir H. Abdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir H. Abdi, Xinye Tang, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing. (2024)<br><strong>NL2KQL: From Natural Language to Kusto Query</strong><br><button class=copy-to-clipboard title="NL2KQL: From Natural Language to Kusto Query" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-CL, cs-DB, cs.DB<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Few-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02933v1.pdf filename=2404.02933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data is growing rapidly in volume and complexity. Proficiency in database query languages is pivotal for crafting effective queries. As coding assistants become more prevalent, there is significant opportunity to enhance database query languages. The Kusto Query Language (KQL) is a widely used query language for <b>large</b> <b>semi-structured</b> <b>data</b> such as logs, telemetries, and time-series for big data analytics platforms. This paper introduces NL2KQL an innovative framework that uses <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to convert natural language queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several key components: Schema Refiner which narrows down the schema to its most pertinent elements; the <b>Few-shot</b> Selector which dynamically selects relevant examples from a <b>few-shot</b> dataset; and the Query Refiner which repairs syntactic and semantic errors in KQL queries. Additionally, this study outlines a method for generating <b>large</b> <b>datasets</b> <b>of</b> synthetic NLQ-KQL pairs which are valid within a specific database contexts. To validate NL2KQL&rsquo;s performance, we utilize an array of online (based on query execution) and offline (based on query parsing) metrics. Through ablation studies, the significance of each framework component is examined, and the datasets used for <b>benchmarking</b> are made publicly available. This work is the first of its kind and is compared with available baselines to demonstrate its effectiveness.</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--202258-meshbrush-painting-the-anatomical-mesh-with-neural-stylization-for-endoscopy-john-j-han-et-al-2024>(1/4 | 202/258) MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy (John J. Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John J. Han, Ayberk Acar, Nicholas Kavoussi, Jie Ying Wu. (2024)<br><strong>MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy</strong><br><button class=copy-to-clipboard title="MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 35<br>Keywords: Geometry, Simulation, Simulator, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02999v1.pdf filename=2404.02999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Style</b> <b>transfer</b> is a promising approach to close the sim-to-real gap in medical endoscopy. Rendering realistic endoscopic videos by traversing pre-operative scans (such as MRI or CT) can generate realistic <b>simulations</b> as well as ground truth camera poses and depth maps. Although image-to-image (I2I) translation models such as CycleGAN perform well, they are unsuitable for video-to-video synthesis due to the lack of temporal consistency, resulting in artifacts between frames. We propose MeshBrush, a neural mesh stylization method to synthesize temporally consistent videos with differentiable rendering. MeshBrush uses the underlying <b>geometry</b> of patient imaging data while leveraging existing I2I methods. With learned per-vertex textures, the stylized mesh guarantees consistency while producing high-fidelity outputs. We demonstrate that mesh stylization is a promising approach for creating realistic <b>simulations</b> for downstream tasks such as training and preoperative planning. Although our method is tested and designed for ureteroscopy, its components are transferable to general endoscopic and laparoscopic procedures.</p></p class="citation"></blockquote><h3 id=24--203258-event-camera-demosaicing-via-swin-transformer-and-pixel-focus-loss-yunfan-lu-et-al-2024>(2/4 | 203/258) Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss (Yunfan Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunfan Lu, Yijie Xu, Wenzong Ma, Weiyu Guo, Hui Xiong. (2024)<br><strong>Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss</strong><br><button class=copy-to-clipboard title="Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-MM, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02731v1.pdf filename=2404.02731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research has highlighted improvements in high-quality imaging guided by event cameras, with most of these efforts concentrating on the RGB domain. However, these advancements frequently neglect the unique challenges introduced by the inherent flaws in the sensor design of event cameras in the RAW domain. Specifically, this sensor design results in the partial loss of pixel values, posing new challenges for RAW domain processes like demosaicing. The challenge intensifies as most research in the RAW domain is based on the premise that each pixel contains a value, making the straightforward adaptation of these methods to event camera demosaicing problematic. To end this, we present a Swin-Transformer-based backbone and a pixel-focus loss function for demosaicing with missing pixel values in RAW domain processing. Our core motivation is to refine a general and widely applicable foundational model from the RGB domain for RAW domain processing, thereby broadening the model&rsquo;s applicability within the entire imaging process. Our method harnesses multi-scale processing and space-to-depth techniques to ensure efficiency and reduce computing complexity. We also proposed the Pixel-focus Loss function for network <b>fine-tuning</b> to improve network convergence based on our discovery of a long-tailed distribution in training loss. Our method has undergone validation on the MIPI Demosaic Challenge dataset, with subsequent analytical experimentation confirming its efficacy. All code and trained models are released here: <a href=https://github.com/yunfanLu/ev-demosaic>https://github.com/yunfanLu/ev-demosaic</a></p></p class="citation"></blockquote><h3 id=34--204258-vestibular-schwannoma-growth-prediction-from-longitudinal-mri-by-time-conditioned-neural-fields-yunjie-chen-et-al-2024>(3/4 | 204/258) Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields (Yunjie Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunjie Chen, Jelmer M. Wolterink, Olaf M. Neve, Stephan R. Romeijn, Berit M. Verbist, Erik F. Hensen, Qian Tao, Marius Staring. (2024)<br><strong>Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields</strong><br><button class=copy-to-clipboard title="Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02614v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02614v2.pdf filename=2404.02614v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination. To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable. In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and <b>recurrent</b> <b>neural</b> <b>networks</b> for prospective tumor growth prediction. In the proposed method, each tumor is represented as a signed distance function (SDF) conditioned on a low-dimensional latent code. Unlike previous studies that perform tumor shape prediction directly in the image space, we predict the latent codes instead and then reconstruct future shapes from it. To deal with irregular time intervals, we introduce a time-conditioned <b>recurrent</b> <b>module</b> <b>based</b> on a ConvLSTM and a novel temporal encoding strategy, which enables the proposed model to output varying tumor shapes over time. The experiments on an in-house longitudinal VS dataset showed that the proposed model significantly improved the performance ($\ge 1.6%$ Dice score and $\ge0.20$ mm 95% Hausdorff distance), in particular for top 20% tumors that grow or shrink the most ($\ge 4.6%$ Dice score and $\ge 0.73$ mm 95% Hausdorff distance). Our code is available at ~\burl{https://github.com/cyjdswx/DeepGrowth}</p></p class="citation"></blockquote><h3 id=44--205258-cohort-individual-cooperative-learning-for-multimodal-cancer-survival-analysis-huajun-zhou-et-al-2024>(4/4 | 205/258) Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis (Huajun Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huajun Zhou, Fengtao Zhou, Hao Chen. (2024)<br><strong>Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis</strong><br><button class=copy-to-clipboard title="Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02394v1.pdf filename=2404.02394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, we have witnessed impressive achievements in cancer survival analysis by integrating <b>multimodal</b> data, e.g., pathology images and genomic profiles. However, the heterogeneity and high dimensionality of these modalities pose significant challenges for extracting discriminative representations while maintaining good generalization. In this paper, we propose a Cohort-individual Cooperative Learning (CCL) framework to advance cancer survival analysis by collaborating knowledge decomposition and cohort guidance. Specifically, first, we propose a <b>Multimodal</b> Knowledge Decomposition (MKD) module to explicitly decompose <b>multimodal</b> knowledge into four distinct components: redundancy, synergy and uniqueness of the two modalities. Such a comprehensive decomposition can enlighten the models to perceive easily overlooked yet important information, facilitating an effective <b>multimodal</b> fusion. Second, we propose a Cohort Guidance Modeling (CGM) to mitigate the risk of overfitting task-irrelevant information. It can promote a more comprehensive and robust understanding of the underlying <b>multimodal</b> data, while avoiding the pitfalls of overfitting and enhancing the generalization ability of the model. By cooperating the knowledge decomposition and cohort guidance methods, we develop a robust <b>multimodal</b> survival analysis model with enhanced discrimination and generalization abilities. Extensive experimental results on five cancer datasets demonstrate the effectiveness of our model in integrating <b>multimodal</b> data for survival analysis.</p></p class="citation"></blockquote><h2 id=csro-10>cs.RO (10)</h2><h3 id=110--206258-learning-quadrupedal-locomotion-via-differentiable-simulation-clemens-schwarke-et-al-2024>(1/10 | 206/258) Learning Quadrupedal Locomotion via Differentiable Simulation (Clemens Schwarke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clemens Schwarke, Victor Klemm, Jesus Tordesillas, Jean-Pierre Sleiman, Marco Hutter. (2024)<br><strong>Learning Quadrupedal Locomotion via Differentiable Simulation</strong><br><button class=copy-to-clipboard title="Learning Quadrupedal Locomotion via Differentiable Simulation" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02887v1.pdf filename=2404.02887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of differentiable simulators enabling analytic gradient computation has motivated a new wave of learning algorithms that hold the potential to significantly increase sample efficiency over traditional <b>Reinforcement</b> <b>Learning</b> (RL) methods. While recent research has demonstrated performance gains in scenarios with comparatively smooth dynamics and, thus, smooth optimization landscapes, research on leveraging differentiable simulators for contact-rich scenarios, such as legged locomotion, is scarce. This may be attributed to the discontinuous nature of contact, which introduces several challenges to optimizing with analytic gradients. The purpose of this paper is to determine if analytic gradients can be beneficial even in the face of contact. Our investigation focuses on the effects of different soft and hard contact models on the learning process, examining optimization challenges through the lens of contact <b>simulation.</b> We demonstrate the viability of employing analytic gradients to learn physically plausible locomotion skills with a quadrupedal robot using Short-Horizon Actor-Critic (SHAC), a learning algorithm leveraging analytic gradients, and draw a comparison to a state-of-the-art RL algorithm, Proximal Policy Optimization (PPO), to understand the benefits of analytic gradients.</p></p class="citation"></blockquote><h3 id=210--207258-unsupervised-learning-of-effective-actions-in-robotics-marko-zaric-et-al-2024>(2/10 | 207/258) Unsupervised Learning of Effective Actions in Robotics (Marko Zaric et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marko Zaric, Jakob Hollenstein, Justus Piater, Erwan Renaudo. (2024)<br><strong>Unsupervised Learning of Effective Actions in Robotics</strong><br><button class=copy-to-clipboard title="Unsupervised Learning of Effective Actions in Robotics" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02728v1.pdf filename=2404.02728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning actions that are relevant to decision-making and can be executed effectively is a key problem in autonomous robotics. Current state-of-the-art action representations in robotics lack proper effect-driven learning of the robot&rsquo;s actions. Although successful in solving manipulation tasks, deep learning methods also lack this ability, in addition to their high cost in terms of memory or training data. In this paper, we propose an <b>unsupervised</b> <b>algorithm</b> to discretize a continuous motion space and generate &ldquo;action prototypes&rdquo;, each producing different effects in the environment. After an exploration phase, the algorithm automatically builds a representation of the effects and groups motions into action prototypes, where motions more likely to produce an effect are represented more than those that lead to negligible changes. We evaluate our method on a simulated stair-climbing <b>reinforcement</b> <b>learning</b> task, and the preliminary results show that our effect driven discretization outperforms uniformly and randomly sampled discretizations in convergence speed and maximum reward.</p></p class="citation"></blockquote><h3 id=310--208258-sliceit----a-dual-simulator-framework-for-learning-robot-food-slicing-cristian-c-beltran-hernandez-et-al-2024>(3/10 | 208/258) SliceIt! &ndash; A Dual Simulator Framework for Learning Robot Food Slicing (Cristian C. Beltran-Hernandez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian C. Beltran-Hernandez, Nicolas Erbetti, Masashi Hamaya. (2024)<br><strong>SliceIt! &ndash; A Dual Simulator Framework for Learning Robot Food Slicing</strong><br><button class=copy-to-clipboard title="SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02569v1.pdf filename=2404.02569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using <b>Reinforcement</b> <b>Learning</b> (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in <b>simulation.</b> Following a real2sim2real approach, our framework consists of collecting a few real food slicing data, calibrating our dual <b>simulation</b> environment (a high-fidelity cutting simulator and a robotic simulator), learning compliant control policies on the calibrated <b>simulation</b> environment, and finally, deploying the policies on the real robot.</p></p class="citation"></blockquote><h3 id=410--209258-versatile-scene-consistent-traffic-scenario-generation-as-optimization-with-diffusion-zhiyu-huang-et-al-2024>(4/10 | 209/258) Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion (Zhiyu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, Jaime Fernández Fisac. (2024)<br><strong>Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion</strong><br><button class=copy-to-clipboard title="Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02524v1.pdf filename=2404.02524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating realistic and controllable agent behaviors in traffic <b>simulation</b> is crucial for the development of autonomous vehicles. This problem is often formulated as imitation learning (IL) from real-world driving data by either directly predicting future trajectories or inferring cost functions with inverse optimal control. In this paper, we draw a conceptual connection between IL and diffusion-based generative modeling and introduce a novel framework Versatile Behavior Diffusion (VBD) to simulate interactive scenarios with multiple traffic participants. Our model not only generates scene-consistent multi-agent interactions but also enables scenario editing through multi-step guidance and refinement. Experimental evaluations show that VBD achieves state-of-the-art performance on the Waymo Sim Agents <b>benchmark.</b> In addition, we illustrate the versatility of our model by adapting it to various applications. VBD is capable of producing scenarios conditioning on priors, integrating with model-based optimization, sampling <b>multi-modal</b> scene-consistent scenarios by fusing marginal predictions, and generating safety-critical scenarios when combined with a game-theoretic solver.</p></p class="citation"></blockquote><h3 id=510--210258-multi-robot-planning-for-filming-groups-of-moving-actors-leveraging-submodularity-and-pixel-density-skyler-hughes-et-al-2024>(5/10 | 210/258) Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density (Skyler Hughes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Skyler Hughes, Rebecca Martin, Micah Corah, Sebastian Scherer. (2024)<br><strong>Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density</strong><br><button class=copy-to-clipboard title="Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03103v1.pdf filename=2404.03103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Observing and filming a group of moving actors with a team of aerial robots is a challenging problem that combines elements of multi-robot coordination, coverage, and view planning. A single camera may observe multiple actors at once, and the robot team may observe individual actors from multiple views. As actors move about, groups may split, merge, and reform, and robots filming these actors should be able to adapt smoothly to such changes in actor formations. Rather than adopt an approach based on explicit formations or assignments, we propose an approach based on optimizing views directly. We model actors as moving polyhedra and compute approximate pixel densities for each face and camera view. Then, we propose an objective that exhibits diminishing returns as pixel densities increase from repeated observation. This gives rise to a multi-robot perception planning problem which we solve via a combination of value iteration and greedy submodular maximization. %using a combination of value iteration to optimize views for individual robots and sequential submodular maximization methods to coordinate the team. We evaluate our approach on challenging scenarios modeled after various kinds of social behaviors and featuring different numbers of robots and actors and observe that robot assignments and formations arise implicitly based on the movements of groups of actors. <b>Simulation</b> results demonstrate that our approach consistently outperforms baselines, and in addition to performing well with the planner&rsquo;s approximation of pixel densities our approach also performs comparably for evaluation based on rendered views. Overall, the multi-round variant of the sequential planner we propose meets (within 1%) or exceeds the formation and assignment baselines in all scenarios we consider.</p></p class="citation"></blockquote><h3 id=610--211258-low-frequency-sampling-in-model-predictive-path-integral-control-bogdan-vlahov-et-al-2024>(6/10 | 211/258) Low Frequency Sampling in Model Predictive Path Integral Control (Bogdan Vlahov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bogdan Vlahov, Jason Gibson, David D. Fan, Patrick Spieler, Ali-akbar Agha-mohammadi, Evangelos A. Theodorou. (2024)<br><strong>Low Frequency Sampling in Model Predictive Path Integral Control</strong><br><button class=copy-to-clipboard title="Low Frequency Sampling in Model Predictive Path Integral Control" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03094v1.pdf filename=2404.03094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling-based model-predictive controllers have become a powerful optimization tool for planning and control problems in various challenging environments. In this paper, we show how the default choice of uncorrelated Gaussian distributions can be improved upon with the use of a colored noise distribution. Our choice of distribution allows for the emphasis on low frequency control signals, which can result in smoother and more exploratory samples. We use this frequency-based sampling distribution with Model Predictive Path Integral (MPPI) in both hardware and <b>simulation</b> experiments to show better or equal performance on systems with various speeds of input response.</p></p class="citation"></blockquote><h3 id=710--212258-fusing-multi-sensor-input-with-state-information-on-tinyml-brains-for-autonomous-nano-drones-luca-crupi-et-al-2024>(7/10 | 212/258) Fusing Multi-sensor Input with State Information on TinyML Brains for Autonomous Nano-drones (Luca Crupi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Crupi, Elia Cereda, Daniele Palossi. (2024)<br><strong>Fusing Multi-sensor Input with State Information on TinyML Brains for Autonomous Nano-drones</strong><br><button class=copy-to-clipboard title="Fusing Multi-sensor Input with State Information on TinyML Brains for Autonomous Nano-drones" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02567v1.pdf filename=2404.02567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous nano-drones (~10 cm in diameter), thanks to their ultra-low power TinyML-based brains, are capable of coping with real-world environments. However, due to their simplified sensors and compute units, they are still far from the sense-and-act capabilities shown in their bigger counterparts. This system paper presents a novel deep learning-based pipeline that fuses multi-sensorial input (i.e., low-resolution images and 8x8 depth map) with the robot&rsquo;s state information to tackle a human pose estimation task. Thanks to our design, the proposed system &ndash; trained in <b>simulation</b> and tested on a real-world dataset &ndash; improves a state-unaware State-of-the-Art baseline by increasing the R^2 regression metric up to 0.10 on the distance&rsquo;s prediction.</p></p class="citation"></blockquote><h3 id=810--213258-self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system-xiwen-dengxiong-et-al-2024>(8/10 | 213/258) Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System (Xiwen Dengxiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiwen Dengxiong, Xueting Wang, Shi Bai, Yunbo Zhang. (2024)<br><strong>Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System</strong><br><button class=copy-to-clipboard title="Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03067v1.pdf filename=2404.03067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a <b>self-supervised</b> 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.</p></p class="citation"></blockquote><h3 id=910--214258-a-survey-of-optimization-based-task-and-motion-planning-from-classical-to-learning-approaches-zhigen-zhao-et-al-2024>(9/10 | 214/258) A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches (Zhigen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhigen Zhao, Shuo Chen, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu, Ye Zhao. (2024)<br><strong>A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches</strong><br><button class=copy-to-clipboard title="A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02817v1.pdf filename=2404.02817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as <b>large</b> <b>language</b> <b>models.</b> Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.</p></p class="citation"></blockquote><h3 id=1010--215258-on-the-go-tree-detection-and-geometric-traits-estimation-with-ground-mobile-robots-in-fruit-tree-groves-dimitrios-chatziparaschis-et-al-2024>(10/10 | 215/258) On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves (Dimitrios Chatziparaschis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Chatziparaschis, Hanzhe Teng, Yipeng Wang, Pamodya Peiris, Elia Scudiero, Konstantinos Karydis. (2024)<br><strong>On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves</strong><br><button class=copy-to-clipboard title="On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02516v1.pdf filename=2404.02516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By-tree information gathering is an essential task in precision agriculture achieved by ground mobile sensors, but it can be time- and labor-intensive. In this paper we present an algorithmic framework to perform real-time and on-the-go detection of trees and key geometric characteristics (namely, width and height) with wheeled mobile robots in the field. Our method is based on the fusion of 2D domain-specific data (normalized difference vegetation index [NDVI] acquired via a red-green-near-infrared [RGN] camera) and 3D LiDAR point clouds, via a customized tree landmark association and parameter estimation algorithm. The proposed system features a <b>multi-modal</b> and entropy-based landmark correspondences approach, integrated into an underlying Kalman filter system to recognize the surrounding trees and jointly estimate their spatial and vegetation-based characteristics. Realistic simulated tests are used to evaluate our proposed algorithm&rsquo;s behavior in a variety of settings. Physical experiments in agricultural fields help validate our method&rsquo;s efficacy in acquiring accurate by-tree information on-the-go and in real-time by employing only onboard computational and sensing resources.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--216258-sound-borrow-checking-for-rust-via-symbolic-semantics-son-ho-et-al-2024>(1/1 | 216/258) Sound Borrow-Checking for Rust via Symbolic Semantics (Son Ho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Son Ho, Aymeric Fromherz, Jonathan Protzenko. (2024)<br><strong>Sound Borrow-Checking for Rust via Symbolic Semantics</strong><br><button class=copy-to-clipboard title="Sound Borrow-Checking for Rust via Symbolic Semantics" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02680v1.pdf filename=2404.02680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Rust programming language continues to rise in popularity, and as such, warrants the close attention of the programming languages community. In this work, we present a new foundational contribution towards the theoretical understanding of Rust&rsquo;s semantics. We prove that LLBC, a high-level, borrow-centric model previously proposed for Rust&rsquo;s semantics and execution, is sound with regards to a low-level pointer-based language `a la CompCert. Specifically, we prove the following: that LLBC is a correct view over a traditional model of execution; that LLBC&rsquo;s symbolic semantics are a correct abstraction of LLBC programs; and that LLBC&rsquo;s symbolic semantics act as a borrow-checker for LLBC, i.e. that symbolically-checked LLBC programs do not get stuck when executed on a heap-and-addresses model of execution. To prove these results, we introduce a new proof style that considerably simplifies our proofs of <b>simulation,</b> which relies on a notion of hybrid states. Equipped with this <b>reasoning</b> framework, we show that a new addition to LLBC&rsquo;s symbolic semantics, namely a join operation, preserves the abstraction and borrow-checking properties. This in turn allows us to add support for loops to the Aeneas framework; we show, using a series of examples and case studies, that this unlocks new expressive power for Aeneas.</p></p class="citation"></blockquote><h2 id=csdc-4>cs.DC (4)</h2><h3 id=14--217258-geot-tensor-centric-library-for-graph-neural-network-via-efficient-segment-reduction-on-gpu-zhongming-yu-et-al-2024>(1/4 | 217/258) GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU (Zhongming Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongming Yu, Genghan Zhang, Hanxian Huang, Xin Chen, Jishen Zhao. (2024)<br><strong>GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU</strong><br><button class=copy-to-clipboard title="GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 26<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03019v1.pdf filename=2404.03019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have ignited a surge of innovation, significantly enhancing the processing of geometric data structures such as <b>graphs,</b> <b>point</b> <b>clouds,</b> and meshes. As the domain continues to evolve, a series of frameworks and libraries are being developed to push <b>GNN</b> efficiency to new heights. While <b>graph-centric</b> <b>libraries</b> <b>have</b> achieved success in the past, the advent of efficient tensor compilers has highlighted the urgent need for tensor-centric libraries. Yet, efficient tensor-centric frameworks for <b>GNNs</b> remain scarce due to unique challenges and limitations encountered when implementing segment reduction in <b>GNN</b> contexts. We introduce GeoT, a cutting-edge tensor-centric library designed specifically for <b>GNNs</b> via efficient segment reduction. GeoT debuts innovative parallel algorithms that not only introduce new design principles but also expand the available design space. Importantly, GeoT is engineered for straightforward fusion within a computation <b>graph,</b> <b>ensuring</b> <b>compatibility</b> with contemporary tensor-centric machine learning frameworks and compilers. Setting a new performance <b>benchmark,</b> GeoT marks a considerable advancement by showcasing an average operator speedup of 1.80x and an end-to-end speedup of 1.68x.</p></p class="citation"></blockquote><h3 id=24--218258-speed-power-and-cost-implications-for-gpu-acceleration-of-computational-fluid-dynamics-on-hpc-systems-zachary-cooper-baldock-et-al-2024>(2/4 | 218/258) Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems (Zachary Cooper-Baldock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zachary Cooper-Baldock, Brenda Vara Almirall, Kiao Inthavong. (2024)<br><strong>Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems</strong><br><button class=copy-to-clipboard title="Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: J-2-9; C-1-4; C-4; G-1-3, cs-DC, cs-NA, cs-PF, cs.DC, math-NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02482v1.pdf filename=2404.02482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational Fluid Dynamics (CFD) is the <b>simulation</b> of fluid flow undertaken with the use of computational hardware. The underlying equations are computationally challenging to solve and necessitate high performance computing (HPC) to resolve in a practical timeframe when a reasonable level of fidelity is required. The <b>simulations</b> are memory intensive, having previously been limited to central processing unit (CPU) solvers, as graphics processing unit (GPU) video random access memory (VRAM) was insufficient. However, with recent developments in GPU design and increases to VRAM, GPU acceleration of CPU solved workflows is now possible. At HPC scale however, many operational details are still unknown. This paper utilizes ANSYS Fluent, a leading commercial code in CFD, to investigate the compute speed, power consumption and service unit (SU) cost considerations for the GPU acceleration of CFD workflows on HPC architectures. To provide a comprehensive analysis, different CPU architectures, and GPUs have been assessed. It is seen that GPU compute speed is faster, however, the initialisation speed, power and cost performance is less clear cut. Whilst the larger A100 cards perform well with respect to power consumption, this is not observed for the V100 cards. In situations where more than one GPU is required, their adoption may not be beneficial from a power or cost perspective.</p></p class="citation"></blockquote><h3 id=34--219258-vpals-towards-verified-performance-aware-learning-system-for-resource-management-guoliang-he-et-al-2024>(3/4 | 219/258) vPALs: Towards Verified Performance-aware Learning System For Resource Management (Guoliang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoliang He, Gingfung Yeung, Sheriffo Ceesay, Adam Barker. (2024)<br><strong>vPALs: Towards Verified Performance-aware Learning System For Resource Management</strong><br><button class=copy-to-clipboard title="vPALs: Towards Verified Performance-aware Learning System For Resource Management" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03079v1.pdf filename=2404.03079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately predicting task performance at runtime in a cluster is advantageous for a resource management system to determine whether a task should be migrated due to performance degradation caused by interference. This is beneficial for both cluster operators and service owners. However, deploying performance prediction systems with learning methods requires sophisticated safeguard mechanisms due to the inherent stochastic and <b>black-box</b> <b>natures</b> of these models, such as Deep Neural Networks (DNNs). Vanilla Neural Networks (NNs) can be vulnerable to <b>out-of-distribution</b> data samples that can lead to sub-optimal decisions. To take a step towards a safe learning system in performance prediction, We propose vPALs that leverage well-correlated system metrics, and verification to produce safe performance prediction at runtime, providing an extra layer of safety to integrate learning techniques to cluster resource management systems. Our experiments show that vPALs can outperform vanilla NNs across our <b>benchmark</b> workload.</p></p class="citation"></blockquote><h3 id=44--220258-a-survey-on-error-bounded-lossy-compression-for-scientific-datasets-sheng-di-et-al-2024>(4/4 | 220/258) A Survey on Error-Bounded Lossy Compression for Scientific Datasets (Sheng Di et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Di, Jinyang Liu, Kai Zhao, Xin Liang, Robert Underwood, Zhaorui Zhang, Milan Shah, Yafan Huang, Jiajun Huang, Xiaodong Yu, Congrong Ren, Hanqi Guo, Grant Wilkins, Dingwen Tao, Jiannan Tian, Sian Jin, Zizhe Jian, Daoce Wang, MD Hasanur Rahman, Boyuan Zhang, Jon C. Calhoun, Guanpeng Li, Kazutomo Yoshii, Khalid Ayed Alharthi, Franck Cappello. (2024)<br><strong>A Survey on Error-Bounded Lossy Compression for Scientific Datasets</strong><br><button class=copy-to-clipboard title="A Survey on Error-Bounded Lossy Compression for Scientific Datasets" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02840v1.pdf filename=2404.02840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Error-bounded lossy compression has been effective in significantly reducing the data storage/transfer burden while preserving the reconstructed data fidelity very well. Many error-bounded lossy compressors have been developed for a wide range of parallel and distributed use cases for years. These lossy compressors are designed with distinct compression models and design principles, such that each of them features particular pros and cons. In this paper we provide a comprehensive survey of emerging error-bounded lossy compression techniques for different use cases each involving big data to process. The key contribution is fourfold. (1) We <b>summarize</b> an insightful taxonomy of lossy compression into 6 classic compression models. (2) We provide a comprehensive survey of 10+ commonly used compression components/modules used in error-bounded lossy compressors. (3) We provide a comprehensive survey of 10+ state-of-the-art error-bounded lossy compressors as well as how they combine the various compression modules in their designs. (4) We provide a comprehensive survey of the lossy compression for 10+ modern scientific applications and use-cases. We believe this survey is useful to multiple communities including scientific applications, high-performance computing, lossy compression, and big data.</p></p class="citation"></blockquote><h2 id=mathoc-4>math.OC (4)</h2><h3 id=14--221258-deep-reinforcement-learning-for-traveling-purchaser-problems-haofeng-yuan-et-al-2024>(1/4 | 221/258) Deep Reinforcement Learning for Traveling Purchaser Problems (Haofeng Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haofeng Yuan, Rongping Zhu, Wanlu Yang, Shiji Song, Keyou You, Yuli Zhang. (2024)<br><strong>Deep Reinforcement Learning for Traveling Purchaser Problems</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Traveling Purchaser Problems" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-AI, cs-LG, math-OC, math.OC<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Meta Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02476v1.pdf filename=2404.02476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep <b>reinforcement</b> <b>learning</b> (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite <b>graph</b> representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite <b>graph</b> and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently construct the route using the policy network, and once the route is determined, the associated purchasing plan can be easily derived through linear programming, while, leveraging DRL, we can train the policy network to optimize the global solution objective. Furthermore, by introducing a <b>meta-learning</b> <b>strategy,</b> the policy network can be trained stably on large-sized TPP instances, and generalize well across instances of varying sizes and distributions, even to much larger instances that are never seen during training. Experiments on various synthetic TPP instances and the TPPLIB <b>benchmark</b> demonstrate that our DRL-based approach can significantly outperform well-established TPP heuristics, reducing the optimality gap by 40%-90%, and also showing an advantage in runtime, especially on large-sized instances.</p></p class="citation"></blockquote><h3 id=24--222258-electric-vehicle-routing-problem-for-emergency-power-supply-towards-telecom-base-station-relief-daisuke-kikuta-et-al-2024>(2/4 | 222/258) Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief (Daisuke Kikuta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daisuke Kikuta, Hiroki Ikeuchi, Kengo Tajiri, Yuta Toyama, Yuusuke Nakano. (2024)<br><strong>Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief</strong><br><button class=copy-to-clipboard title="Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-AI, cs-LG, cs-MA, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02448v1.pdf filename=2404.02448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a <b>reinforcement</b> <b>learning</b> (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on both synthetic datasets and real datasets. The results show that our solver outperforms baselines in terms of the objective value and computation time. Moreover, we analyze the generalization and scalability of our solver, demonstrating the capability toward unseen settings and large-scale problems. Check also our project page: <a href=https://ntt-dkiku.github.io/rl-evrpeps>https://ntt-dkiku.github.io/rl-evrpeps</a>.</p></p class="citation"></blockquote><h3 id=34--223258-faster-convergence-of-stochastic-accelerated-gradient-descent-under-interpolation-aaron-mishkin-et-al-2024>(3/4 | 223/258) Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation (Aaron Mishkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Mishkin, Mert Pilanci, Mark Schmidt. (2024)<br><strong>Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation</strong><br><button class=copy-to-clipboard title="Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02378v1.pdf filename=2404.02378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated <b>SGD</b> under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\rho$ to $\sqrt{\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for <b>SGD.</b></p></p class="citation"></blockquote><h3 id=44--224258-nonlinear-integral-extension-of-pid-control-with-improved-convergence-of-perturbed-second-order-dynamic-systems-michael-ruderman-2024>(4/4 | 224/258) Nonlinear integral extension of PID control with improved convergence of perturbed second-order dynamic systems (Michael Ruderman, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Ruderman. (2024)<br><strong>Nonlinear integral extension of PID control with improved convergence of perturbed second-order dynamic systems</strong><br><button class=copy-to-clipboard title="Nonlinear integral extension of PID control with improved convergence of perturbed second-order dynamic systems" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02502v1.pdf filename=2404.02502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nonlinear extension of the integral part of PID feedback control is proposed for the perturbed second-order systems. For the matched constant perturbations, the global asymptotic stability is shown, and for Lipschitz perturbations an ultimately bounded output error is guaranteed. The second-order system plants can also be expanded by an additional (parasitic) actuator dynamics with low-pass characteristics. The proposed nonlinear control is proven to outperform its linear (PID) <b>benchmarking</b> counterpart during the settling phase, i.e. at convergence of the residual output error. An experimental case study of the second-order system with an additional actuator dynamics and considerable perturbation is demonstrated to confirm and <b>benchmark</b> the control performance.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--225258-promptcodec-high-fidelity-neural-speech-codec-using-disentangled-representation-learning-based-adaptive-feature-aware-prompt-encoders-yu-pan-et-al-2024>(1/1 | 225/258) PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders (Yu Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Pan, Lei Ma, Jianjun Zhao. (2024)<br><strong>PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders</strong><br><button class=copy-to-clipboard title="PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD<br>Keyword Score: 25<br>Keywords: Representation Learning, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02702v1.pdf filename=2404.02702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, <b>text-to-speech</b> synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled <b>representation</b> <b>learning</b> based feature-aware <b>prompt</b> encoders. By incorporating additional feature <b>representations</b> <b>from</b> <b>prompt</b> encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled <b>representation</b> <b>learning</b> strategy based on cosine distance to optimize PromptCodec&rsquo;s encoders to ensure their efficiency, thereby further improving the performance of PromptCodec. Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently outperforms state-of-the-art neural speech codec models under all different bitrate conditions while achieving impressive performance with low bitrates.</p></p class="citation"></blockquote><h2 id=statco-1>stat.CO (1)</h2><h3 id=11--226258-alaamee-open-source-software-for-fitting-autologistic-actor-attribute-models-alex-stivala-et-al-2024>(1/1 | 226/258) ALAAMEE: Open-source software for fitting autologistic actor attribute models (Alex Stivala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Stivala, Peng Wang, Alessandro Lomi. (2024)<br><strong>ALAAMEE: Open-source software for fitting autologistic actor attribute models</strong><br><button class=copy-to-clipboard title="ALAAMEE: Open-source software for fitting autologistic actor attribute models" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.CO<br>Categories: cs-SI, stat-CO, stat-ME, stat.CO<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03116v1.pdf filename=2404.03116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The autologistic actor attribute model (ALAAM) is a model for social influence, derived from the more widely known exponential-family random <b>graph</b> model (ERGM). ALAAMs can be used to estimate parameters corresponding to multiple forms of social contagion associated with network structure and actor covariates. This work introduces ALAAMEE, open-source Python software for estimation, <b>simulation,</b> and goodness-of-fit testing for ALAAM models. ALAAMEE implements both the stochastic approximation and equilibrium expectation (EE) algorithms for ALAAM parameter estimation, including estimation from snowball sampled network data. It implements data structures and statistics for undirected, directed, and bipartite networks. We use a <b>simulation</b> study to assess the accuracy of the EE algorithm for ALAAM parameter estimation and statistical inference, and demonstrate the use of ALAAMEE with empirical examples using both small (fewer than 100 nodes) and large (more than 10 000 nodes) networks.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--227258-polynomial-graphical-lasso-learning-edges-from-gaussian-graph-stationary-signals-andrei-buciulea-et-al-2024>(1/2 | 227/258) Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals (Andrei Buciulea et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Buciulea, Jiaxi Ying, Antonio G. Marques, Daniel P. Palomar. (2024)<br><strong>Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals</strong><br><button class=copy-to-clipboard title="Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02621v1.pdf filename=2404.02621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Polynomial Graphical Lasso (PGL), a new approach to learning <b>graph</b> structures from nodal signals. Our key contribution lies in modeling the signals as Gaussian and stationary on the <b>graph,</b> enabling the development of a <b>graph-learning</b> formulation that combines the strengths of graphical lasso with a more encompassing model. Specifically, we assume that the precision matrix can take any polynomial form of the sought <b>graph,</b> allowing for increased flexibility in modeling nodal relationships. Given the resulting complexity and nonconvexity of the resulting optimization problem, we (i) propose a low-complexity algorithm that alternates between estimating the <b>graph</b> and precision matrices, and (ii) characterize its convergence. We evaluate the performance of PGL through comprehensive numerical <b>simulations</b> using both synthetic and real data, demonstrating its superiority over several alternatives. Overall, this approach presents a significant advancement in <b>graph</b> learning and holds promise for various applications in <b>graph-aware</b> signal analysis and beyond.</p></p class="citation"></blockquote><h3 id=22--228258-ground-to-uav-140-ghz-channel-measurement-and-modeling-da-li-et-al-2024>(2/2 | 228/258) Ground-to-UAV 140 GHz channel measurement and modeling (Da Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Da Li, Peian Li, Jiabiao Zhao, Jianjian Liang, Jiacheng Liu, Guohao Liu, Yuanshuai Lei, Wenbo Liu, Jianqin Deng, Fuyong Liu, Jianjun Ma. (2024)<br><strong>Ground-to-UAV 140 GHz channel measurement and modeling</strong><br><button class=copy-to-clipboard title="Ground-to-UAV 140 GHz channel measurement and modeling" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02663v1.pdf filename=2404.02663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned Aerial Vehicle (UAV) assisted terahertz (THz) wireless communications have been expected to play a vital role in the next generation of wireless networks. UAVs can serve as either repeaters or data collectors within the communication link, thereby potentially augmenting the efficacy of communication systems. Despite their promise, the channel analysis and modeling specific to THz wireless channels leveraging UAVs remain under explored. This work delves into a ground-to-UAV channel at 140 GHz, with a specific focus on the influence of UAV hovering behavior on channel performance. Employing experimental measurements through an unmodulated channel setup and a <b>geometry-based</b> stochastic model (GBSM) that integrates three-dimensional positional coordinates and beamwidth, this work evaluates the impact of UAV dynamic movements and antenna orientation on channel performance. Our findings highlight the minimal impact of UAV orientation adjustments on channel performance and underscore the diminishing necessity for precise alignment between UAVs and ground stations as beamwidth increases.</p></p class="citation"></blockquote><h2 id=statap-1>stat.AP (1)</h2><h3 id=11--229258-auditing-the-use-of-language-models-to-guide-hiring-decisions-johann-d-gaebler-et-al-2024>(1/1 | 229/258) Auditing the Use of Language Models to Guide Hiring Decisions (Johann D. Gaebler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johann D. Gaebler, Sharad Goel, Aziz Huq, Prasanna Tambe. (2024)<br><strong>Auditing the Use of Language Models to Guide Hiring Decisions</strong><br><button class=copy-to-clipboard title="Auditing the Use of Language Models to Guide Hiring Decisions" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.AP<br>Categories: cs-CL, stat-AP, stat.AP<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03086v1.pdf filename=2404.03086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regulatory efforts to protect against algorithmic bias have taken on increased urgency with rapid advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which are machine learning models that can achieve performance rivaling human experts on a wide array of tasks. A key theme of these initiatives is algorithmic &ldquo;auditing,&rdquo; but current regulations &ndash; as well as the scientific literature &ndash; provide little guidance on how to conduct these assessments. Here we propose and investigate one approach for auditing algorithms: correspondence experiments, a widely applied tool for detecting bias in human judgements. In the employment context, correspondence experiments aim to measure the extent to which race and gender impact decisions by experimentally manipulating elements of submitted application materials that suggest an applicant&rsquo;s demographic traits, such as their listed name. We apply this method to audit candidate assessments produced by several state-of-the-art <b>LLMs,</b> using a novel corpus of applications to K-12 teaching positions in a <b>large</b> <b>public</b> <b>school</b> district. We find evidence of moderate race and gender disparities, a pattern largely robust to varying the types of application material input to the models, as well as the framing of the task to the <b>LLMs.</b> We conclude by discussing some important limitations of correspondence experiments for auditing algorithms.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--230258-human-mobility-in-the-metaverse-kishore-vasan-et-al-2024>(1/1 | 230/258) Human Mobility in the Metaverse (Kishore Vasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kishore Vasan, Marton Karsai, Albert-Laszlo Barabasi. (2024)<br><strong>Human Mobility in the Metaverse</strong><br><button class=copy-to-clipboard title="Human Mobility in the Metaverse" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 20<br>Keywords: Prompt, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03071v1.pdf filename=2404.03071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The metaverse promises a shift in the way humans interact with each other, and with their digital and physical environments. The lack of geographical boundaries and travel costs in the metaverse <b>prompts</b> us to ask if the fundamental laws that govern human mobility in the physical world apply. We collected data on avatar movements, along with their network mobility extracted from NFT purchases. We find that despite the absence of commuting costs, an individuals inclination to explore new locations diminishes over time, limiting movement to a small fraction of the metaverse. We also find a lack of correlation between land prices and visitation, a deviation from the patterns characterizing the physical world. Finally, we identify the <b>scaling</b> <b>laws</b> that characterize meta mobility and show that we need to add preferential selection to the existing models to explain quantitative patterns of metaverse mobility. Our ability to predict the characteristics of the emerging meta mobility network implies that the laws governing human mobility are rooted in fundamental patterns of human dynamics, rather than the nature of space and cost of movement.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--231258-traffic-divergence-theory-an-analysis-formalism-for-dynamic-networks-matin-macktoobian-et-al-2024>(1/2 | 231/258) Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks (Matin Macktoobian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matin Macktoobian, Zhan Shu, Qing Zhao. (2024)<br><strong>Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks</strong><br><button class=copy-to-clipboard title="Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-NI, cs.MA, math-DS<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03066v1.pdf filename=2404.03066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic dynamics is universally crucial in analyzing and designing almost any network. This article introduces a novel theoretical approach to analyzing network traffic dynamics. This theory&rsquo;s machinery is based on the notion of traffic divergence, which captures the flow (im)balance of network nodes and links. It features various analytical probes to investigate both spatial and temporal traffic dynamics. In particular, the maximal traffic distribution in a network can be characterized by spatial traffic divergence rate, which reveals the relative difference among node traffic divergence. To illustrate the usefulness, we apply the theory to two network-driven problems: throughput estimation of data center networks and power-optimized communication planning for robot networks, and show the merits of the proposed theory through <b>simulations.</b></p></p class="citation"></blockquote><h3 id=22--232258-marl-lns-cooperative-multi-agent-reinforcement-learning-via-large-neighborhoods-search-weizhe-chen-et-al-2024>(2/2 | 232/258) MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search (Weizhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhe Chen, Sven Koenig, Bistra Dilkina. (2024)<br><strong>MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search</strong><br><button class=copy-to-clipboard title="MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03101v1.pdf filename=2404.03101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cooperative multi-agent <b>reinforcement</b> <b>learning</b> (MARL) has been an increasingly important research topic in the last half-decade because of its great potential for real-world applications. Because of the curse of dimensionality, the popular &ldquo;centralized training decentralized execution&rdquo; framework requires a long time in training, yet still cannot converge efficiently. In this paper, we propose a general training framework, MARL-LNS, to algorithmically address these issues by training on alternating subsets of agents using existing deep MARL algorithms as low-level trainers, while not involving any additional parameters to be trained. Based on this framework, we provide three algorithm variants based on the framework: random large neighborhood search (RLNS), batch large neighborhood search (BLNS), and adaptive large neighborhood search (ALNS), which alternate the subsets of agents differently. We test our algorithms on both the StarCraft Multi-Agent Challenge and Google Research Football, showing that our algorithms can automatically reduce at least 10% of training time while reaching the same final skill level as the original algorithm.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--233258-a-mean-field-game-model-for-timely-computation-in-edge-computing-systems-shubham-aggarwal-et-al-2024>(1/6 | 233/258) A Mean Field Game Model for Timely Computation in Edge Computing Systems (Shubham Aggarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Aggarwal, Muhammad Aneeq uz Zaman, Melih Bastopcu, Sennur Ulukus, Tamer Başar. (2024)<br><strong>A Mean Field Game Model for Timely Computation in Edge Computing Systems</strong><br><button class=copy-to-clipboard title="A Mean Field Game Model for Timely Computation in Edge Computing Systems" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-GT, cs-IT, cs-NI, cs-SY, cs.IT, eess-SY, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02898v1.pdf filename=2404.02898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of task offloading in multi-access edge computing (MEC) systems constituting $N$ devices assisted by an edge server (ES), where the devices can split task execution between a local processor and the ES. Since the local task execution and communication with the ES both consume power, each device must judiciously choose between the two. We model the problem as a large population non-cooperative game among the $N$ devices. Since computation of an equilibrium in this scenario is difficult due to the presence of a large number of devices, we employ the mean-field game framework to reduce the finite-agent game problem to a generic user&rsquo;s multi-objective optimization problem, with a coupled consistency condition. By leveraging the novel age of information (AoI) metric, we invoke techniques from stochastic hybrid systems (SHS) theory and study the tradeoffs between increasing information freshness and reducing power consumption. In numerical <b>simulations,</b> we validate that a higher load at the ES may lead devices to upload their task to the ES less often.</p></p class="citation"></blockquote><h3 id=26--234258-optimizing-peak-age-of-information-in-mec-systems-computing-preemption-and-non-preemption-jianhang-zhu-et-al-2024>(2/6 | 234/258) Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption (Jianhang Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhang Zhu, Jie Gong. (2024)<br><strong>Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption</strong><br><button class=copy-to-clipboard title="Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02700v1.pdf filename=2404.02700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The freshness of information in real-time monitoring systems has received increasing attention, with Age of Information (AoI) emerging as a novel metric for measuring information freshness. In many applications, update packets need to be computed before being delivered to a destination. Mobile edge computing (MEC) is a promising approach for efficiently accomplishing the computing process, where the transmission process and computation process are coupled, jointly affecting freshness. In this paper, we aim to minimize the average peak AoI (PAoI) in an MEC system. We consider the generate-at-will source model and study when to generate a new update in two edge server setups: 1) computing preemption, where the packet in the computing process will be preempted by the newly arrived one, and 2) non-preemption, where the newly arrived packet will wait in the queue until the current one completes computing. We prove that the fixed threshold policy is optimal in a non-preemptive system for arbitrary transmission time and computation time distributions. In a preemptive system, we show that the transmission-aware threshold policy is optimal when the computing time follows an exponential distribution. Our numerical <b>simulation</b> results not only validate the theoretical findings but also demonstrate that: 1) in our problem, preemptive systems are not always superior to non-preemptive systems, even with exponential distribution, and 2) as the ratio of the mean transmission time to the mean computation time increases, the optimal threshold increases in preemptive systems but decreases in non-preemptive systems.</p></p class="citation"></blockquote><h3 id=36--235258-performance-analysis-and-isi-mitigation-with-imperfect-transmitter-in-molecular-communication-dongliang-jing-et-al-2024>(3/6 | 235/258) Performance Analysis and ISI Mitigation with Imperfect Transmitter in Molecular Communication (Dongliang Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongliang Jing, Lin Lin, Andrew W. Eckford. (2024)<br><strong>Performance Analysis and ISI Mitigation with Imperfect Transmitter in Molecular Communication</strong><br><button class=copy-to-clipboard title="Performance Analysis and ISI Mitigation with Imperfect Transmitter in Molecular Communication" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02383v1.pdf filename=2404.02383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In molecular communication (MC), molecules are released from the transmitter to convey information. This paper considers a realistic molecule shift keying (MoSK) scenario with two species of molecule in two reservoirs, where the molecules are harvested from the environment and placed into different reservoirs, which are purified by exchanging molecules between the reservoirs. This process consumes energy, and for a reasonable energy cost, the reservoirs cannot be pure; thus, our MoSK transmitter is imperfect, releasing mixtures of both molecules for every symbol, resulting in inter-symbol interference (ISI). To mitigate ISI, the properties of the receiver are analyzed and a detection method based on the ratio of different molecules is proposed. Theoretical and <b>simulation</b> results are provided, showing that with the increase of energy cost, the system achieves better performance. The good performance of the proposed detection scheme is also demonstrated.</p></p class="citation"></blockquote><h3 id=46--236258-computationally-efficient-unsupervised-deep-learning-for-robust-joint-ap-clustering-and-beamforming-design-in-cell-free-systems-guanghui-chen-et-al-2024>(4/6 | 236/258) Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems (Guanghui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanghui Chen, Zheng Wang, Hongxin Lin, Yongming Huang, Luxi Yang. (2024)<br><strong>Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems</strong><br><button class=copy-to-clipboard title="Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02531v1.pdf filename=2404.02531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider robust joint access point (AP) <b>clustering</b> and beamforming design with imperfect channel state information (CSI) in cell-free systems. Specifically, we jointly optimize AP <b>clustering</b> and beamforming with imperfect CSI to simultaneously maximize the worst-case sum rate and minimize the number of AP <b>clustering</b> under power constraint and the sparsity constraint of AP <b>clustering.</b> By transformations, the semi-infinite constraints caused by the imperfect CSI are converted into more tractable forms for facilitating a computationally efficient <b>unsupervised</b> deep learning algorithm. In addition, to further reduce the computational complexity, a computationally effective <b>unsupervised</b> deep learning algorithm is proposed to implement robust joint AP <b>clustering</b> and beamforming design with imperfect CSI in cell-free systems. Numerical results demonstrate that the proposed <b>unsupervised</b> deep learning algorithm achieves a higher worst-case sum rate under a smaller number of AP <b>clustering</b> with computational efficiency.</p></p class="citation"></blockquote><h3 id=56--237258-an-error-bounded-lossy-compression-method-with-bit-adaptive-quantization-for-particle-data-congrong-ren-et-al-2024>(5/6 | 237/258) An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data (Congrong Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congrong Ren, Sheng Di, Longtao Zhang, Kai Zhao, Hanqi Guo. (2024)<br><strong>An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data</strong><br><button class=copy-to-clipboard title="An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: astro-ph-IM, cs-GR, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02826v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02826v2.pdf filename=2404.02826v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents error-bounded lossy compression tailored for particle datasets from diverse scientific applications in cosmology, fluid dynamics, and fusion energy sciences. As today&rsquo;s high-performance computing capabilities advance, these datasets often reach trillions of points, posing significant visualization, analysis, and storage challenges. While error-bounded lossy compression makes it possible to represent floating-point values with strict pointwise accuracy guarantees, the lack of correlations in particle data&rsquo;s storage ordering often limits the compression ratio. Inspired by <b>quantization-encoding</b> schemes in SZ lossy compressors, we dynamically determine the number of bits to encode particles of the dataset to increase the compression ratio. Specifically, we utilize a k-d tree to partition particles into subregions and generate ``bit boxes&rsquo;&rsquo; centered at particles for each subregion to encode their positions. These bit boxes ensure error control while reducing the bit count used for compression. We comprehensively evaluate our method against state-of-the-art compressors on cosmology, fluid dynamics, and fusion plasma datasets.</p></p class="citation"></blockquote><h3 id=66--238258-multiple-uav-assisted-cooperative-df-relaying-in-multi-user-massive-mimo-iot-systems-mobeen-mahmood-et-al-2024>(6/6 | 238/258) Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems (Mobeen Mahmood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mobeen Mahmood, Yicheng Yuan, Tho Le-Ngoc. (2024)<br><strong>Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems</strong><br><button class=copy-to-clipboard title="Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03068v1.pdf filename=2404.03068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work considers a multi-user massive multiple-input multiple-output (MU-mMIMO) Internet-of-Things (IoT) system, where multiple unmanned aerial vehicles (UAVs) operating as decode-and-forward (DF) relays connect the base station (BS) to a large number of IoT devices. To maximize the total achievable rate, we propose a novel joint optimization problem of hybrid beamforming (HBF), multiple UAV relay positioning, and power allocation (PA) to multiple IoT users. The study adopts a <b>geometry-based</b> millimeter-wave (mmWave) channel model for both links and utilizes sequential optimization based on K-means UAV-user association. The radio frequency (RF) stages are designed based on the slow time-varying angular information, while the baseband (BB) stages are designed utilizing the reduced-dimension effective channel matrices. The illustrative results show that multiple UAV-assisted cooperative relaying systems outperform a single UAV system in practical user distributions. Moreover, compared to fixed positions and equal PA of UAVs and BS, the joint optimization of UAV location and PA substantially enhances the total achievable rate.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--239258-residual-based-a-posteriori-error-estimators-for-algebraic-stabilizations-abhinav-jha-2024>(1/5 | 239/258) Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations (Abhinav Jha, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhinav Jha. (2024)<br><strong>Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations</strong><br><button class=copy-to-clipboard title="Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02804v1.pdf filename=2404.02804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA]. Numerical <b>simulations</b> on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme.</p></p class="citation"></blockquote><h3 id=25--240258-proper-implicit-discretization-of-arbitrary-order-robust-exact-differentiators-richard-seeber-2024>(2/5 | 240/258) Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators (Richard Seeber, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Seeber. (2024)<br><strong>Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators</strong><br><button class=copy-to-clipboard title="Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, cs-SY, eess-SY, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02770v1.pdf filename=2404.02770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the implicit Euler discretization of Levant&rsquo;s arbitrary order robust exact differentiator in presence of sampled measurements. Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization. A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator&rsquo;s outputs as appropriately designed linear combinations of its state variables. A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given. The influence of bounded measurement noise and numerical approximation errors is formally analyzed. Numerical <b>simulations</b> confirm the obtained results.</p></p class="citation"></blockquote><h3 id=35--241258-a-neural-multigrid-solver-for-helmholtz-equations-with-high-wavenumber-and-heterogeneous-media-chen-cui-et-al-2024>(3/5 | 241/258) A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media (Chen Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Cui, Kai Jiang, Shi Shu. (2024)<br><strong>A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media</strong><br><button class=copy-to-clipboard title="A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N22, 65N55, 68T07, cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02493v1.pdf filename=2404.02493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Solving high-wavenumber and heterogeneous Helmholtz equations presents a long-standing challenge in scientific computing. In this paper, we introduce a deep learning-enhanced multigrid solver to address this issue. By conducting error analysis on standard multigrid applied to a discrete Helmholtz equation, we devise a strategy to handle errors with different frequencies separately. For error components with frequencies distant from the wavenumber, we perform simple smoothing based on local operations at different levels to eliminate them. On the other hand, to address error components with frequencies near the wavenumber, we utilize another multigrid V-cycle to solve an advection-diffusion-reaction (ADR) equation at a coarse scale. The resulting solver, named Wave-ADR-NS, involves parameters learned through <b>unsupervised</b> training. Numerical results demonstrate that Wave-ADR-NS effectively resolves heterogeneous 2D Helmholtz equation with wavenumber up to 2000. Comparative experiments against classical multigrid preconditioners and existing deep learning-based multigrid preconditioners reveals the superior performance of Wave-ADR-NS.</p></p class="citation"></blockquote><h3 id=45--242258-locking-free-hybrid-high-order-method-for-linear-elasticity-carsten-carstensen-et-al-2024>(4/5 | 242/258) Locking-free hybrid high-order method for linear elasticity (Carsten Carstensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carsten Carstensen, Ngoc Tien Tran. (2024)<br><strong>Locking-free hybrid high-order method for linear elasticity</strong><br><button class=copy-to-clipboard title="Locking-free hybrid high-order method for linear elasticity" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N12, 65N30, 65Y20, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02768v1.pdf filename=2404.02768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics. The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization. The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam'e parameter $\lambda\to\infty$ becomes very large. This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior. The a priori error analysis provides quasi-best approximation with $\lambda$-independent equivalence constants. The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\lambda$-robust. The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms. Numerical <b>benchmarks</b> provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit.</p></p class="citation"></blockquote><h3 id=55--243258-adaptive-hp-polynomial-based-sparse-grid-collocation-algorithms-for-piecewise-smooth-functions-with-kinks-hendrik-wilka-et-al-2024>(5/5 | 243/258) Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks (Hendrik Wilka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hendrik Wilka, Jens Lang. (2024)<br><strong>Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks</strong><br><button class=copy-to-clipboard title="Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65D40, 65D15, 65D05, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02556v1.pdf filename=2404.02556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning. Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality. However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions. For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere. Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order. Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there. The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure. Three numerical <b>benchmark</b> examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--244258-ai-and-personalized-learning-bridging-the-gap-with-modern-educational-goals-kristjan-julius-laak-et-al-2024>(1/3 | 244/258) AI and personalized learning: bridging the gap with modern educational goals (Kristjan-Julius Laak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristjan-Julius Laak, Jaan Aru. (2024)<br><strong>AI and personalized learning: bridging the gap with modern educational goals</strong><br><button class=copy-to-clipboard title="AI and personalized learning: bridging the gap with modern educational goals" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-HC, cs.CY<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02798v1.pdf filename=2404.02798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized learning (PL) aspires to provide an alternative to the one-size-fits-all approach in education. Technology-based PL solutions have shown notable effectiveness in enhancing learning performance. However, their alignment with the broader goals of modern education is inconsistent across technologies and research areas. In this paper, we examine the characteristics of AI-driven PL solutions in light of the OECD Learning Compass 2030 goals. Our analysis indicates a gap between the objectives of modern education and the current direction of PL. We identify areas where most present-day PL technologies could better embrace essential elements of contemporary education, such as collaboration, cognitive engagement, and the development of general competencies. While the present PL solutions are instrumental in aiding learning processes, the PL envisioned by educational experts extends beyond simple technological tools and requires a holistic change in the educational system. Finally, we explore the potential of <b>large</b> <b>language</b> <b>models,</b> such as <b>ChatGPT,</b> and propose a hybrid model that blends artificial intelligence with a collaborative, teacher-facilitated approach to personalized learning.</p></p class="citation"></blockquote><h3 id=23--245258-enhancing-student-engagement-in-large-scale-capstone-courses-an-experience-report-asma-shakil-et-al-2024>(2/3 | 245/258) Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report (Asma Shakil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asma Shakil, Paul Denny. (2024)<br><strong>Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report</strong><br><button class=copy-to-clipboard title="Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03120v1.pdf filename=2404.03120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer science (CS) capstone courses offer students a valuable opportunity to gain hands-on experience in software development, practice essential soft skills, and enhance their employability prospects. They are a core component in many CS undergraduate degrees and address the ACM curricula requirements of inculcating professional dispositions in students and making them aware of the broader societal implications of computing. However, coordinating a capstone course, especially for a large student cohort, can be a daunting task for academic staff. It demands considerable time and energy for planning and coordinating activities between students, academic staff, and any external stakeholders. In this experience report, we outline the iterative development and refinement of our capstone course as it grew substantially in size over a span of six consecutive sessions. We outline the pedagogies that helped us to enhance student engagement and motivation in the course as assessed by end-of-course surveys and students&rsquo; written reflections. We share the lessons that we have learnt and provide <b>recommendations</b> to educators who are designing new capstone courses or looking to scale existing ones.</p></p class="citation"></blockquote><h3 id=33--246258-decentralised-moderation-for-interoperable-social-networks-a-conversation-based-approach-for-pleroma-and-the-fediverse-vibhor-agarwal-et-al-2024>(3/3 | 246/258) Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse (Vibhor Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vibhor Agarwal, Aravindh Raman, Nishanth Sastry, Ahmed M. Abdelmoniem, Gareth Tyson, Ignacio Castro. (2024)<br><strong>Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse</strong><br><button class=copy-to-clipboard title="Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs.CY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.03048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.03048v1.pdf filename=2404.03048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent development of decentralised and interoperable social networks (such as the &ldquo;fediverse&rdquo;) creates new challenges for content moderators. This is because millions of posts generated on one server can easily &ldquo;spread&rdquo; to another, even if the recipient server has very different moderation policies. An obvious solution would be to leverage moderation tools to automatically tag (and filter) posts that contravene moderation policies, e.g. related to toxic speech. Recent work has exploited the conversational context of a post to improve this automatic tagging, e.g. using the replies to a post to help classify if it contains toxic speech. This has shown particular potential in environments with large training sets that contain complete conversations. This, however, creates challenges in a decentralised context, as a single conversation may be fragmented across multiple servers. Thus, each server only has a partial view of an entire conversation because conversations are often federated across servers in a non-synchronized fashion. To address this, we propose a decentralised conversation-aware content moderation approach suitable for the fediverse. Our approach employs a <b>graph</b> deep learning model (GraphNLI) trained locally on each server. The model exploits local data to train a model that combines post and conversational information captured through random walks to detect toxicity. We evaluate our approach with data from Pleroma, a major decentralised and interoperable micro-blogging network containing 2 million conversations. Our model effectively detects toxicity on larger instances, exclusively trained using their local post information (0.8837 macro-F1). Our approach has considerable scope to improve moderation in decentralised and interoperable social networks such as Pleroma or Mastodon.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--247258-closing-the-implementation-gap-in-mc-fully-chemical-synchronization-and-detection-for-cellular-receivers-bastian-heinlein-et-al-2024>(1/1 | 247/258) Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers (Bastian Heinlein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bastian Heinlein, Lukas Brand, Malcolm Egan, Maximilian Schäfer, Robert Schober, Sebastian Lotter. (2024)<br><strong>Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers</strong><br><button class=copy-to-clipboard title="Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-ET, cs.ET<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02765v1.pdf filename=2404.02765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of the Internet of Bio-Nano Things (IoBNT), nano-devices are envisioned to perform complex tasks collaboratively, i.e., by communicating with each other. One candidate for the implementation of such devices are engineered cells due to their inherent biocompatibility. However, because each engineered cell has only little computational capabilities, transmitter and receiver (RX) functionalities can afford only limited complexity. In this paper, we propose a simple, yet modular, architecture for a cellular RX that is capable of processing a stream of observed symbols using chemical reaction networks. Furthermore, we propose two specific detector implementations for the RX. The first detector is based on a machine learning model that is trained offline, i.e., before the cellular RX is deployed. The second detector utilizes pilot symbol-based training and is therefore able to continuously adapt to changing channel conditions online, i.e., after deployment. To coordinate the different chemical processing steps involved in symbol detection, the proposed cellular RX leverages an internal chemical timer. Furthermore, the RX is synchronized with the transmitter via external, i.e., extracellular, signals. Finally, the proposed architecture is validated using theoretical analysis and stochastic <b>simulations.</b> The presented results confirm the feasibility of both proposed implementations and reveal that the proposed online learning-based RX is able to perform reliable detection even in initially unknown or slowly changing channels. By its modular design and exclusively chemical implementation, the proposed RX contributes towards the realization of versatile and biocompatible nano-scale communication networks for IoBNT applications narrowing the existing implementation gap in cellular molecular communication (MC).</p></p class="citation"></blockquote><h2 id=econgn-1>econ.GN (1)</h2><h3 id=11--248258-karma-an-experimental-study-ezzat-elokda-et-al-2024>(1/1 | 248/258) Karma: An Experimental Study (Ezzat Elokda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ezzat Elokda, Heinrich Nax, Saverio Bolognani, Florian Dörfler. (2024)<br><strong>Karma: An Experimental Study</strong><br><button class=copy-to-clipboard title="Karma: An Experimental Study" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-GT, cs-SY, econ-GN, econ.GN, eess-SY, q-fin-EC<br>Keyword Score: 20<br>Keywords: Fairness, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02687v1.pdf filename=2404.02687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A system of non-tradable credits that flow between individuals like karma, hence proposed under that name, is a mechanism for repeated resource allocation that comes with attractive efficiency and <b>fairness</b> properties, in theory. In this study, we test karma in an online experiment in which human subjects repeatedly compete for a resource with time-varying and stochastic individual preferences or urgency to acquire the resource. We confirm that karma has significant and sustained welfare benefits even in a population with no prior training. We identify mechanism usage in contexts with sporadic high urgency, more so than with frequent moderate urgency, and implemented as an easy (binary) karma bidding scheme as particularly effective for welfare improvements: relatively larger aggregate efficiency gains are realized that are (almost) Pareto superior. These findings provide guidance for further testing and for future implementation plans of such mechanisms in the real world.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--249258-solar-synthetic-imaging-introducing-denoising-diffusion-probabilistic-models-on-sdoaia-data-francesco-p-ramunno-et-al-2024>(1/1 | 249/258) Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data (Francesco P. Ramunno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco P. Ramunno, S. Hackstein, V. Kinakh, M. Drozdova, G. Quetant, A. Csillaghy, S. Voloshynovskiy. (2024)<br><strong>Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data</strong><br><button class=copy-to-clipboard title="Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-IM, astro-ph-SR, astro-ph.SR, cs-AI<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02552v1.pdf filename=2404.02552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the rarity of significant solar flares compared to smaller ones, training effective machine learning models for solar activity forecasting is challenging due to insufficient data. This study proposes using generative deep learning models, specifically a Denoising Diffusion <b>Probabilistic</b> <b>Model</b> (DDPM), to create synthetic images of solar phenomena, including flares of varying intensities. By employing a dataset from the AIA instrument aboard the SDO spacecraft, focusing on the 171 {\AA} band that captures various solar activities, and classifying images with GOES X-ray measurements based on flare intensity, we aim to address the data scarcity issue. The DDPM&rsquo;s performance is evaluated using cluster metrics, Frechet Inception Distance (FID), and F1-score, showcasing promising results in generating realistic solar imagery. We conduct two experiments: one to train a <b>supervised</b> classifier for event identification and another for basic flare prediction, demonstrating the value of synthetic data in managing imbalanced datasets. This research underscores the potential of DDPMs in solar data analysis and forecasting, suggesting further exploration into their capabilities for solar flare prediction and application in other deep learning and physical tasks.</p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--250258-convergence-analysis-of-flow-matching-in-latent-space-with-transformers-yuling-jiao-et-al-2024>(1/2 | 250/258) Convergence Analysis of Flow Matching in Latent Space with Transformers (Yuling Jiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuling Jiao, Yanming Lai, Yang Wang, Bokai Yan. (2024)<br><strong>Convergence Analysis of Flow Matching in Latent Space with Transformers</strong><br><button class=copy-to-clipboard title="Convergence Analysis of Flow Matching in Latent Space with Transformers" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02538v1.pdf filename=2404.02538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained <b>autoencoder</b> network to map high-dimensional original inputs to a low-dimensional latent space, where a <b>transformer</b> network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by <b>transformer</b> networks with Lipschitz continuity, which may be of independent interest.</p></p class="citation"></blockquote><h3 id=22--251258-gaussian-process-regression-with-soft-inequality-and-monotonicity-constraints-didem-kochan-et-al-2024>(2/2 | 251/258) Gaussian Process Regression with Soft Inequality and Monotonicity Constraints (Didem Kochan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Didem Kochan, Xiu Yang. (2024)<br><strong>Gaussian Process Regression with Soft Inequality and Monotonicity Constraints</strong><br><button class=copy-to-clipboard title="Gaussian Process Regression with Soft Inequality and Monotonicity Constraints" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02873v1.pdf filename=2404.02873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Gaussian</b> <b>process</b> (GP) regression is a non-parametric, Bayesian framework to approximate complex models. Standard GP regression can lead to an unbounded model in which some points can take infeasible values. We introduce a new GP method that enforces the physical constraints in a probabilistic manner. This GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC). QHMC is an efficient way to sample from a broad class of distributions. Unlike the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed mass, QHMC allows a particle to have a random mass matrix with a probability distribution. Introducing the QHMC method to the inequality and monotonicity constrained GP regression in the probabilistic sense, our approach improves the accuracy and reduces the variance in the resulting GP model. According to our experiments on several datasets, the proposed approach serves as an efficient method as it accelerates the sampling process while maintaining the accuracy, and it is applicable to high dimensional problems.</p></p class="citation"></blockquote><h2 id=physicsdata-an-1>physics.data-an (1)</h2><h3 id=11--252258-an-inversion-problem-for-optical-spectrum-data-via-physics-guided-machine-learning-hwiwoo-park-et-al-2024>(1/1 | 252/258) An inversion problem for optical spectrum data via physics-guided machine learning (Hwiwoo Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hwiwoo Park, Jun H. Park, Jungseek Hwang. (2024)<br><strong>An inversion problem for optical spectrum data via physics-guided machine learning</strong><br><button class=copy-to-clipboard title="An inversion problem for optical spectrum data via physics-guided machine learning" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.data-an<br>Categories: cond-mat-str-el, cs-LG, physics-comp-ph, physics-data-an, physics.data-an<br>Keyword Score: 20<br>Keywords: Out-of-distribution, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02387v1.pdf filename=2404.02387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose the regularized recurrent inference machine (rRIM), a novel machine-learning approach to solve the challenging problem of deriving the pairing <b>glue</b> function from measured optical spectra. The rRIM incorporates physical principles into both training and inference and affords noise robustness, flexibility with <b>out-of-distribution</b> data, and reduced data requirements. It effectively obtains reliable pairing <b>glue</b> functions from experimental optical spectra and yields promising solutions for similar inverse problems of the Fredholm integral equation of the first kind.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--253258-automated-inference-of-graph-transformation-rules-jakob-l-andersen-et-al-2024>(1/1 | 253/258) Automated Inference of Graph Transformation Rules (Jakob L. Andersen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakob L. Andersen, Akbar Davoodi, Rolf Fagerberg, Christoph Flamm, Walter Fontana, Juri Kolčák, Christophe V. F. P. Laurent, Daniel Merkle, Nikolai Nøjgaard. (2024)<br><strong>Automated Inference of Graph Transformation Rules</strong><br><button class=copy-to-clipboard title="Automated Inference of Graph Transformation Rules" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs-LG, cs.DM, q-bio-MN<br>Keyword Score: 13<br>Keywords: Graph, Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02692v1.pdf filename=2404.02692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The explosion of data available in life sciences is fueling an increasing demand for expressive <b>models</b> <b>and</b> computational methods. <b>Graph</b> transformation is a <b>model</b> <b>for</b> dynamic systems with a large variety of applications. We introduce a novel method of the <b>graph</b> transformation <b>model</b> <b>construction,</b> combining generative and dynamical viewpoints to give a fully automated data-driven <b>model</b> <b>inference</b> method. The method takes the input dynamical properties, given as a &ldquo;snapshot&rdquo; of the dynamics encoded by explicit transitions, and constructs a compatible <b>model.</b> <b>The</b> obtained <b>model</b> <b>is</b> guaranteed to be minimal, thus framing the approach as <b>model</b> <b>compression</b> (from a set of transitions into a set of rules). The compression is permissive to a lossy case, where the constructed <b>model</b> <b>is</b> allowed to exhibit behavior outside of the input transitions, thus suggesting a completion of the input dynamics. The task of <b>graph</b> transformation <b>model</b> <b>inference</b> is naturally highly challenging due to the combinatorics involved. We tackle the exponential explosion by proposing a heuristically minimal translation of the task into a well-established problem, set cover, for which highly optimized solutions exist. We further showcase how our results relate to Kolmogorov complexity expressed in terms of <b>graph</b> transformation.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--254258-qfnn-ffd-quantum-federated-neural-network-for-financial-fraud-detection-nouhaila-innan-et-al-2024>(1/2 | 254/258) QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection (Nouhaila Innan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nouhaila Innan, Alberto Marchisio, Muhammad Shafique, Mohamed Bennai. (2024)<br><strong>QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection</strong><br><button class=copy-to-clipboard title="QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, q-fin-RM, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02595v1.pdf filename=2404.02595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces the Quantum <b>Federated</b> <b>Neural</b> Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with <b>Federated</b> <b>Learning</b> (FL) to innovate financial fraud detection. Using quantum technologies&rsquo; computational power and FL&rsquo;s data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.</p></p class="citation"></blockquote><h3 id=22--255258-scalable-quantum-detector-tomography-by-high-performance-computing-timon-schapeler-et-al-2024>(2/2 | 255/258) Scalable quantum detector tomography by high-performance computing (Timon Schapeler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timon Schapeler, Robert Schade, Michael Lass, Christian Plessl, Tim J. Bartley. (2024)<br><strong>Scalable quantum detector tomography by high-performance computing</strong><br><button class=copy-to-clipboard title="Scalable quantum detector tomography by high-performance computing" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DC, quant-ph, quant-ph<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02844v1.pdf filename=2404.02844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>At large scales, quantum systems may become advantageous over their classical counterparts at performing certain tasks. Developing tools to analyse these systems at the relevant scales, in a manner consistent with quantum mechanics, is therefore critical to <b>benchmarking</b> performance and characterising their operation. While classical computational approaches cannot perform like-for-like computations of quantum systems beyond a certain scale, classical high-performance computing (HPC) may nevertheless be useful for precisely these characterisation and certification tasks. By developing open-source customised algorithms using high-performance computing, we perform quantum tomography on a megascale quantum photonic detector covering a Hilbert space of $10^6$. This requires finding $10^8$ elements of the matrix corresponding to the positive operator valued measure (POVM), the quantum description of the detector, and is achieved in minutes of computation time. Moreover, by exploiting the structure of the problem, we achieve highly efficient parallel scaling, paving the way for quantum objects up to a system size of $10^{12}$ elements to be reconstructed using this method. In general, this shows that a consistent quantum mechanical description of quantum phenomena is applicable at everyday scales. More concretely, this enables the reconstruction of large-scale quantum sources, processes and detectors used in computation and sampling tasks, which may be necessary to prove their nonclassical character or quantum computational advantage.</p></p class="citation"></blockquote><h2 id=mathct-1>math.CT (1)</h2><h3 id=11--256258-rendering-string-diagrams-recursively-celia-rubio-madrigal-et-al-2024>(1/1 | 256/258) Rendering string diagrams recursively (Celia Rubio-Madrigal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Celia Rubio-Madrigal, Jules Hedges. (2024)<br><strong>Rendering string diagrams recursively</strong><br><button class=copy-to-clipboard title="Rendering string diagrams recursively" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CT<br>Categories: cs-CG, math-CT, math.CT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02679v1.pdf filename=2404.02679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>String diagrams are a graphical language used to represent processes that can be composed sequentially or in parallel, which correspond graphically to horizontal or vertical juxtaposition. In this paper we demonstrate how to compute the layout of a string diagram by folding over its algebraic representation in terms of sequential and parallel composition operators. The algebraic representation can be seen as a term of a free monoidal category or a proof tree for a small fragment of linear logic. This contrasts to existing non-compositional approaches that use <b>graph</b> layout techniques. The key innovation is storing the diagrams in binary space-partition trees, maintaining a right-trapezoidal shape for the diagram&rsquo;s outline as an invariant. We provide an implementation in Haskell, using an existing denotational graphics library called Diagrams. Our renderer also supports adding semantics to diagrams to serve as a compiler, with matrix algebra used as an example.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--257258-degree-sequence-optimization-and-extremal-degree-enumerators-shmuel-onn-2024>(1/2 | 257/258) Degree Sequence Optimization and Extremal Degree Enumerators (Shmuel Onn, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shmuel Onn. (2024)<br><strong>Degree Sequence Optimization and Extremal Degree Enumerators</strong><br><button class=copy-to-clipboard title="Degree Sequence Optimization and Extremal Degree Enumerators" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C, cs-DM, cs-DS, math-CO, math-OC, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02551v1.pdf filename=2404.02551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The degree sequence optimization problem is to find a subgraph of a given <b>graph</b> which maximizes the sum of given functions evaluated at the subgraph degrees. Here we study this problem by replacing degree sequences, via suitable nonlinear transformations, by suitable degree enumerators, and we introduce suitable degree enumerator polytopes. We characterize their vertices, that is, the extremal degree enumerators, for complete <b>graphs</b> and some complete bipartite <b>graphs,</b> and use these characterizations to obtain simpler and faster algorithms for optimization over degree sequences for such <b>graphs.</b></p></p class="citation"></blockquote><h3 id=22--258258-grid-drawings-of-graphs-in-three-dimensions-jozsef-balogh-et-al-2024>(2/2 | 258/258) Grid-drawings of graphs in three-dimensions (Jozsef Balogh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jozsef Balogh, Ethan Patrick White. (2024)<br><strong>Grid-drawings of graphs in three-dimensions</strong><br><button class=copy-to-clipboard title="Grid-drawings of graphs in three-dimensions" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 68R10, cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.02369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.02369v1.pdf filename=2404.02369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using probabilistic methods, we obtain grid-drawings of <b>graphs</b> without crossings with low volume and small aspect ratio. We show that every $D$-degenerate <b>graph</b> on $n$ vertices can be drawn in $[m]^3$ where $m = O(D^{5/3} n^{1/3}\log^{4/3}n)$. In particular, every <b>graph</b> of bounded maximum degree can be drawn in a grid with volume $O(n \log^{4}n)$.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.04</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.06</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-60>cs.CL (60)</a><ul><li><a href=#160--1258-gpt-detox-an-in-context-learning-based-paraphraser-for-text-detoxification-ali-pesaranghader-et-al-2024>(1/60 | 1/258) GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification (Ali Pesaranghader et al., 2024)</a></li><li><a href=#260--2258-utebc-nlp-at-semeval-2024-task-9-can-llms-be-lateral-thinkers-pouya-sadeghi-et-al-2024>(2/60 | 2/258) uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers? (Pouya Sadeghi et al., 2024)</a></li><li><a href=#360--3258-benchmarking-large-language-models-for-persian-a-preliminary-study-focusing-on-chatgpt-amirhossein-abaskohi-et-al-2024>(3/60 | 3/258) Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT (Amirhossein Abaskohi et al., 2024)</a></li><li><a href=#460--4258-enhancing-low-resource-llms-classification-with-peft-and-synthetic-data-parth-patwa-et-al-2024>(4/60 | 4/258) Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data (Parth Patwa et al., 2024)</a></li><li><a href=#560--5258-an-incomplete-loop-deductive-inductive-and-abductive-learning-in-large-language-models-emmy-liu-et-al-2024>(5/60 | 5/258) An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models (Emmy Liu et al., 2024)</a></li><li><a href=#660--6258-towards-large-language-model-driven-reference-less-translation-evaluation-for-english-and-indian-languages-vandan-mujadia-et-al-2024>(6/60 | 6/258) Towards Large Language Model driven Reference-less Translation Evaluation for English and Indian Languages (Vandan Mujadia et al., 2024)</a></li><li><a href=#760--7258-low-resource-neural-machine-translation-with-morphological-modeling-antoine-nzeyimana-2024>(7/60 | 7/258) Low-resource neural machine translation with morphological modeling (Antoine Nzeyimana, 2024)</a></li><li><a href=#860--8258-fpt-feature-prompt-tuning-for-few-shot-readability-assessment-ziyang-wang-et-al-2024>(8/60 | 8/258) FPT: Feature Prompt Tuning for Few-shot Readability Assessment (Ziyang Wang et al., 2024)</a></li><li><a href=#960--9258-rethinking-kullback-leibler-divergence-in-knowledge-distillation-for-large-language-models-taiqiang-wu-et-al-2024>(9/60 | 9/258) Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models (Taiqiang Wu et al., 2024)</a></li><li><a href=#1060--10258-knowhalu-hallucination-detection-via-multi-form-knowledge-based-factual-checking-jiawei-zhang-et-al-2024>(10/60 | 10/258) KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking (Jiawei Zhang et al., 2024)</a></li><li><a href=#1160--11258-conifer-improving-complex-constrained-instruction-following-ability-of-large-language-models-haoran-sun-et-al-2024>(11/60 | 11/258) Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models (Haoran Sun et al., 2024)</a></li><li><a href=#1260--12258-adaptive-cross-lingual-text-classification-through-in-context-one-shot-demonstrations-emilio-villa-cueva-et-al-2024>(12/60 | 12/258) Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations (Emilio Villa-Cueva et al., 2024)</a></li><li><a href=#1360--13258-on-linearizing-structured-data-in-encoder-decoder-language-models-insights-from-text-to-sql-yutong-shao-et-al-2024>(13/60 | 13/258) On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL (Yutong Shao et al., 2024)</a></li><li><a href=#1460--14258-automatic-prompt-selection-for-large-language-models-viet-tung-do-et-al-2024>(14/60 | 14/258) Automatic Prompt Selection for Large Language Models (Viet-Tung Do et al., 2024)</a></li><li><a href=#1560--15258-pejorativity-disambiguating-pejorative-epithets-to-improve-misogyny-detection-in-italian-tweets-arianna-muti-et-al-2024>(15/60 | 15/258) PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny Detection in Italian Tweets (Arianna Muti et al., 2024)</a></li><li><a href=#1660--16258-large-language-models-for-expansion-of-spoken-language-understanding-systems-to-new-languages-jakub-hoscilowicz-et-al-2024>(16/60 | 16/258) Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages (Jakub Hoscilowicz et al., 2024)</a></li><li><a href=#1760--17258-mai-hoomāuna-i-ka-ai-language-models-improve-automatic-speech-recognition-in-hawaiian-kaavya-chaparala-et-al-2024>(17/60 | 17/258) Mai Ho&rsquo;omāuna i ka &lsquo;Ai: Language Models Improve Automatic Speech Recognition in Hawaiian (Kaavya Chaparala et al., 2024)</a></li><li><a href=#1860--18258-cherry-on-top-parameter-heterogeneity-and-quantization-in-large-language-models-wanyun-cui-et-al-2024>(18/60 | 18/258) Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models (Wanyun Cui et al., 2024)</a></li><li><a href=#1960--19258-retrieving-examples-from-memory-for-retrieval-augmented-neural-machine-translation-a-systematic-comparison-maxime-bouthors-et-al-2024>(19/60 | 19/258) Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison (Maxime Bouthors et al., 2024)</a></li><li><a href=#2060--20258-affective-nli-towards-accurate-and-interpretable-personality-recognition-in-conversation-zhiyuan-wen-et-al-2024>(20/60 | 20/258) Affective-NLI: Towards Accurate and Interpretable Personality Recognition in Conversation (Zhiyuan Wen et al., 2024)</a></li><li><a href=#2160--21258-angofa-leveraging-ofa-embedding-initialization-and-synthetic-data-for-angolan-language-model-osvaldo-luamba-quinjica-et-al-2024>(21/60 | 21/258) ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model (Osvaldo Luamba Quinjica et al., 2024)</a></li><li><a href=#2260--22258-optical-text-recognition-in-nepali-and-bengali-a-transformer-based-approach-s-m-rakib-hasan-et-al-2024>(22/60 | 22/258) Optical Text Recognition in Nepali and Bengali: A Transformer-based Approach (S M Rakib Hasan et al., 2024)</a></li><li><a href=#2360--23258-bcamirs-at-semeval-2024-task-4-beyond-words-a-multimodal-and-multilingual-exploration-of-persuasion-in-memes-amirhossein-abaskohi-et-al-2024>(23/60 | 23/258) BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and Multilingual Exploration of Persuasion in Memes (Amirhossein Abaskohi et al., 2024)</a></li><li><a href=#2460--24258-cseprompts-a-benchmark-of-introductory-computer-science-prompts-nishat-raihan-et-al-2024>(24/60 | 24/258) CSEPrompts: A Benchmark of Introductory Computer Science Prompts (Nishat Raihan et al., 2024)</a></li><li><a href=#2560--25258-chatglm-math-improving-math-problem-solving-in-large-language-models-with-a-self-critique-pipeline-yifan-xu-et-al-2024>(25/60 | 25/258) ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline (Yifan Xu et al., 2024)</a></li><li><a href=#2660--26258-multi-granularity-guided-fusion-in-decoder-eunseong-choi-et-al-2024>(26/60 | 26/258) Multi-Granularity Guided Fusion-in-Decoder (Eunseong Choi et al., 2024)</a></li><li><a href=#2760--27258-cmulab-an-open-source-framework-for-training-and-deployment-of-natural-language-processing-models-zaid-sheikh-et-al-2024>(27/60 | 27/258) CMULAB: An Open-Source Framework for Training and Deployment of Natural Language Processing Models (Zaid Sheikh et al., 2024)</a></li><li><a href=#2860--28258-greedllama-performance-of-financial-value-aligned-large-language-models-in-moral-reasoning-jeffy-yu-et-al-2024>(28/60 | 28/258) GreedLlama: Performance of Financial Value-Aligned Large Language Models in Moral Reasoning (Jeffy Yu et al., 2024)</a></li><li><a href=#2960--29258-a-differentiable-integer-linear-programming-solver-for-explanation-based-natural-language-inference-mokanarangan-thayaparan-et-al-2024>(29/60 | 29/258) A Differentiable Integer Linear Programming Solver for Explanation-Based Natural Language Inference (Mokanarangan Thayaparan et al., 2024)</a></li><li><a href=#3060--30258-cross-architecture-transfer-learning-for-linear-cost-inference-transformers-sehyun-choi-2024>(30/60 | 30/258) Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers (Sehyun Choi, 2024)</a></li><li><a href=#3160--31258-estimating-the-causal-effects-of-natural-logic-features-in-transformer-based-nli-models-julia-rozanova-et-al-2024>(31/60 | 31/258) Estimating the Causal Effects of Natural Logic Features in Transformer-Based NLI Models (Julia Rozanova et al., 2024)</a></li><li><a href=#3260--32258-measuring-social-norms-of-large-language-models-ye-yuan-et-al-2024>(32/60 | 32/258) Measuring Social Norms of Large Language Models (Ye Yuan et al., 2024)</a></li><li><a href=#3360--33258-prompting-for-numerical-sequences-a-case-study-on-market-comment-generation-masayuki-kawarada-et-al-2024>(33/60 | 33/258) Prompting for Numerical Sequences: A Case Study on Market Comment Generation (Masayuki Kawarada et al., 2024)</a></li><li><a href=#3460--34258-exploring-the-trade-off-between-model-performance-and-explanation-plausibility-of-text-classifiers-using-human-rationales-lucas-e-resck-et-al-2024>(34/60 | 34/258) Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales (Lucas E. Resck et al., 2024)</a></li><li><a href=#3560--35258-blessing-or-curse-a-survey-on-the-impact-of-generative-ai-on-fake-news-alexander-loth-et-al-2024>(35/60 | 35/258) Blessing or curse? A survey on the Impact of Generative AI on Fake News (Alexander Loth et al., 2024)</a></li><li><a href=#3660--36258-leveraging-the-interplay-between-syntactic-and-acoustic-cues-for-optimizing-korean-tts-pause-formation-yejin-jeon-et-al-2024>(36/60 | 36/258) Leveraging the Interplay Between Syntactic and Acoustic Cues for Optimizing Korean TTS Pause Formation (Yejin Jeon et al., 2024)</a></li><li><a href=#3760--37258-language-models-as-compilers-simulating-pseudocode-execution-improves-algorithmic-reasoning-in-language-models-hyungjoo-chae-et-al-2024>(37/60 | 37/258) Language Models as Compilers: Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models (Hyungjoo Chae et al., 2024)</a></li><li><a href=#3860--38258-mainlp-at-semeval-2024-task-1-analyzing-source-language-selection-in-cross-lingual-textual-relatedness-shijia-zhou-et-al-2024>(38/60 | 38/258) MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness (Shijia Zhou et al., 2024)</a></li><li><a href=#3960--39258-enhancing-cross-lingual-sentence-embedding-for-low-resource-languages-with-word-alignment-zhongtao-miao-et-al-2024>(39/60 | 39/258) Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment (Zhongtao Miao et al., 2024)</a></li><li><a href=#4060--40258-on-the-multilingual-ability-of-decoder-based-pre-trained-language-models-finding-and-controlling-language-specific-neurons-takeshi-kojima-et-al-2024>(40/60 | 40/258) On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons (Takeshi Kojima et al., 2024)</a></li><li><a href=#4160--41258-revisiting-subword-tokenization-a-case-study-on-affixal-negation-in-large-language-models-thinh-hung-truong-et-al-2024>(41/60 | 41/258) Revisiting subword tokenization: A case study on affixal negation in large language models (Thinh Hung Truong et al., 2024)</a></li><li><a href=#4260--42258-token-trails-navigating-contextual-depths-in-conversational-ai-with-chatllm-md-kowsher-et-al-2024>(42/60 | 42/258) Token Trails: Navigating Contextual Depths in Conversational AI with ChatLLM (Md. Kowsher et al., 2024)</a></li><li><a href=#4360--43258-backdoor-attack-on-multilingual-machine-translation-jun-wang-et-al-2024>(43/60 | 43/258) Backdoor Attack on Multilingual Machine Translation (Jun Wang et al., 2024)</a></li><li><a href=#4460--44258-construction-of-functional-materials-knowledge-graph-in-multidisciplinary-materials-science-via-large-language-model-yanpeng-ye-et-al-2024>(44/60 | 44/258) Construction of Functional Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model (Yanpeng Ye et al., 2024)</a></li><li><a href=#4560--45258-mulan-a-study-of-fact-mutability-in-language-models-constanza-fierro-et-al-2024>(45/60 | 45/258) MuLan: A Study of Fact Mutability in Language Models (Constanza Fierro et al., 2024)</a></li><li><a href=#4660--46258-phonologybench-evaluating-phonological-skills-of-large-language-models-ashima-suvarna-et-al-2024>(46/60 | 46/258) PhonologyBench: Evaluating Phonological Skills of Large Language Models (Ashima Suvarna et al., 2024)</a></li><li><a href=#4760--47258-min-k-improved-baseline-for-detecting-pre-training-data-from-large-language-models-jingyang-zhang-et-al-2024>(47/60 | 47/258) Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models (Jingyang Zhang et al., 2024)</a></li><li><a href=#4860--48258-unsupervised-bottom-up-category-discovery-for-symbol-grounding-with-a-curious-robot-catherine-henry-et-al-2024>(48/60 | 48/258) Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a Curious Robot (Catherine Henry et al., 2024)</a></li><li><a href=#4960--49258-towards-a-fully-interpretable-and-more-scalable-rsa-model-for-metaphor-understanding-gaia-carenini-et-al-2024>(49/60 | 49/258) Towards a Fully Interpretable and More Scalable RSA Model for Metaphor Understanding (Gaia Carenini et al., 2024)</a></li><li><a href=#5060--50258-on-few-shot-prompting-for-controllable-question-answer-generation-in-narrative-comprehension-bernardo-leite-et-al-2024>(50/60 | 50/258) On Few-Shot Prompting for Controllable Question-Answer Generation in Narrative Comprehension (Bernardo Leite et al., 2024)</a></li><li><a href=#5160--51258-calibrating-the-confidence-of-large-language-models-by-eliciting-fidelity-mozhi-zhang-et-al-2024>(51/60 | 51/258) Calibrating the Confidence of Large Language Models by Eliciting Fidelity (Mozhi Zhang et al., 2024)</a></li><li><a href=#5260--52258-lifelong-event-detection-with-embedding-space-separation-and-compaction-chengwei-qin-et-al-2024>(52/60 | 52/258) Lifelong Event Detection with Embedding Space Separation and Compaction (Chengwei Qin et al., 2024)</a></li><li><a href=#5360--53258-dynamic-demonstration-retrieval-and-cognitive-understanding-for-emotional-support-conversation-zhe-xu-et-al-2024>(53/60 | 53/258) Dynamic Demonstration Retrieval and Cognitive Understanding for Emotional Support Conversation (Zhe Xu et al., 2024)</a></li><li><a href=#5460--54258-the-promises-and-pitfalls-of-using-language-models-to-measure-instruction-quality-in-education-paiheng-xu-et-al-2024>(54/60 | 54/258) The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education (Paiheng Xu et al., 2024)</a></li><li><a href=#5560--55258-from-narratives-to-numbers-valid-inference-using-language-model-predictions-from-verbal-autopsy-narratives-shuxian-fan-et-al-2024>(55/60 | 55/258) From Narratives to Numbers: Valid Inference Using Language Model Predictions from Verbal Autopsy Narratives (Shuxian Fan et al., 2024)</a></li><li><a href=#5660--56258-language-environment-and-robotic-navigation-johnathan-e-avery-2024>(56/60 | 56/258) Language, Environment, and Robotic Navigation (Johnathan E. Avery, 2024)</a></li><li><a href=#5760--57258-aqua----combining-experts-and-non-experts-views-to-assess-deliberation-quality-in-online-discussions-using-llms-maike-behrendt-et-al-2024>(57/60 | 57/258) AQuA &ndash; Combining Experts&rsquo; and Non-Experts&rsquo; Views To Assess Deliberation Quality in Online Discussions Using LLMs (Maike Behrendt et al., 2024)</a></li><li><a href=#5860--58258-scalable-model-editing-via-customized-expert-networks-zihan-yao-et-al-2024>(58/60 | 58/258) Scalable Model Editing via Customized Expert Networks (Zihan Yao et al., 2024)</a></li><li><a href=#5960--59258-a-school-student-essay-corpus-for-analyzing-interactions-of-argumentative-structure-and-quality-maja-stahl-et-al-2024>(59/60 | 59/258) A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality (Maja Stahl et al., 2024)</a></li><li><a href=#6060--60258-auxiliary-task-demands-mask-the-capabilities-of-smaller-language-models-jennifer-hu-et-al-2024>(60/60 | 60/258) Auxiliary task demands mask the capabilities of smaller language models (Jennifer Hu et al., 2024)</a></li></ul></li><li><a href=#cscv-50>cs.CV (50)</a><ul><li><a href=#150--61258-deit-lt-distillation-strikes-back-for-vision-transformer-training-on-long-tailed-datasets-harsh-rangwani-et-al-2024>(1/50 | 61/258) DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets (Harsh Rangwani et al., 2024)</a></li><li><a href=#250--62258-multi-scale-spatial-temporal-self-attention-graph-convolutional-networks-for-skeleton-based-action-recognition-ikuo-nakamura-2024>(2/50 | 62/258) Multi-Scale Spatial-Temporal Self-Attention Graph Convolutional Networks for Skeleton-based Action Recognition (Ikuo Nakamura, 2024)</a></li><li><a href=#350--63258-enhancing-human-computer-interaction-in-chest-x-ray-analysis-using-vision-and-language-model-with-eye-gaze-patterns-yunsoo-kim-et-al-2024>(3/50 | 63/258) Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns (Yunsoo Kim et al., 2024)</a></li><li><a href=#450--64258-asap-interpretable-analysis-and-summarization-of-ai-generated-image-patterns-at-scale-jinbin-huang-et-al-2024>(4/50 | 64/258) ASAP: Interpretable Analysis and Summarization of AI-generated Image Patterns at Scale (Jinbin Huang et al., 2024)</a></li><li><a href=#550--65258-weakly-supervised-3d-scene-graph-generation-via-visual-linguistic-assisted-pseudo-labeling-xu-wang-et-al-2024>(5/50 | 65/258) Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling (Xu Wang et al., 2024)</a></li><li><a href=#650--66258-on-the-scalability-of-diffusion-based-text-to-image-generation-hao-li-et-al-2024>(6/50 | 66/258) On the Scalability of Diffusion-based Text-to-Image Generation (Hao Li et al., 2024)</a></li><li><a href=#750--67258-a-unified-membership-inference-method-for-visual-self-supervised-encoder-via-part-aware-capability-jie-zhu-et-al-2024>(7/50 | 67/258) A Unified Membership Inference Method for Visual Self-supervised Encoder via Part-aware Capability (Jie Zhu et al., 2024)</a></li><li><a href=#850--68258-visual-autoregressive-modeling-scalable-image-generation-via-next-scale-prediction-keyu-tian-et-al-2024>(8/50 | 68/258) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction (Keyu Tian et al., 2024)</a></li><li><a href=#950--69258-viassist-adapting-multi-modal-large-language-models-for-users-with-visual-impairments-bufang-yang-et-al-2024>(9/50 | 69/258) VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments (Bufang Yang et al., 2024)</a></li><li><a href=#1050--70258-scaling-laws-for-galaxy-images-mike-walmsley-et-al-2024>(10/50 | 70/258) Scaling Laws for Galaxy Images (Mike Walmsley et al., 2024)</a></li><li><a href=#1150--71258-knowledge-distillation-with-multi-granularity-mixture-of-priors-for-image-super-resolution-simiao-li-et-al-2024>(11/50 | 71/258) Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution (Simiao Li et al., 2024)</a></li><li><a href=#1250--72258-rs3mamba-visual-state-space-model-for-remote-sensing-images-semantic-segmentation-xianping-ma-et-al-2024>(12/50 | 72/258) RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation (Xianping Ma et al., 2024)</a></li><li><a href=#1350--73258-aloha-a-new-measure-for-hallucination-in-captioning-models-suzanne-petryk-et-al-2024>(13/50 | 73/258) ALOHa: A New Measure for Hallucination in Captioning Models (Suzanne Petryk et al., 2024)</a></li><li><a href=#1450--74258-matatlas-text-driven-consistent-geometry-texturing-and-material-assignment-duygu-ceylan-et-al-2024>(14/50 | 74/258) MatAtlas: Text-driven Consistent Geometry Texturing and Material Assignment (Duygu Ceylan et al., 2024)</a></li><li><a href=#1550--75258-lvlm-intrepret-an-interpretability-tool-for-large-vision-language-models-gabriela-ben-melech-stan-et-al-2024>(15/50 | 75/258) LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models (Gabriela Ben Melech Stan et al., 2024)</a></li><li><a href=#1650--76258-what-are-we-measuring-when-we-evaluate-large-vision-language-models-an-analysis-of-latent-factors-and-biases-anthony-meng-huat-tiong-et-al-2024>(16/50 | 76/258) What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases (Anthony Meng Huat Tiong et al., 2024)</a></li><li><a href=#1750--77258-lidardm-generative-lidar-simulation-in-a-generated-world-vlas-zyrianov-et-al-2024>(17/50 | 77/258) LidarDM: Generative LiDAR Simulation in a Generated World (Vlas Zyrianov et al., 2024)</a></li><li><a href=#1850--78258-mulan-a-multi-layer-annotated-dataset-for-controllable-text-to-image-generation-petru-daniel-tudosiu-et-al-2024>(18/50 | 78/258) MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation (Petru-Daniel Tudosiu et al., 2024)</a></li><li><a href=#1950--79258-adaptive-affinity-based-generalization-for-mri-imaging-segmentation-across-resource-limited-settings-eddardaa-b-loussaief-et-al-2024>(19/50 | 79/258) Adaptive Affinity-Based Generalization For MRI Imaging Segmentation Across Resource-Limited Settings (Eddardaa B. Loussaief et al., 2024)</a></li><li><a href=#2050--80258-rs-mamba-for-large-remote-sensing-image-dense-prediction-sijie-zhao-et-al-2024>(20/50 | 80/258) RS-Mamba for Large Remote Sensing Image Dense Prediction (Sijie Zhao et al., 2024)</a></li><li><a href=#2150--81258-non-negative-subspace-feature-representation-for-few-shot-learning-in-medical-imaging-keqiang-fan-et-al-2024>(21/50 | 81/258) Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging (Keqiang Fan et al., 2024)</a></li><li><a href=#2250--82258-diffexplainer-towards-cross-modal-global-explanations-with-diffusion-models-matteo-pennisi-et-al-2024>(22/50 | 82/258) Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models (Matteo Pennisi et al., 2024)</a></li><li><a href=#2350--83258-salfom-dynamic-saliency-prediction-with-video-foundation-models-morteza-moradi-et-al-2024>(23/50 | 83/258) SalFoM: Dynamic Saliency Prediction with Video Foundation Models (Morteza Moradi et al., 2024)</a></li><li><a href=#2450--84258-flightscope-a-deep-comprehensive-assessment-of-aircraft-detection-algorithms-in-satellite-imagery-safouane-el-ghazouali-et-al-2024>(24/50 | 84/258) FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery (Safouane El Ghazouali et al., 2024)</a></li><li><a href=#2550--85258-active-learning-for-efficient-annotation-in-precision-agriculture-a-use-case-on-crop-weed-semantic-segmentation-bart-m-van-marrewijk-et-al-2024>(25/50 | 85/258) Active learning for efficient annotation in precision agriculture: a use-case on crop-weed semantic segmentation (Bart M. van Marrewijk et al., 2024)</a></li><li><a href=#2650--86258-semi-supervised-unconstrained-head-pose-estimation-in-the-wild-huayi-zhou-et-al-2024>(26/50 | 86/258) Semi-Supervised Unconstrained Head Pose Estimation in the Wild (Huayi Zhou et al., 2024)</a></li><li><a href=#2750--87258-te-tad-towards-full-end-to-end-temporal-action-detection-via-time-aligned-coordinate-expression-ho-joong-kim-et-al-2024>(27/50 | 87/258) TE-TAD: Towards Full End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression (Ho-Joong Kim et al., 2024)</a></li><li><a href=#2850--88258-many-to-many-image-generation-with-auto-regressive-diffusion-models-ying-shen-et-al-2024>(28/50 | 88/258) Many-to-many Image Generation with Auto-regressive Diffusion Models (Ying Shen et al., 2024)</a></li><li><a href=#2950--89258-dpft-dual-perspective-fusion-transformer-for-camera-radar-based-object-detection-felix-fent-et-al-2024>(29/50 | 89/258) DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection (Felix Fent et al., 2024)</a></li><li><a href=#3050--90258-genn2n-generative-nerf2nerf-translation-xiangyue-liu-et-al-2024>(30/50 | 90/258) GenN2N: Generative NeRF2NeRF Translation (Xiangyue Liu et al., 2024)</a></li><li><a href=#3150--91258-dibs-enhancing-dense-video-captioning-with-unlabeled-videos-via-pseudo-boundary-enrichment-and-online-refinement-hao-wu-et-al-2024>(31/50 | 91/258) DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement (Hao Wu et al., 2024)</a></li><li><a href=#3250--92258-cross-attention-makes-inference-cumbersome-in-text-to-image-diffusion-models-wentian-zhang-et-al-2024>(32/50 | 92/258) Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models (Wentian Zhang et al., 2024)</a></li><li><a href=#3350--93258-harnessing-the-power-of-large-vision-language-models-for-synthetic-image-detection-mamadou-keita-et-al-2024>(33/50 | 93/258) Harnessing the Power of Large Vision Language Models for Synthetic Image Detection (Mamadou Keita et al., 2024)</a></li><li><a href=#3450--94258-3dstyleglip-part-tailored-text-guided-3d-neural-stylization-seungjeh-chung-et-al-2024>(34/50 | 94/258) 3DStyleGLIP: Part-Tailored Text-Guided 3D Neural Stylization (SeungJeh Chung et al., 2024)</a></li><li><a href=#3550--95258-neural-radiance-fields-with-torch-units-bingnan-ni-et-al-2024>(35/50 | 95/258) Neural Radiance Fields with Torch Units (Bingnan Ni et al., 2024)</a></li><li><a href=#3650--96258-unsegment-anything-by-simulating-deformation-jiahao-lu-et-al-2024>(36/50 | 96/258) Unsegment Anything by Simulating Deformation (Jiahao Lu et al., 2024)</a></li><li><a href=#3750--97258-severity-controlled-text-to-image-generative-model-bias-manipulation-jordan-vice-et-al-2024>(37/50 | 97/258) Severity Controlled Text-to-Image Generative Model Bias Manipulation (Jordan Vice et al., 2024)</a></li><li><a href=#3850--98258-unsupervised-occupancy-learning-from-sparse-point-cloud-amine-ouasfi-et-al-2024>(38/50 | 98/258) Unsupervised Occupancy Learning from Sparse Point Cloud (Amine Ouasfi et al., 2024)</a></li><li><a href=#3950--99258-independently-keypoint-learning-for-small-object-semantic-correspondence-hailong-jin-et-al-2024>(39/50 | 99/258) Independently Keypoint Learning for Small Object Semantic Correspondence (Hailong Jin et al., 2024)</a></li><li><a href=#4050--100258-henet-hybrid-encoding-for-end-to-end-multi-task-3d-perception-from-multi-view-cameras-zhongyu-xia-et-al-2024>(40/50 | 100/258) HENet: Hybrid Encoding for End-to-end Multi-task 3D Perception from Multi-view Cameras (Zhongyu Xia et al., 2024)</a></li><li><a href=#4150--101258-awol-analysis-without-synthesis-using-language-silvia-zuffi-et-al-2024>(41/50 | 101/258) AWOL: Analysis WithOut synthesis using Language (Silvia Zuffi et al., 2024)</a></li><li><a href=#4250--102258-instantstyle-free-lunch-towards-style-preserving-in-text-to-image-generation-haofan-wang-et-al-2024>(42/50 | 102/258) InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation (Haofan Wang et al., 2024)</a></li><li><a href=#4350--103258-model-agnostic-origin-attribution-of-generated-images-with-few-shot-examples-fengyuan-liu-et-al-2024>(43/50 | 103/258) Model-agnostic Origin Attribution of Generated Images with Few-shot Examples (Fengyuan Liu et al., 2024)</a></li><li><a href=#4450--104258-representation-alignment-contrastive-regularization-for-multi-object-tracking-shujie-chen-et-al-2024>(44/50 | 104/258) Representation Alignment Contrastive Regularization for Multi-Object Tracking (Shujie Chen et al., 2024)</a></li><li><a href=#4550--105258-enhancing-diffusion-based-point-cloud-generation-with-smoothness-constraint-yukun-li-et-al-2024>(45/50 | 105/258) Enhancing Diffusion-based Point Cloud Generation with Smoothness Constraint (Yukun Li et al., 2024)</a></li><li><a href=#4650--106258-cape-cam-as-a-probabilistic-ensemble-for-enhanced-dnn-interpretation-townim-faisal-chowdhury-et-al-2024>(46/50 | 106/258) CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation (Townim Faisal Chowdhury et al., 2024)</a></li><li><a href=#4750--107258-behind-the-veil-enhanced-indoor-3d-scene-reconstruction-with-occluded-surfaces-completion-su-sun-et-al-2024>(47/50 | 107/258) Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion (Su Sun et al., 2024)</a></li><li><a href=#4850--108258-lidar4d-dynamic-neural-fields-for-novel-space-time-view-lidar-synthesis-zehan-zheng-et-al-2024>(48/50 | 108/258) LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis (Zehan Zheng et al., 2024)</a></li><li><a href=#4950--109258-tclc-gs-tightly-coupled-lidar-camera-gaussian-splatting-for-surrounding-autonomous-driving-scenes-cheng-zhao-et-al-2024>(49/50 | 109/258) TCLC-GS: Tightly Coupled LiDAR-Camera Gaussian Splatting for Surrounding Autonomous Driving Scenes (Cheng Zhao et al., 2024)</a></li><li><a href=#5050--110258-regional-biases-in-image-geolocation-estimation-a-case-study-with-the-sensecity-africa-dataset-ximena-salgado-uribe-et-al-2024>(50/50 | 110/258) Regional biases in image geolocation estimation: a case study with the SenseCity Africa dataset (Ximena Salgado Uribe et al., 2024)</a></li></ul></li><li><a href=#cslg-48>cs.LG (48)</a><ul><li><a href=#148--111258-foundation-models-for-structural-health-monitoring-luca-benfenati-et-al-2024>(1/48 | 111/258) Foundation Models for Structural Health Monitoring (Luca Benfenati et al., 2024)</a></li><li><a href=#248--112258-badam-a-memory-efficient-full-parameter-training-method-for-large-language-models-qijun-luo-et-al-2024>(2/48 | 112/258) BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models (Qijun Luo et al., 2024)</a></li><li><a href=#348--113258-generative-contrastive-heterogeneous-graph-neural-network-yu-wang-et-al-2024>(3/48 | 113/258) Generative-Contrastive Heterogeneous Graph Neural Network (Yu Wang et al., 2024)</a></li><li><a href=#448--114258-task-agnostic-architecture-for-algorithm-induction-via-implicit-composition-sahil-j-sindhi-et-al-2024>(4/48 | 114/258) Task Agnostic Architecture for Algorithm Induction via Implicit Composition (Sahil J. Sindhi et al., 2024)</a></li><li><a href=#548--115258-deep-privacy-funnel-model-from-a-discriminative-to-a-generative-approach-with-an-application-to-face-recognition-behrooz-razeghi-et-al-2024>(5/48 | 115/258) Deep Privacy Funnel Model: From a Discriminative to a Generative Approach with an Application to Face Recognition (Behrooz Razeghi et al., 2024)</a></li><li><a href=#648--116258-explainable-traffic-flow-prediction-with-large-language-models-xusen-guo-et-al-2024>(6/48 | 116/258) Explainable Traffic Flow Prediction with Large Language Models (Xusen Guo et al., 2024)</a></li><li><a href=#748--117258-on-the-efficiency-and-robustness-of-vibration-based-foundation-models-for-iot-sensing-a-case-study-tomoyoshi-kimura-et-al-2024>(7/48 | 117/258) On the Efficiency and Robustness of Vibration-based Foundation Models for IoT Sensing: A Case Study (Tomoyoshi Kimura et al., 2024)</a></li><li><a href=#848--118258-ressa-repair-sparse-vision-language-models-via-sparse-cross-modality-adaptation-shwai-he-et-al-2024>(8/48 | 118/258) RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation (Shwai He et al., 2024)</a></li><li><a href=#948--119258-transfer-learning-applications-for-anomaly-detection-in-wind-turbines-cyriana-m-a-roelofs-et-al-2024>(9/48 | 119/258) Transfer learning applications for anomaly detection in wind turbines (Cyriana M. A. Roelofs et al., 2024)</a></li><li><a href=#1048--120258-towards-detecting-unanticipated-bias-in-large-language-models-anna-kruspe-2024>(10/48 | 120/258) Towards detecting unanticipated bias in Large Language Models (Anna Kruspe, 2024)</a></li><li><a href=#1148--121258-masked-completion-via-structured-diffusion-with-white-box-transformers-druv-pai-et-al-2024>(11/48 | 121/258) Masked Completion via Structured Diffusion with White-Box Transformers (Druv Pai et al., 2024)</a></li><li><a href=#1248--122258-robust-federated-learning-for-wireless-networks-a-demonstration-with-channel-estimation-zexin-fang-et-al-2024>(12/48 | 122/258) Robust Federated Learning for Wireless Networks: A Demonstration with Channel Estimation (Zexin Fang et al., 2024)</a></li><li><a href=#1348--123258-rethinking-teacher-student-curriculum-learning-through-the-cooperative-mechanics-of-experience-manfred-diaz-et-al-2024>(13/48 | 123/258) Rethinking Teacher-Student Curriculum Learning through the Cooperative Mechanics of Experience (Manfred Diaz et al., 2024)</a></li><li><a href=#1448--124258-deep-generative-models-through-the-lens-of-the-manifold-hypothesis-a-survey-and-new-connections-gabriel-loaiza-ganem-et-al-2024>(14/48 | 124/258) Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections (Gabriel Loaiza-Ganem et al., 2024)</a></li><li><a href=#1548--125258-end-to-end-self-tuning-self-supervised-time-series-anomaly-detection-boje-deforce-et-al-2024>(15/48 | 125/258) End-To-End Self-tuning Self-supervised Time Series Anomaly Detection (Boje Deforce et al., 2024)</a></li><li><a href=#1648--126258-toward-inference-optimal-mixture-of-expert-large-language-models-longfei-yun-et-al-2024>(16/48 | 126/258) Toward Inference-optimal Mixture-of-Expert Large Language Models (Longfei Yun et al., 2024)</a></li><li><a href=#1748--127258-pissa-principal-singular-values-and-singular-vectors-adaptation-of-large-language-models-fanxu-meng-et-al-2024>(17/48 | 127/258) PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models (Fanxu Meng et al., 2024)</a></li><li><a href=#1848--128258-first-order-pdes-for-graph-neural-networks-advection-and-burgers-equation-models-yifan-qu-et-al-2024>(18/48 | 128/258) First-order PDES for Graph Neural Networks: Advection And Burgers Equation Models (Yifan Qu et al., 2024)</a></li><li><a href=#1948--129258-the-satml-24-cnn-interpretability-competition-new-innovations-for-concept-level-interpretability-stephen-casper-et-al-2024>(19/48 | 129/258) The SaTML &lsquo;24 CNN Interpretability Competition: New Innovations for Concept-Level Interpretability (Stephen Casper et al., 2024)</a></li><li><a href=#2048--130258-optimizing-the-deployment-of-tiny-transformers-on-low-power-mcus-victor-j-b-jung-et-al-2024>(20/48 | 130/258) Optimizing the Deployment of Tiny Transformers on Low-Power MCUs (Victor J. B. Jung et al., 2024)</a></li><li><a href=#2148--131258-learning-in-convolutional-neural-networks-accelerated-by-transfer-entropy-adrian-moldovan-et-al-2024>(21/48 | 131/258) Learning in Convolutional Neural Networks Accelerated by Transfer Entropy (Adrian Moldovan et al., 2024)</a></li><li><a href=#2248--132258-attention-is-naturally-sparse-with-gaussian-distributed-input-yichuan-deng-et-al-2024>(22/48 | 132/258) Attention is Naturally Sparse with Gaussian Distributed Input (Yichuan Deng et al., 2024)</a></li><li><a href=#2348--133258-on-the-importance-of-uncertainty-in-decision-making-with-large-language-models-nicolò-felicioni-et-al-2024>(23/48 | 133/258) On the Importance of Uncertainty in Decision-Making with Large Language Models (Nicolò Felicioni et al., 2024)</a></li><li><a href=#2448--134258-grid-mapping-pseudo-count-constraint-for-offline-reinforcement-learning-yi-shen-et-al-2024>(24/48 | 134/258) Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning (Yi Shen et al., 2024)</a></li><li><a href=#2548--135258-ad4rl-autonomous-driving-benchmarks-for-offline-reinforcement-learning-with-value-based-dataset-dongsu-lee-et-al-2024>(25/48 | 135/258) AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning with Value-based Dataset (Dongsu Lee et al., 2024)</a></li><li><a href=#2648--136258-the-artificial-intelligence-ontology-llm-assisted-construction-of-ai-concept-hierarchies-marcin-p-joachimiak-et-al-2024>(26/48 | 136/258) The Artificial Intelligence Ontology: LLM-assisted construction of AI concept hierarchies (Marcin P. Joachimiak et al., 2024)</a></li><li><a href=#2748--137258-domain-generalization-through-meta-learning-a-survey-arsham-gholamzadeh-khoee-et-al-2024>(27/48 | 137/258) Domain Generalization through Meta-Learning: A Survey (Arsham Gholamzadeh Khoee et al., 2024)</a></li><li><a href=#2848--138258-federated-computing----survey-on-building-blocks-extensions-and-systems-rené-schwermer-et-al-2024>(28/48 | 138/258) Federated Computing &ndash; Survey on Building Blocks, Extensions and Systems (René Schwermer et al., 2024)</a></li><li><a href=#2948--139258-adversarial-attacks-and-dimensionality-in-text-classifiers-nandish-chattopadhyay-et-al-2024>(29/48 | 139/258) Adversarial Attacks and Dimensionality in Text Classifiers (Nandish Chattopadhyay et al., 2024)</a></li><li><a href=#3048--140258-solving-a-real-world-optimization-problem-using-proximal-policy-optimization-with-curriculum-learning-and-reward-engineering-abhijeet-pendyala-et-al-2024>(30/48 | 140/258) Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering (Abhijeet Pendyala et al., 2024)</a></li><li><a href=#3148--141258-fedselect-personalized-federated-learning-with-customized-selection-of-parameters-for-fine-tuning-rishub-tamirisa-et-al-2024>(31/48 | 141/258) FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning (Rishub Tamirisa et al., 2024)</a></li><li><a href=#3248--142258-decision-predicate-graphs-enhancing-interpretability-in-tree-ensembles-leonardo-arrighi-et-al-2024>(32/48 | 142/258) Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles (Leonardo Arrighi et al., 2024)</a></li><li><a href=#3348--143258-model-based-reinforcement-learning-for-parameterized-action-spaces-renhao-zhang-et-al-2024>(33/48 | 143/258) Model-based Reinforcement Learning for Parameterized Action Spaces (Renhao Zhang et al., 2024)</a></li><li><a href=#3448--144258-spectral-clustering-in-convex-and-constrained-settings-swarup-ranjan-behera-et-al-2024>(34/48 | 144/258) Spectral Clustering in Convex and Constrained Settings (Swarup Ranjan Behera et al., 2024)</a></li><li><a href=#3548--145258-incremental-learning-with-concept-drift-detection-and-prototype-based-embeddings-for-graph-stream-classification-kleanthis-malialis-et-al-2024>(35/48 | 145/258) Incremental Learning with Concept Drift Detection and Prototype-based Embeddings for Graph Stream Classification (Kleanthis Malialis et al., 2024)</a></li><li><a href=#3648--146258-methodology-for-interpretable-reinforcement-learning-for-optimizing-mechanical-ventilation-joo-seung-lee-et-al-2024>(36/48 | 146/258) Methodology for Interpretable Reinforcement Learning for Optimizing Mechanical Ventilation (Joo Seung Lee et al., 2024)</a></li><li><a href=#3748--147258-universal-functional-regression-with-neural-operator-flows-yaozhong-shi-et-al-2024>(37/48 | 147/258) Universal Functional Regression with Neural Operator Flows (Yaozhong Shi et al., 2024)</a></li><li><a href=#3848--148258-modno-multi-operator-learning-with-distributed-neural-operators-zecheng-zhang-2024>(38/48 | 148/258) MODNO: Multi Operator Learning With Distributed Neural Operators (Zecheng Zhang, 2024)</a></li><li><a href=#3948--149258-guarantees-of-confidentiality-via-hammersley-chapman-robbins-bounds-kamalika-chaudhuri-et-al-2024>(39/48 | 149/258) Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds (Kamalika Chaudhuri et al., 2024)</a></li><li><a href=#4048--150258-dnn-memory-footprint-reduction-via-post-training-intra-layer-multi-precision-quantization-behnam-ghavami-et-al-2024>(40/48 | 150/258) DNN Memory Footprint Reduction via Post-Training Intra-Layer Multi-Precision Quantization (Behnam Ghavami et al., 2024)</a></li><li><a href=#4148--151258-continual-learning-of-numerous-tasks-from-long-tail-distributions-liwei-kang-et-al-2024>(41/48 | 151/258) Continual Learning of Numerous Tasks from Long-tail Distributions (Liwei Kang et al., 2024)</a></li><li><a href=#4248--152258-reinforcement-learning-in-categorical-cybernetics-jules-hedges-et-al-2024>(42/48 | 152/258) Reinforcement Learning in Categorical Cybernetics (Jules Hedges et al., 2024)</a></li><li><a href=#4348--153258-transformer-based-stagewise-decomposition-for-large-scale-multistage-stochastic-optimization-chanyeong-kim-et-al-2024>(43/48 | 153/258) Transformer-based Stagewise Decomposition for Large-Scale Multistage Stochastic Optimization (Chanyeong Kim et al., 2024)</a></li><li><a href=#4448--154258-an-interpretable-client-decision-tree-aggregation-process-for-federated-learning-alberto-argente-garrido-et-al-2024>(44/48 | 154/258) An Interpretable Client Decision Tree Aggregation process for Federated Learning (Alberto Argente-Garrido et al., 2024)</a></li><li><a href=#4548--155258-optimal-batch-allocation-for-wireless-federated-learning-jaeyoung-song-et-al-2024>(45/48 | 155/258) Optimal Batch Allocation for Wireless Federated Learning (Jaeyoung Song et al., 2024)</a></li><li><a href=#4648--156258-on-line-conformalized-neural-networks-ensembles-for-probabilistic-forecasting-of-day-ahead-electricity-prices-alessandro-brusaferri-et-al-2024>(46/48 | 156/258) On-line conformalized neural networks ensembles for probabilistic forecasting of day-ahead electricity prices (Alessandro Brusaferri et al., 2024)</a></li><li><a href=#4748--157258-effector-a-python-package-for-regional-explanations-vasilis-gkolemis-et-al-2024>(47/48 | 157/258) Effector: A Python package for regional explanations (Vasilis Gkolemis et al., 2024)</a></li><li><a href=#4848--158258-adaptive-sampling-policies-imply-biased-beliefs-a-generalization-of-the-hot-stove-effect-jerker-denrell-2024>(48/48 | 158/258) Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect (Jerker Denrell, 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--159258-clam-tts-improving-neural-codec-language-model-for-zero-shot-text-to-speech-jaehyeon-kim-et-al-2024>(1/1 | 159/258) CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech (Jaehyeon Kim et al., 2024)</a></li></ul></li><li><a href=#csai-7>cs.AI (7)</a><ul><li><a href=#17--160258-empowering-biomedical-discovery-with-ai-agents-shanghua-gao-et-al-2024>(1/7 | 160/258) Empowering Biomedical Discovery with AI Agents (Shanghua Gao et al., 2024)</a></li><li><a href=#27--161258-i-design-personalized-llm-interior-designer-ata-çelen-et-al-2024>(2/7 | 161/258) I-Design: Personalized LLM Interior Designer (Ata Çelen et al., 2024)</a></li><li><a href=#37--162258-learn-to-disguise-avoid-refusal-responses-in-llms-defense-via-a-multi-agent-attacker-disguiser-game-qianqiao-xu-et-al-2024>(3/7 | 162/258) Learn to Disguise: Avoid Refusal Responses in LLM&rsquo;s Defense via a Multi-agent Attacker-Disguiser Game (Qianqiao Xu et al., 2024)</a></li><li><a href=#47--163258-integrating-explanations-in-learning-ltl-specifications-from-demonstrations-ashutosh-gupta-et-al-2024>(4/7 | 163/258) Integrating Explanations in Learning LTL Specifications from Demonstrations (Ashutosh Gupta et al., 2024)</a></li><li><a href=#57--164258-data-driven-goal-recognition-design-for-general-behavioral-agents-robert-kasumba-et-al-2024>(5/7 | 164/258) Data-Driven Goal Recognition Design for General Behavioral Agents (Robert Kasumba et al., 2024)</a></li><li><a href=#67--165258-shield-a-regularization-technique-for-explainable-artificial-intelligence-iván-sevillano-garcía-et-al-2024>(6/7 | 165/258) SHIELD: A regularization technique for eXplainable Artificial Intelligence (Iván Sevillano-García et al., 2024)</a></li><li><a href=#77--166258-learning-generalized-policies-for-fully-observable-non-deterministic-planning-domains-till-hofmann-et-al-2024>(7/7 | 166/258) Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains (Till Hofmann et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--167258-ai-tutoring-in-software-engineering-education-eduard-frankford-et-al-2024>(1/5 | 167/258) AI-Tutoring in Software Engineering Education (Eduard Frankford et al., 2024)</a></li><li><a href=#25--168258-testing-the-effect-of-code-documentation-on-large-language-model-code-understanding-william-macke-et-al-2024>(2/5 | 168/258) Testing the Effect of Code Documentation on Large Language Model Code Understanding (William Macke et al., 2024)</a></li><li><a href=#35--169258-large-language-model-for-vulnerability-detection-and-repair-literature-review-and-roadmap-xin-zhou-et-al-2024>(3/5 | 169/258) Large Language Model for Vulnerability Detection and Repair: Literature Review and Roadmap (Xin Zhou et al., 2024)</a></li><li><a href=#45--170258-the-realhumaneval-evaluating-large-language-models-abilities-to-support-programmers-hussein-mozannar-et-al-2024>(4/5 | 170/258) The RealHumanEval: Evaluating Large Language Models&rsquo; Abilities to Support Programmers (Hussein Mozannar et al., 2024)</a></li><li><a href=#55--171258-creating-a-trajectory-for-code-writing-algorithmic-reasoning-tasks-shruthi-ravikumar-et-al-2024>(5/5 | 171/258) Creating a Trajectory for Code Writing: Algorithmic Reasoning Tasks (Shruthi Ravikumar et al., 2024)</a></li></ul></li><li><a href=#eesssy-8>eess.SY (8)</a><ul><li><a href=#18--172258-decision-transformer-as-a-foundation-model-for-partially-observable-continuous-control-xiangyuan-zhang-et-al-2024>(1/8 | 172/258) Decision Transformer as a Foundation Model for Partially Observable Continuous Control (Xiangyuan Zhang et al., 2024)</a></li><li><a href=#28--173258-distributionally-robust-policy-and-lyapunov-certificate-learning-kehan-long-et-al-2024>(2/8 | 173/258) Distributionally Robust Policy and Lyapunov-Certificate Learning (Kehan Long et al., 2024)</a></li><li><a href=#38--174258-joint-optimization-on-uplink-ofdma-and-mu-mimo-for-ieee-80211ax-deep-hierarchical-reinforcement-learning-approach-hyeonho-noh-et-al-2024>(3/8 | 174/258) Joint Optimization on Uplink OFDMA and MU-MIMO for IEEE 802.11ax: Deep Hierarchical Reinforcement Learning Approach (Hyeonho Noh et al., 2024)</a></li><li><a href=#48--175258-powersimulationsjl----a-power-systems-operations-simulation-library-jose-daniel-lara-et-al-2024>(4/8 | 175/258) PowerSimulations.jl &ndash; A Power Systems operations simulation Library (Jose Daniel Lara et al., 2024)</a></li><li><a href=#58--176258-network-aware-and-welfare-maximizing-dynamic-pricing-for-energy-sharing-ahmed-s-alahmed-et-al-2024>(5/8 | 176/258) Network-Aware and Welfare-Maximizing Dynamic Pricing for Energy Sharing (Ahmed S. Alahmed et al., 2024)</a></li><li><a href=#68--177258-impact-and-integration-of-mini-photovoltaic-systems-on-electric-power-distribution-grids-gökhan-demirel-et-al-2024>(6/8 | 177/258) Impact and Integration of Mini Photovoltaic Systems on Electric Power Distribution Grids (Gökhan Demirel et al., 2024)</a></li><li><a href=#78--178258-learning-with-errors-based-dynamic-encryption-that-discloses-residue-signal-for-anomaly-detection-yeongjun-jang-et-al-2024>(7/8 | 178/258) Learning with errors based dynamic encryption that discloses residue signal for anomaly detection (Yeongjun Jang et al., 2024)</a></li><li><a href=#88--179258-stabilizing-switched-nonlinear-systems-under-restricted-but-arbitrary-switching-signals-atreyee-kundu-2024>(8/8 | 179/258) Stabilizing switched nonlinear systems under restricted but arbitrary switching signals (Atreyee Kundu, 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--180258-jailbreakv-28k-a-benchmark-for-assessing-the-robustness-of-multimodal-large-language-models-against-jailbreak-attacks-weidi-luo-et-al-2024>(1/7 | 180/258) JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks (Weidi Luo et al., 2024)</a></li><li><a href=#27--181258-exploring-backdoor-vulnerabilities-of-chat-models-yunzhuo-hao-et-al-2024>(2/7 | 181/258) Exploring Backdoor Vulnerabilities of Chat Models (Yunzhuo Hao et al., 2024)</a></li><li><a href=#37--182258-differentially-private-verification-of-survey-weighted-estimates-tong-lin-et-al-2024>(3/7 | 182/258) Differentially Private Verification of Survey-Weighted Estimates (Tong Lin et al., 2024)</a></li><li><a href=#47--183258-vocabulary-attack-to-hijack-large-language-model-applications-patrick-levi-et-al-2024>(4/7 | 183/258) Vocabulary Attack to Hijack Large Language Model Applications (Patrick Levi et al., 2024)</a></li><li><a href=#57--184258-novel_authentication_protocols_tailored_for_ambient_iot_devices_in_3gpp_5g_networks-xiongpeng-ren-et-al-2024>(5/7 | 184/258) Novel_Authentication_Protocols_Tailored_for_Ambient_IoT_Devices_in_3GPP_5G_Networks (Xiongpeng Ren et al., 2024)</a></li><li><a href=#67--185258-lightfat-mitigating-control-flow-explosion-via-lightweight-pmu-based-control-flow-attestation-jeferson-gonzalez-gomez-et-al-2024>(6/7 | 185/258) LightFAt: Mitigating Control-flow Explosion via Lightweight PMU-based Control-flow Attestation (Jeferson Gonzalez-Gomez et al., 2024)</a></li><li><a href=#77--186258-designing-a-photonic-physically-unclonable-function-having-resilience-to-machine-learning-attacks-elena-r-henderson-et-al-2024>(7/7 | 186/258) Designing a Photonic Physically Unclonable Function Having Resilience to Machine Learning Attacks (Elena R. Henderson et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#16--187258-a-neuroergonomics-model-to-evaluating-nuclear-power-plants-operators-performance-under-heat-stress-driven-by-ecg-time-frequency-spectrums-and-fnirs-prefrontal-cortex-network-a-cnn-gat-fusion-model-yan-zhang-et-al-2024>(1/6 | 187/258) A neuroergonomics model to evaluating nuclear power plants operators&rsquo; performance under heat stress driven by ECG time-frequency spectrums and fNIRS prefrontal cortex network: a CNN-GAT fusion model (Yan Zhang et al., 2024)</a></li><li><a href=#26--188258-unblind-text-inputs-predicting-hint-text-of-text-input-in-mobile-apps-via-llm-zhe-liu-et-al-2024>(2/6 | 188/258) Unblind Text Inputs: Predicting Hint-text of Text Input in Mobile Apps via LLM (Zhe Liu et al., 2024)</a></li><li><a href=#36--189258-writing-with-ai-lowers-psychological-ownership-but-longer-prompts-can-help-nikhita-joshi-et-al-2024>(3/6 | 189/258) Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help (Nikhita Joshi et al., 2024)</a></li><li><a href=#46--190258-evolving-agents-interactive-simulation-of-dynamic-and-diverse-human-personalities-jiale-li-et-al-2024>(4/6 | 190/258) Evolving Agents: Interactive Simulation of Dynamic and Diverse Human Personalities (Jiale Li et al., 2024)</a></li><li><a href=#56--191258-promptrpa-generating-robotic-process-automation-on-smartphones-from-textual-prompts-tian-huang-et-al-2024>(5/6 | 191/258) PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts (Tian Huang et al., 2024)</a></li><li><a href=#66--192258-a-unified-editing-method-for-co-speech-gesture-generation-via-diffusion-inversion-zeyu-zhao-et-al-2024>(6/6 | 192/258) A Unified Editing Method for Co-Speech Gesture Generation via Diffusion Inversion (Zeyu Zhao et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--193258-a-universal-deep-neural-network-for-signal-detection-in-wireless-communication-systems-khalid-albagami-et-al-2024>(1/3 | 193/258) A Universal Deep Neural Network for Signal Detection in Wireless Communication Systems (Khalid Albagami et al., 2024)</a></li><li><a href=#23--194258-exploring-opportunistic-routing-for-remote-sea-emergencies-cleon-liew-et-al-2024>(2/3 | 194/258) Exploring Opportunistic Routing for Remote Sea Emergencies (Cleon Liew et al., 2024)</a></li><li><a href=#33--195258-when-digital-twin-meets-generative-ai-intelligent-closed-loop-network-management-xinyu-huang-et-al-2024>(3/3 | 195/258) When Digital Twin Meets Generative AI: Intelligent Closed-Loop Network Management (Xinyu Huang et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--196258-improving-topic-relevance-model-by-mix-structured-summarization-and-llm-based-data-augmentation-yizhu-liu-et-al-2024>(1/5 | 196/258) Improving Topic Relevance Model by Mix-structured Summarization and LLM-based Data Augmentation (Yizhu Liu et al., 2024)</a></li><li><a href=#25--197258-duqgen-effective-unsupervised-domain-adaptation-of-neural-rankers-by-diversifying-synthetic-query-generation-ramraj-chandradevan-et-al-2024>(2/5 | 197/258) DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation (Ramraj Chandradevan et al., 2024)</a></li><li><a href=#35--198258-efficient-multi-vector-dense-retrieval-using-bit-vectors-franco-maria-nardini-et-al-2024>(3/5 | 198/258) Efficient Multi-Vector Dense Retrieval Using Bit Vectors (Franco Maria Nardini et al., 2024)</a></li><li><a href=#45--199258-the-surprising-effectiveness-of-rankers-trained-on-expanded-queries-abhijit-anand-et-al-2024>(4/5 | 199/258) The Surprising Effectiveness of Rankers Trained on Expanded Queries (Abhijit Anand et al., 2024)</a></li><li><a href=#55--200258-unbiased-learning-to-rank-meets-reality-lessons-from-baidus-large-scale-search-dataset-philipp-hager-et-al-2024>(5/5 | 200/258) Unbiased Learning to Rank Meets Reality: Lessons from Baidu&rsquo;s Large-Scale Search Dataset (Philipp Hager et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--201258-nl2kql-from-natural-language-to-kusto-query-amir-h-abdi-et-al-2024>(1/1 | 201/258) NL2KQL: From Natural Language to Kusto Query (Amir H. Abdi et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--202258-meshbrush-painting-the-anatomical-mesh-with-neural-stylization-for-endoscopy-john-j-han-et-al-2024>(1/4 | 202/258) MeshBrush: Painting the Anatomical Mesh with Neural Stylization for Endoscopy (John J. Han et al., 2024)</a></li><li><a href=#24--203258-event-camera-demosaicing-via-swin-transformer-and-pixel-focus-loss-yunfan-lu-et-al-2024>(2/4 | 203/258) Event Camera Demosaicing via Swin Transformer and Pixel-focus Loss (Yunfan Lu et al., 2024)</a></li><li><a href=#34--204258-vestibular-schwannoma-growth-prediction-from-longitudinal-mri-by-time-conditioned-neural-fields-yunjie-chen-et-al-2024>(3/4 | 204/258) Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields (Yunjie Chen et al., 2024)</a></li><li><a href=#44--205258-cohort-individual-cooperative-learning-for-multimodal-cancer-survival-analysis-huajun-zhou-et-al-2024>(4/4 | 205/258) Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis (Huajun Zhou et al., 2024)</a></li></ul></li><li><a href=#csro-10>cs.RO (10)</a><ul><li><a href=#110--206258-learning-quadrupedal-locomotion-via-differentiable-simulation-clemens-schwarke-et-al-2024>(1/10 | 206/258) Learning Quadrupedal Locomotion via Differentiable Simulation (Clemens Schwarke et al., 2024)</a></li><li><a href=#210--207258-unsupervised-learning-of-effective-actions-in-robotics-marko-zaric-et-al-2024>(2/10 | 207/258) Unsupervised Learning of Effective Actions in Robotics (Marko Zaric et al., 2024)</a></li><li><a href=#310--208258-sliceit----a-dual-simulator-framework-for-learning-robot-food-slicing-cristian-c-beltran-hernandez-et-al-2024>(3/10 | 208/258) SliceIt! &ndash; A Dual Simulator Framework for Learning Robot Food Slicing (Cristian C. Beltran-Hernandez et al., 2024)</a></li><li><a href=#410--209258-versatile-scene-consistent-traffic-scenario-generation-as-optimization-with-diffusion-zhiyu-huang-et-al-2024>(4/10 | 209/258) Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion (Zhiyu Huang et al., 2024)</a></li><li><a href=#510--210258-multi-robot-planning-for-filming-groups-of-moving-actors-leveraging-submodularity-and-pixel-density-skyler-hughes-et-al-2024>(5/10 | 210/258) Multi-Robot Planning for Filming Groups of Moving Actors Leveraging Submodularity and Pixel Density (Skyler Hughes et al., 2024)</a></li><li><a href=#610--211258-low-frequency-sampling-in-model-predictive-path-integral-control-bogdan-vlahov-et-al-2024>(6/10 | 211/258) Low Frequency Sampling in Model Predictive Path Integral Control (Bogdan Vlahov et al., 2024)</a></li><li><a href=#710--212258-fusing-multi-sensor-input-with-state-information-on-tinyml-brains-for-autonomous-nano-drones-luca-crupi-et-al-2024>(7/10 | 212/258) Fusing Multi-sensor Input with State Information on TinyML Brains for Autonomous Nano-drones (Luca Crupi et al., 2024)</a></li><li><a href=#810--213258-self-supervised-6-dof-robot-grasping-by-demonstration-via-augmented-reality-teleoperation-system-xiwen-dengxiong-et-al-2024>(8/10 | 213/258) Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System (Xiwen Dengxiong et al., 2024)</a></li><li><a href=#910--214258-a-survey-of-optimization-based-task-and-motion-planning-from-classical-to-learning-approaches-zhigen-zhao-et-al-2024>(9/10 | 214/258) A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches (Zhigen Zhao et al., 2024)</a></li><li><a href=#1010--215258-on-the-go-tree-detection-and-geometric-traits-estimation-with-ground-mobile-robots-in-fruit-tree-groves-dimitrios-chatziparaschis-et-al-2024>(10/10 | 215/258) On-the-Go Tree Detection and Geometric Traits Estimation with Ground Mobile Robots in Fruit Tree Groves (Dimitrios Chatziparaschis et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--216258-sound-borrow-checking-for-rust-via-symbolic-semantics-son-ho-et-al-2024>(1/1 | 216/258) Sound Borrow-Checking for Rust via Symbolic Semantics (Son Ho et al., 2024)</a></li></ul></li><li><a href=#csdc-4>cs.DC (4)</a><ul><li><a href=#14--217258-geot-tensor-centric-library-for-graph-neural-network-via-efficient-segment-reduction-on-gpu-zhongming-yu-et-al-2024>(1/4 | 217/258) GeoT: Tensor Centric Library for Graph Neural Network via Efficient Segment Reduction on GPU (Zhongming Yu et al., 2024)</a></li><li><a href=#24--218258-speed-power-and-cost-implications-for-gpu-acceleration-of-computational-fluid-dynamics-on-hpc-systems-zachary-cooper-baldock-et-al-2024>(2/4 | 218/258) Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems (Zachary Cooper-Baldock et al., 2024)</a></li><li><a href=#34--219258-vpals-towards-verified-performance-aware-learning-system-for-resource-management-guoliang-he-et-al-2024>(3/4 | 219/258) vPALs: Towards Verified Performance-aware Learning System For Resource Management (Guoliang He et al., 2024)</a></li><li><a href=#44--220258-a-survey-on-error-bounded-lossy-compression-for-scientific-datasets-sheng-di-et-al-2024>(4/4 | 220/258) A Survey on Error-Bounded Lossy Compression for Scientific Datasets (Sheng Di et al., 2024)</a></li></ul></li><li><a href=#mathoc-4>math.OC (4)</a><ul><li><a href=#14--221258-deep-reinforcement-learning-for-traveling-purchaser-problems-haofeng-yuan-et-al-2024>(1/4 | 221/258) Deep Reinforcement Learning for Traveling Purchaser Problems (Haofeng Yuan et al., 2024)</a></li><li><a href=#24--222258-electric-vehicle-routing-problem-for-emergency-power-supply-towards-telecom-base-station-relief-daisuke-kikuta-et-al-2024>(2/4 | 222/258) Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief (Daisuke Kikuta et al., 2024)</a></li><li><a href=#34--223258-faster-convergence-of-stochastic-accelerated-gradient-descent-under-interpolation-aaron-mishkin-et-al-2024>(3/4 | 223/258) Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation (Aaron Mishkin et al., 2024)</a></li><li><a href=#44--224258-nonlinear-integral-extension-of-pid-control-with-improved-convergence-of-perturbed-second-order-dynamic-systems-michael-ruderman-2024>(4/4 | 224/258) Nonlinear integral extension of PID control with improved convergence of perturbed second-order dynamic systems (Michael Ruderman, 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--225258-promptcodec-high-fidelity-neural-speech-codec-using-disentangled-representation-learning-based-adaptive-feature-aware-prompt-encoders-yu-pan-et-al-2024>(1/1 | 225/258) PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders (Yu Pan et al., 2024)</a></li></ul></li><li><a href=#statco-1>stat.CO (1)</a><ul><li><a href=#11--226258-alaamee-open-source-software-for-fitting-autologistic-actor-attribute-models-alex-stivala-et-al-2024>(1/1 | 226/258) ALAAMEE: Open-source software for fitting autologistic actor attribute models (Alex Stivala et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--227258-polynomial-graphical-lasso-learning-edges-from-gaussian-graph-stationary-signals-andrei-buciulea-et-al-2024>(1/2 | 227/258) Polynomial Graphical Lasso: Learning Edges from Gaussian Graph-Stationary Signals (Andrei Buciulea et al., 2024)</a></li><li><a href=#22--228258-ground-to-uav-140-ghz-channel-measurement-and-modeling-da-li-et-al-2024>(2/2 | 228/258) Ground-to-UAV 140 GHz channel measurement and modeling (Da Li et al., 2024)</a></li></ul></li><li><a href=#statap-1>stat.AP (1)</a><ul><li><a href=#11--229258-auditing-the-use-of-language-models-to-guide-hiring-decisions-johann-d-gaebler-et-al-2024>(1/1 | 229/258) Auditing the Use of Language Models to Guide Hiring Decisions (Johann D. Gaebler et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--230258-human-mobility-in-the-metaverse-kishore-vasan-et-al-2024>(1/1 | 230/258) Human Mobility in the Metaverse (Kishore Vasan et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--231258-traffic-divergence-theory-an-analysis-formalism-for-dynamic-networks-matin-macktoobian-et-al-2024>(1/2 | 231/258) Traffic Divergence Theory: An Analysis Formalism for Dynamic Networks (Matin Macktoobian et al., 2024)</a></li><li><a href=#22--232258-marl-lns-cooperative-multi-agent-reinforcement-learning-via-large-neighborhoods-search-weizhe-chen-et-al-2024>(2/2 | 232/258) MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search (Weizhe Chen et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--233258-a-mean-field-game-model-for-timely-computation-in-edge-computing-systems-shubham-aggarwal-et-al-2024>(1/6 | 233/258) A Mean Field Game Model for Timely Computation in Edge Computing Systems (Shubham Aggarwal et al., 2024)</a></li><li><a href=#26--234258-optimizing-peak-age-of-information-in-mec-systems-computing-preemption-and-non-preemption-jianhang-zhu-et-al-2024>(2/6 | 234/258) Optimizing Peak Age of Information in MEC Systems: Computing Preemption and Non-preemption (Jianhang Zhu et al., 2024)</a></li><li><a href=#36--235258-performance-analysis-and-isi-mitigation-with-imperfect-transmitter-in-molecular-communication-dongliang-jing-et-al-2024>(3/6 | 235/258) Performance Analysis and ISI Mitigation with Imperfect Transmitter in Molecular Communication (Dongliang Jing et al., 2024)</a></li><li><a href=#46--236258-computationally-efficient-unsupervised-deep-learning-for-robust-joint-ap-clustering-and-beamforming-design-in-cell-free-systems-guanghui-chen-et-al-2024>(4/6 | 236/258) Computationally Efficient Unsupervised Deep Learning for Robust Joint AP Clustering and Beamforming Design in Cell-Free Systems (Guanghui Chen et al., 2024)</a></li><li><a href=#56--237258-an-error-bounded-lossy-compression-method-with-bit-adaptive-quantization-for-particle-data-congrong-ren-et-al-2024>(5/6 | 237/258) An Error-Bounded Lossy Compression Method with Bit-Adaptive Quantization for Particle Data (Congrong Ren et al., 2024)</a></li><li><a href=#66--238258-multiple-uav-assisted-cooperative-df-relaying-in-multi-user-massive-mimo-iot-systems-mobeen-mahmood-et-al-2024>(6/6 | 238/258) Multiple UAV-Assisted Cooperative DF Relaying in Multi-User Massive MIMO IoT Systems (Mobeen Mahmood et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--239258-residual-based-a-posteriori-error-estimators-for-algebraic-stabilizations-abhinav-jha-2024>(1/5 | 239/258) Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations (Abhinav Jha, 2024)</a></li><li><a href=#25--240258-proper-implicit-discretization-of-arbitrary-order-robust-exact-differentiators-richard-seeber-2024>(2/5 | 240/258) Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators (Richard Seeber, 2024)</a></li><li><a href=#35--241258-a-neural-multigrid-solver-for-helmholtz-equations-with-high-wavenumber-and-heterogeneous-media-chen-cui-et-al-2024>(3/5 | 241/258) A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media (Chen Cui et al., 2024)</a></li><li><a href=#45--242258-locking-free-hybrid-high-order-method-for-linear-elasticity-carsten-carstensen-et-al-2024>(4/5 | 242/258) Locking-free hybrid high-order method for linear elasticity (Carsten Carstensen et al., 2024)</a></li><li><a href=#55--243258-adaptive-hp-polynomial-based-sparse-grid-collocation-algorithms-for-piecewise-smooth-functions-with-kinks-hendrik-wilka-et-al-2024>(5/5 | 243/258) Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks (Hendrik Wilka et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--244258-ai-and-personalized-learning-bridging-the-gap-with-modern-educational-goals-kristjan-julius-laak-et-al-2024>(1/3 | 244/258) AI and personalized learning: bridging the gap with modern educational goals (Kristjan-Julius Laak et al., 2024)</a></li><li><a href=#23--245258-enhancing-student-engagement-in-large-scale-capstone-courses-an-experience-report-asma-shakil-et-al-2024>(2/3 | 245/258) Enhancing Student Engagement in Large-Scale Capstone Courses: An Experience Report (Asma Shakil et al., 2024)</a></li><li><a href=#33--246258-decentralised-moderation-for-interoperable-social-networks-a-conversation-based-approach-for-pleroma-and-the-fediverse-vibhor-agarwal-et-al-2024>(3/3 | 246/258) Decentralised Moderation for Interoperable Social Networks: A Conversation-based Approach for Pleroma and the Fediverse (Vibhor Agarwal et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--247258-closing-the-implementation-gap-in-mc-fully-chemical-synchronization-and-detection-for-cellular-receivers-bastian-heinlein-et-al-2024>(1/1 | 247/258) Closing the Implementation Gap in MC: Fully Chemical Synchronization and Detection for Cellular Receivers (Bastian Heinlein et al., 2024)</a></li></ul></li><li><a href=#econgn-1>econ.GN (1)</a><ul><li><a href=#11--248258-karma-an-experimental-study-ezzat-elokda-et-al-2024>(1/1 | 248/258) Karma: An Experimental Study (Ezzat Elokda et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--249258-solar-synthetic-imaging-introducing-denoising-diffusion-probabilistic-models-on-sdoaia-data-francesco-p-ramunno-et-al-2024>(1/1 | 249/258) Solar synthetic imaging: Introducing denoising diffusion probabilistic models on SDO/AIA data (Francesco P. Ramunno et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--250258-convergence-analysis-of-flow-matching-in-latent-space-with-transformers-yuling-jiao-et-al-2024>(1/2 | 250/258) Convergence Analysis of Flow Matching in Latent Space with Transformers (Yuling Jiao et al., 2024)</a></li><li><a href=#22--251258-gaussian-process-regression-with-soft-inequality-and-monotonicity-constraints-didem-kochan-et-al-2024>(2/2 | 251/258) Gaussian Process Regression with Soft Inequality and Monotonicity Constraints (Didem Kochan et al., 2024)</a></li></ul></li><li><a href=#physicsdata-an-1>physics.data-an (1)</a><ul><li><a href=#11--252258-an-inversion-problem-for-optical-spectrum-data-via-physics-guided-machine-learning-hwiwoo-park-et-al-2024>(1/1 | 252/258) An inversion problem for optical spectrum data via physics-guided machine learning (Hwiwoo Park et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--253258-automated-inference-of-graph-transformation-rules-jakob-l-andersen-et-al-2024>(1/1 | 253/258) Automated Inference of Graph Transformation Rules (Jakob L. Andersen et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--254258-qfnn-ffd-quantum-federated-neural-network-for-financial-fraud-detection-nouhaila-innan-et-al-2024>(1/2 | 254/258) QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection (Nouhaila Innan et al., 2024)</a></li><li><a href=#22--255258-scalable-quantum-detector-tomography-by-high-performance-computing-timon-schapeler-et-al-2024>(2/2 | 255/258) Scalable quantum detector tomography by high-performance computing (Timon Schapeler et al., 2024)</a></li></ul></li><li><a href=#mathct-1>math.CT (1)</a><ul><li><a href=#11--256258-rendering-string-diagrams-recursively-celia-rubio-madrigal-et-al-2024>(1/1 | 256/258) Rendering string diagrams recursively (Celia Rubio-Madrigal et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--257258-degree-sequence-optimization-and-extremal-degree-enumerators-shmuel-onn-2024>(1/2 | 257/258) Degree Sequence Optimization and Extremal Degree Enumerators (Shmuel Onn, 2024)</a></li><li><a href=#22--258258-grid-drawings-of-graphs-in-three-dimensions-jozsef-balogh-et-al-2024>(2/2 | 258/258) Grid-drawings of graphs in three-dimensions (Jozsef Balogh et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>