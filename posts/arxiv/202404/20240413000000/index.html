<!doctype html><html><head><title>arXiv @ 2024.04.13</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202404/20240413000000/"><meta property="og:site_name" content="Akitenkrad's Blog"><meta property="og:title" content="arXiv @ 2024.04.13"><meta property="og:description" content="Primary Categories cs.AI (10) cs.CC (1) cs.CG (1) cs.CL (38) cs.CR (12) cs.CV (77) cs.CY (3) cs.DB (1) cs.DM (1) cs.DS (3) cs.FL (1) cs.GT (2) cs.HC (4) cs.IR (7) cs.IT (2) cs.LG (45) cs.LO (1) cs.MA (1) cs.MM (1) cs.NE (2) cs.NI (8) cs.PL (2) cs.RO (15) cs.SD (1) cs.SE (5) cs.SI (1) econ.GN (2) eess.AS (1) eess.IV (7) eess.SP (2) eess.SY (4) math.CO (1) math.NA (3) math.OC (2) math."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-13T00:00:00+00:00"><meta property="article:tag" content="ArXiv"><meta property="article:tag" content="Published:2024"><meta name=description content="arXiv @ 2024.04.13"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE")}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14">arXiv @ 2024.04.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240415000000/ title="arXiv @ 2024.04.15">arXiv @ 2024.04.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240416000000/ title="arXiv @ 2024.04.16">arXiv @ 2024.04.16</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202404/20240413000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Apr 13, 2024</p></div><div class=title><h1>arXiv @ 2024.04.13</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csai-10>cs.AI (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscl-38>cs.CL (38)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscr-12>cs.CR (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscv-77>cs.CV (77)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cslg-45>cs.LG (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csni-8>cs.NI (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cspl-2>cs.PL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#econgn-2>econ.GN (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#mathna-3>math.NA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#mathoc-2>math.OC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#mathpr-1>math.PR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#q-bioqm-2>q-bio.QM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#q-finrm-1>q-fin.RM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CR</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>5</td><td>4</td><td>1</td><td>23</td><td>11</td><td>3</td></tr><tr><td>Black Box</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Chain-of-thought</td><td></td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td></td><td></td><td>6</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td><td>4</td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td></td><td>11</td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td></td><td>25</td><td>8</td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>4</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Deep Neural Network</td><td></td><td></td><td>2</td><td>4</td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td></td><td>8</td><td>1</td><td>1</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Document Embedding</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td>1</td><td>5</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>10</td><td></td><td>10</td><td>3</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td><td>5</td><td></td><td>2</td></tr><tr><td>GLUE</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td></td><td>6</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td></td><td>1</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3.5</td><td></td><td>1</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>GPT-4</td><td>1</td><td>4</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td>3</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td></td><td>7</td><td>2</td><td></td></tr><tr><td>Graph</td><td>4</td><td>3</td><td></td><td>2</td><td>7</td><td>1</td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td>4</td><td></td><td></td><td></td><td>6</td><td></td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>8</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Information Compression</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>1</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td></td><td>9</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Large Language Model</td><td>6</td><td>50</td><td>1</td><td>9</td><td>6</td><td>6</td></tr><tr><td>Low-Resource</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Markov Game</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>6</td><td></td><td>8</td><td>5</td><td>1</td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>5</td><td></td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td></td><td>8</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Planning Domain Descrition Language</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>3</td><td>9</td><td></td><td>9</td><td>1</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Question Answering</td><td></td><td>3</td><td></td><td>4</td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>5</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td></td><td></td><td>6</td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td><td>3</td><td>3</td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Security</td><td></td><td></td><td>7</td><td>2</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td></td><td>13</td><td>1</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>1</td><td></td><td>2</td><td>6</td><td>4</td></tr><tr><td>Simulator</td><td>1</td><td>1</td><td></td><td>2</td><td>6</td><td>4</td></tr><tr><td>Stance Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td></td><td>2</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td></td><td>4</td><td>1</td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Text Understanding</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text-to-speech</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td></td><td>9</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>1</td><td>3</td><td></td><td>11</td><td>2</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td></td><td>1</td><td>2</td><td>3</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td></td><td>6</td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>2</td><td></td><td>7</td><td>1</td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-38>cs.CL (38)</h2><h3 id=138--1276-mm-phyqa-multimodal-physics-question-answering-with-multi-image-cot-prompting-avinash-anand-et-al-2024>(1/38 | 1/276) MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting (Avinash Anand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinash Anand, Janak Kapuriya, Apoorv Singh, Jay Saraf, Naman Lal, Astha Verma, Rushali Gupta, Rajiv Shah. (2024)<br><strong>MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting</strong><br><button class=copy-to-clipboard title="MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 126<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Zero-shot, GPT, GPT-4, Mistral, Question Answering, Reasoning, Chain-of-thought, Chain-of-thought, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08704v1.pdf filename=2404.08704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can achieve human-level performance in various tasks, they continue to face challenges when it comes to effectively tackling multi-step physics <b>reasoning</b> tasks. To identify the shortcomings of existing models and facilitate further research in this area, we curated a novel dataset, MM-PhyQA, which comprises well-constructed, high schoollevel <b>multimodal</b> physics problems. By evaluating the performance of contemporary <b>LLMs</b> that are publicly available, both with and without the incorporation of <b>multimodal</b> elements in these problems, we aim to shed light on their capabilities. For generating answers for <b>questions</b> <b>consisting</b> of <b>multimodal</b> input (in this case, images and text) we employed <b>Zero-shot</b> prediction using <b>GPT-4</b> and utilized LLaVA (LLaVA and LLaVA-1.5), the latter of which were <b>fine-tuned</b> on our dataset. For evaluating the performance of <b>LLMs</b> consisting solely of textual input, we tested the performance of the base and <b>fine-tuned</b> versions of the <b>Mistral-7B</b> and LLaMA2-7b models. We also showcased the performance of the novel Multi-Image <b>Chain-of-Thought</b> (MI-CoT) <b>Prompting</b> technique, which when used to train LLaVA-1.5 13b yielded the best results when tested on our dataset, with superior scores in most metrics and the highest accuracy of 71.65% on the test set.</p></p class="citation"></blockquote><h3 id=238--2276-guiding-large-language-models-to-post-edit-machine-translation-with-error-annotations-dayeon-ki-et-al-2024>(2/38 | 2/276) Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations (Dayeon Ki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayeon Ki, Marine Carpuat. (2024)<br><strong>Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations</strong><br><button class=copy-to-clipboard title="Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, Supervised Learning, LLaMA, Neural Machine Translation, Neural Machine Translation, BLEU, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07851v1.pdf filename=2404.07851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Translation</b> <b>(MT)</b> remains one of the last NLP tasks where <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have not yet replaced dedicated <b>supervised</b> systems. This work exploits the complementary strengths of <b>LLMs</b> and <b>supervised</b> <b>MT</b> by guiding <b>LLMs</b> to automatically post-edit <b>MT</b> with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with <b>LLaMA-2</b> models, we consider <b>prompting</b> strategies varying the nature of feedback provided and then <b>fine-tune</b> the <b>LLM</b> to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that <b>prompting</b> <b>LLMs</b> to post-edit <b>MT</b> improves TER, <b>BLEU</b> and COMET scores, although the benefits of fine-grained feedback are not clear. <b>Fine-tuning</b> helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.</p></p class="citation"></blockquote><h3 id=338--3276-mscinli-a-diverse-benchmark-for-scientific-natural-language-inference-mobashir-sadat-et-al-2024>(3/38 | 3/276) MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference (Mobashir Sadat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mobashir Sadat, Cornelia Caragea. (2024)<br><strong>MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference</strong><br><button class=copy-to-clipboard title="MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Fine-tuning, Transfer Learning, Natural Language Inference, Natural Language Inference, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08066v1.pdf filename=2404.08066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The task of scientific <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific <b>NLI</b> task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific <b>NLI.</b> We establish strong baselines on MSciNLI by <b>fine-tuning</b> <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> and <b>prompting</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> The highest Macro F1 scores of <b>PLM</b> and <b>LLM</b> baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific <b>NLI</b> models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific <b>NLI</b> datasets in an intermediate task <b>transfer</b> <b>learning</b> setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.</p></p class="citation"></blockquote><h3 id=438--4276-amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbreaking-both-open-and-closed-llms-zeyi-liao-et-al-2024>(4/38 | 4/276) AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs (Zeyi Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyi Liao, Huan Sun. (2024)<br><strong>AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs</strong><br><button class=copy-to-clipboard title="AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Automatic Speech Recognition, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07921v1.pdf filename=2404.07921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become increasingly prevalent and integrated into autonomous systems, ensuring their safety is imperative. Despite significant strides toward safety alignment, recent work GCG~\citep{zou2023universal} proposes a discrete token optimization algorithm and selects the single suffix with the lowest loss to successfully jailbreak aligned <b>LLMs.</b> In this work, we first discuss the drawbacks of solely picking the suffix with the lowest loss during GCG optimization for jailbreaking and uncover the missed successful suffixes during the intermediate steps. Moreover, we utilize those successful suffixes as training data to learn a generative model, named AmpleGCG, which captures the distribution of adversarial suffixes given a harmful query and enables the rapid generation of hundreds of suffixes for any harmful queries in seconds. AmpleGCG achieves near 100% attack success rate <b>(ASR)</b> on two aligned <b>LLMs</b> <b>(Llama-2-7B-chat</b> and Vicuna-7B), surpassing two strongest attack baselines. More interestingly, AmpleGCG also transfers seamlessly to attack different models, including closed-source <b>LLMs,</b> achieving a 99% <b>ASR</b> on the latest <b>GPT-3.5.</b> To <b>summarize,</b> our work amplifies the impact of GCG by training a generative model of adversarial suffixes that is universal to any harmful queries and transferable from attacking open-source <b>LLMs</b> to closed-source <b>LLMs.</b> In addition, it can generate 200 adversarial suffixes for one harmful query in only 4 seconds, rendering it more challenging to defend.</p></p class="citation"></blockquote><h3 id=538--5276-data-augmentation-based-dialectal-adaptation-for-llms-fahim-faisal-et-al-2024>(5/38 | 5/276) Data-Augmentation-Based Dialectal Adaptation for LLMs (Fahim Faisal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fahim Faisal, Antonios Anastasopoulos. (2024)<br><strong>Data-Augmentation-Based Dialectal Adaptation for LLMs</strong><br><button class=copy-to-clipboard title="Data-Augmentation-Based Dialectal Adaptation for LLMs" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Low-Resource, Common-sense Reasoning, Natural Language Understanding, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08092v1.pdf filename=2404.08092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This report presents GMUNLP&rsquo;s participation to the Dialect-Copa shared task at VarDial 2024, which focuses on evaluating the <b>commonsense</b> <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on South Slavic micro-dialects. The task aims to assess how well <b>LLMs</b> can handle non-standard dialectal varieties, as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages <b>data</b> <b>augmentation</b> techniques to improve task performance on three South Slavic dialects: Chakavian, Cherkano, and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTi'c) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed <b>data</b> <b>augmentation</b> techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of <b>data</b> <b>augmentation</b> and the potential of <b>LLMs</b> in handling non-standard dialectal varieties, contributing to the broader goal of advancing <b>natural</b> <b>language</b> <b>understanding</b> in <b>low-resource</b> and dialectal settings. Code:https://github.com/ffaisal93/dialect_copa</p></p class="citation"></blockquote><h3 id=638--6276-lloco-learning-long-contexts-offline-sijun-tan-et-al-2024>(6/38 | 6/276) LLoCO: Learning Long Contexts Offline (Sijun Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa. (2024)<br><strong>LLoCO: Learning Long Contexts Offline</strong><br><button class=copy-to-clipboard title="LLoCO: Learning Long Contexts Offline" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Question Answering, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07979v1.pdf filename=2404.07979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing long contexts remains a challenge for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> due to the quadratic computational and memory overhead of the <b>self-attention</b> mechanism and the substantial KV cache sizes during generation. We propose a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient <b>finetuning.</b> Our method enables an <b>LLM</b> to create a concise representation of the original context and efficiently retrieve relevant information to answer <b>questions</b> <b>accurately.</b> We introduce LLoCO, a technique that combines context compression, retrieval, and parameter-efficient <b>finetuning</b> using LoRA. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context <b>question-answering</b> <b>datasets,</b> demonstrating that LLoCO significantly outperforms <b>in-context</b> <b>learning</b> while using $30\times$ fewer tokens during inference. LLoCO achieves up to $7.62\times$ speed-up and substantially reduces the cost of long document <b>question</b> <b>answering,</b> making it a promising solution for efficient long context processing. Our code is publicly available at <a href=https://github.com/jeffreysijuntan/lloco>https://github.com/jeffreysijuntan/lloco</a>.</p></p class="citation"></blockquote><h3 id=738--7276-automatic-generation-and-evaluation-of-reading-comprehension-test-items-with-large-language-models-andreas-säuberli-et-al-2024>(7/38 | 7/276) Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models (Andreas Säuberli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andreas Säuberli, Simon Clematide. (2024)<br><strong>Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models</strong><br><button class=copy-to-clipboard title="Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Automatic Evaluation, Zero-shot, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07720v1.pdf filename=2404.07720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reading comprehension tests are used in a variety of applications, reaching from education to assessing the comprehensibility of simplified texts. However, creating such tests manually and ensuring their quality is difficult and time-consuming. In this paper, we explore how <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can be used to generate and evaluate multiple-choice reading comprehension items. To this end, we compiled a dataset of German reading comprehension items and developed a new protocol for human and <b>automatic</b> <b>evaluation,</b> including a metric we call text informativity, which is based on guessability and answerability. We then used this protocol and the dataset to evaluate the quality of items generated by <b>Llama</b> 2 and <b>GPT-4.</b> Our results suggest that both models are capable of generating items of acceptable quality in a <b>zero-shot</b> setting, but <b>GPT-4</b> clearly outperforms <b>Llama</b> 2. We also show that <b>LLMs</b> can be used for <b>automatic</b> <b>evaluation</b> by eliciting item reponses from them. In this scenario, evaluation results with <b>GPT-4</b> were the most similar to human annotators. Overall, <b>zero-shot</b> generation with <b>LLMs</b> is a promising approach for generating and evaluating reading comprehension test items, in particular for languages without <b>large</b> <b>amounts</b> <b>of</b> available data.</p></p class="citation"></blockquote><h3 id=838--8276-comments-as-natural-logic-pivots-improve-code-generation-via-comment-perspective-yijie-chen-et-al-2024>(8/38 | 8/276) Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective (Yijie Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou. (2024)<br><strong>Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective</strong><br><button class=copy-to-clipboard title="Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, Code Generation, Chain-of-thought, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07549v1.pdf filename=2404.07549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Code</b> <b>generation</b> aims to understand the problem description and generate corresponding <b>code</b> <b>snippets,</b> where existing works generally decompose such complex tasks into intermediate steps by <b>prompting</b> strategies, such as <b>Chain-of-Thought</b> and its variants. While these studies have achieved some success, their effectiveness is highly dependent on the capabilities of advanced <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>GPT-4,</b> particularly in terms of API calls, which significantly limits their practical applicability. Consequently, how to enhance the <b>code</b> <b>generation</b> capabilities of small and medium-scale <b>code</b> <b>LLMs</b> without significantly increasing training costs is an appealing challenge. In this paper, we suggest that <b>code</b> <b>comments</b> are the natural logic pivot between natural language and <b>code</b> <b>language</b> and propose using comments to boost the <b>code</b> <b>generation</b> ability of <b>code</b> <b>LLMs.</b> Concretely, we propose MANGO (comMents As Natural loGic pivOts), including a comment contrastive training strategy and a corresponding logical comment decoding strategy. Experiments are performed on HumanEval and MBPP, utilizing StarCoder and WizardCoder as backbone models, and encompassing model parameter sizes between 3B and 7B. The results indicate that MANGO significantly improves the <b>code</b> <b>pass</b> rate based on the strong baselines. Meanwhile, the robustness of the logical comment decoding strategy is notably higher than the <b>Chain-of-thoughts</b> <b>prompting.</b> The <b>code</b> <b>is</b> publicly available at \url{https://github.com/pppa2019/Mango}.</p></p class="citation"></blockquote><h3 id=938--9276-from-words-to-numbers-your-large-language-model-is-secretly-a-capable-regressor-when-given-in-context-examples-robert-vacareanu-et-al-2024>(9/38 | 9/276) From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples (Robert Vacareanu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, Mihai Surdeanu. (2024)<br><strong>From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples</strong><br><button class=copy-to-clipboard title="From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Supervised Learning, Claude, GPT, GPT-4, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07544v1.pdf filename=2404.07544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze how well pre-trained <b>large</b> <b>language</b> <b>models</b> (e.g., Llama2, <b>GPT-4,</b> <b>Claude</b> 3, etc) can do linear and non-linear regression when given <b>in-context</b> examples, without any additional training or gradient updates. Our findings reveal that several <b>large</b> <b>language</b> <b>models</b> (e.g., <b>GPT-4,</b> <b>Claude</b> 3) are able to perform regression tasks with a performance rivaling (or even outperforming) that of traditional <b>supervised</b> methods such as Random Forest, Bagging, or Gradient Boosting. For example, on the challenging Friedman #2 regression dataset, <b>Claude</b> 3 outperforms many <b>supervised</b> methods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boosting. We then investigate how well the performance of <b>large</b> <b>language</b> <b>models</b> scales with the number of <b>in-context</b> exemplars. We borrow from the notion of regret from online learning and empirically show that <b>LLMs</b> are capable of obtaining a sub-linear regret.</p></p class="citation"></blockquote><h3 id=1038--10276-distilling-algorithmic-reasoning-from-llms-via-explaining-solution-programs-jierui-li-et-al-2024>(10/38 | 10/276) Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs (Jierui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jierui Li, Raymond Mooney. (2024)<br><strong>Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs</strong><br><button class=copy-to-clipboard title="Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Knowledge Distillation, Reasoning, Chain-of-thought, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08148v1.pdf filename=2404.08148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distilling</b> explicit <b>chain-of-thought</b> <b>reasoning</b> paths has emerged as an effective method for improving the <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> across various tasks. However, when tackling complex tasks that pose significant challenges for state-of-the-art models, this technique often struggles to produce effective chains of thought that lead to correct answers. In this work, we propose a novel approach to <b>distill</b> <b>reasoning</b> abilities from <b>LLMs</b> by leveraging their capacity to explain solutions. We apply our method to solving competitive-level programming challenges. More specifically, we employ an <b>LLM</b> to generate explanations for a set of &lt;problem, solution-program> pairs, then use &lt;problem, explanation> pairs to <b>fine-tune</b> a smaller language model, which we refer to as the Reasoner, to learn algorithmic <b>reasoning</b> that can generate &ldquo;how-to-solve&rdquo; hints for unseen problems. Our experiments demonstrate that learning from explanations enables the Reasoner to more effectively guide program implementation by a Coder, resulting in higher solve rates than strong <b>chain-of-thought</b> baselines on competitive-level programming problems. It also outperforms models that learn directly from &lt;problem, solution-program> pairs. We curated an additional test set in the CodeContests format, which includes 246 more recent problems posted after the models&rsquo; knowledge cutoff.</p></p class="citation"></blockquote><h3 id=1138--11276-on-training-data-influence-of-gpt-models-qingyi-liu-et-al-2024>(11/38 | 11/276) On Training Data Influence of GPT Models (Qingyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun, Qiwei Peng, Keze Wang, Hua Wu. (2024)<br><strong>On Training Data Influence of GPT Models</strong><br><button class=copy-to-clipboard title="On Training Data Influence of GPT Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Simulation, Simulator, GPT, Natural Language Understanding, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07840v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07840v2.pdf filename=2404.07840v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amidst the rapid advancements in generative language models, the investigation of how training data shapes the performance of <b>GPT</b> models is still emerging. This paper presents GPTfluence, a novel approach that leverages a featurized <b>simulation</b> to assess the impact of training examples on the training dynamics of <b>GPT</b> models. Our approach not only traces the influence of individual training instances on performance trajectories, such as loss and other key metrics, on targeted test points but also enables a comprehensive comparison with existing methods across various training scenarios in <b>GPT</b> models, ranging from 14 million to 2.8 billion parameters, across a range of downstream tasks. Contrary to earlier methods that struggle with generalization to new data, GPTfluence introduces a parameterized <b>simulation</b> of training dynamics, demonstrating robust generalization capabilities to unseen training data. This adaptability is evident across both <b>fine-tuning</b> and <b>instruction-tuning</b> <b>scenarios,</b> spanning tasks in <b>natural</b> <b>language</b> <b>understanding</b> and generation. We will make our code and data publicly available.</p></p class="citation"></blockquote><h3 id=1238--12276-decomposing-label-space-format-and-discrimination-rethinking-how-llms-respond-and-solve-tasks-via-in-context-learning-quanyu-long-et-al-2024>(12/38 | 12/276) Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning (Quanyu Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quanyu Long, Yin Wu, Wenya Wang, Sinno Jialin Pan. (2024)<br><strong>Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning</strong><br><button class=copy-to-clipboard title="Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07546v1.pdf filename=2404.07546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>Learning</b> <b>(ICL)</b> has emerged as a powerful capability alongside the development of scaled-up <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> By instructing <b>LLMs</b> using <b>few-shot</b> demonstrative examples, <b>ICL</b> enables them to perform a wide range of tasks without updating millions of parameters. However, the precise contributions of demonstrations towards improving end-task performance have not been thoroughly investigated in recent analytical studies. In this paper, we empirically decompose the overall performance of <b>ICL</b> into three dimensions, label space, format, and discrimination, and we evaluate four general-purpose <b>LLMs</b> across a diverse range of tasks. Counter-intuitively, we find that the demonstrations have a marginal impact on provoking discriminative knowledge of language models. However, <b>ICL</b> exhibits significant efficacy in regulating the label space and format which helps <b>LLMs</b> to respond in desired label words. We then demonstrate this ability functions similar to detailed instructions for <b>LLMs</b> to follow. We additionally provide an in-depth analysis of the mechanism of retrieval helping with <b>ICL</b> and find that retrieving the most semantically similar examples notably boosts model&rsquo;s discriminative capability.</p></p class="citation"></blockquote><h3 id=1338--13276-sqbc-active-learning-using-llm-generated-synthetic-data-for-stance-detection-in-online-political-discussions-stefan-sylvius-wagner-et-al-2024>(13/38 | 13/276) SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions (Stefan Sylvius Wagner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling. (2024)<br><strong>SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions</strong><br><button class=copy-to-clipboard title="SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Active Learning, Fine-tuning, Transformer, Stance Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08078v1.pdf filename=2404.08078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stance</b> <b>detection</b> is an important task for many applications that analyse or support online political discussions. Common approaches include <b>fine-tuning</b> <b>transformer</b> based models. However, these models require a large amount of labelled data, which might not be available. In this work, we present two different ways to leverage <b>LLM-generated</b> synthetic data to train and improve <b>stance</b> <b>detection</b> agents for online political discussions: first, we show that augmenting a small <b>fine-tuning</b> dataset with synthetic data can improve the performance of the <b>stance</b> <b>detection</b> model. Second, we propose a new <b>active</b> <b>learning</b> method called SQBC based on the &ldquo;Query-by-Comittee&rdquo; approach. The key idea is to use <b>LLM-generated</b> synthetic data as an oracle to identify the most informative unlabelled samples, that are selected for manual labelling. Comprehensive experiments show that both ideas can improve the <b>stance</b> <b>detection</b> performance. Curiously, we observed that <b>fine-tuning</b> on actively selected samples can exceed the performance of using the full dataset.</p></p class="citation"></blockquote><h3 id=1438--14276-high-dimension-human-value-representation-in-large-language-models-samuel-cahyawijaya-et-al-2024>(14/38 | 14/276) High-Dimension Human Value Representation in Large Language Models (Samuel Cahyawijaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung. (2024)<br><strong>High-Dimension Human Value Representation in Large Language Models</strong><br><button class=copy-to-clipboard title="High-Dimension Human Value Representation in Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07900v1.pdf filename=2404.07900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> across various tasks and fields has necessitated the alignment of these models with human values and preferences. Given various approaches of human value alignment, ranging from <b>Reinforcement</b> <b>Learning</b> with Human Feedback <b>(RLHF),</b> to constitutional learning, etc. there is an urgent need to understand the scope and nature of human values injected into these models before their release. There is also a need for model alignment without a costly <b>large</b> <b>scale</b> <b>human</b> annotation effort. We propose UniVaR, a high-dimensional representation of human value distributions in <b>LLMs,</b> orthogonal to model architecture and training data. Trained from the value-relevant output of eight multilingual <b>LLMs</b> and tested on the output from four multilingual <b>LLMs,</b> namely LlaMA2, <b>ChatGPT,</b> JAIS and Yi, we show that UniVaR is a powerful tool to compare the distribution of human values embedded in different <b>LLMs</b> with different langauge sources. Through UniVaR, we explore how different <b>LLMs</b> prioritize various values in different languages and cultures, shedding light on the complex interplay between human values and language modeling.</p></p class="citation"></blockquote><h3 id=1538--15276-discourse-aware-in-context-learning-for-temporal-expression-normalization-akash-kumar-gautam-et-al-2024>(15/38 | 15/276) Discourse-Aware In-Context Learning for Temporal Expression Normalization (Akash Kumar Gautam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Kumar Gautam, Lukas Lange, Jannik Strötgen. (2024)<br><strong>Discourse-Aware In-Context Learning for Temporal Expression Normalization</strong><br><button class=copy-to-clipboard title="Discourse-Aware In-Context Learning for Temporal Expression Normalization" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07775v1.pdf filename=2404.07775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal expression (TE) normalization is a well-studied problem. However, the predominately used rule-based systems are highly restricted to specific settings, and upcoming machine learning approaches suffer from a lack of labeled data. In this work, we explore the feasibility of proprietary and open-source <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for TE normalization using <b>in-context</b> <b>learning</b> to inject task, document, and example information into the model. We explore various sample selection strategies to retrieve the most relevant set of examples. By using a window-based <b>prompt</b> design approach, we can perform TE normalization across sentences, while leveraging the <b>LLM</b> knowledge without training the model. Our experiments show competitive results to models designed for this task. In particular, our method achieves <b>large</b> <b>performance</b> <b>improvements</b> for non-standard settings by dynamically including relevant examples during inference.</p></p class="citation"></blockquote><h3 id=1638--16276-introducing-l2m3-a-multilingual-medical-large-language-model-to-advance-health-equity-in-low-resource-regions-agasthya-gangavarapu-2024>(16/38 | 16/276) Introducing L2M3, A Multilingual Medical Large Language Model to Advance Health Equity in Low-Resource Regions (Agasthya Gangavarapu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agasthya Gangavarapu. (2024)<br><strong>Introducing L2M3, A Multilingual Medical Large Language Model to Advance Health Equity in Low-Resource Regions</strong><br><button class=copy-to-clipboard title="Introducing L2M3, A Multilingual Medical Large Language Model to Advance Health Equity in Low-Resource Regions" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Low-Resource, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08705v1.pdf filename=2404.08705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the imminent shortfall of 10 million health workers by 2030, predominantly in Low- and Middle-Income Countries (LMICs), this paper introduces an innovative approach that harnesses the power of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> integrated with <b>machine</b> <b>translation</b> models. This solution is engineered to meet the unique needs of Community Health Workers (CHWs), overcoming language barriers, cultural sensitivities, and the limited availability of medical dialog datasets. I have crafted a model that not only boasts superior translation capabilities but also undergoes rigorous <b>fine-tuning</b> on open-source datasets to ensure medical accuracy and is equipped with comprehensive safety features to counteract the risks of misinformation. Featuring a modular design, this approach is specifically structured for swift adaptation across various linguistic and cultural contexts, utilizing open-source components to significantly reduce healthcare operational costs. This strategic innovation markedly improves the accessibility and quality of healthcare services by providing CHWs with contextually appropriate medical knowledge and diagnostic tools. This paper highlights the transformative impact of this context-aware <b>LLM,</b> underscoring its crucial role in addressing the global healthcare workforce deficit and propelling forward healthcare outcomes in LMICs.</p></p class="citation"></blockquote><h3 id=1738--17276-interactive-prompt-debugging-with-sequence-salience-ian-tenney-et-al-2024>(17/38 | 17/276) Interactive Prompt Debugging with Sequence Salience (Ian Tenney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ian Tenney, Ryan Mullins, Bin Du, Shree Pandya, Minsuk Kahng, Lucas Dixon. (2024)<br><strong>Interactive Prompt Debugging with Sequence Salience</strong><br><button class=copy-to-clipboard title="Interactive Prompt Debugging with Sequence Salience" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Text Classification, Chain-of-thought, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07498v1.pdf filename=2404.07498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Sequence Salience, a visual tool for interactive <b>prompt</b> debugging with input salience methods. Sequence Salience builds on widely used salience methods for <b>text</b> <b>classification</b> and single-token prediction, and extends this to a system tailored for debugging complex <b>LLM</b> <b>prompts.</b> Our system is well-suited for long <b>texts,</b> <b>and</b> expands on previous work by 1) providing controllable aggregation of token-level salience to the word, sentence, or paragraph level, making salience over long inputs tractable; and 2) supporting rapid iteration where practitioners can act on salience results, refine <b>prompts,</b> and run salience on the new output. We include case studies showing how Sequence Salience can help practitioners work with several complex <b>prompting</b> strategies, including <b>few-shot,</b> <b>chain-of-thought,</b> and constitutional principles. Sequence Salience is built on the Learning Interpretability Tool, an open-source platform for ML model visualizations, and code, notebooks, and tutorials are available at <a href=http://goo.gle/sequence-salience>http://goo.gle/sequence-salience</a>.</p></p class="citation"></blockquote><h3 id=1838--18276-oda-observation-driven-agent-for-integrating-llms-and-knowledge-graphs-lei-sun-et-al-2024>(18/38 | 18/276) ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs (Lei Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Sun, Zhengwei Tao, Youdi Li, Hiroshi Arakawa. (2024)<br><strong>ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs</strong><br><button class=copy-to-clipboard title="ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07677v1.pdf filename=2404.07677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate <b>LLMs</b> and <b>KGs</b> often navigate the task-solving process solely based on the <b>LLM&rsquo;s</b> analysis of the question, overlooking the rich cognitive potential inherent in the vast <b>knowledge</b> <b>encapsulated</b> in <b>KGs.</b> To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving <b>KGs.</b> ODA incorporates <b>KG</b> <b>reasoning</b> abilities via global observation that enhances <b>reasoning</b> capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of <b>knowledge</b> <b>during</b> observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed <b>knowledge</b> <b>into</b> the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.</p></p class="citation"></blockquote><h3 id=1938--19276-scalable-language-model-with-generalized-continual-learning-bohao-peng-et-al-2024>(19/38 | 19/276) Scalable Language Model with Generalized Continual Learning (Bohao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohao Peng, Zhuotao Tian, Shu Liu, Mingchang Yang, Jiaya Jia. (2024)<br><strong>Scalable Language Model with Generalized Continual Learning</strong><br><button class=copy-to-clipboard title="Scalable Language Model with Generalized Continual Learning" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Continual Learning, Few-shot, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07470v1.pdf filename=2404.07470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID. In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for <b>continual</b> <b>learning.</b> Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks. This approach leverages the task distribution within the vector space, aiming to achieve a smooth and effortless <b>continual</b> <b>learning</b> process. Our method demonstrates state-of-the-art performance on diverse backbones and <b>benchmarks,</b> achieving effective <b>continual</b> <b>learning</b> in both full-set and <b>few-shot</b> scenarios with minimal forgetting. Moreover, while prior research primarily focused on a single task type such as classification, our study goes beyond, with the <b>large</b> <b>language</b> <b>model,</b> i.e., <b>LLaMA-2,</b> to explore the effects across diverse domains and task types, such that a single language model can be decently scaled to broader applications.</p></p class="citation"></blockquote><h3 id=2038--20276-hgrn2-gated-linear-rnns-with-state-expansion-zhen-qin-et-al-2024>(20/38 | 20/276) HGRN2: Gated Linear RNNs with State Expansion (Zhen Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong. (2024)<br><strong>HGRN2: Gated Linear RNNs with State Expansion</strong><br><button class=copy-to-clipboard title="HGRN2: Gated Linear RNNs with State Expansion" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Graph Attention Networks, LLaMA, Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07904v1.pdf filename=2404.07904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hierarchically <b>gated</b> linear <b>RNN</b> (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and <b>LLaMa</b> Architecture <b>Transformer</b> for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.</p></p class="citation"></blockquote><h3 id=2138--21276-nostra-domina-at-evalatin-2024-improving-latin-polarity-detection-through-data-augmentation-stephen-bothwell-et-al-2024>(21/38 | 21/276) Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation (Stephen Bothwell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephen Bothwell, Abigail Swenor, David Chiang. (2024)<br><strong>Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation</strong><br><button class=copy-to-clipboard title="Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Data Augmentation, Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07792v1.pdf filename=2404.07792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes submissions from the team Nostra Domina to the EvaLatin 2024 shared task of emotion polarity detection. Given the <b>low-resource</b> environment of Latin and the complexity of sentiment in rhetorical genres like poetry, we augmented the available <b>data</b> <b>through</b> automatic polarity annotation. We present two methods for doing so on the basis of the $k$-means algorithm, and we employ a variety of Latin <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in a neural architecture to better capture the underlying contextual sentiment representations. Our best approach achieved the second highest macro-averaged Macro-$F_1$ score on the shared task&rsquo;s test set.</p></p class="citation"></blockquote><h3 id=2238--22276-audio-dialogues-dialogues-dataset-for-audio-and-music-understanding-arushi-goel-et-al-2024>(22/38 | 22/276) Audio Dialogues: Dialogues dataset for audio and music understanding (Arushi Goel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arushi Goel, Zhifeng Kong, Rafael Valle, Bryan Catanzaro. (2024)<br><strong>Audio Dialogues: Dialogues dataset for audio and music understanding</strong><br><button class=copy-to-clipboard title="Audio Dialogues: Dialogues dataset for audio and music understanding" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 40<br>Keywords: Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07616v1.pdf filename=2404.07616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing datasets for audio understanding primarily focus on single-turn interactions (i.e. audio captioning, audio <b>question</b> <b>answering)</b> for describing audio in natural language, thus limiting understanding audio via interactive dialogue. To address this gap, we introduce Audio Dialogues: a multi-turn dialogue dataset containing 163.8k samples for general audio sounds and music. In addition to dialogues, Audio Dialogues also has <b>question-answer</b> <b>pairs</b> to understand and compare multiple input audios together. Audio Dialogues leverages a <b>prompting-based</b> approach and caption annotations from existing datasets to generate multi-turn dialogues using a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM).</b> We evaluate existing audio-augmented <b>large</b> <b>language</b> <b>models</b> on our proposed dataset to demonstrate the complexity and applicability of Audio Dialogues. Our code for generating the dataset will be made publicly available. Detailed <b>prompts</b> and generated dialogues can be found on the demo website <a href=https://audiodialogues.github.io/>https://audiodialogues.github.io/</a>.</p></p class="citation"></blockquote><h3 id=2338--23276-lavy-vietnamese-multimodal-large-language-model-chi-tran-et-al-2024>(23/38 | 23/276) LaVy: Vietnamese Multimodal Large Language Model (Chi Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi Tran, Huong Le Thanh. (2024)<br><strong>LaVy: Vietnamese Multimodal Large Language Model</strong><br><button class=copy-to-clipboard title="LaVy: Vietnamese Multimodal Large Language Model" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07922v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07922v3.pdf filename=2404.07922v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Multimodal</b> <b>Large</b> <b>language</b> <b>models</b> (MLLMs) have taken the world by storm with impressive abilities in complex <b>reasoning</b> and linguistic comprehension. Meanwhile there are plethora of works related to Vietnamese <b>Large</b> <b>Language</b> <b>Models,</b> the lack of high-quality resources in multimodality limits the progress of Vietnamese MLLMs. In this paper, we pioneer in address this by introducing LaVy, a state-of-the-art Vietnamese MLLM, and we also introduce LaVy-Bench <b>benchmark</b> designated for evaluating MLLMs&rsquo;s understanding on Vietnamese visual language tasks. Our project is public at <a href=https://github.com/baochi0212/LaVy>https://github.com/baochi0212/LaVy</a></p></p class="citation"></blockquote><h3 id=2438--24276-medical-mt5-an-open-source-multilingual-text-to-text-llm-for-the-medical-domain-iker-garcía-ferrero-et-al-2024>(24/38 | 24/276) Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain (Iker García-Ferrero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iker García-Ferrero, Rodrigo Agerri, Aitziber Atutxa Salazar, Elena Cabrio, Iker de la Iglesia, Alberto Lavelli, Bernardo Magnini, Benjamin Molinet, Johana Ramirez-Romero, German Rigau, Jose Maria Villa-Gonzalez, Serena Villata, Andrea Zaninello. (2024)<br><strong>Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain</strong><br><button class=copy-to-clipboard title="Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Natural Language Understanding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07613v1.pdf filename=2404.07613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research on language technology for the development of medical applications is currently a hot topic in <b>Natural</b> <b>Language</b> <b>Understanding</b> and Generation. Thus, a number of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently been adapted to the medical domain, so that they can be used as a tool for mediating in human-AI interaction. While these <b>LLMs</b> display competitive performance on automated medical texts <b>benchmarks,</b> they have been pre-trained and evaluated with a focus on a single language (English mostly). This is particularly true of text-to-text models, which typically require <b>large</b> <b>amounts</b> <b>of</b> domain-specific pre-training data, often not easily accessible for many languages. In this paper, we address these shortcomings by compiling, to the best of our knowledge, the largest multilingual corpus for the medical domain in four languages, namely English, French, Italian and Spanish. This new corpus has been used to train Medical mT5, the first open-source text-to-text multilingual model for the medical domain. Additionally, we present two new evaluation <b>benchmarks</b> for all four languages with the aim of facilitating multilingual research in this domain. A comprehensive evaluation shows that Medical mT5 outperforms both encoders and similarly sized text-to-text models for the Spanish, French, and Italian <b>benchmarks,</b> while being competitive with current state-of-the-art <b>LLMs</b> in English.</p></p class="citation"></blockquote><h3 id=2538--25276-noticia-a-clickbait-article-summarization-dataset-in-spanish-iker-garcía-ferrero-et-al-2024>(25/38 | 25/276) NoticIA: A Clickbait Article Summarization Dataset in Spanish (Iker García-Ferrero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iker García-Ferrero, Begoña Altuna. (2024)<br><strong>NoticIA: A Clickbait Article Summarization Dataset in Spanish</strong><br><button class=copy-to-clipboard title="NoticIA: A Clickbait Article Summarization Dataset in Spanish" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Summarization, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07611v1.pdf filename=2404.07611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present NoticIA, a dataset consisting of 850 Spanish news articles featuring prominent clickbait headlines, each paired with high-quality, single-sentence generative <b>summarizations</b> written by humans. This task demands advanced <b>text</b> <b>understanding</b> and <b>summarization</b> abilities, challenging the models&rsquo; capacity to infer and connect diverse pieces of information to meet the user&rsquo;s informational needs generated by the clickbait headline. We evaluate the Spanish <b>text</b> <b>comprehension</b> capabilities of a wide range of state-of-the-art <b>large</b> <b>language</b> <b>models.</b> Additionally, we use the dataset to train ClickbaitFighter, a task-specific model that achieves near-human performance in this task.</p></p class="citation"></blockquote><h3 id=2638--26276-ultraeval-a-lightweight-platform-for-flexible-and-comprehensive-evaluation-for-llms-chaoqun-he-et-al-2024>(26/38 | 26/276) UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs (Chaoqun He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs</strong><br><button class=copy-to-clipboard title="UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07584v1.pdf filename=2404.07584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluation is pivotal for honing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> pinpointing their capabilities and guiding enhancements. The rapid development of <b>LLMs</b> calls for a lightweight and easy-to-use framework for swift evaluation deployment. However, due to the various implementation details to consider, developing a comprehensive evaluation platform is never easy. Existing platforms are often complex and poorly modularized, hindering seamless incorporation into researcher&rsquo;s workflows. This paper introduces UltraEval, a user-friendly evaluation framework characterized by lightweight, comprehensiveness, modularity, and efficiency. We identify and reimplement three core components of model evaluation (models, data, and metrics). The resulting composability allows for the free combination of different models, tasks, <b>prompts,</b> and metrics within a unified evaluation workflow. Additionally, UltraEval supports diverse models owing to a unified HTTP service and provides sufficient inference acceleration. UltraEval is now available for researchers publicly~\footnote{Website is at \url{https://github.com/OpenBMB/UltraEval}}.</p></p class="citation"></blockquote><h3 id=2738--27276-leveraging-data-augmentation-for-process-information-extraction-julian-neuberger-et-al-2024>(27/38 | 27/276) Leveraging Data Augmentation for Process Information Extraction (Julian Neuberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Neuberger, Leonie Doll, Benedict Engelmann, Lars Ackermann, Stefan Jablonski. (2024)<br><strong>Leveraging Data Augmentation for Process Information Extraction</strong><br><button class=copy-to-clipboard title="Leveraging Data Augmentation for Process Information Extraction" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Data Augmentation, Information Retrieval, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07501v1.pdf filename=2404.07501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Business Process Modeling projects often require formal process models as a central component. High costs associated with the creation of such formal process models motivated many different fields of research aimed at automated generation of process models from readily available <b>data.</b> <b>These</b> include process mining on event logs, and generating business process models from natural language texts. Research in the latter field is regularly faced with the problem of limited <b>data</b> <b>availability,</b> hindering both evaluation and development of new techniques, especially learning-based ones. To overcome this <b>data</b> <b>scarcity</b> issue, in this paper we investigate the application of <b>data</b> <b>augmentation</b> for natural language text <b>data.</b> <b>Data</b> <b>augmentation</b> methods are well established in machine learning for creating new, synthetic <b>data</b> <b>without</b> human assistance. We find that many of these methods are applicable to the task of business process <b>information</b> <b>extraction,</b> improving the accuracy of extraction. Our study shows, that <b>data</b> <b>augmentation</b> is an important component in enabling machine learning methods for the task of business process model generation from natural language text, where currently mostly rule-based systems are still state of the art. Simple <b>data</b> <b>augmentation</b> techniques improved the $F_1$ score of mention extraction by 2.9 percentage points, and the $F_1$ of <b>relation</b> <b>extraction</b> by $4.5$. To better understand how <b>data</b> <b>augmentation</b> alters human annotated texts, we analyze the resulting text, visualizing and discussing the properties of augmented textual <b>data.</b> <b>We</b> make all code and experiments results publicly available.</p></p class="citation"></blockquote><h3 id=2838--28276-jetmoe-reaching-llama2-performance-with-01m-dollars-yikang-shen-et-al-2024>(28/38 | 28/276) JetMoE: Reaching Llama2 Performance with 0.1M Dollars (Yikang Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikang Shen, Zhen Guo, Tianle Cai, Zengyi Qin. (2024)<br><strong>JetMoE: Reaching Llama2 Performance with 0.1M Dollars</strong><br><button class=copy-to-clipboard title="JetMoE: Reaching Llama2 Performance with 0.1M Dollars" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07413v1.pdf filename=2404.07413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new <b>LLM</b> trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that <b>LLM</b> training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open <b>foundation</b> <b>models.</b> This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient <b>LLMs.</b> The model weights are publicly available at <a href=https://github.com/myshell-ai/JetMoE>https://github.com/myshell-ai/JetMoE</a>.</p></p class="citation"></blockquote><h3 id=2938--29276-rho-1-not-all-tokens-are-what-you-need-zhenghao-lin-et-al-2024>(29/38 | 29/276) Rho-1: Not All Tokens Are What You Need (Zhenghao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, Weizhu Chen. (2024)<br><strong>Rho-1: Not All Tokens Are What You Need</strong><br><button class=copy-to-clipboard title="Rho-1: Not All Tokens Are What You Need" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Few-shot, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07965v1.pdf filename=2404.07965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that &ldquo;Not all tokens in a corpus are equally important for language model training&rdquo;. Our initial analysis delves into token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher excess loss. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in <b>few-shot</b> accuracy of up to 30% in 9 math tasks. After <b>fine-tuning,</b> Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively - matching DeepSeekMath with only 3% of the pretraining tokens. Furthermore, when pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.</p></p class="citation"></blockquote><h3 id=3038--30276-annoctr-a-dataset-for-detecting-and-linking-entities-tactics-and-techniques-in-cyber-threat-reports-lukas-lange-et-al-2024>(30/38 | 30/276) AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports (Lukas Lange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Lange, Marc Müller, Ghazaleh Haratinezhad Torbati, Dragan Milchevski, Patrick Grau, Subhash Pujari, Annemarie Friedrich. (2024)<br><strong>AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports</strong><br><button class=copy-to-clipboard title="AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Data Augmentation, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07765v1.pdf filename=2404.07765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monitoring the threat landscape to be aware of actual or potential attacks is of utmost importance to cybersecurity professionals. Information about cyber threats is typically distributed using natural language reports. Natural language processing can help with managing this large amount of unstructured information, yet to date, the topic has received little attention. With this paper, we present AnnoCTR, a new CC-BY-SA-licensed dataset of cyber threat reports. The reports have been annotated by a domain expert with named entities, temporal expressions, and cybersecurity-specific concepts including implicitly mentioned techniques and tactics. Entities and concepts are linked to Wikipedia and the MITRE ATT&amp;CK knowledge base, the most widely-used taxonomy for classifying types of attacks. Prior datasets linking to MITRE ATT&amp;CK either provide a single label per document or annotate sentences out-of-context; our dataset annotates entire documents in a much finer-grained way. In an experimental study, we model the annotations of our dataset using state-of-the-art neural models. In our <b>few-shot</b> scenario, we find that for identifying the MITRE ATT&amp;CK concepts that are mentioned explicitly or implicitly in a text, concept descriptions from MITRE ATT&amp;CK are an effective source for training <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=3138--31276-curated-datasets-and-neural-models-for-machine-translation-of-informal-registers-between-mayan-and-spanish-vernaculars-andrés-lou-et-al-2024>(31/38 | 31/276) Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars (Andrés Lou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrés Lou, Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez, Víctor M. Sánchez-Cartagena. (2024)<br><strong>Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars</strong><br><button class=copy-to-clipboard title="Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07673v1.pdf filename=2404.07673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mayan languages comprise a language family with an ancient history, millions of speakers, and immense cultural value, that, nevertheless, remains severely underrepresented in terms of resources and global exposure. In this paper we develop, curate, and publicly release a set of corpora in several Mayan languages spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are parallel with Spanish, the dominant language of the region, and are taken from official native sources focused on representing informal, day-to-day, and non-domain-specific language. As such, and according to our dialectometric analysis, they differ in register from most other available resources. Additionally, we present <b>neural</b> <b>machine</b> <b>translation</b> models, trained on as many resources and Mayan languages as possible, and evaluated exclusively on our datasets. We observe lexical divergences between the dialects of Spanish in our resources and the more widespread written standard of Spanish, and that resources other than the ones we present do not seem to improve translation performance, indicating that many such resources may not accurately capture common, real-life language usage. The MayanV dataset is available at <a href=https://github.com/transducens/mayanv>https://github.com/transducens/mayanv</a>.</p></p class="citation"></blockquote><h3 id=3238--32276-rollama-an-r-package-for-using-generative-large-language-models-through-ollama-johannes-b-gruber-et-al-2024>(32/38 | 32/276) rollama: An R package for using generative large language models through Ollama (Johannes B. Gruber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johannes B. Gruber, Maximilian Weber. (2024)<br><strong>rollama: An R package for using generative large language models through Ollama</strong><br><button class=copy-to-clipboard title="rollama: An R package for using generative large language models through Ollama" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Document Embedding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07654v1.pdf filename=2404.07654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>rollama is an R package that wraps the Ollama API, which allows you to run different Generative <b>Large</b> <b>Language</b> <b>Models</b> (GLLM) locally. The package and learning material focus on making it easy to use Ollama for annotating textual or imagine data with open-source models as well as use these models for <b>document</b> <b>embedding.</b> But users can use or extend rollama to do essentially anything else that is possible through OpenAI&rsquo;s API, yet more private, reproducible and for free.</p></p class="citation"></blockquote><h3 id=3338--33276-confidently-nonsensical-a-critical-survey-on-the-perspectives-and-challenges-of-hallucinations-in-nlp-pranav-narayanan-venkit-et-al-2024>(33/38 | 33/276) &lsquo;Confidently Nonsensical?&rsquo;&rsquo;: A Critical Survey on the Perspectives and Challenges of &lsquo;Hallucinations&rsquo; in NLP (Pranav Narayanan Venkit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul Gupta, Heidi Biggs, Mukund Srinath, Koustava Goswami, Sarah Rajtmajer, Shomir Wilson. (2024)<br><strong>&lsquo;Confidently Nonsensical?&rsquo;&rsquo;: A Critical Survey on the Perspectives and Challenges of &lsquo;Hallucinations&rsquo; in NLP</strong><br><button class=copy-to-clipboard title="'Confidently Nonsensical?'': A Critical Survey on the Perspectives and Challenges of 'Hallucinations' in NLP" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07461v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07461v1.pdf filename=2404.07461v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate how hallucination in <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> is characterized in peer-reviewed literature using a critical examination of 103 publications across NLP research. Through a comprehensive review of sociological and technological literature, we identify a lack of agreement with the term `hallucination.&rsquo; Additionally, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis underscores the necessity for explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.</p></p class="citation"></blockquote><h3 id=3438--34276-graph-integrated-language-transformers-for-next-action-prediction-in-complex-phone-calls-amin-hosseiny-marani-et-al-2024>(34/38 | 34/276) Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls (Amin Hosseiny Marani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Hosseiny Marani, Ulie Schnaithmann, Youngseo Son, Akil Iyer, Manas Paldhe, Arushi Raghuvanshi. (2024)<br><strong>Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls</strong><br><button class=copy-to-clipboard title="Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08155v1.pdf filename=2404.08155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current Conversational AI systems employ different machine learning pipelines, as well as external knowledge sources and business logic to predict the next action. Maintaining various components in dialogue managers&rsquo; pipeline adds complexity in expansion and updates, increases processing time, and causes additive noise through the pipeline that can lead to incorrect next action prediction. This paper investigates <b>graph</b> integration into language <b>transformers</b> to improve understanding the relationships between humans&rsquo; utterances, previous, and next actions without the dependency on external sources or components. Experimental analyses on real calls indicate that the proposed <b>Graph</b> Integrated Language <b>Transformer</b> models can achieve higher performance compared to other production level conversational AI systems in driving interactive calls with human users in real-world settings.</p></p class="citation"></blockquote><h3 id=3538--35276-researchagent-iterative-research-idea-generation-over-scientific-literature-with-large-language-models-jinheon-baek-et-al-2024>(35/38 | 35/276) ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models (Jinheon Baek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang. (2024)<br><strong>ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models</strong><br><button class=copy-to-clipboard title="ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07738v1.pdf filename=2404.07738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a <b>large</b> <b>language</b> <b>model-powered</b> research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic <b>graph</b> but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned <b>large</b> <b>language</b> <b>models</b> whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.</p></p class="citation"></blockquote><h3 id=3638--36276-hltcoe-at-trec-2023-neuclir-track-eugene-yang-et-al-2024>(36/38 | 36/276) HLTCOE at TREC 2023 NeuCLIR Track (Eugene Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugene Yang, Dawn Lawrie, James Mayfield. (2024)<br><strong>HLTCOE at TREC 2023 NeuCLIR Track</strong><br><button class=copy-to-clipboard title="HLTCOE at TREC 2023 NeuCLIR Track" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08118v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08118v1.pdf filename=2404.08118v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The HLTCOE team applied PLAID, an mT5 reranker, and document translation to the TREC 2023 NeuCLIR track. For PLAID we included a variety of models and training techniques &ndash; the English model released with ColBERT v2, translate-train~(TT), Translate Distill~(TD) and multilingual translate-train~(MTT). TT trains a ColBERT model with English queries and passages automatically translated into the document language from the MS-MARCO v1 collection. This results in three cross-language models for the track, one per language. MTT creates a single model for all three document languages by combining the translations of MS-MARCO passages in all three languages into mixed-language batches. Thus the model learns about matching queries to passages simultaneously in all languages. <b>Distillation</b> uses scores from the mT5 model over non-English translated document pairs to learn how to score query-document pairs. The team submitted runs to all NeuCLIR tasks: the CLIR and MLIR news task as well as the technical documents task.</p></p class="citation"></blockquote><h3 id=3738--37276-laissez-faire-harms-algorithmic-biases-in-generative-language-models-evan-shieh-et-al-2024>(37/38 | 37/276) Laissez-Faire Harms: Algorithmic Biases in Generative Language Models (Evan Shieh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evan Shieh, Faye-Marie Vassel, Cassidy Sugimoto, Thema Monroe-White. (2024)<br><strong>Laissez-Faire Harms: Algorithmic Biases in Generative Language Models</strong><br><button class=copy-to-clipboard title="Laissez-Faire Harms: Algorithmic Biases in Generative Language Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07475v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07475v2.pdf filename=2404.07475v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid deployment of generative language models (LMs) has raised concerns about social biases affecting the well-being of diverse consumers. The extant literature on generative LMs has primarily examined bias via explicit identity <b>prompting.</b> However, prior research on bias in earlier language-based technology platforms, including search engines, has shown that discrimination can occur even when identity terms are not specified explicitly. Studies of bias in LM responses to open-ended <b>prompts</b> (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms. Here, we advance studies of generative LM bias by considering a broader set of natural use cases via open-ended <b>prompting.</b> In this &ldquo;laissez-faire&rdquo; setting, we find that synthetically generated texts from five of the most pervasive LMs (ChatGPT3.5, ChatGPT4, Claude2.0, Llama2, and PaLM2) perpetuate harms of omission, subordination, and stereotyping for minoritized individuals with intersectional race, gender, and/or sexual orientation identities (AI/AN, Asian, Black, Latine, MENA, NH/PI, Female, Non-binary, Queer). We find widespread evidence of bias to an extent that such individuals are hundreds to thousands of times more likely to encounter LM-generated outputs that portray their identities in a subordinated manner compared to representative or empowering portrayals. We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals. These include stereotype threat, which leads to impaired cognitive performance and increased negative self-perception. Our findings highlight the urgent need to protect consumers from discriminatory harms caused by language models and invest in critical AI education programs tailored towards empowering diverse consumers.</p></p class="citation"></blockquote><h3 id=3838--38276-multimodal-contextual-dialogue-breakdown-detection-for-conversational-ai-models-md-messal-monem-miah-et-al-2024>(38/38 | 38/276) Multimodal Contextual Dialogue Breakdown Detection for Conversational AI Models (Md Messal Monem Miah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Messal Monem Miah, Ulie Schnaithmann, Arushi Raghuvanshi, Youngseo Son. (2024)<br><strong>Multimodal Contextual Dialogue Breakdown Detection for Conversational AI Models</strong><br><button class=copy-to-clipboard title="Multimodal Contextual Dialogue Breakdown Detection for Conversational AI Models" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08156v1.pdf filename=2404.08156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting dialogue breakdown in real time is critical for conversational AI systems, because it enables taking corrective action to successfully complete a task. In spoken dialog systems, this breakdown can be caused by a variety of unexpected situations including high levels of background noise, causing STT mistranscriptions, or unexpected user flows. In particular, industry settings like healthcare, require high precision and high flexibility to navigate differently based on the conversation history and dialogue states. This makes it both more challenging and more critical to accurately detect dialog breakdown. To accurately detect breakdown, we found it requires processing audio inputs along with downstream NLP model inferences on transcribed text in real time. In this paper, we introduce a <b>Multimodal</b> Contextual Dialogue Breakdown (MultConDB) model. This model significantly outperforms other known best models by achieving an F1 of 69.27.</p></p class="citation"></blockquote><h2 id=cscv-77>cs.CV (77)</h2><h3 id=177--39276-learning-to-localize-objects-improves-spatial-reasoning-in-visual-llms-kanchana-ranasinghe-et-al-2024>(1/77 | 39/276) Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs (Kanchana Ranasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanchana Ranasinghe, Satya Narayan Shukla, Omid Poursaeed, Michael S. Ryoo, Tsung-Yu Lin. (2024)<br><strong>Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs</strong><br><button class=copy-to-clipboard title="Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Fine-tuning, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07449v1.pdf filename=2404.07449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into <b>visual</b> <b>domain</b> <b>tasks,</b> resulting in <b>visual-LLMs</b> <b>(V-LLMs),</b> <b>has</b> enabled exceptional performance in <b>vision-language</b> tasks, particularly for <b>visual</b> <b>question</b> <b>answering</b> <b>(VQA).</b> However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial <b>reasoning</b> and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction <b>fine-tuning</b> objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction <b>fine-tuning</b> objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves <b>VQA</b> across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 <b>vision-language</b> tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.</p></p class="citation"></blockquote><h3 id=277--40276-tbsn-transformer-based-blind-spot-network-for-self-supervised-image-denoising-junyi-li-et-al-2024>(2/77 | 40/276) TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising (Junyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Li, Zhilu Zhang, Wangmeng Zuo. (2024)<br><strong>TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising</strong><br><button class=copy-to-clipboard title="TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 70<br>Keywords: Convolution, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07846v1.pdf filename=2404.07846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blind-spot networks (BSN) have been prevalent network architectures in <b>self-supervised</b> image denoising (SSID). Existing BSNs are mostly conducted with <b>convolution</b> layers. Although <b>transformers</b> offer potential solutions to the limitations of <b>convolutions</b> and have demonstrated success in various image restoration tasks, their attention mechanisms may violate the blind-spot requirement, thus restricting their applicability in SSID. In this paper, we present a <b>transformer-based</b> blind-spot network (TBSN) by analyzing and redesigning the <b>transformer</b> operators that meet the blind-spot requirement. Specifically, TBSN follows the architectural principles of dilated BSNs, and incorporates spatial as well as channel <b>self-attention</b> layers to enhance the network capability. For spatial <b>self-attention,</b> an elaborate mask is applied to the attention matrix to restrict its receptive field, thus mimicking the dilated <b>convolution.</b> For channel <b>self-attention,</b> we observe that it may leak the blind-spot information when the channel number is greater than spatial size in the deep layers of multi-scale architectures. To eliminate this effect, we divide the channel into several groups and perform channel attention separately. Furthermore, we introduce a <b>knowledge</b> <b>distillation</b> strategy that <b>distills</b> TBSN into smaller denoisers to improve computational efficiency while maintaining performance. Extensive experiments on real-world image denoising datasets show that TBSN largely extends the receptive field and exhibits favorable performance against state-of-the-art SSID methods. The code and pre-trained models will be publicly available at <a href=https://github.com/nagejacob/TBSN>https://github.com/nagejacob/TBSN</a>.</p></p class="citation"></blockquote><h3 id=377--41276-glid-pre-training-a-generalist-encoder-decoder-vision-model-jihao-liu-et-al-2024>(3/77 | 41/276) GLID: Pre-training a Generalist Encoder-Decoder Vision Model (Jihao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihao Liu, Jinliang Zheng, Yu Liu, Hongsheng Li. (2024)<br><strong>GLID: Pre-training a Generalist Encoder-Decoder Vision Model</strong><br><button class=copy-to-clipboard title="GLID: Pre-training a Generalist Encoder-Decoder Vision Model" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Object Detection, Autoencoder, Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Pre-training, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07603v1.pdf filename=2404.07603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks. While <b>self-supervised</b> <b>pre-training</b> approaches, e.g., Masked <b>Autoencoder,</b> have shown success in <b>transfer</b> <b>learning,</b> task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training. GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be <b>fine-tuned</b> on various vision tasks with minimal task-specific architecture modifications. In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as &ldquo;query-to-answer&rdquo; problems, including the pre-training pretext task and other downstream tasks. We pre-train a task-agnostic encoder-decoder with query-mask pairs. During <b>fine-tuning,</b> GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads. This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks. GLID achieves competitive performance on various vision tasks, including <b>object</b> <b>detection,</b> image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer.</p></p class="citation"></blockquote><h3 id=477--42276-dgmamba-domain-generalization-via-generalized-state-space-model-shaocong-long-et-al-2024>(4/77 | 42/276) DGMamba: Domain Generalization via Generalized State Space Model (Shaocong Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaocong Long, Qianyu Zhou, Xiangtai Li, Xuequan Lu, Chenhao Ying, Yuan Luo, Lizhuang Ma, Shuicheng Yan. (2024)<br><strong>DGMamba: Domain Generalization via Generalized State Space Model</strong><br><button class=copy-to-clipboard title="DGMamba: Domain Generalization via Generalized State Space Model" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 68<br>Keywords: Convolutional Neural Network, Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Distribution Shift, Distribution Shift, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07794v1.pdf filename=2404.07794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain generalization~(DG) aims at solving <b>distribution</b> <b>shift</b> problems in various scenes. Existing approaches are based on <b>Convolution</b> Neural Networks <b>(CNNs)</b> or <b>Vision</b> <b>Transformers</b> (ViTs), which suffer from limited receptive fields or quadratic complexities issues. Mamba, as an emerging state space model (SSM), possesses superior linear complexity and global receptive fields. Despite this, it can hardly be applied to DG to address <b>distribution</b> <b>shifts,</b> due to the hidden state issues and inappropriate scan mechanisms. In this paper, we propose a novel framework for DG, named DGMamba, that excels in strong generalizability toward unseen domains and meanwhile has the advantages of global receptive fields, and efficient linear complexity. Our DGMamba compromises two core components: Hidden State Suppressing~(HSS) and Semantic-aware Patch refining~(SPR). In particular, HSS is introduced to mitigate the influence of hidden states associated with domain-specific features during output prediction. SPR strives to encourage the model to concentrate more on objects rather than context, consisting of two designs: Prior-Free Scanning~(PFS), and Domain Context Interchange~(DCI). Concretely, PFS aims to shuffle the non-semantic patches within images, creating more flexible and effective sequences from images, and DCI is designed to regularize Mamba with the combination of mismatched non-semantic and semantic information by fusing patches among domains. Extensive experiments on four commonly used DG <b>benchmarks</b> demonstrate that the proposed DGMamba achieves remarkably superior results to state-of-the-art models. The code will be made publicly available.</p></p class="citation"></blockquote><h3 id=577--43276-progressive-semantic-guided-vision-transformer-for-zero-shot-learning-shiming-chen-et-al-2024>(5/77 | 43/276) Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning (Shiming Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiming Chen, Wenjin Hou, Salman Khan, Fahad Shahbaz Khan. (2024)<br><strong>Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning</strong><br><button class=copy-to-clipboard title="Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 68<br>Keywords: Convolutional Neural Network, Vision Transformer, Benchmarking, Convolutional Neural Network, Zero-shot, Transformer, Vision Transformer, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07713v1.pdf filename=2404.07713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> <b>learning</b> (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer semantic knowledge from seen classes to unseen ones, supported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., <b>CNN</b> or ViT), which fail to learn matched visual-semantic correspondences for representing semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we propose a progressive semantic-guided <b>vision</b> <b>transformer</b> for <b>zero-shot</b> <b>learning</b> (dubbed ZSLViT). ZSLViT mainly considers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specifically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual information for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular <b>benchmark</b> datasets, i.e., CUB, SUN, and AWA2.</p></p class="citation"></blockquote><h3 id=677--44276-finding-dino-a-plug-and-play-framework-for-unsupervised-detection-of-out-of-distribution-objects-using-prototypes-poulami-sinhamahapatra-et-al-2024>(6/77 | 44/276) Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes (Poulami Sinhamahapatra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Poulami Sinhamahapatra, Franziska Schwaiger, Shirsha Bose, Huiyu Wang, Karsten Roscher, Stephan Guennemann. (2024)<br><strong>Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes</strong><br><button class=copy-to-clipboard title="Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Object Detection, Benchmarking, Out-of-distribution, Self-supervised Learning, Supervised Learning, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07664v1.pdf filename=2404.07664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting and localising unknown or <b>Out-of-distribution</b> (OOD) <b>objects</b> <b>in</b> any scene can be a challenging task in vision. Particularly, in safety-critical cases involving autonomous systems like automated vehicles or trains. <b>Supervised</b> anomaly segmentation or open-world <b>object</b> <b>detection</b> models depend on training on exhaustively annotated datasets for every domain and still struggle in distinguishing between background and OOD <b>objects.</b> <b>In</b> this work, we present a plug-and-play generalised framework - PRototype-based <b>zero-shot</b> OOD detection Without Labels (PROWL). It is an inference-based method that does not require training on the domain dataset and relies on extracting relevant features from <b>self-supervised</b> pre-trained models. PROWL can be easily adapted to detect OOD <b>objects</b> <b>in</b> any operational design domain by specifying a list of known classes from this domain. PROWL, as an <b>unsupervised</b> method, outperforms other <b>supervised</b> methods trained without auxiliary OOD data on the RoadAnomaly and RoadObstacle datasets provided in SegmentMeIfYouCan (SMIYC) <b>benchmark.</b> We also demonstrate its suitability for other domains such as rail and maritime scenes.</p></p class="citation"></blockquote><h3 id=777--45276-self-supervised-dataset-distillation-a-good-compression-is-all-you-need-muxin-zhou-et-al-2024>(7/77 | 45/276) Self-supervised Dataset Distillation: A Good Compression Is All You Need (Muxin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muxin Zhou, Zeyuan Yin, Shitong Shao, Zhiqiang Shen. (2024)<br><strong>Self-supervised Dataset Distillation: A Good Compression Is All You Need</strong><br><button class=copy-to-clipboard title="Self-supervised Dataset Distillation: A Good Compression Is All You Need" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning, Information Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07976v1.pdf filename=2404.07976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> aims to compress <b>information</b> <b>from</b> a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of the original data <b>informational</b> <b>essence.</b> Previous studies have predominantly concentrated on aligning the intermediate statistics between the original and <b>distilled</b> data, such as weight trajectory, features, gradient, BatchNorm, etc. In this work, we consider addressing this task through the new lens of model informativeness in the compression stage on the original dataset pretraining. We observe that with the prior state-of-the-art SRe$^2$L, as model sizes increase, it becomes increasingly challenging for <b>supervised</b> <b>pretrained</b> models to recover learned <b>information</b> <b>during</b> data synthesis, as the channel-wise mean and variance inside the model are flatting and less informative. We further notice that larger variances in BN statistics from <b>self-supervised</b> models enable larger loss signals to update the recovered data by gradients, enjoying more informativeness during synthesis. Building on this observation, we introduce SC-DD, a simple yet effective <b>Self-supervised</b> Compression framework for Dataset <b>Distillation</b> that facilitates diverse <b>information</b> <b>compression</b> and recovery compared to traditional <b>supervised</b> <b>learning</b> schemes, further reaps the potential of large pretrained models with enhanced capabilities. Extensive experiments are conducted on CIFAR-100, Tiny-ImageNet and ImageNet-1K datasets to demonstrate the superiority of our proposed approach. The proposed SC-DD outperforms all previous state-of-the-art <b>supervised</b> <b>dataset</b> <b>distillation</b> methods when employing larger models, such as SRe$^2$L, MTT, TESLA, DC, CAFE, etc., by large margins under the same recovery and post-training budgets. Code is available at <a href=https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/>https://github.com/VILA-Lab/SRe2L/tree/main/SCDD/</a>.</p></p class="citation"></blockquote><h3 id=877--46276-simba-mamba-augmented-u-shiftgcn-for-skeletal-action-recognition-in-videos-soumyabrata-chaudhuri-et-al-2024>(8/77 | 46/276) Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos (Soumyabrata Chaudhuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soumyabrata Chaudhuri, Saumik Bhattacharya. (2024)<br><strong>Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos</strong><br><button class=copy-to-clipboard title="Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07645v1.pdf filename=2404.07645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton Action Recognition (SAR) involves identifying human actions using skeletal joint coordinates and their interconnections. While plain <b>Transformers</b> have been attempted for this task, they still fall short compared to the current leading methods, which are rooted in <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> due to the absence of structural priors. Recently, a novel selective state space model, Mamba, has surfaced as a compelling alternative to the attention mechanism in <b>Transformers,</b> offering efficient modeling of long sequences. In this work, to the utmost extent of our awareness, we present the first SAR framework incorporating Mamba. Each fundamental block of our model adopts a novel U-ShiftGCN architecture with Mamba as its core component. The encoder segment of the U-ShiftGCN is devised to extract spatial features from the skeletal data using downsampling vanilla Shift S-GCN blocks. These spatial features then undergo intermediate temporal modeling facilitated by the Mamba block before progressing to the encoder section, which comprises vanilla upsampling Shift S-GCN blocks. Additionally, a Shift T-GCN (ShiftTCN) temporal modeling unit is employed before the exit of each fundamental block to refine temporal representations. This particular integration of downsampling spatial, intermediate temporal, upsampling spatial, and ultimate temporal subunits yields promising results for skeleton action recognition. We dub the resulting model \textbf{Simba}, which attains state-of-the-art performance across three well-known <b>benchmark</b> skeleton action recognition datasets: NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA. Interestingly, U-ShiftGCN (Simba without Intermediate Mamba Block) by itself is capable of performing reasonably well and surpasses our baseline.</p></p class="citation"></blockquote><h3 id=977--47276-voice-assisted-real-time-traffic-sign-recognition-system-using-convolutional-neural-network-mayura-manawadu-et-al-2024>(9/77 | 47/276) Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network (Mayura Manawadu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mayura Manawadu, Udaya Wijenayake. (2024)<br><strong>Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network</strong><br><button class=copy-to-clipboard title="Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07807v1.pdf filename=2404.07807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic signs are important in communicating information to drivers. Thus, comprehension of traffic signs is essential for road safety and ignorance may result in road accidents. Traffic sign detection has been a research spotlight over the past few decades. Real-time and accurate detections are the preliminaries of robust traffic sign detection system which is yet to be achieved. This study presents a voice-assisted real-time traffic sign recognition system which is capable of assisting drivers. This system functions under two subsystems. Initially, the detection and recognition of the traffic signs are carried out using a trained <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN).</b> After recognizing the specific traffic sign, it is narrated to the driver as a voice message using a <b>text-to-speech</b> engine. An efficient <b>CNN</b> model for a <b>benchmark</b> dataset is developed for real-time detection and recognition using Deep Learning techniques. The advantage of this system is that even if the driver misses a traffic sign, or does not look at the traffic sign, or is unable to comprehend the sign, the system detects it and narrates it to the driver. A system of this type is also important in the development of autonomous vehicles.</p></p class="citation"></blockquote><h3 id=1077--48276-attention-based-end-to-end-network-for-offline-writer-identification-on-word-level-data-vineet-kumar-et-al-2024>(10/77 | 48/276) Attention based End to end network for Offline Writer Identification on Word level data (Vineet Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vineet Kumar, Suresh Sundaram. (2024)<br><strong>Attention based End to end network for Offline Writer Identification on Word level data</strong><br><button class=copy-to-clipboard title="Attention based End to end network for Offline Writer Identification on Word level data" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07602v1.pdf filename=2404.07602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Writer identification due to its widespread application in various fields has gained popularity over the years. In scenarios where optimum handwriting samples are available, whether they be in the form of a single line, a sentence, or an entire page, writer identification algorithms have demonstrated noteworthy levels of accuracy. However, in scenarios where only a limited number of handwritten samples are available, particularly in the form of word images, there is a significant scope for improvement. In this paper, we propose a writer identification system based on an attention-driven <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN).</b> The system is trained utilizing image segments, known as fragments, extracted from word images, employing a pyramid-based strategy. This methodology enables the system to capture a comprehensive representation of the data, encompassing both fine-grained details and coarse features across various levels of abstraction. These extracted fragments serve as the training data for the <b>convolutional</b> <b>network,</b> <b>enabling</b> it to learn a more robust representation compared to traditional <b>convolution-based</b> networks trained on word images. Additionally, the paper explores the integration of an attention mechanism to enhance the representational power of the learned features. The efficacy of the proposed algorithm is evaluated on three <b>benchmark</b> databases, demonstrating its proficiency in writer identification tasks, particularly in scenarios with limited access to handwriting data.</p></p class="citation"></blockquote><h3 id=1177--49276-openbias-open-set-bias-detection-in-text-to-image-generative-models-moreno-dincà-et-al-2024>(11/77 | 49/276) OpenBias: Open-set Bias Detection in Text-to-Image Generative Models (Moreno D&rsquo;Incà et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moreno D&rsquo;Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, Nicu Sebe. (2024)<br><strong>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</strong><br><button class=copy-to-clipboard title="OpenBias: Open-set Bias Detection in Text-to-Image Generative Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fairness, Question Answering, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07990v1.pdf filename=2404.07990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generative models are becoming increasingly popular and accessible to the general public. As these models see <b>large-scale</b> <b>deployments,</b> <b>it</b> is necessary to deeply investigate their safety and <b>fairness</b> to not disseminate and perpetuate any kind of biases. However, existing works focus on detecting closed sets of biases defined a priori, limiting the studies to well-known concepts. In this paper, we tackle the challenge of open-set bias detection in <b>text-to-image</b> generative models presenting OpenBias, a new pipeline that identifies and quantifies the severity of biases agnostically, without access to any precompiled set. OpenBias has three stages. In the first phase, we leverage a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to propose biases given a set of captions. Secondly, the target generative model produces images using the same set of captions. Lastly, a Vision <b>Question</b> <b>Answering</b> model recognizes the presence and extent of the previously proposed biases. We study the behavior of Stable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated before. Via quantitative experiments, we demonstrate that OpenBias agrees with current closed-set bias detection methods and human judgement.</p></p class="citation"></blockquote><h3 id=1277--50276-implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception-hefeng-wang-et-al-2024>(12/77 | 50/276) Implicit and Explicit Language Guidance for Diffusion-based Visual Perception (Hefeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hefeng Wang, Jiale Cao, Jin Xie, Aiping Yang, Yanwei Pang. (2024)<br><strong>Implicit and Explicit Language Guidance for Diffusion-based Visual Perception</strong><br><button class=copy-to-clipboard title="Implicit and Explicit Language Guidance for Diffusion-based Visual Perception" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Text2image, Prompt, Text Embedding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07600v1.pdf filename=2404.07600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have shown powerful ability on conditional image synthesis. With large-scale <b>vision-language</b> pre-training, <b>diffusion</b> <b>models</b> are able to generate high-quality images with rich texture and reasonable structure under different <b>text</b> <b>prompts.</b> However, it is an open problem to adapt the pre-trained <b>diffusion</b> <b>model</b> for visual perception. In this paper, we propose an implicit and explicit language guidance framework for <b>diffusion-based</b> <b>perception,</b> named IEDP. Our IEDP comprises of an implicit language guidance branch and an explicit language guidance branch. The implicit branch employs frozen CLIP image encoder to directly generate implicit <b>text</b> <b>embeddings</b> that are fed to <b>diffusion</b> <b>model,</b> without using explicit <b>text</b> <b>prompts.</b> The explicit branch utilizes the ground-truth labels of corresponding images as <b>text</b> <b>prompts</b> to condition feature extraction of <b>diffusion</b> <b>model.</b> During training, we jointly train <b>diffusion</b> <b>model</b> by sharing the model weights of these two branches. As a result, implicit and explicit branches can jointly guide feature learning. During inference, we only employ implicit branch for final prediction, which does not require any ground-truth labels. Experiments are performed on two typical perception tasks, including semantic segmentation and depth estimation. Our IEDP achieves promising performance on both tasks. For semantic segmentation, our IEDP has the mIoU score of 55.9% on AD20K validation set, which outperforms the baseline method VPD by 2.2%. For depth estimation, our IEDP outperforms the baseline method VPD with a relative gain of 10.2%.</p></p class="citation"></blockquote><h3 id=1377--51276-encoding-urban-ecologies-automated-building-archetype-generation-through-self-supervised-learning-for-energy-modeling-xinwei-zhuang-et-al-2024>(13/77 | 51/276) Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling (Xinwei Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinwei Zhuang, Zixun Huang, Wentao Zeng, Luisa Caldas. (2024)<br><strong>Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling</strong><br><button class=copy-to-clipboard title="Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07435v1.pdf filename=2404.07435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the global population and urbanization expand, the building sector has emerged as the predominant energy consumer and carbon emission contributor. The need for innovative Urban Building Energy Modeling grows, yet existing building archetypes often fail to capture the unique attributes of local buildings and the nuanced distinctions between different cities, jeopardizing the precision of energy modeling. This paper presents an alternative tool employing <b>self-supervised</b> <b>learning</b> to <b>distill</b> complex geometric data into representative, locale-specific archetypes. This study attempts to foster a new paradigm of interaction with built environments, incorporating local parameters to conduct bespoke energy <b>simulations</b> at the community level. The catered archetypes can augment the precision and applicability of energy consumption modeling at different scales across diverse building inventories. This tool provides a potential solution that encourages the exploration of emerging local ecologies. By integrating building envelope characteristics and cultural granularity into the building archetype generation process, we seek a future where architecture and urban design are intricately interwoven with the energy sector in shaping our built environments.</p></p class="citation"></blockquote><h3 id=1477--52276-boosting-self-supervision-for-single-view-scene-completion-via-knowledge-distillation-keonhee-han-et-al-2024>(14/77 | 52/276) Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation (Keonhee Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keonhee Han, Dominik Muhle, Felix Wimbauer, Daniel Cremers. (2024)<br><strong>Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Geometry, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07933v1.pdf filename=2404.07933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inferring scene <b>geometry</b> from images via Structure from Motion is a long-standing and fundamental problem in computer vision. While classical approaches and, more recently, depth map predictions only focus on the visible parts of a scene, the task of scene completion aims to reason about <b>geometry</b> even in occluded regions. With the popularity of neural radiance fields (NeRFs), implicit representations also became popular for scene completion by predicting so-called density fields. Unlike explicit approaches. e.g. voxel-based methods, density fields also allow for accurate depth prediction and novel-view synthesis via image-based rendering. In this work, we propose to fuse the scene reconstruction from multiple images and <b>distill</b> this <b>knowledge</b> <b>into</b> a more accurate single-view scene reconstruction. To this end, we propose Multi-View Behind the Scenes (MVBTS) to fuse density fields from multiple posed images, trained fully <b>self-supervised</b> only from image data. Using <b>knowledge</b> <b>distillation,</b> we use MVBTS to train a single-view scene completion network via direct supervision called KDBTS. It achieves state-of-the-art performance on occupancy prediction, especially in occluded regions.</p></p class="citation"></blockquote><h3 id=1577--53276-promptsync-bridging-domain-gaps-in-vision-language-models-through-class-aware-prototype-alignment-and-discrimination-anant-khandelwal-2024>(15/77 | 53/276) PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination (Anant Khandelwal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anant Khandelwal. (2024)<br><strong>PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination</strong><br><button class=copy-to-clipboard title="PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Zero-shot, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07520v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07520v2.pdf filename=2404.07520v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The potential for <b>zero-shot</b> generalization in <b>vision-language</b> (V-L) models such as CLIP has spurred their widespread adoption in addressing numerous downstream tasks. Previous methods have employed test-time <b>prompt</b> tuning to adapt the model to unseen domains, but they overlooked the issue of imbalanced class distributions. In this study, we explicitly address this problem by employing class-aware prototype alignment weighted by mean class probabilities obtained for the test sample and filtered augmented views. Additionally, we ensure that the class probabilities are as accurate as possible by performing prototype discrimination using <b>contrastive</b> <b>learning.</b> The combination of alignment and discriminative loss serves as a geometric regularizer, preventing the <b>prompt</b> representation from collapsing onto a single class and effectively bridging the distribution gap between the source and test domains. Our method, named PromptSync, synchronizes the <b>prompts</b> for each test sample on both the text and vision branches of the V-L model. In empirical evaluations on the domain generalization <b>benchmark,</b> our method outperforms previous best methods by 2.33% in overall performance, by 1% in base-to-novel generalization, and by 2.84% in cross-dataset transfer tasks.</p></p class="citation"></blockquote><h3 id=1677--54276-transferable-and-principled-efficiency-for-open-vocabulary-segmentation-jingxuan-xu-et-al-2024>(16/77 | 54/276) Transferable and Principled Efficiency for Open-Vocabulary Segmentation (Jingxuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxuan Xu, Wuyang Chen, Yao Zhao, Yunchao Wei. (2024)<br><strong>Transferable and Principled Efficiency for Open-Vocabulary Segmentation</strong><br><button class=copy-to-clipboard title="Transferable and Principled Efficiency for Open-Vocabulary Segmentation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV, eess-IV<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Model Compression, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07448v1.pdf filename=2404.07448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent success of pre-trained <b>foundation</b> <b>vision-language</b> <b>models</b> <b>makes</b> Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance, this approach introduces heavy computational overheads for two challenges: 1) large <b>model</b> <b>sizes</b> of the backbone; 2) expensive costs during the <b>fine-tuning.</b> These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as <b>model</b> <b>compression</b> and efficient <b>fine-tuning</b> can address these challenges, they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different <b>models,</b> <b>which</b> comes at a cost. In the context of efficient OVS, we target achieving performance that is comparable to or even better than prior OVS works based on large <b>vision-language</b> <b>foundation</b> <b>models,</b> <b>by</b> utilizing smaller <b>models</b> <b>that</b> incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS <b>benchmarks</b> demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on <a href=https://github.com/Xujxyang/OpenTrans>https://github.com/Xujxyang/OpenTrans</a></p></p class="citation"></blockquote><h3 id=1777--55276-improving-shift-invariance-in-convolutional-neural-networks-with-translation-invariant-polyphase-sampling-sourajit-saha-et-al-2024>(17/77 | 55/276) Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling (Sourajit Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourajit Saha, Tejas Gokhale. (2024)<br><strong>Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling</strong><br><button class=copy-to-clipboard title="Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07410v1.pdf filename=2404.07410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Downsampling operators break the shift invariance of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> and this affects the robustness of features learned by <b>CNNs</b> when dealing with even small pixel-level shift. Through a large-scale correlation analysis framework, we study shift invariance of <b>CNNs</b> by inspecting existing downsampling operators in terms of their maximum-sampling bias (MSB), and find that MSB is negatively correlated with shift invariance. Based on this crucial insight, we propose a learnable pooling operator called Translation Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps of TIPS to reduce MSB and learn translation-invariant representations. TIPS can be integrated into any <b>CNN</b> and can be trained end-to-end with marginal computational overhead. Our experiments demonstrate that TIPS results in consistent performance gains in terms of accuracy, shift consistency, and shift fidelity on multiple <b>benchmarks</b> for image classification and semantic segmentation compared to previous methods and also leads to improvements in adversarial and distributional robustness. TIPS results in the lowest MSB compared to all previous methods, thus explaining our strong empirical results.</p></p class="citation"></blockquote><h3 id=1877--56276-latent-guard-a-safety-framework-for-text-to-image-generation-runtao-liu-et-al-2024>(18/77 | 56/276) Latent Guard: a Safety Framework for Text-to-image Generation (Runtao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, Fabio Pizzati. (2024)<br><strong>Latent Guard: a Safety Framework for Text-to-image Generation</strong><br><button class=copy-to-clipboard title="Latent Guard: a Safety Framework for Text-to-image Generation" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Text2image, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08031v1.pdf filename=2404.08031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the ability to generate high-quality images, <b>text-to-image</b> <b>(T2I)</b> models can be exploited for creating inappropriate content. To prevent misuse, existing safety measures are either based on <b>text</b> <b>blacklists,</b> which can be easily circumvented, or harmful content classification, requiring <b>large</b> <b>datasets</b> <b>for</b> training and offering low flexibility. Hence, we propose Latent Guard, a framework designed to improve safety measures in <b>text-to-image</b> <b>generation.</b> Inspired by blacklist-based approaches, Latent Guard learns a latent space on top of the T2I model&rsquo;s <b>text</b> <b>encoder,</b> where it is possible to check the presence of harmful concepts in the input <b>text</b> <b>embeddings.</b> Our proposed framework is composed of a data generation pipeline specific to the task using <b>large</b> <b>language</b> <b>models,</b> ad-hoc architectural components, and a <b>contrastive</b> <b>learning</b> strategy to benefit from the generated data. The effectiveness of our method is verified on three datasets and against four baselines. Code and data will be shared at <a href=https://github.com/rt219/LatentGuard>https://github.com/rt219/LatentGuard</a>.</p></p class="citation"></blockquote><h3 id=1977--57276-controlnet-improving-conditional-controls-with-efficient-consistency-feedback-ming-li-et-al-2024>(19/77 | 57/276) ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback (Ming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen. (2024)<br><strong>ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback</strong><br><button class=copy-to-clipboard title="ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: ControlNet, Diffusion Model, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07987v1.pdf filename=2404.07987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enhance the controllability of <b>text-to-image</b> <b>diffusion</b> <b>models,</b> existing efforts like <b>ControlNet</b> incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward <b>fine-tuning.</b> This avoids the extensive costs associated with image sampling, allowing for more efficient reward <b>fine-tuning.</b> Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over <b>ControlNet</b> by 7.9% mIoU, 13.4% SSIM, and 7.6% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions.</p></p class="citation"></blockquote><h3 id=2077--58276-view-selection-for-3d-captioning-via-diffusion-ranking-tiange-luo-et-al-2024>(20/77 | 58/276) View Selection for 3D Captioning via Diffusion Ranking (Tiange Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiange Luo, Justin Johnson, Honglak Lee. (2024)<br><strong>View Selection for 3D Captioning via Diffusion Ranking</strong><br><button class=copy-to-clipboard title="View Selection for 3D Captioning via Diffusion Ranking" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: GPT-4, Question Answering, Text2image, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07984v1.pdf filename=2404.07984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scalable annotation approaches are crucial for constructing extensive 3D-text datasets, facilitating a broader range of applications. However, existing methods sometimes lead to the generation of hallucinated captions, compromising caption quality. This paper explores the issue of hallucination in 3D object captioning, with a focus on Cap3D method, which renders 3D objects into 2D views for captioning using pre-trained models. We pinpoint a major challenge: certain rendered views of 3D objects are atypical, deviating from the training data of standard image captioning models and causing hallucinations. To tackle this, we present DiffuRank, a method that leverages a pre-trained text-to-3D model to assess the alignment between 3D objects and their 2D rendered views, where the view with high alignment closely represent the object&rsquo;s characteristics. By ranking all rendered views and feeding the top-ranked ones into <b>GPT4-Vision,</b> we enhance the accuracy and detail of captions, enabling the correction of 200k captions in the Cap3D dataset and extending it to 1 million captions across Objaverse and Objaverse-XL datasets. Additionally, we showcase the adaptability of DiffuRank by applying it to pre-trained <b>text-to-image</b> models for a <b>Visual</b> <b>Question</b> <b>Answering</b> task, where it outperforms the CLIP model.</p></p class="citation"></blockquote><h3 id=2177--59276-ferret-v2-an-improved-baseline-for-referring-and-grounding-with-large-language-models-haotian-zhang-et-al-2024>(21/77 | 59/276) Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models (Haotian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, Yinfei Yang. (2024)<br><strong>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models</strong><br><button class=copy-to-clipboard title="Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Grounding, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07973v1.pdf filename=2404.07973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While Ferret seamlessly integrates regional understanding into the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> to facilitate its referring and <b>grounding</b> capability, it poses certain limitations: constrained by the pre-trained fixed visual encoder and failed to perform well on broader tasks. In this work, we unveil Ferret-v2, a significant upgrade to Ferret, with three key designs. (1) Any resolution <b>grounding</b> and referring: A flexible approach that effortlessly handles higher image resolution, improving the model&rsquo;s ability to process and understand images in greater detail. (2) Multi-granularity visual encoding: By integrating the additional DINOv2 encoder, the model learns better and diverse underlying contexts for global and fine-grained visual information. (3) A three-stage training paradigm: Besides image-caption alignment, an additional stage is proposed for high-resolution dense alignment before the final <b>instruction</b> <b>tuning.</b> Experiments show that Ferret-v2 provides substantial improvements over Ferret and other state-of-the-art methods, thanks to its high-resolution scaling and fine-grained visual processing.</p></p class="citation"></blockquote><h3 id=2277--60276-multi-image-visual-question-answering-for-unsupervised-anomaly-detection-jun-li-et-al-2024>(22/77 | 60/276) Multi-Image Visual Question Answering for Unsupervised Anomaly Detection (Jun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Li, Cosmin I. Bercea, Philip Müller, Lina Felsner, Suhwan Kim, Daniel Rueckert, Benedikt Wiestler, Julia A. Schnabel. (2024)<br><strong>Multi-Image Visual Question Answering for Unsupervised Anomaly Detection</strong><br><button class=copy-to-clipboard title="Multi-Image Visual Question Answering for Unsupervised Anomaly Detection" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Unsupervised Learning, Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07622v1.pdf filename=2404.07622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>anomaly</b> <b>detection</b> enables the identification of potential pathological areas by juxtaposing original images with their pseudo-healthy reconstructions generated by models trained exclusively on normal images. However, the clinical interpretation of resultant <b>anomaly</b> <b>maps</b> presents a challenge due to a lack of detailed, understandable explanations. Recent advancements in language models have shown the capability of mimicking human-like understanding and providing detailed descriptions. This raises an interesting <b>question:</b> <b>\textit{How</b> can language models be employed to make the <b>anomaly</b> <b>maps</b> more explainable?} To the best of our knowledge, we are the first to leverage a language model for <b>unsupervised</b> <b>anomaly</b> <b>detection,</b> for which we construct a dataset with different <b>questions</b> <b>and</b> answers. Additionally, we present a novel multi-image <b>visual</b> <b>question</b> <b>answering</b> framework tailored for <b>anomaly</b> <b>detection,</b> incorporating diverse feature fusion strategies to enhance <b>visual</b> <b>knowledge</b> <b>extraction.</b> Our experiments reveal that the framework, augmented by our new Knowledge Q-Former module, adeptly answers <b>questions</b> <b>on</b> the <b>anomaly</b> <b>detection</b> dataset. Besides, integrating <b>anomaly</b> <b>maps</b> as inputs distinctly aids in improving the detection of unseen pathologies.</p></p class="citation"></blockquote><h3 id=2377--61276-heron-bench-a-benchmark-for-evaluating-vision-language-models-in-japanese-yuichi-inoue-et-al-2024>(23/77 | 61/276) Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese (Yuichi Inoue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, Yu Yamaguchi. (2024)<br><strong>Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese</strong><br><button class=copy-to-clipboard title="Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Instruction Tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07824v1.pdf filename=2404.07824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision Language Models (VLMs) have undergone a rapid evolution, giving rise to significant advancements in the realm of <b>multimodal</b> understanding tasks. However, the majority of these models are trained and evaluated on English-centric datasets, leaving a gap in the development and evaluation of VLMs for other languages, such as Japanese. This gap can be attributed to the lack of methodologies for constructing VLMs and the absence of <b>benchmarks</b> to accurately measure their performance. To address this issue, we introduce a novel <b>benchmark,</b> Japanese Heron-Bench, for evaluating Japanese capabilities of VLMs. The Japanese Heron-Bench consists of a variety of imagequestion answer pairs tailored to the Japanese context. Additionally, we present a baseline Japanese VLM that has been trained with Japanese visual <b>instruction</b> <b>tuning</b> datasets. Our Heron-Bench reveals the strengths and limitations of the proposed VLM across various ability dimensions. Furthermore, we clarify the capability gap between strong closed models like <b>GPT-4V</b> and the baseline model, providing valuable insights for future research in this domain. We release the <b>benchmark</b> dataset and training code to facilitate further developments in Japanese VLM research.</p></p class="citation"></blockquote><h3 id=2477--62276-any2point-empowering-any-modality-large-models-for-efficient-3d-understanding-yiwen-tang-et-al-2024>(24/77 | 62/276) Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding (Yiwen Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwen Tang, Jiaming Liu, Dong Wang, Zhigang Wang, Shanghang Zhang, Bin Zhao, Xuelong Li. (2024)<br><strong>Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</strong><br><button class=copy-to-clipboard title="Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-SD, cs.CV, eess-AS<br>Keyword Score: 35<br>Keywords: Fine-tuning, Foundation Model, Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07989v1.pdf filename=2404.07989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>foundation</b> <b>models</b> have recently emerged as a prominent focus of interest, attaining superior performance in widespread scenarios. Due to the scarcity of 3D data, many efforts have been made to adapt pre-trained <b>transformers</b> from vision to 3D domains. However, such 2D-to-3D approaches are still limited, due to the potential loss of spatial geometries and high computation cost. More importantly, their frameworks are mainly designed for 2D models, lacking a general any-to-3D paradigm. In this paper, we introduce Any2Point, a parameter-efficient method to empower any-modality large models (vision, language, audio) for 3D understanding. Given a frozen <b>transformer</b> from any source modality, we propose a 3D-to-any (1D or 2D) virtual projection strategy that correlates the input 3D points to the original 1D or 2D positions within the source modality. This mechanism enables us to assign each 3D token with a positional encoding paired with the pre-trained model, which avoids 3D <b>geometry</b> loss caused by the true projection and better motivates the <b>transformer</b> for 3D learning with 1D/2D positional priors. Then, within each <b>transformer</b> block, we insert an any-to-3D guided adapter module for parameter-efficient <b>fine-tuning.</b> The adapter incorporates prior spatial knowledge from the source modality to guide the local feature aggregation of 3D tokens, compelling the semantic adaption of any-modality <b>transformers.</b> We conduct extensive experiments to showcase the effectiveness and efficiency of our method. Code and models are released at <a href=https://github.com/Ivan-Tang-3D/Any2Point>https://github.com/Ivan-Tang-3D/Any2Point</a>.</p></p class="citation"></blockquote><h3 id=2577--63276-contrastive-based-deep-embeddings-for-label-noise-resilient-histopathology-image-classification-lucas-dedieu-et-al-2024>(25/77 | 63/276) Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification (Lucas Dedieu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Dedieu, Nicolas Nerrienet, Adrien Nivaggioli, Clara Simmat, Marceau Clavel, Arnaud Gauthier, Stéphane Sockeel, Rémy Peyret. (2024)<br><strong>Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification</strong><br><button class=copy-to-clipboard title="Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Deep Neural Network, Foundation Model, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07605v1.pdf filename=2404.07605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>deep</b> <b>learning</b> <b>have</b> proven highly effective in medical image classification, notably within histopathology. However, noisy labels represent a critical challenge in histopathology image classification, where accurate annotations are vital for training robust <b>deep</b> <b>learning</b> <b>models.</b> Indeed, <b>deep</b> <b>neural</b> <b>networks</b> can easily overfit label noise, leading to severe degradations in model performance. While numerous public pathology <b>foundation</b> <b>models</b> have emerged recently, none have evaluated their resilience to label noise. Through thorough empirical analyses across multiple datasets, we exhibit the label noise resilience property of embeddings extracted from <b>foundation</b> <b>models</b> trained in a <b>self-supervised</b> <b>contrastive</b> <b>manner.</b> We demonstrate that training with such embeddings substantially enhances label noise robustness when compared to non-contrastive-based ones as well as commonly used noise-resilient methods. Our results unequivocally underline the superiority of <b>contrastive</b> <b>learning</b> in effectively mitigating the label noise challenge. Code is publicly available at <a href=https://github.com/LucasDedieu/NoiseResilientHistopathology>https://github.com/LucasDedieu/NoiseResilientHistopathology</a>.</p></p class="citation"></blockquote><h3 id=2677--64276-context-aware-video-anomaly-detection-in-long-term-datasets-zhengye-yang-et-al-2024>(26/77 | 64/276) Context-aware Video Anomaly Detection in Long-Term Datasets (Zhengye Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengye Yang, Richard Radke. (2024)<br><strong>Context-aware Video Anomaly Detection in Long-Term Datasets</strong><br><button class=copy-to-clipboard title="Context-aware Video Anomaly Detection in Long-Term Datasets" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Anomaly Detection, Benchmarking, Contrastive Learning, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07887v1.pdf filename=2404.07887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video <b>anomaly</b> <b>detection</b> research is generally evaluated on short, isolated <b>benchmark</b> videos only a few minutes long. However, in real-world environments, <b>security</b> cameras observe the same scene for months or years at a time, and the notion of anomalous behavior critically depends on context, such as the time of day, day of week, or schedule of events. Here, we propose a context-aware video <b>anomaly</b> <b>detection</b> algorithm, Trinity, specifically targeted to these scenarios. Trinity is especially well-suited to crowded scenes in which individuals cannot be easily tracked, and anomalies are due to speed, direction, or absence of group motion. Trinity is a <b>contrastive</b> <b>learning</b> framework that aims to learn alignments between context, appearance, and motion, and uses alignment quality to classify videos as normal or anomalous. We evaluate our algorithm on both conventional <b>benchmarks</b> and a public webcam-based dataset we collected that spans more than three months of activity.</p></p class="citation"></blockquote><h3 id=2777--65276-aug-a-new-dataset-and-an-efficient-model-for-aerial-image-urban-scene-graph-generation-yansheng-li-et-al-2024>(27/77 | 65/276) AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation (Yansheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yansheng Li, Kun Li, Yongjun Zhang, Linlin Wang, Dingwen Zhang. (2024)<br><strong>AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation</strong><br><button class=copy-to-clipboard title="AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07788v1.pdf filename=2404.07788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>graph</b> <b>generation</b> <b>(SGG)</b> aims to understand the visual objects and their semantic relationships from one given image. Until now, lots of SGG datasets with the eyelevel view are released but the SGG dataset with the overhead view is scarcely studied. By contrast to the object occlusion problem in the eyelevel view, which impedes the SGG, the overhead view provides a new perspective that helps to promote the SGG by providing a clear perception of the spatial relationships of objects in the ground scene. To fill in the gap of the overhead view dataset, this paper constructs and releases an aerial image urban scene <b>graph</b> <b>generation</b> <b>(AUG)</b> dataset. Images from the AUG dataset are captured with the low-attitude overhead view. In the AUG dataset, 25,594 objects, 16,970 relationships, and 27,175 attributes are manually annotated. To avoid the local context being overwhelmed in the complex aerial urban scene, this paper proposes one new locality-preserving <b>graph</b> <b>convolutional</b> <b>network</b> (LPG). Different from the traditional <b>graph</b> <b>convolutional</b> <b>network,</b> which has the natural advantage of capturing the global context for SGG, the <b>convolutional</b> <b>layer</b> in the LPG integrates the non-destructive initial features of the objects with dynamically updated neighborhood information to preserve the local context under the premise of mining the global context. To address the problem that there exists an extra-large number of potential object relationship pairs but only a small part of them is meaningful in AUG, we propose the adaptive bounding box scaling factor for potential relationship detection (ABS-PRD) to intelligently prune the meaningless relationship pairs. Extensive experiments on the AUG dataset show that our LPG can significantly outperform the state-of-the-art methods and the effectiveness of the proposed locality-preserving strategy.</p></p class="citation"></blockquote><h3 id=2877--66276-weakly-supervised-learning-via-multi-lateral-decoder-branching-for-guidewire-segmentation-in-robot-assisted-cardiovascular-catheterization-olatunji-mumini-omisore-et-al-2024>(28/77 | 66/276) Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization (Olatunji Mumini Omisore et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olatunji Mumini Omisore, Toluwanimi Akinyemi, Anh Nguyen, Lei Wang. (2024)<br><strong>Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization</strong><br><button class=copy-to-clipboard title="Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07594v1.pdf filename=2404.07594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although robot-assisted cardiovascular catheterization is commonly performed for intervention of cardiovascular diseases, more studies are needed to support the procedure with automated tool segmentation. This can aid surgeons on tool tracking and visualization during intervention. Learning-based segmentation has recently offered state-of-the-art segmentation performances however, generating ground-truth signals for fully-supervised methods is labor-intensive and time consuming for the interventionists. In this study, a <b>weakly-supervised</b> <b>learning</b> <b>method</b> with multi-lateral pseudo labeling is proposed for tool segmentation in cardiac angiograms. The method includes a modified U-Net model with one encoder and multiple lateral-branched decoders that produce pseudo labels as supervision signals under different perturbation. The pseudo labels are self-generated through a mixed loss function and shared consistency in the decoders. We trained the model end-to-end with <b>weakly-annotated</b> <b>data</b> <b>obtained</b> during robotic cardiac catheterization. Experiments with the proposed model shows <b>weakly</b> <b>annotated</b> <b>data</b> has closer performance to when fully annotated data is used. Compared to three existing <b>weakly-supervised</b> <b>methods,</b> <b>our</b> approach yielded higher segmentation performance across three different cardiac angiogram data. With ablation study, we showed consistent performance under different parameters. Thus, we offer a less expensive method for real-time tool segmentation and tracking during robot-assisted cardiac catheterization.</p></p class="citation"></blockquote><h3 id=2977--67276-multi-rater-prompting-for-ambiguous-medical-image-segmentation-jinhong-wang-et-al-2024>(29/77 | 67/276) Multi-rater Prompting for Ambiguous Medical Image Segmentation (Jinhong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhong Wang, Yi Cheng, Jintai Chen, Hongxia Xu, Danny Chen, Jian Wu. (2024)<br><strong>Multi-rater Prompting for Ambiguous Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Multi-rater Prompting for Ambiguous Medical Image Segmentation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07580v1.pdf filename=2404.07580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-rater annotations commonly occur when medical images are independently annotated by multiple experts (raters). In this paper, we tackle two challenges arisen in multi-rater annotations for medical image segmentation (called ambiguous medical image segmentation): (1) How to train a deep learning model when a group of raters produces a set of diverse but plausible annotations, and (2) how to <b>fine-tune</b> the model efficiently when computation resources are not available for re-training the entire model on a different dataset domain. We propose a multi-rater <b>prompt-based</b> approach to address these two challenges altogether. Specifically, we introduce a series of rater-aware <b>prompts</b> that can be plugged into the U-Net model for uncertainty estimation to handle multi-annotation cases. During the <b>prompt-based</b> <b>fine-tuning</b> process, only 0.3% of learnable parameters are required to be updated comparing to training the entire model. Further, in order to integrate expert consensus and disagreement, we explore different multi-rater incorporation strategies and design a mix-training strategy for comprehensive insight learning. Extensive experiments verify the effectiveness of our new approach for ambiguous medical image segmentation on two public datasets while alleviating the heavy burden of model re-training.</p></p class="citation"></blockquote><h3 id=3077--68276-objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation-stanislav-frolov-et-al-2024>(30/77 | 68/276) ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation (Stanislav Frolov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stanislav Frolov, Brian B. Moser, Sebastian Palacio, Andreas Dengel. (2024)<br><strong>ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation</strong><br><button class=copy-to-clipboard title="ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Curriculum Learning, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07564v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07564v1.pdf filename=2404.07564v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ObjBlur, a novel <b>curriculum</b> <b>learning</b> approach to improve layout-to-image generation models, where the task is to produce realistic images from layouts composed of boxes and labels. Our method is based on progressive object-level blurring, which effectively stabilizes training and enhances the quality of generated images. This <b>curriculum</b> <b>learning</b> strategy systematically applies varying degrees of blurring to individual objects or the background during training, starting from strong blurring to progressively cleaner images. Our findings reveal that this approach yields significant performance improvements, stabilized training, smoother convergence, and reduced variance between multiple runs. Moreover, our technique demonstrates its versatility by being compatible with <b>generative</b> <b>adversarial</b> <b>networks</b> and <b>diffusion</b> <b>models,</b> underlining its applicability across various <b>generative</b> <b>modeling</b> <b>paradigms.</b> With ObjBlur, we reach new state-of-the-art results on the complex COCO and Visual Genome datasets.</p></p class="citation"></blockquote><h3 id=3177--69276-copilotcad-empowering-radiologists-with-report-completion-models-and-quantitative-evidence-from-medical-image-foundation-models-sheng-wang-et-al-2024>(31/77 | 69/276) CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models (Sheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Wang, Tianming Du, Katherine Fischer, Gregory E Tasian, Justin Ziemba, Joanie M Garratt, Hersh Sagreiya, Yong Fan. (2024)<br><strong>CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models</strong><br><button class=copy-to-clipboard title="CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07424v1.pdf filename=2404.07424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer-aided diagnosis systems hold great promise to aid radiologists and clinicians in radiological clinical practice and enhance diagnostic accuracy and efficiency. However, the conventional systems primarily focus on delivering diagnostic results through text report generation or medical image classification, positioning them as standalone decision-makers rather than helpers and ignoring radiologists&rsquo; expertise. This study introduces an innovative paradigm to create an assistive co-pilot system for empowering radiologists by leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and medical image analysis tools. Specifically, we develop a collaborative framework to integrate <b>LLMs</b> and quantitative medical image analysis results generated by <b>foundation</b> <b>models</b> with radiologists in the loop, achieving efficient and safe generation of radiology reports and effective utilization of computational power of AI and the expertise of medical professionals. This approach empowers radiologists to generate more precise and detailed diagnostic reports, enhancing patient outcomes while reducing the burnout of clinicians. Our methodology underscores the potential of AI as a supportive tool in medical diagnostics, promoting a harmonious integration of technology and human expertise to advance the field of radiology.</p></p class="citation"></blockquote><h3 id=3277--70276-vifnet-an-end-to-end-visible-infrared-fusion-network-for-image-dehazing-meng-yu-et-al-2024>(32/77 | 70/276) VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing (Meng Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Yu, Te Cui, Haoyang Lu, Yufeng Yue. (2024)<br><strong>VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing</strong><br><button class=copy-to-clipboard title="VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07790v1.pdf filename=2404.07790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image dehazing poses significant challenges in environmental perception. Recent research mainly focus on deep learning-based methods with single modality, while they may result in severe information loss especially in dense-haze scenarios. The infrared image exhibits robustness to the haze, however, existing methods have primarily treated the infrared modality as auxiliary information, failing to fully explore its rich information in dehazing. To address this challenge, the key insight of this study is to design a visible-infrared fusion network for image dehazing. In particular, we propose a multi-scale Deep Structure Feature Extraction (DSFE) module, which incorporates the Channel-Pixel Attention Block (CPAB) to restore more spatial and marginal information within the deep structural features. Additionally, we introduce an inconsistency weighted fusion strategy to merge the two modalities by leveraging the more reliable information. To validate this, we construct a visible-infrared <b>multimodal</b> dataset called AirSim-VID based on the AirSim <b>simulation</b> platform. Extensive experiments performed on challenging real and simulated image datasets demonstrate that VIFNet can outperform many state-of-the-art competing methods. The code and dataset are available at <a href=https://github.com/mengyu212/VIFNet_dehazing>https://github.com/mengyu212/VIFNet_dehazing</a>.</p></p class="citation"></blockquote><h3 id=3377--71276-two-effects-one-trigger-on-the-modality-gap-object-bias-and-information-imbalance-in-contrastive-vision-language-representation-learning-simon-schrodi-et-al-2024>(33/77 | 71/276) Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning (Simon Schrodi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, Thomas Brox. (2024)<br><strong>Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning</strong><br><button class=copy-to-clipboard title="Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 25<br>Keywords: Representation Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07983v1.pdf filename=2404.07983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive <b>vision-language</b> models like CLIP have gained popularity for their versatile applicable learned <b>representations</b> <b>in</b> various downstream tasks. Despite their successes in some tasks, like <b>zero-shot</b> image recognition, they also perform surprisingly poor on other tasks, like attribute detection. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared <b>representation</b> <b>space,</b> and a bias towards objects over other factors, such as attributes. In this work we investigate both phenomena. We find that only a few embedding dimensions drive the modality gap. Further, we propose a measure for object bias and find that object bias does not lead to worse performance on other concepts, such as attributes. But what leads to the emergence of the modality gap and object bias? To answer this question we carefully designed an experimental setting which allows us to control the amount of shared information between the modalities. This revealed that the driving factor behind both, the modality gap and the object bias, is the information imbalance between images and captions.</p></p class="citation"></blockquote><h3 id=3477--72276-fusionmamba-efficient-image-fusion-with-state-space-model-siran-peng-et-al-2024>(34/77 | 72/276) FusionMamba: Efficient Image Fusion with State Space Model (Siran Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siran Peng, Xiangyu Zhu, Haoyu Deng, Zhen Lei, Liang-Jian Deng. (2024)<br><strong>FusionMamba: Efficient Image Fusion with State Space Model</strong><br><button class=copy-to-clipboard title="FusionMamba: Efficient Image Fusion with State Space Model" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 25<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07932v1.pdf filename=2404.07932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image fusion aims to generate a high-resolution multi/hyper-spectral image by combining a high-resolution image with limited spectral information and a low-resolution image with abundant spectral data. Current deep learning (DL)-based methods for image fusion primarily rely on <b>CNNs</b> or <b>Transformers</b> to extract features and merge different types of data. While <b>CNNs</b> are efficient, their receptive fields are limited, restricting their capacity to capture global context. Conversely, <b>Transformers</b> excel at learning global information but are hindered by their quadratic complexity. Fortunately, recent advancements in the State Space Model (SSM), particularly Mamba, offer a promising solution to this issue by enabling global awareness with linear complexity. However, there have been few attempts to explore the potential of SSM in information fusion, which is a crucial ability in domains like image fusion. Therefore, we propose FusionMamba, an innovative method for efficient image fusion. Our contributions mainly focus on two aspects. Firstly, recognizing that images from different sources possess distinct properties, we incorporate Mamba blocks into two U-shaped networks, presenting a novel architecture that extracts spatial and spectral features in an efficient, independent, and hierarchical manner. Secondly, to effectively combine spatial and spectral information, we extend the Mamba block to accommodate dual inputs. This expansion leads to the creation of a new module called the FusionMamba block, which outperforms existing fusion techniques such as concatenation and cross-attention. To validate FusionMamba&rsquo;s effectiveness, we conduct a series of experiments on five datasets related to three image fusion tasks. The quantitative and qualitative evaluation results demonstrate that our method achieves state-of-the-art (SOTA) performance, underscoring the superiority of FusionMamba.</p></p class="citation"></blockquote><h3 id=3577--73276-the-power-of-properties-uncovering-the-influential-factors-in-emotion-classification-tim-büchner-et-al-2024>(35/77 | 73/276) The Power of Properties: Uncovering the Influential Factors in Emotion Classification (Tim Büchner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler. (2024)<br><strong>The Power of Properties: Uncovering the Influential Factors in Emotion Classification</strong><br><button class=copy-to-clipboard title="The Power of Properties: Uncovering the Influential Factors in Emotion Classification" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Emotion Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07867v1.pdf filename=2404.07867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial expression-based human <b>emotion</b> <b>recognition</b> is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such <b>black-box</b> <b>models</b> lack transparency in their decision-making processes, <b>prompting</b> efforts to ascertain the rules that underlie classifiers&rsquo; decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model&rsquo;s prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences <b>emotion</b> <b>prediction.</b> We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.</p></p class="citation"></blockquote><h3 id=3677--74276-pram-place-recognition-anywhere-model-for-efficient-visual-localization-fei-xue-et-al-2024>(36/77 | 74/276) PRAM: Place Recognition Anywhere Model for Efficient Visual Localization (Fei Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Xue, Ignas Budvytis, Roberto Cipolla. (2024)<br><strong>PRAM: Place Recognition Anywhere Model for Efficient Visual Localization</strong><br><button class=copy-to-clipboard title="PRAM: Place Recognition Anywhere Model for Efficient Visual Localization" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 25<br>Keywords: Deep Neural Network, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07785v1.pdf filename=2404.07785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans localize themselves efficiently in known environments by first recognizing landmarks defined on certain objects and their spatial relationships, and then verifying the location by aligning detailed structures of recognized objects with those in the memory. Inspired by this, we propose the place recognition anywhere model (PRAM) to perform visual localization as efficiently as humans do. PRAM consists of two main components - recognition and registration. In detail, first of all, a <b>self-supervised</b> map-centric landmark definition strategy is adopted, making places in either indoor or outdoor scenes act as unique landmarks. Then, sparse keypoints extracted from images, are utilized as the input to a <b>transformer-based</b> <b>deep</b> <b>neural</b> <b>network</b> for landmark recognition; these keypoints enable PRAM to recognize hundreds of landmarks with high time and memory efficiency. Keypoints along with recognized landmark labels are further used for registration between query images and the 3D landmark map. Different from previous hierarchical methods, PRAM discards global and local descriptors, and reduces over 90% storage. Since PRAM utilizes recognition and landmark-wise verification to replace global reference search and exhaustive matching respectively, it runs 2.4 times faster than prior state-of-the-art approaches. Moreover, PRAM opens new directions for visual localization including multi-modality localization, map-centric feature learning, and hierarchical scene coordinate regression.</p></p class="citation"></blockquote><h3 id=3777--75276-vim-unet-vision-mamba-for-biomedical-segmentation-anwai-archit-et-al-2024>(37/77 | 75/276) ViM-UNet: Vision Mamba for Biomedical Segmentation (Anwai Archit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anwai Archit, Constantin Pape. (2024)<br><strong>ViM-UNet: Vision Mamba for Biomedical Segmentation</strong><br><button class=copy-to-clipboard title="ViM-UNet: Vision Mamba for Biomedical Segmentation" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07705v1.pdf filename=2404.07705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>CNNs,</b> most notably the UNet, are the default architecture for biomedical segmentation. <b>Transformer-based</b> approaches, such as UNETR, have been proposed to replace them, benefiting from a global field of view, but suffering from larger runtimes and higher parameter counts. The recent Vision Mamba architecture offers a compelling alternative to <b>transformers,</b> also providing a global field of view, but at higher efficiency. Here, we introduce ViM-UNet, a novel segmentation architecture based on it and compare it to UNet and UNETR for two challenging microscopy instance segmentation tasks. We find that it performs similarly or better than UNet, depending on the task, and outperforms UNETR while being more efficient. Our code is open source and documented at <a href=https://github.com/constantinpape/torch-em/blob/main/vimunet.md>https://github.com/constantinpape/torch-em/blob/main/vimunet.md</a>.</p></p class="citation"></blockquote><h3 id=3877--76276-automatic-detection-of-dark-ship-to-ship-transfers-using-deep-learning-and-satellite-imagery-ollie-ballinger-2024>(38/77 | 76/276) Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery (Ollie Ballinger, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ollie Ballinger. (2024)<br><strong>Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery</strong><br><button class=copy-to-clipboard title="Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07607v1.pdf filename=2404.07607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite extensive research into ship detection via remote sensing, no studies identify ship-to-ship transfers in satellite imagery. Given the importance of transshipment in illicit shipping practices, this is a significant gap. In what follows, I train a <b>convolutional</b> <b>neural</b> <b>network</b> to accurately detect 4 different types of cargo vessel and two different types of Ship-to-Ship transfer in PlanetScope satellite imagery. I then elaborate a pipeline for the automatic detection of suspected illicit ship-to-ship transfers by cross-referencing satellite detections with vessel borne GPS data. Finally, I apply this method to the Kerch Strait between Ukraine and Russia to identify over 400 dark transshipment events since 2022.</p></p class="citation"></blockquote><h3 id=3977--77276-consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model-lifan-jiang-et-al-2024>(39/77 | 77/276) ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model (Lifan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lifan Jiang, Zhihui Wang, Changmiao Wang, Ming Li, Jiaxu Leng, Xindong Wu. (2024)<br><strong>ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model</strong><br><button class=copy-to-clipboard title="ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07773v1.pdf filename=2404.07773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection,</b> a quintessential task in the realm of perceptual computing, can be tackled using a generative methodology. In the present study, we introduce a novel framework designed to articulate <b>object</b> <b>detection</b> as a denoising <b>diffusion</b> <b>process,</b> which operates on perturbed bounding boxes of annotated entities. This framework, termed ConsistencyDet, leverages an innovative denoising concept known as the Consistency Model. The hallmark of this model is its self-consistency feature, which empowers the model to map distorted information from any temporal stage back to its pristine state, thereby realizing a ``one-step denoising&rsquo;&rsquo; mechanism. Such an attribute markedly elevates the operational efficiency of the model, setting it apart from the conventional <b>Diffusion</b> <b>Model.</b> Throughout the training phase, ConsistencyDet initiates the <b>diffusion</b> <b>sequence</b> with noise-infused boxes derived from the ground-truth annotations and conditions the model to perform the denoising task. Subsequently, in the inference stage, the model employs a denoising sampling strategy that commences with bounding boxes randomly sampled from a normal distribution. Through iterative refinement, the model transforms an assortment of arbitrarily generated boxes into the definitive detections. Comprehensive evaluations employing standard <b>benchmarks,</b> such as MS-COCO and LVIS, corroborate that ConsistencyDet surpasses other leading-edge detectors in performance metrics.</p></p class="citation"></blockquote><h3 id=4077--78276-opentrench3d-a-photogrammetric-3d-point-cloud-dataset-for-semantic-segmentation-of-underground-utilities-lasse-h-hansen-et-al-2024>(40/77 | 78/276) OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities (Lasse H. Hansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lasse H. Hansen, Simon B. Jensen, Mark P. Philipsen, Andreas Møgelmose, Lars Bodum, Thomas B. Moeslund. (2024)<br><strong>OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities</strong><br><button class=copy-to-clipboard title="OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07711v1.pdf filename=2404.07711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying and classifying underground utilities is an important task for efficient and effective urban planning and infrastructure maintenance. We present OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point cloud dataset, designed to advance research and development in underground utility surveying and mapping. OpenTrench3D covers a completely novel domain for public 3D point cloud datasets and is unique in its focus, scope, and cost-effective capturing method. The dataset consists of 310 point clouds collected across 7 distinct areas. These include 5 water utility areas and 2 district heating utility areas. The inclusion of different geographical areas and main utilities (water and district heating utilities) makes OpenTrench3D particularly valuable for inter-domain <b>transfer</b> <b>learning</b> experiments. We provide <b>benchmark</b> results for the dataset using three state-of-the-art semantic segmentation models, PointNeXt, PointVector and PointMetaBase. <b>Benchmarks</b> are conducted by training on data from water areas, <b>fine-tuning</b> on district heating area 1 and evaluating on district heating area 2. The dataset is publicly available. With OpenTrench3D, we seek to foster innovation and progress in the field of 3D semantic segmentation in applications related to detection and documentation of underground utilities as well as in <b>transfer</b> <b>learning</b> methods in general.</p></p class="citation"></blockquote><h3 id=4177--79276-dealing-with-subject-similarity-in-differential-morphing-attack-detection-nicolò-di-domenico-et-al-2024>(41/77 | 79/276) Dealing with Subject Similarity in Differential Morphing Attack Detection (Nicolò Di Domenico et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Di Domenico, Guido Borghi, Annalisa Franco, Davide Maltoni. (2024)<br><strong>Dealing with Subject Similarity in Differential Morphing Attack Detection</strong><br><button class=copy-to-clipboard title="Dealing with Subject Similarity in Differential Morphing Attack Detection" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Face Recognition, Benchmarking, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07667v1.pdf filename=2404.07667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of morphing attacks has posed significant <b>security</b> concerns for automated <b>Face</b> <b>Recognition</b> systems, raising the pressing need for robust and effective Morphing Attack Detection (MAD) methods able to effectively address this issue. In this paper, we focus on Differential MAD (D-MAD), where a trusted live capture, usually representing the criminal, is compared with the document image to classify it as morphed or bona fide. We show these approaches based on identity features are effective when the morphed image and the live one are sufficiently diverse; unfortunately, the effectiveness is significantly reduced when the same approaches are applied to look-alike subjects or in all those cases when the similarity between the two compared images is high (e.g. comparison between the morphed image and the accomplice). Therefore, in this paper, we propose ACIdA, a modular D-MAD system, consisting of a module for the attempt type classification, and two modules for the identity and artifacts analysis on input images. Successfully addressing this task would allow broadening the D-MAD applications including, for instance, the document enrollment stage, which currently relies entirely on human evaluation, thus limiting the possibility of releasing ID documents with manipulated images, as well as the automated gates to detect both accomplices and criminals. An extensive cross-dataset experimental evaluation conducted on the introduced scenario shows that ACIdA achieves state-of-the-art results, outperforming literature competitors, while maintaining good performance in traditional D-MAD <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4277--80276-separated-attention-an-improved-cycle-gan-based-under-water-image-enhancement-method-tashmoy-ghosh-2024>(42/77 | 80/276) Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method (Tashmoy Ghosh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tashmoy Ghosh. (2024)<br><strong>Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method</strong><br><button class=copy-to-clipboard title="Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07649v1.pdf filename=2404.07649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we have present an improved Cycle <b>GAN</b> based model for under water image enhancement. We have utilized the cycle consistent learning technique of the state-of-the-art Cycle <b>GAN</b> model with modification in the loss function in terms of depth-oriented attention which enhance the contrast of the overall image, keeping global content, color, local texture, and style information intact. We trained the Cycle <b>GAN</b> model with the modified loss functions on the <b>benchmarked</b> Enhancing Underwater Visual Perception (EUPV) dataset a large dataset including paired and unpaired sets of underwater images (poor and good quality) taken with seven distinct cameras in a range of visibility situation during research on ocean exploration and human-robot cooperation. In addition, we perform qualitative and quantitative evaluation which supports the given technique applied and provided a better contrast enhancement model of underwater imagery. More significantly, the upgraded images provide better results from conventional models and further for under water navigation, pose estimation, saliency prediction, <b>object</b> <b>detection</b> and tracking. The results validate the appropriateness of the model for autonomous underwater vehicles (AUV) in visual navigation.</p></p class="citation"></blockquote><h3 id=4377--81276-connecting-nerfs-images-and-text-francesco-ballerini-et-al-2024>(43/77 | 81/276) Connecting NeRFs, Images, and Text (Francesco Ballerini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Ballerini, Pierluigi Zama Ramirez, Roberto Mirabella, Samuele Salti, Luigi Di Stefano. (2024)<br><strong>Connecting NeRFs, Images, and Text</strong><br><button class=copy-to-clipboard title="Connecting NeRFs, Images, and Text" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Multi-modal, Multi-modal, Representation Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07993v1.pdf filename=2404.07993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRFs) have emerged as a standard framework for representing 3D scenes and objects, introducing a novel data type for information exchange and storage. Concurrently, significant progress has been made in <b>multimodal</b> <b>representation</b> <b>learning</b> for text and image data. This paper explores a novel research direction that aims to connect the NeRF modality with other modalities, similar to established methodologies for images and text. To this end, we propose a simple framework that exploits pre-trained models for NeRF <b>representations</b> <b>alongside</b> <b>multimodal</b> models for text and image processing. Our framework learns a bidirectional mapping between NeRF embeddings and those obtained from corresponding images and text. This mapping unlocks several novel and useful applications, including NeRF <b>zero-shot</b> classification and NeRF retrieval from images or text.</p></p class="citation"></blockquote><h3 id=4477--82276-post-hurricane-building-damage-assessment-using-street-view-imagery-and-structured-data-a-multi-modal-deep-learning-approach-zhuoqun-xue-et-al-2024>(44/77 | 82/276) Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach (Zhuoqun Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoqun Xue, Xiaojian Zhang, David O. Prevatt, Jennifer Bridge, Susu Xu, Xilei Zhao. (2024)<br><strong>Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach</strong><br><button class=copy-to-clipboard title="Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Benchmarking, Geometry, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07399v1.pdf filename=2404.07399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately assessing building damage is critical for disaster response and recovery. However, many existing models for detecting building damage have poor prediction accuracy due to their limited capabilities of identifying detailed, comprehensive structural and/or non-structural damage from the street-view image. Additionally, these models mainly rely on the imagery data for damage classification, failing to account for other critical information, such as wind speed, building characteristics, evacuation zones, and distance of the building to the hurricane track. To address these limitations, in this study, we propose a novel <b>multi-modal</b> (i.e., imagery and structured data) approach for post-hurricane building damage classification, named the <b>Multi-Modal</b> Swin <b>Transformer</b> (MMST). We empirically train and evaluate the proposed MMST using data collected from the 2022 Hurricane Ian in Florida, USA. Results show that MMST outperforms all selected state-of-the-art <b>benchmark</b> models and can achieve an accuracy of 92.67%, which are 7.71% improvement in accuracy compared to Visual <b>Geometry</b> Group 16 (VGG-16). In addition to the street-view imagery data, building value, building age, and wind speed are the most important predictors for damage level classification. The proposed MMST can be deployed to assist in rapid damage assessment and guide reconnaissance efforts in future hurricanes.</p></p class="citation"></blockquote><h3 id=4577--83276-self-supervised-learning-of-color-constancy-markus-r-ernst-et-al-2024>(45/77 | 83/276) Self-Supervised Learning of Color Constancy (Markus R. Ernst et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus R. Ernst, Francisco M. López, Arthur Aubret, Roland W. Fleming, Jochen Triesch. (2024)<br><strong>Self-Supervised Learning of Color Constancy</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning of Color Constancy" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08127v1.pdf filename=2404.08127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Color constancy (CC) describes the ability of the visual system to perceive an object as having a relatively constant color despite changes in lighting conditions. While CC and its limitations have been carefully characterized in humans, it is still unclear how the visual system acquires this ability during development. Here, we present a first study showing that CC develops in a neural network trained in a <b>self-supervised</b> <b>manner</b> through an invariance learning objective. During learning, objects are presented under changing illuminations, while the network aims to map subsequent views of the same object onto close-by latent representations. This gives rise to representations that are largely invariant to the illumination conditions, offering a plausible example of how CC could emerge during human cognitive development via a form of <b>self-supervised</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=4677--84276-rethinking-artistic-copyright-infringements-in-the-era-of-text-to-image-generative-models-mazda-moayeri-et-al-2024>(46/77 | 84/276) Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models (Mazda Moayeri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mazda Moayeri, Samyadeep Basu, Sriram Balasubramanian, Priyatham Kattakinda, Atoosa Chengini, Robert Brauneis, Soheil Feizi. (2024)<br><strong>Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models</strong><br><button class=copy-to-clipboard title="Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08030v1.pdf filename=2404.08030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>text-to-image</b> generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy &ldquo;artistic style&rdquo; is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of &ldquo;artistic copyright infringement&rdquo; to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular <b>text-to-image</b> generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple <b>prompting</b> of today&rsquo;s popular <b>text-to-image</b> generative models.</p></p class="citation"></blockquote><h3 id=4777--85276-taming-stable-diffusion-for-text-to-360-panorama-image-generation-cheng-zhang-et-al-2024>(47/77 | 85/276) Taming Stable Diffusion for Text to 360° Panorama Image Generation (Cheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, Jianfei Cai. (2024)<br><strong>Taming Stable Diffusion for Text to 360° Panorama Image Generation</strong><br><button class=copy-to-clipboard title="Taming Stable Diffusion for Text to 360° Panorama Image Generation" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07949v1.pdf filename=2404.07949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models, e.g., Stable <b>Diffusion,</b> <b>have</b> enabled the creation of photorealistic images from text <b>prompts.</b> Yet, the generation of 360-degree panorama images from text remains a challenge, particularly due to the dearth of paired text-panorama data and the domain gap between panorama and perspective images. In this paper, we introduce a novel dual-branch <b>diffusion</b> <b>model</b> named PanFusion to generate a 360-degree image from a text <b>prompt.</b> We leverage the stable <b>diffusion</b> <b>model</b> as one branch to provide prior knowledge in natural image generation and register it to another panorama branch for holistic image generation. We propose a unique cross-attention mechanism with projection awareness to minimize distortion during the collaborative denoising process. Our experiments validate that PanFusion surpasses existing methods and, thanks to its dual-branch structure, can integrate additional constraints like room layout for customized panorama outputs. Code is available at <a href=https://chengzhag.github.io/publication/panfusion>https://chengzhag.github.io/publication/panfusion</a>.</p></p class="citation"></blockquote><h3 id=4877--86276-resolve-domain-conflicts-for-generalizable-remote-physiological-measurement-weiyu-sun-et-al-2024>(48/77 | 86/276) Resolve Domain Conflicts for Generalizable Remote Physiological Measurement (Weiyu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiyu Sun, Xinyu Zhang, Hao Lu, Ying Chen, Yun Ge, Xiaolin Huang, Jie Yuan, Yingcong Chen. (2024)<br><strong>Resolve Domain Conflicts for Generalizable Remote Physiological Measurement</strong><br><button class=copy-to-clipboard title="Resolve Domain Conflicts for Generalizable Remote Physiological Measurement" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07855v1.pdf filename=2404.07855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote photoplethysmography (rPPG) technology has become increasingly popular due to its non-invasive monitoring of various physiological indicators, making it widely applicable in multimedia interaction, healthcare, and emotion analysis. Existing rPPG methods utilize multiple datasets for training to enhance the generalizability of models. However, they often overlook the underlying conflict issues across different datasets, such as (1) label conflict resulting from different phase delays between physiological signal labels and face videos at the instance level, and (2) attribute conflict <b>stemming</b> from <b>distribution</b> <b>shifts</b> caused by head movements, illumination changes, skin types, etc. To address this, we introduce the DOmain-HArmonious framework (DOHA). Specifically, we first propose a harmonious phase strategy to eliminate uncertain phase delays and preserve the temporal variation of physiological signals. Next, we design a harmonious hyperplane optimization that reduces irrelevant attribute shifts and encourages the model&rsquo;s optimization towards a global solution that fits more valid scenarios. Our experiments demonstrate that DOHA significantly improves the performance of existing methods under multiple protocols. Our code is available at <a href=https://github.com/SWY666/rPPG-DOHA>https://github.com/SWY666/rPPG-DOHA</a>.</p></p class="citation"></blockquote><h3 id=4977--87276-streamlined-photoacoustic-image-processing-with-foundation-models-a-training-free-solution-handi-deng-et-al-2024>(49/77 | 87/276) Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution (Handi Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Handi Deng, Yucheng Zhou, Jiaxuan Xiang, Liujie Gu, Yan Luo, Hai Feng, Mingyuan Liu, Cheng Ma. (2024)<br><strong>Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution</strong><br><button class=copy-to-clipboard title="Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07833v1.pdf filename=2404.07833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have rapidly evolved and have achieved significant accomplishments in computer vision tasks. Specifically, the <b>prompt</b> mechanism conveniently allows users to integrate image prior information into the model, making it possible to apply models without any training. Therefore, we propose a method based on <b>foundation</b> <b>models</b> and zero training to solve the tasks of photoacoustic (PA) image segmentation. We employed the segment anything model (SAM) by setting simple <b>prompts</b> and integrating the model&rsquo;s outputs with prior knowledge of the imaged objects to accomplish various tasks, including: (1) removing the skin signal in three-dimensional PA image rendering; (2) dual speed-of-sound reconstruction, and (3) segmentation of finger blood vessels. Through these demonstrations, we have concluded that deep learning can be directly applied in PA imaging without the requirement for network design and training. This potentially allows for a hands-on, convenient approach to achieving efficient and accurate segmentation of PA images. This letter serves as a comprehensive tutorial, facilitating the mastery of the technique through the provision of code and sample datasets.</p></p class="citation"></blockquote><h3 id=5077--88276-depth-estimation-using-weighted-loss-and-transfer-learning-muhammad-adeel-hafeez-et-al-2024>(50/77 | 88/276) Depth Estimation using Weighted-loss and Transfer Learning (Muhammad Adeel Hafeez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah. (2024)<br><strong>Depth Estimation using Weighted-loss and Transfer Learning</strong><br><button class=copy-to-clipboard title="Depth Estimation using Weighted-loss and Transfer Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07686v1.pdf filename=2404.07686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a <b>supervised</b> depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using <b>transfer</b> <b>learning</b> and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the <b>supervised</b> depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.</p></p class="citation"></blockquote><h3 id=5177--89276-run-time-monitoring-of-3d-object-detection-in-automated-driving-systems-using-early-layer-neural-activation-patterns-hakan-yekta-yatbaz-et-al-2024>(51/77 | 89/276) Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns (Hakan Yekta Yatbaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman. (2024)<br><strong>Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns</strong><br><button class=copy-to-clipboard title="Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Deep Neural Network, Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07685v1.pdf filename=2404.07685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monitoring the integrity of <b>object</b> <b>detection</b> for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in <b>deep</b> <b>neural</b> <b>network</b> <b>(DNN)-based</b> <b>object</b> <b>detectors,</b> their susceptibility to detection errors, particularly in the less-explored realm of 3D <b>object</b> <b>detection,</b> remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D <b>object</b> <b>detection</b> mainly utilise the activation patterns in the final layer of the <b>DNN-based</b> detector&rsquo;s backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D <b>object</b> <b>detection.</b> To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D <b>object</b> <b>detectors.</b> Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers&rsquo; activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector&rsquo;s backbone and report its performance.</p></p class="citation"></blockquote><h3 id=5277--90276-model-based-cleaning-of-the-quilt-1m-pathology-dataset-for-text-conditional-image-synthesis-marc-aubreville-et-al-2024>(52/77 | 90/276) Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis (Marc Aubreville et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Aubreville, Jonathan Ganz, Jonas Ammeling, Christopher C. Kaltenecker, Christof A. Bertram. (2024)<br><strong>Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis</strong><br><button class=copy-to-clipboard title="Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07676v1.pdf filename=2404.07676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The QUILT-1M dataset is the first openly available dataset containing images harvested from various online sources. While it provides a huge data variety, the image quality and composition is highly heterogeneous, impacting its utility for text-conditional image synthesis. We propose an automatic pipeline that provides predictions of the most common impurities within the images, e.g., visibility of narrators, desktop environment and pathology software, or text within the image. Additionally, we propose to use semantic alignment filtering of the <b>image-text</b> pairs. Our findings demonstrate that by rigorously filtering the dataset, there is a substantial enhancement of image fidelity in <b>text-to-image</b> tasks.</p></p class="citation"></blockquote><h3 id=5377--91276-sfsort-scene-features-based-simple-online-real-time-tracker-m-m-morsali-et-al-2024>(53/77 | 91/276) SFSORT: Scene Features-based Simple Online Real-Time Tracker (M. M. Morsali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. M. Morsali, Z. Sharifi, F. Fallah, S. Hashembeiki, H. Mohammadzade, S. Bagheri Shouraki. (2024)<br><strong>SFSORT: Scene Features-based Simple Online Real-Time Tracker</strong><br><button class=copy-to-clipboard title="SFSORT: Scene Features-based Simple Online Real-Time Tracker" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07553v1.pdf filename=2404.07553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces SFSORT, the world&rsquo;s fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets. To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature. By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements. Additionally, this paper demonstrates the impact of scene features on enhancing <b>object-track</b> <b>association</b> and improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9% with a processing speed of 304 Hz on the MOT20 dataset. The tracker&rsquo;s source code, <b>fine-tuned</b> <b>object</b> <b>detection</b> model, and tutorials are available at \url{https://github.com/gitmehrdad/SFSORT}.</p></p class="citation"></blockquote><h3 id=5477--92276-mitigating-object-dependencies-improving-point-cloud-self-supervised-learning-through-object-exchange-yanhao-wu-et-al-2024>(54/77 | 92/276) Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange (Yanhao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanhao Wu, Tong Zhang, Wei Ke, Congpei Qiu, Sabine Susstrunk, Mathieu Salzmann. (2024)<br><strong>Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange</strong><br><button class=copy-to-clipboard title="Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07504v1.pdf filename=2404.07504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of point cloud scene understanding, particularly in indoor scenes, objects are arranged following human habits, resulting in objects of certain semantics being closely positioned and displaying notable inter-object correlations. This can create a tendency for neural networks to exploit these strong dependencies, bypassing the individual object patterns. To address this challenge, we introduce a novel <b>self-supervised</b> <b>learning</b> (SSL) strategy. Our approach leverages both object patterns and contextual cues to produce robust features. It begins with the formulation of an object-exchanging strategy, where pairs of objects with comparable sizes are exchanged across different scenes, effectively disentangling the strong contextual dependencies. Subsequently, we introduce a context-aware feature learning strategy, which encodes object patterns without relying on their specific context by aggregating object features across various scenes. Our extensive experiments demonstrate the superiority of our method over existing SSL techniques, further showing its better robustness to environmental changes. Moreover, we showcase the applicability of our approach by transferring pre-trained models to diverse point cloud datasets.</p></p class="citation"></blockquote><h3 id=5577--93276-fine-grained-side-information-guided-dual-prompts-for-zero-shot-skeleton-action-recognition-yang-chen-et-al-2024>(55/77 | 93/276) Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition (Yang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Chen, Jingcai Guo, Tian He, Ling Wang. (2024)<br><strong>Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition</strong><br><button class=copy-to-clipboard title="Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07487v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07487v2.pdf filename=2404.07487v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based <b>zero-shot</b> action recognition aims to recognize unknown human actions based on the learned priors of the known skeleton-based actions and a semantic descriptor space shared by both known and unknown categories. However, previous works focus on establishing the bridges between the known skeleton representation space and semantic descriptions space at the coarse-grained level for recognizing unknown action categories, ignoring the fine-grained alignment of these two spaces, resulting in suboptimal performance in distinguishing high-similarity action categories. To address these challenges, we propose a novel method via Side information and dual-prompts learning for skeleton-based <b>zero-shot</b> action recognition (STAR) at the fine-grained level. Specifically, 1) we decompose the skeleton into several parts based on its topology structure and introduce the side information concerning multi-part descriptions of human body movements for alignment between the skeleton and the semantic space at the fine-grained level; 2) we design the visual-attribute and semantic-part <b>prompts</b> to improve the intra-class compactness within the skeleton space and inter-class separability within the semantic space, respectively, to distinguish the high-similarity actions. Extensive experiments show that our method achieves state-of-the-art performance in ZSL and GZSL settings on NTU RGB+D, NTU RGB+D 120, and PKU-MMD datasets.</p></p class="citation"></blockquote><h3 id=5677--94276-trashbusters-deep-learning-approach-for-litter-detection-and-tracking-kashish-jain-et-al-2024>(56/77 | 94/276) Trashbusters: Deep Learning Approach for Litter Detection and Tracking (Kashish Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kashish Jain, Manthan Juthani, Jash Jain, Anant V. Nimkar. (2024)<br><strong>Trashbusters: Deep Learning Approach for Litter Detection and Tracking</strong><br><button class=copy-to-clipboard title="Trashbusters: Deep Learning Approach for Litter Detection and Tracking" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07467v1.pdf filename=2404.07467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The illegal disposal of trash is a major public health and environmental concern. Disposing of trash in unplanned places poses serious health and environmental risks. We should try to restrict public trash cans as much as possible. This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places. Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues. To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and <b>face</b> <b>recognition.</b> The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through <b>face</b> <b>recognition,</b> and enables efficient enforcement of anti-littering policies. By reducing reliance on manual intervention, minimizing human error, and providing <b>prompt</b> identification, the proposed system offers significant advantages in addressing littering incidents. The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs.</p></p class="citation"></blockquote><h3 id=5777--95276-simplifying-two-stage-detectors-for-on-device-inference-in-remote-sensing-jaemin-kang-et-al-2024>(57/77 | 95/276) Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing (Jaemin Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaemin Kang, Hoeseok Yang, Hyungshin Kim. (2024)<br><strong>Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing</strong><br><button class=copy-to-clipboard title="Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07405v1.pdf filename=2404.07405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning has been successfully applied to <b>object</b> <b>detection</b> from remotely sensed images. Images are typically processed on the ground rather than on-board due to the computation power of the ground system. Such offloaded processing causes delays in acquiring target mission information, which hinders its application to real-time use cases. For on-device <b>object</b> <b>detection,</b> researches have been conducted on designing efficient detectors or <b>model</b> <b>compression</b> to reduce inference latency. However, highly accurate two-stage detectors still need further exploitation for acceleration. In this paper, we propose a <b>model</b> <b>simplification</b> method for two-stage <b>object</b> <b>detectors.</b> Instead of constructing a general feature pyramid, we utilize only one feature extraction in the two-stage detector. To compensate for the accuracy drop, we apply a high pass filter to the RPN&rsquo;s score map. Our approach is applicable to any two-stage detector using a feature pyramid network. In the experiments with state-of-the-art two-stage detectors such as ReDet, Oriented-RCNN, and LSKNet, our method reduced computation costs upto 61.2% with the accuracy loss within 2.1% on the DOTAv1.5 dataset. Source code will be released.</p></p class="citation"></blockquote><h3 id=5877--96276-gomvs-geometrically-consistent-cost-aggregation-for-multi-view-stereo-jiang-wu-et-al-2024>(58/77 | 96/276) GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo (Jiang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiang Wu, Rui Li, Haofei Xu, Wenxun Zhao, Yu Zhu, Jinqiu Sun, Yanning Zhang. (2024)<br><strong>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</strong><br><button class=copy-to-clipboard title="GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07992v1.pdf filename=2404.07992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Matching cost aggregation plays a fundamental role in learning-based multi-view stereo networks. However, directly aggregating adjacent costs can lead to suboptimal results due to local geometric inconsistency. Related methods either seek selective aggregation or improve aggregated depth in the 2D space, both are unable to handle geometric inconsistency in the cost volume effectively. In this paper, we propose GoMVS to aggregate geometrically consistent costs, yielding better utilization of adjacent geometries. More specifically, we correspond and propagate adjacent costs to the reference pixel by leveraging the local geometric smoothness in conjunction with surface normals. We achieve this by the geometric consistent propagation (GCP) module. It computes the correspondence from the adjacent depth hypothesis space to the reference depth space using surface normals, then uses the correspondence to propagate adjacent costs to the reference <b>geometry,</b> followed by a <b>convolution</b> for aggregation. Our method achieves new state-of-the-art performance on DTU, Tanks & Temple, and ETH3D datasets. Notably, our method ranks 1st on the Tanks & Temple Advanced <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=5977--97276-rmaff-psn-a-residual-multi-scale-attention-feature-fusion-photometric-stereo-network-kai-luo-et-al-2024>(59/77 | 97/276) RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network (Kai Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Luo, Yakun Ju, Lin Qi, Kaixuan Wang, Junyu Dong. (2024)<br><strong>RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network</strong><br><button class=copy-to-clipboard title="RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07766v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07766v2.pdf filename=2404.07766v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting accurate normal maps of objects from two-dimensional images in regions of complex structure and spatial material variations is challenging using photometric stereo methods due to the influence of surface reflection properties caused by variations in object <b>geometry</b> and surface materials. To address this issue, we propose a photometric stereo network called a RMAFF-PSN that uses residual multiscale attentional feature fusion to handle the ``difficult&rsquo;&rsquo; regions of the object. Unlike previous approaches that only use stacked <b>convolutional</b> layers to extract deep features from the input image, our method integrates feature information from different resolution stages and scales of the image. This approach preserves more physical information, such as texture and <b>geometry</b> of the object in complex regions, through shallow-deep stage feature extraction, double branching enhancement, and attention optimization. To test the network structure under real-world conditions, we propose a new real dataset called Simple PS data, which contains multiple objects with varying structures and materials. Experimental results on a publicly available <b>benchmark</b> dataset demonstrate that our method outperforms most existing calibrated photometric stereo methods for the same number of input images, especially in the case of highly non-convex object structures. Our method also obtains good results under sparse lighting conditions.</p></p class="citation"></blockquote><h3 id=6077--98276-mindbridge-a-cross-subject-brain-decoding-framework-shizun-wang-et-al-2024>(60/77 | 98/276) MindBridge: A Cross-Subject Brain Decoding Framework (Shizun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shizun Wang, Songhua Liu, Zhenxiong Tan, Xinchao Wang. (2024)<br><strong>MindBridge: A Cross-Subject Brain Decoding Framework</strong><br><button class=copy-to-clipboard title="MindBridge: A Cross-Subject Brain Decoding Framework" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Data Augmentation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07850v1.pdf filename=2404.07850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain decoding, a pivotal field in neuroscience, aims to reconstruct stimuli from acquired brain signals, primarily utilizing functional magnetic resonance imaging (fMRI). Currently, brain decoding is confined to a per-subject-per-model paradigm, limiting its applicability to the same individual for whom the decoding model is trained. This constraint stems from three key challenges: 1) the inherent variability in input dimensions across subjects due to differences in brain size; 2) the unique intrinsic neural patterns, influencing how different individuals perceive and process sensory information; 3) limited <b>data</b> <b>availability</b> for new subjects in real-world scenarios hampers the performance of decoding models. In this paper, we present a novel approach, MindBridge, that achieves cross-subject brain decoding by employing only one model. Our proposed framework establishes a generic paradigm capable of addressing these challenges by introducing biological-inspired aggregation function and novel cyclic fMRI reconstruction mechanism for subject-invariant <b>representation</b> <b>learning.</b> Notably, by cycle reconstruction of fMRI, MindBridge can enable novel fMRI synthesis, which also can serve as pseudo <b>data</b> <b>augmentation.</b> Within the framework, we also devise a novel reset-tuning method for adapting a pretrained model to a new subject. Experimental results demonstrate MindBridge&rsquo;s ability to reconstruct images for multiple subjects, which is competitive with dedicated subject-specific models. Furthermore, with limited <b>data</b> <b>for</b> a new subject, we achieve a high level of decoding accuracy, surpassing that of subject-specific models. This advancement in cross-subject brain decoding suggests promising directions for wider applications in neuroscience and indicates potential for more efficient utilization of limited fMRI <b>data</b> <b>in</b> real-world scenarios. Project page: <a href=https://littlepure2333.github.io/MindBridge>https://littlepure2333.github.io/MindBridge</a></p></p class="citation"></blockquote><h3 id=6177--99276-g-nerf-geometry-enhanced-novel-view-synthesis-from-single-view-images-zixiong-huang-et-al-2024>(61/77 | 99/276) G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images (Zixiong Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zixiong Huang, Qi Chen, Libo Sun, Yifan Yang, Naizhou Wang, Mingkui Tan, Qi Wu. (2024)<br><strong>G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images</strong><br><button class=copy-to-clipboard title="G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Generative Adversarial Network, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07474v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07474v1.pdf filename=2404.07474v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel view synthesis aims to generate new view images of a given view image collection. Recent attempts address this problem relying on 3D <b>geometry</b> priors (e.g., shapes, sizes, and positions) learned from multi-view images. However, such methods encounter the following limitations: 1) they require a set of multi-view images as training data for a specific scene (e.g., face, car or chair), which is often unavailable in many real-world scenarios; 2) they fail to extract the <b>geometry</b> priors from single-view images due to the lack of multi-view supervision. In this paper, we propose a <b>Geometry-enhanced</b> NeRF (G-NeRF), which seeks to enhance the <b>geometry</b> priors by a <b>geometry-guided</b> multi-view synthesis approach, followed by a depth-aware training. In the synthesis process, inspired that existing 3D <b>GAN</b> models can unconditionally synthesize high-fidelity multi-view images, we seek to adopt off-the-shelf 3D <b>GAN</b> models, such as EG3D, as a free source to provide <b>geometry</b> priors through synthesizing multi-view data. Simultaneously, to further improve the <b>geometry</b> quality of the synthetic data, we introduce a truncation method to effectively sample latent codes within 3D <b>GAN</b> models. To tackle the absence of multi-view supervision for single-view images, we design the depth-aware training approach, incorporating a depth-aware discriminator to guide <b>geometry</b> priors through depth maps. Experiments demonstrate the effectiveness of our method in terms of both qualitative and quantitative results.</p></p class="citation"></blockquote><h3 id=6277--100276-3d-csad-untrained-3d-anomaly-detection-for-complex-manufacturing-surfaces-xuanming-cao-et-al-2024>(62/77 | 100/276) 3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces (Xuanming Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanming Cao, Chengyu Tao, Juan Du. (2024)<br><strong>3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces</strong><br><button class=copy-to-clipboard title="3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07748v1.pdf filename=2404.07748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D <b>anomaly</b> <b>detection</b> is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient <b>anomaly</b> <b>samples.</b> To address these challenges, we propose a novel untrained <b>anomaly</b> <b>detection</b> method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate <b>anomaly</b> <b>detection</b> in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D <b>anomaly</b> <b>detection</b> can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the <b>benchmark</b> methods.</p></p class="citation"></blockquote><h3 id=6377--101276-exploiting-object-based-and-segmentation-based-semantic-features-for-deep-learning-based-indoor-scene-classification-ricardo-pereira-et-al-2024>(63/77 | 101/276) Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification (Ricardo Pereira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo Pereira, Luís Garrote, Tiago Barros, Ana Lopes, Urbano J. Nunes. (2024)<br><strong>Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification</strong><br><button class=copy-to-clipboard title="Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07739v1.pdf filename=2404.07739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Indoor scenes are usually characterized by scattered <b>objects</b> <b>and</b> their relationships, which turns the indoor scene classification task into a challenging computer vision task. Despite the significant performance boost in classification tasks achieved in recent years, provided by the use of deep-learning-based methods, limitations such as inter-category ambiguity and intra-category variation have been holding back their performance. To overcome such issues, gathering semantic information has been shown to be a promising source of information towards a more complete and discriminative feature representation of indoor scenes. Therefore, the work described in this paper uses both semantic information, obtained from <b>object</b> <b>detection,</b> and semantic segmentation techniques. While <b>object</b> <b>detection</b> techniques provide the 2D location of <b>objects</b> <b>allowing</b> to obtain spatial distributions between <b>objects,</b> <b>semantic</b> segmentation techniques provide pixel-level information that allows to obtain, at a pixel-level, a spatial distribution and shape-related features of the segmentation categories. Hence, a novel approach that uses a semantic segmentation mask to provide Hu-moments-based segmentation categories&rsquo; shape characterization, designated by Segmentation-based Hu-Moments Features (SHMFs), is proposed. Moreover, a three-main-branch network, designated by GOS$^2$F$^2$App, that exploits deep-learning-based global features, <b>object-based</b> <b>features,</b> and semantic segmentation-based features is also proposed. GOS$^2$F$^2$App was evaluated in two indoor scene <b>benchmark</b> datasets: SUN RGB-D and NYU Depth V2, where, to the best of our knowledge, state-of-the-art results were achieved on both datasets, which present evidences of the effectiveness of the proposed approach.</p></p class="citation"></blockquote><h3 id=6477--102276-how-is-visual-attention-influenced-by-text-guidance-database-and-model-yinan-sun-et-al-2024>(64/77 | 102/276) How is Visual Attention Influenced by Text Guidance? Database and Model (Yinan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai. (2024)<br><strong>How is Visual Attention Influenced by Text Guidance? Database and Model</strong><br><button class=copy-to-clipboard title="How is Visual Attention Influenced by Text Guidance? Database and Model" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07537v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07537v2.pdf filename=2404.07537v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 <b>text-image</b> pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a <b>benchmark</b> for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: <a href=https://github.com/IntMeGroup/TGSal>https://github.com/IntMeGroup/TGSal</a>.</p></p class="citation"></blockquote><h3 id=6577--103276-gaga-group-any-gaussians-via-3d-aware-memory-bank-weijie-lyu-et-al-2024>(65/77 | 103/276) Gaga: Group Any Gaussians via 3D-aware Memory Bank (Weijie Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang. (2024)<br><strong>Gaga: Group Any Gaussians via 3D-aware Memory Bank</strong><br><button class=copy-to-clipboard title="Gaga: Group Any Gaussians via 3D-aware Memory Bank" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07977v1.pdf filename=2404.07977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by <b>zero-shot</b> segmentation models. Contrasted to prior 3D scene segmentation approaches that heavily rely on video object tracking, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world <b>zero-shot</b> segmentation models, enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as scene understanding and manipulation.</p></p class="citation"></blockquote><h3 id=6677--104276-sparse-laneformer-ji-liu-et-al-2024>(66/77 | 104/276) Sparse Laneformer (Ji Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ji Liu, Zifeng Zhang, Mingjie Lu, Hongyang Wei, Dong Li, Yile Xie, Jinzhang Peng, Lu Tian, Ashish Sirasao, Emad Barsoum. (2024)<br><strong>Sparse Laneformer</strong><br><button class=copy-to-clipboard title="Sparse Laneformer" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07821v1.pdf filename=2404.07821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lane detection is a fundamental task in autonomous driving, and has achieved great progress as deep learning emerges. Previous anchor-based methods often design dense anchors, which highly depend on the training dataset and remain fixed during inference. We analyze that dense anchors are not necessary for lane detection, and propose a <b>transformer-based</b> lane detection framework based on a sparse anchor mechanism. To this end, we generate sparse anchors with position-aware lane queries and angle queries instead of traditional explicit anchors. We adopt Horizontal Perceptual Attention (HPA) to aggregate the lane features along the horizontal direction, and adopt Lane-Angle Cross Attention (LACA) to perform interactions between lane queries and angle queries. We also propose Lane Perceptual Attention (LPA) based on deformable cross attention to further refine the lane predictions. Our method, named Sparse Laneformer, is easy-to-implement and end-to-end trainable. Extensive experiments demonstrate that Sparse Laneformer performs favorably against the state-of-the-art methods, e.g., surpassing Laneformer by 3.0% F1 score and O2SFormer by 0.7% F1 score with fewer MACs on CULane with the same ResNet-34 backbone.</p></p class="citation"></blockquote><h3 id=6777--105276-joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations-yufeng-yue-et-al-2024>(67/77 | 105/276) Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations (Yufeng Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufeng Yue, Meng Yu, Luojie Yang, Yi Yang. (2024)<br><strong>Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations</strong><br><button class=copy-to-clipboard title="Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07770v1.pdf filename=2404.07770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image restoration is rather challenging in adverse weather conditions, especially when multiple degradations occur simultaneously. Blind image decomposition was proposed to tackle this issue, however, its effectiveness heavily relies on the accurate estimation of each component. Although <b>diffusion-based</b> <b>models</b> exhibit strong generative abilities in image restoration tasks, they may generate irrelevant contents when the degraded images are severely corrupted. To address these issues, we leverage physical constraints to guide the whole restoration process, where a mixed degradation model based on atmosphere scattering model is constructed. Then we formulate our Joint Conditional <b>Diffusion</b> <b>Model</b> (JCDM) by incorporating the degraded image and degradation mask to provide precise guidance. To achieve better color and detail recovery results, we further integrate a refinement network to reconstruct the restored image, where Uncertainty Estimation Block (UEB) is employed to enhance the features. Extensive experiments performed on both multi-weather and weather-specific datasets demonstrate the superiority of our method over state-of-the-art competing methods.</p></p class="citation"></blockquote><h3 id=6877--106276-generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models----technical-challenges-and-implications-for-monitoring-and-verification-tuong-vy-nguyen-et-al-2024>(68/77 | 106/276) Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models &ndash; Technical Challenges and Implications for Monitoring and Verification (Tuong Vy Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann. (2024)<br><strong>Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models &ndash; Technical Challenges and Implications for Monitoring and Verification</strong><br><button class=copy-to-clipboard title="Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07754v1.pdf filename=2404.07754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large <b>text-to-image</b> models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.</p></p class="citation"></blockquote><h3 id=6977--107276-applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models-tuomas-kynkäänniemi-et-al-2024>(69/77 | 107/276) Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models (Tuomas Kynkäänniemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen. (2024)<br><strong>Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models</strong><br><button class=copy-to-clipboard title="Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.CV, stat-ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07724v1.pdf filename=2404.07724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Guidance is a crucial technique for extracting the best performance out of image-generating <b>diffusion</b> <b>models.</b> Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable <b>Diffusion</b> <b>XL.</b> We thus suggest exposing the guidance interval as a hyperparameter in all <b>diffusion</b> <b>models</b> that use guidance.</p></p class="citation"></blockquote><h3 id=7077--108276-cat-contrastive-adapter-training-for-personalized-image-generation-jae-wan-park-et-al-2024>(70/77 | 108/276) CAT: Contrastive Adapter Training for Personalized Image Generation (Jae Wan Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae Wan Park, Sang Hyun Park, Jun Young Koh, Junha Lee, Min Song. (2024)<br><strong>CAT: Contrastive Adapter Training for Personalized Image Generation</strong><br><button class=copy-to-clipboard title="CAT: Contrastive Adapter Training for Personalized Image Generation" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07554v1.pdf filename=2404.07554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of various adapters, including Low-Rank Adaptation (LoRA) applied from the field of natural language processing, has allowed <b>diffusion</b> <b>models</b> to personalize image generation at a low cost. However, due to the various challenges including limited datasets and shortage of regularization and computation resources, adapter training often results in unsatisfactory outcomes, leading to the corruption of the backbone model&rsquo;s prior knowledge. One of the well known phenomena is the loss of diversity in object generation, especially within the same class which leads to generating almost identical objects with minor variations. This poses challenges in generation capabilities. To solve this issue, we present Contrastive Adapter Training (CAT), a simple yet effective strategy to enhance adapter training through the application of CAT loss. Our approach facilitates the preservation of the base model&rsquo;s original knowledge when the model initiates adapters. Furthermore, we introduce the Knowledge Preservation Score (KPS) to evaluate CAT&rsquo;s ability to keep the former information. We qualitatively and quantitatively compare CAT&rsquo;s improvement. Finally, we mention the possibility of CAT in the aspects of multi-concept adapter and optimization.</p></p class="citation"></blockquote><h3 id=7177--109276-content-adaptive-non-local-convolution-for-remote-sensing-pansharpening-yule-duan-et-al-2024>(71/77 | 109/276) Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening (Yule Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yule Duan, Xiao Wu, Haoyu Deng, Liang-Jian Deng. (2024)<br><strong>Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening</strong><br><button class=copy-to-clipboard title="Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07543v1.pdf filename=2404.07543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Currently, machine learning-based methods for remote sensing pansharpening have progressed rapidly. However, existing pansharpening methods often do not fully exploit differentiating regional information in non-local spaces, thereby limiting the effectiveness of the methods and resulting in redundant learning parameters. In this paper, we introduce a so-called content-adaptive non-local <b>convolution</b> (CANConv), a novel method tailored for remote sensing image pansharpening. Specifically, CANConv employs adaptive <b>convolution,</b> ensuring spatial adaptability, and incorporates non-local self-similarity through the similarity relationship partition (SRP) and the partition-wise adaptive <b>convolution</b> (PWAC) sub-modules. Furthermore, we also propose a corresponding network architecture, called CANNet, which mainly utilizes the multi-scale self-similarity. Extensive experiments demonstrate the superior performance of CANConv, compared with recent promising fusion methods. Besides, we substantiate the method&rsquo;s effectiveness through visualization, ablation experiments, and comparison with existing methods on multiple test sets. The source code is publicly available at <a href=https://github.com/duanyll/CANConv>https://github.com/duanyll/CANConv</a>.</p></p class="citation"></blockquote><h3 id=7277--110276-generalization-gap-in-data-augmentation-insights-from-illumination-jianqiang-xiao-et-al-2024>(72/77 | 110/276) Generalization Gap in Data Augmentation: Insights from Illumination (Jianqiang Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianqiang Xiao, Weiwen Guo, Junfeng Liu, Mengze Li. (2024)<br><strong>Generalization Gap in Data Augmentation: Insights from Illumination</strong><br><button class=copy-to-clipboard title="Generalization Gap in Data Augmentation: Insights from Illumination" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07514v1.pdf filename=2404.07514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of computer vision, <b>data</b> <b>augmentation</b> is widely used to enrich the feature complexity of training datasets with deep learning techniques. However, regarding the generalization capabilities of models, the difference in artificial features generated by <b>data</b> <b>augmentation</b> and natural visual features has not been fully revealed. This study focuses on the visual representation variable &lsquo;illumination&rsquo;, by simulating its distribution degradation and examining how <b>data</b> <b>augmentation</b> techniques enhance model performance on a classification task. Our goal is to investigate the differences in generalization between models trained with augmented <b>data</b> <b>and</b> those trained under real-world illumination conditions. Results indicate that after undergoing various <b>data</b> <b>augmentation</b> methods, model performance has been significantly improved. Yet, a noticeable generalization gap still exists after utilizing various <b>data</b> <b>augmentation</b> methods, emphasizing the critical role of feature diversity in the training set for enhancing model generalization.</p></p class="citation"></blockquote><h3 id=7377--111276-pillartrack-redesigning-pillar-based-transformer-network-for-single-object-tracking-on-point-clouds-weisheng-xu-et-al-2024>(73/77 | 111/276) PillarTrack: Redesigning Pillar-based Transformer Network for Single Object Tracking on Point Clouds (Weisheng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weisheng Xu, Sifan Zhou, Zhihang Yuan. (2024)<br><strong>PillarTrack: Redesigning Pillar-based Transformer Network for Single Object Tracking on Point Clouds</strong><br><button class=copy-to-clipboard title="PillarTrack: Redesigning Pillar-based Transformer Network for Single Object Tracking on Point Clouds" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07495v1.pdf filename=2404.07495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR-based 3D single object tracking (3D SOT) is a critical issue in robotics and autonomous driving. It aims to obtain accurate 3D BBox from the search area based on similarity or motion. However, existing 3D SOT methods usually follow the point-based pipeline, where the sampling operation inevitably leads to redundant or lost information, resulting in unexpected performance. To address these issues, we propose PillarTrack, a pillar-based 3D single object tracking framework. Firstly, we transform sparse point clouds into dense pillars to preserve the local and global geometrics. Secondly, we introduce a Pyramid-type Encoding Pillar Feature Encoder (PE-PFE) design to help the feature representation of each pillar. Thirdly, we present an efficient <b>Transformer-based</b> backbone from the perspective of modality differences. Finally, we construct our PillarTrack tracker based above designs. Extensive experiments on the KITTI and nuScenes dataset demonstrate the superiority of our proposed method. Notably, our method achieves state-of-the-art performance on the KITTI and nuScenes dataset and enables real-time tracking speed. We hope our work could encourage the community to rethink existing 3D SOT tracker designs.We will open source our code to the research community in <a href=https://github.com/StiphyJay/PillarTrack>https://github.com/StiphyJay/PillarTrack</a>.</p></p class="citation"></blockquote><h3 id=7477--112276-gomavatar-efficient-animatable-human-modeling-from-monocular-video-using-gaussians-on-mesh-jing-wen-et-al-2024>(74/77 | 112/276) GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh (Jing Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Wen, Xiaoming Zhao, Zhongzheng Ren, Alexander G. Schwing, Shenlong Wang. (2024)<br><strong>GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</strong><br><button class=copy-to-clipboard title="GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07991v1.pdf filename=2404.07991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GoMAvatar, a novel approach for real-time, memory-efficient, high-quality animatable human modeling. GoMAvatar takes as input a single monocular video to create a digital avatar capable of re-articulation in new poses and real-time rendering from novel viewpoints, while seamlessly integrating with rasterization-based graphics pipelines. Central to our method is the Gaussians-on-Mesh representation, a hybrid 3D model combining rendering quality and speed of Gaussian splatting with <b>geometry</b> modeling and compatibility of deformable meshes. We assess GoMAvatar on ZJU-MoCap data and various YouTube videos. GoMAvatar matches or surpasses current monocular human modeling algorithms in rendering quality and significantly outperforms them in computational efficiency (43 FPS) while being memory-efficient (3.63 MB per subject).</p></p class="citation"></blockquote><h3 id=7577--113276-real-time-detection-and-analysis-of-vehicles-and-pedestrians-using-deep-learning-md-nahid-sadik-et-al-2024>(75/77 | 113/276) Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep Learning (Md Nahid Sadik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Nahid Sadik, Tahmim Hossain, Faisal Sayeed. (2024)<br><strong>Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep Learning</strong><br><button class=copy-to-clipboard title="Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep Learning" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08081v1.pdf filename=2404.08081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer vision, particularly vehicle and pedestrian identification is critical to the evolution of autonomous driving, artificial intelligence, and video surveillance. Current traffic monitoring systems confront major difficulty in recognizing small objects and pedestrians effectively in real-time, posing a serious risk to public safety and contributing to traffic inefficiency. Recognizing these difficulties, our project focuses on the creation and validation of an advanced deep-learning framework capable of processing complex visual input for precise, real-time recognition of cars and people in a variety of environmental situations. On a dataset representing complicated urban settings, we trained and evaluated different versions of the YOLOv8 and RT-DETR models. The YOLOv8 Large version proved to be the most effective, especially in pedestrian recognition, with great precision and robustness. The results, which include Mean Average Precision and recall rates, demonstrate the model&rsquo;s ability to dramatically improve traffic monitoring and safety. This study makes an important addition to real-time, reliable detection in computer vision, establishing new <b>benchmarks</b> for traffic management systems.</p></p class="citation"></blockquote><h3 id=7677--114276-survmamba-state-space-model-with-multi-grained-multi-modal-interaction-for-survival-prediction-ying-chen-et-al-2024>(76/77 | 114/276) SurvMamba: State Space Model with Multi-grained Multi-modal Interaction for Survival Prediction (Ying Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Chen, Jiajing Xie, Yuxiang Lin, Yuhang Song, Wenxian Yang, Rongshan Yu. (2024)<br><strong>SurvMamba: State Space Model with Multi-grained Multi-modal Interaction for Survival Prediction</strong><br><button class=copy-to-clipboard title="SurvMamba: State Space Model with Multi-grained Multi-modal Interaction for Survival Prediction" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-QM<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08027v1.pdf filename=2404.08027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> learning that combines pathological images with genomic data has significantly enhanced the accuracy of survival prediction. Nevertheless, existing methods have not fully utilized the inherent hierarchical structure within both whole slide images (WSIs) and transcriptomic data, from which better intra-modal representations and inter-modal integration could be derived. Moreover, many existing studies attempt to improve <b>multi-modal</b> representations through attention mechanisms, which inevitably lead to high complexity when processing high-dimensional WSIs and transcriptomic data. Recently, a structured state space model named Mamba emerged as a promising approach for its superior performance in modeling long sequences with low complexity. In this study, we propose Mamba with multi-grained <b>multi-modal</b> interaction (SurvMamba) for survival prediction. SurvMamba is implemented with a Hierarchical Interaction Mamba (HIM) module that facilitates efficient intra-modal interactions at different granularities, thereby capturing more detailed local features as well as rich global representations. In addition, an Interaction Fusion Mamba (IFM) module is used for cascaded inter-modal interactive fusion, yielding more comprehensive features for survival prediction. Comprehensive evaluations on five TCGA datasets demonstrate that SurvMamba outperforms other existing methods in terms of performance and computational cost.</p></p class="citation"></blockquote><h3 id=7777--115276-stereo-lidar-depth-estimation-with-deformable-propagation-and-learned-disparity-depth-conversion-ang-li-et-al-2024>(77/77 | 115/276) Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion (Ang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ang Li, Anning Hu, Wei Xi, Wenxian Yu, Danping Zou. (2024)<br><strong>Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion</strong><br><button class=copy-to-clipboard title="Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07545v1.pdf filename=2404.07545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and dense depth estimation with stereo cameras and LiDAR is an important task for automatic driving and robotic perception. While sparse hints from LiDAR points have improved cost aggregation in stereo matching, their effectiveness is limited by the low density and non-uniform distribution. To address this issue, we propose a novel stereo-LiDAR depth estimation network with Semi-Dense hint Guidance, named SDG-Depth. Our network includes a deformable propagation module for generating a semi-dense hint map and a confidence map by propagating sparse hints using a learned deformable window. These maps then guide cost aggregation in stereo matching. To reduce the triangulation error in depth recovery from disparity, especially in distant regions, we introduce a disparity-depth conversion module. Our method is both accurate and efficient. The experimental results on <b>benchmark</b> tests show its superior performance. Our code is available at <a href=https://github.com/SJTU-ViSYS/SDG-Depth>https://github.com/SJTU-ViSYS/SDG-Depth</a>.</p></p class="citation"></blockquote><h2 id=cslg-45>cs.LG (45)</h2><h3 id=145--116276-variance-reduced-zeroth-order-methods-for-fine-tuning-language-models-tanmay-gautam-et-al-2024>(1/45 | 116/276) Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models (Tanmay Gautam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmay Gautam, Youngsuk Park, Hao Zhou, Parameswaran Raman, Wooseok Ha. (2024)<br><strong>Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models</strong><br><button class=copy-to-clipboard title="Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, math-OC<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Stochastic Gradient Descent, Zero-shot, GLUE, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08080v1.pdf filename=2404.08080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> language models (LMs) has demonstrated success in a wide array of downstream tasks. However, as LMs are scaled up, the memory requirements for backpropagation become prohibitively high. Zeroth-order (ZO) optimization methods can leverage memory-efficient forward passes to estimate gradients. More recently, MeZO, an adaptation of ZO-SGD, has been shown to consistently outperform <b>zero-shot</b> and <b>in-context</b> <b>learning</b> when combined with suitable task <b>prompts.</b> In this work, we couple ZO methods with variance reduction techniques to enhance stability and convergence for inference-based LM <b>fine-tuning.</b> We introduce Memory-Efficient Zeroth-Order Stochastic Variance-Reduced Gradient (MeZO-SVRG) and demonstrate its efficacy across multiple LM <b>fine-tuning</b> tasks, eliminating the reliance on task-specific <b>prompts.</b> Evaluated across a range of both masked and autoregressive LMs on <b>benchmark</b> <b>GLUE</b> tasks, MeZO-SVRG outperforms MeZO with up to 20% increase in test accuracies in both full- and partial-parameter <b>fine-tuning</b> settings. MeZO-SVRG benefits from reduced computation time as it often surpasses MeZO&rsquo;s peak test accuracy with a $2\times$ reduction in GPU-hours. MeZO-SVRG significantly reduces the required memory footprint compared to first-order <b>SGD,</b> i.e. by $2\times$ for autoregressive models. Our experiments highlight that MeZO-SVRG&rsquo;s memory savings progressively improve compared to <b>SGD</b> with larger batch sizes.</p></p class="citation"></blockquote><h3 id=245--117276-bayesian-federated-model-compression-for-communication-and-computation-efficiency-chengyu-xia-et-al-2024>(2/45 | 117/276) Bayesian Federated Model Compression for Communication and Computation Efficiency (Chengyu Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyu Xia, Danny H. K. Tsang, Vincent K. N. Lau. (2024)<br><strong>Bayesian Federated Model Compression for Communication and Computation Efficiency</strong><br><button class=copy-to-clipboard title="Bayesian Federated Model Compression for Communication and Computation Efficiency" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Message-Passing, Federated Learning, Model Compression, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07532v1.pdf filename=2404.07532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate Bayesian <b>model</b> <b>compression</b> in <b>federated</b> <b>learning</b> (FL) to construct sparse <b>models</b> <b>that</b> can achieve both communication and computation efficiencies. We propose a decentralized Turbo variational Bayesian inference (D-Turbo-VBI) FL framework where we firstly propose a hierarchical sparse prior to promote a clustered sparse structure in the weight matrix. Then, by carefully integrating message passing and VBI with a decentralized turbo framework, we propose the D-Turbo-VBI algorithm which can (i) reduce both upstream and downstream communication overhead during <b>federated</b> <b>training,</b> and (ii) reduce the computational complexity during local inference. Additionally, we establish the convergence property for thr proposed D-Turbo-VBI algorithm. <b>Simulation</b> results show the significant gain of our proposed algorithm over the baselines in reducing communication overhead during <b>federated</b> <b>training</b> and computational complexity of final model.</p></p class="citation"></blockquote><h3 id=345--118276-continual-learning-of-range-dependent-transmission-loss-for-underwater-acoustic-using-conditional-convolutional-neural-net-indu-kant-deo-et-al-2024>(3/45 | 118/276) Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net (Indu Kant Deo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Indu Kant Deo, Akash Venkateshwaran, Rajeev K. Jaiman. (2024)<br><strong>Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net</strong><br><button class=copy-to-clipboard title="Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP, physics-flu-dyn<br>Keyword Score: 48<br>Keywords: Convolutional Neural Network, Benchmarking, Continual Learning, Convolution, Convolutional Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08091v1.pdf filename=2404.08091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a significant need for precise and reliable forecasting of the far-field noise emanating from shipping vessels. Conventional full-order models based on the Navier-Stokes equations are unsuitable, and sophisticated model reduction methods may be ineffective for accurately predicting far-field noise in environments with seamounts and significant variations in bathymetry. Recent advances in reduced-order models, particularly those based on <b>convolutional</b> <b>and</b> <b>recurrent</b> <b>neural</b> <b>networks,</b> offer a faster and more accurate alternative. These models use <b>convolutional</b> <b>neural</b> <b>networks</b> to reduce data dimensions effectively. However, current deep-learning models face challenges in predicting wave propagation over long periods and for remote locations, often relying on auto-regressive prediction and lacking far-field bathymetry information. This research aims to improve the accuracy of deep-learning models for predicting underwater radiated noise in far-field scenarios. We propose a novel range-conditional <b>convolutional</b> <b>neural</b> <b>network</b> that incorporates ocean bathymetry data into the input. By integrating this architecture into a <b>continual</b> <b>learning</b> framework, we aim to generalize the model for varying bathymetry worldwide. To demonstrate the effectiveness of our approach, we analyze our model on several test cases and a <b>benchmark</b> scenario involving far-field prediction over Dickin&rsquo;s seamount in the Northeast Pacific. Our proposed architecture effectively captures transmission loss over a range-dependent, varying bathymetry profile. This architecture can be integrated into an adaptive management system for underwater radiated noise, providing real-time end-to-end mapping between near-field ship noise sources and received noise at the marine mammal&rsquo;s location.</p></p class="citation"></blockquote><h3 id=445--119276-vetrass-vehicle-trajectory-similarity-search-through-graph-modeling-and-representation-learning-ming-cheng-et-al-2024>(4/45 | 119/276) VeTraSS: Vehicle Trajectory Similarity Search Through Graph Modeling and Representation Learning (Ming Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Cheng, Bowen Zhang, Ziyu Wang, Ziyi Zhou, Weiqi Feng, Yi Lyu, Xingjian Diao. (2024)<br><strong>VeTraSS: Vehicle Trajectory Similarity Search Through Graph Modeling and Representation Learning</strong><br><button class=copy-to-clipboard title="VeTraSS: Vehicle Trajectory Similarity Search Through Graph Modeling and Representation Learning" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 48<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Representation Learning, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08021v1.pdf filename=2404.08021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory similarity search plays an essential role in autonomous driving, as it enables vehicles to analyze the information and characteristics of different trajectories to make informed decisions and navigate safely in dynamic environments. Existing work on the trajectory similarity search task primarily utilizes sequence-processing algorithms or <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs),</b> which suffer from the inevitable issues of complicated architecture and heavy training costs. Considering the intricate connections between trajectories, using <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> for data modeling is feasible. However, most methods directly use existing mathematical <b>graph</b> <b>structures</b> <b>as</b> the input instead of constructing specific <b>graphs</b> <b>from</b> <b>certain</b> vehicle trajectory data. This ignores such data&rsquo;s unique and dynamic characteristics. To bridge such a research gap, we propose VeTraSS &ndash; an end-to-end pipeline for Vehicle Trajectory Similarity Search. Specifically, VeTraSS models the original trajectory data into multi-scale <b>graphs,</b> <b>and</b> <b>generates</b> comprehensive embeddings through a novel multi-layer attention-based <b>GNN.</b> The learned embeddings can be used for searching similar vehicle trajectories. Extensive experiments on the Porto and Geolife datasets demonstrate the effectiveness of VeTraSS, where our model outperforms existing work and reaches the state-of-the-art. This demonstrates the potential of VeTraSS for trajectory analysis and safe navigation in self-driving vehicles in the real world.</p></p class="citation"></blockquote><h3 id=545--120276-can-contrastive-learning-refine-embeddings-lihui-liu-et-al-2024>(5/45 | 120/276) Can Contrastive Learning Refine Embeddings (Lihui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lihui Liu, Jinha Kim, Vidit Bansal. (2024)<br><strong>Can Contrastive Learning Refine Embeddings</strong><br><button class=copy-to-clipboard title="Can Contrastive Learning Refine Embeddings" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 48<br>Keywords: Benchmarking, Contrastive Learning, Representation Learning, Self-supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08701v1.pdf filename=2404.08701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>contrastive</b> <b>learning</b> have revolutionized <b>self-supervised</b> <b>representation</b> <b>learning</b> and achieved state-of-the-art performance on <b>benchmark</b> tasks. While most existing methods focus on applying <b>contrastive</b> <b>learning</b> to input data modalities such as images, natural language sentences, or networks, they overlook the potential of utilizing outputs from previously trained encoders. In this paper, we introduce SIMSKIP, a novel <b>contrastive</b> <b>learning</b> framework that specifically refines input embeddings for downstream tasks. Unlike traditional <b>unsupervised</b> <b>learning</b> approaches, SIMSKIP takes advantage of the output embeddings of encoder models as its input. Through theoretical analysis, we provide evidence that applying SIMSKIP does not result in larger upper bounds on downstream task errors than those of the original embeddings, which serve as SIMSKIP&rsquo;s input. Experimental results on various open datasets demonstrate that the embeddings produced by SIMSKIP improve performance on downstream tasks.</p></p class="citation"></blockquote><h3 id=645--121276-graph-attention-network-for-lane-wise-and-topology-invariant-intersection-traffic-simulation-nooshin-yousefzadeh-et-al-2024>(6/45 | 121/276) Graph Attention Network for Lane-Wise and Topology-Invariant Intersection Traffic Simulation (Nooshin Yousefzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nooshin Yousefzadeh, Rahul Sengupta, Yashaswi Karnati, Anand Rangarajan, Sanjay Ranka. (2024)<br><strong>Graph Attention Network for Lane-Wise and Topology-Invariant Intersection Traffic Simulation</strong><br><button class=copy-to-clipboard title="Graph Attention Network for Lane-Wise and Topology-Invariant Intersection Traffic Simulation" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Graph, Counter-factual, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07446v1.pdf filename=2404.07446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic congestion has significant economic, environmental, and social ramifications. Intersection traffic flow dynamics are influenced by numerous factors. While microscopic traffic simulators are valuable tools, they are computationally intensive and challenging to calibrate. Moreover, existing machine-learning approaches struggle to provide lane-specific waveforms or adapt to intersection topology and traffic patterns. In this study, we propose two efficient and accurate &ldquo;Digital Twin&rdquo; models for intersections, leveraging <b>Graph</b> Attention Neural Networks <b>(GAT).</b> These attentional <b>graph</b> auto-encoder digital twins capture temporal, spatial, and contextual aspects of traffic within intersections, incorporating various influential factors such as high-resolution loop detector waveforms, signal state records, driving behaviors, and turning-movement counts. Trained on diverse <b>counterfactual</b> scenarios across multiple intersections, our models generalize well, enabling the estimation of detailed traffic waveforms for any intersection approach and exit lanes. Multi-scale error metrics demonstrate that our models perform comparably to microsimulations. The primary application of our study lies in traffic signal optimization, a pivotal area in transportation systems research. These lightweight digital twins can seamlessly integrate into corridor and network signal timing optimization frameworks. Furthermore, our study&rsquo;s applications extend to lane reconfiguration, driving behavior analysis, and facilitating informed decisions regarding intersection safety and efficiency enhancements. A promising avenue for future research involves extending this approach to urban freeway corridors and integrating it with measures of effectiveness metrics.</p></p class="citation"></blockquote><h3 id=745--122276-efficient-duple-perturbation-robustness-in-low-rank-mdps-yang-hu-et-al-2024>(7/45 | 122/276) Efficient Duple Perturbation Robustness in Low-rank MDPs (Yang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Hu, Haitong Ma, Bo Dai, Na Li. (2024)<br><strong>Efficient Duple Perturbation Robustness in Low-rank MDPs</strong><br><button class=copy-to-clipboard title="Efficient Duple Perturbation Robustness in Low-rank MDPs" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08089v1.pdf filename=2404.08089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pursuit of robustness has recently been a popular topic in <b>reinforcement</b> <b>learning</b> (RL) research, yet the existing methods generally suffer from efficiency issues that obstruct their real-world implementation. In this paper, we introduce duple perturbation robustness, i.e. perturbation on both the feature and factor vectors for low-rank Markov decision processes <b>(MDPs),</b> via a novel characterization of $(\xi,\eta)$-ambiguity sets. The novel robust MDP formulation is compatible with the function representation view, and therefore, is naturally applicable to practical RL problems with large or even continuous state-action spaces. Meanwhile, it also gives rise to a provably efficient and practical algorithm with theoretical convergence rate guarantee. Examples are designed to justify the new robustness concept, and algorithmic efficiency is supported by both theoretical bounds and numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=845--123276-large-language-model-can-continue-evolving-from-mistakes-haokun-zhao-et-al-2024>(8/45 | 123/276) Large Language Model Can Continue Evolving From Mistakes (Haokun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haokun Zhao, Haixia Han, Jie Shi, Chengyu Du, Jiaqing Liang, Yanghua Xiao. (2024)<br><strong>Large Language Model Can Continue Evolving From Mistakes</strong><br><button class=copy-to-clipboard title="Large Language Model Can Continue Evolving From Mistakes" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Continual Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08707v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08707v1.pdf filename=2404.08707v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate impressive performance in various downstream tasks. However, they may still generate incorrect responses in certain scenarios due to the knowledge deficiencies and the flawed pre-training data. <b>Continual</b> <b>Learning</b> (CL) is a commonly used method to address this issue. Traditional CL is task-oriented, using novel or factually accurate data to retrain <b>LLMs</b> from scratch. However, this method requires more task-related training data and incurs expensive training costs. To address this challenge, we propose the Continue Evolving from Mistakes (CEM) method, inspired by the <b>&lsquo;summarize</b> mistakes&rsquo; learning skill, to achieve iterative refinement of <b>LLMs.</b> Specifically, the incorrect responses of <b>LLMs</b> indicate knowledge deficiencies related to the questions. Therefore, we collect corpora with these knowledge from multiple data sources and follow it up with iterative supplementary training for continuous, targeted knowledge updating and supplementation. Meanwhile, we developed two strategies to construct supplementary training sets to enhance the <b>LLM&rsquo;s</b> understanding of the corpus and prevent catastrophic forgetting. We conducted extensive experiments to validate the effectiveness of this CL method. In the best case, our method resulted in a 17.00% improvement in the accuracy of the <b>LLM.</b></p></p class="citation"></blockquote><h3 id=945--124276-a-multi-expert-large-language-model-architecture-for-verilog-code-generation-bardia-nadimi-et-al-2024>(9/45 | 124/276) A Multi-Expert Large Language Model Architecture for Verilog Code Generation (Bardia Nadimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bardia Nadimi, Hao Zheng. (2024)<br><strong>A Multi-Expert Large Language Model Architecture for Verilog Code Generation</strong><br><button class=copy-to-clipboard title="A Multi-Expert Large Language Model Architecture for Verilog Code Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08029v1.pdf filename=2404.08029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a surging interest in using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for Verilog <b>code</b> <b>generation.</b> However, the existing approaches are limited in terms of the quality of the generated Verilog <b>code.</b> <b>To</b> address such limitations, this paper introduces an innovative multi-expert <b>LLM</b> architecture for Verilog <b>code</b> <b>generation</b> (MEV-LLM). Our architecture uniquely integrates multiple <b>LLMs,</b> each specifically <b>fine-tuned</b> with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog <b>code</b> <b>for</b> each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.</p></p class="citation"></blockquote><h3 id=1045--125276-generating-comprehensive-lithium-battery-charging-data-with-generative-ai-lidang-jiang-et-al-2024>(10/45 | 125/276) Generating Comprehensive Lithium Battery Charging Data with Generative AI (Lidang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lidang Jiang, Changyan Hu, Sibei Ji, Hang Zhao, Junxiong Chen, Ge He. (2024)<br><strong>Generating Comprehensive Lithium Battery Charging Data with Generative AI</strong><br><button class=copy-to-clipboard title="Generating Comprehensive Lithium Battery Charging Data with Generative AI" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 40<br>Keywords: Autoencoder, Generative AI, Supervised Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07577v1.pdf filename=2404.07577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In optimizing performance and extending the lifespan of lithium batteries, accurate state prediction is pivotal. Traditional regression and classification methods have achieved some success in battery state prediction. However, the efficacy of these data-driven approaches heavily relies on the availability and quality of public datasets. Additionally, generating electrochemical data predominantly through battery experiments is a lengthy and costly process, making it challenging to acquire high-quality electrochemical data. This difficulty, coupled with data incompleteness, significantly impacts prediction accuracy. Addressing these challenges, this study introduces the End of Life (EOL) and Equivalent Cycle Life (ECL) as conditions for <b>generative</b> <b>AI</b> models. By integrating an embedding layer into the CVAE model, we developed the Refined Conditional <b>Variational</b> <b>Autoencoder</b> (RCVAE). Through preprocessing data into a quasi-video format, our study achieves an integrated synthesis of electrochemical data, including voltage, current, temperature, and charging capacity, which is then processed by the RCVAE model. Coupled with customized training and inference algorithms, this model can generate specific electrochemical data for EOL and ECL under <b>supervised</b> conditions. This method provides users with a comprehensive electrochemical dataset, pioneering a new research domain for the artificial synthesis of lithium battery data. Furthermore, based on the detailed synthetic data, various battery state indicators can be calculated, offering new perspectives and possibilities for lithium battery performance prediction.</p></p class="citation"></blockquote><h3 id=1145--126276-a-parsimonious-setup-for-streamflow-forecasting-using-cnn-lstm-sudan-pokharel-et-al-2024>(11/45 | 126/276) A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM (Sudan Pokharel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sudan Pokharel, Tirthankar Roy. (2024)<br><strong>A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM</strong><br><button class=copy-to-clipboard title="A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07924v1.pdf filename=2404.07924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant strides have been made in advancing streamflow predictions, notably with the introduction of cutting-edge machine-learning models. Predominantly, Long Short-Term Memories <b>(LSTMs)</b> and <b>Convolution</b> Neural Networks <b>(CNNs)</b> have been widely employed in this domain. While <b>LSTMs</b> are applicable in both rainfall-runoff and time series settings, <b>CNN-LSTMs</b> have primarily been utilized in rainfall-runoff scenarios. In this study, we extend the application of <b>CNN-LSTMs</b> to time series settings, leveraging lagged streamflow data in conjunction with precipitation and temperature data to predict streamflow. Our results show a substantial improvement in predictive performance in 21 out of 32 HUC8 basins in Nebraska, showcasing noteworthy increases in the Kling-Gupta Efficiency (KGE) values. These results highlight the effectiveness of <b>CNN-LSTMs</b> in time series settings, particularly for spatiotemporal hydrological modeling, for more accurate and robust streamflow predictions.</p></p class="citation"></blockquote><h3 id=1245--127276-post-hoc-reversal-are-we-selecting-models-prematurely-rishabh-ranjan-et-al-2024>(12/45 | 127/276) Post-Hoc Reversal: Are We Selecting Models Prematurely? (Rishabh Ranjan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishabh Ranjan, Saurabh Garg, Mrigank Raman, Carlos Guestrin, Zachary Chase Lipton. (2024)<br><strong>Post-Hoc Reversal: Are We Selecting Models Prematurely?</strong><br><button class=copy-to-clipboard title="Post-Hoc Reversal: Are We Selecting Models Prematurely?" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Graph, Massive Multitask Language Understanding (MMLU), Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07815v1.pdf filename=2404.07815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trained models are often composed with post-hoc transforms such as temperature scaling (TS), ensembling and stochastic weight averaging (SWA) to improve performance, robustness, uncertainty estimation, etc. However, such transforms are typically applied only after the base models have already been finalized by standard means. In this paper, we challenge this practice with an extensive empirical study. In particular, we demonstrate a phenomenon that we call post-hoc reversal, where performance trends are reversed after applying these post-hoc transforms. This phenomenon is especially prominent in high-noise settings. For example, while base models overfit badly early in training, both conventional ensembling and SWA favor base models trained for more epochs. Post-hoc reversal can also suppress the appearance of double descent and mitigate mismatches between test loss and test error seen in base models. Based on our findings, we propose post-hoc selection, a simple technique whereby post-hoc metrics inform model development decisions such as early stopping, checkpointing, and broader hyperparameter choices. Our experimental analyses span real-world vision, language, tabular and <b>graph</b> datasets from domains like satellite imaging, language modeling, census prediction and social network analysis. On an <b>LLM</b> <b>instruction</b> <b>tuning</b> dataset, post-hoc selection results in > 1.5x <b>MMLU</b> improvement compared to naive selection. Code is available at <a href=https://github.com/rishabh-ranjan/post-hoc-reversal>https://github.com/rishabh-ranjan/post-hoc-reversal</a>.</p></p class="citation"></blockquote><h3 id=1345--128276-sketch-plan-generalize-continual-few-shot-learning-of-inductively-generalizable-spatial-concepts-for-language-guided-robot-manipulation-namasivayam-kalithasan-et-al-2024>(13/45 | 128/276) Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation (Namasivayam Kalithasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Namasivayam Kalithasan, Sachit Sachdeva, Himanshu Gaurav Singh, Divyanshu Aggarwal, Gurarmaan Singh Panjeta, Vishal Bindal, Arnav Tuli, Rohan Paul, Parag Singla. (2024)<br><strong>Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation</strong><br><button class=copy-to-clipboard title="Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 33<br>Keywords: Continual Learning, Few-shot, Few-shot Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07774v1.pdf filename=2404.07774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our goal is to build embodied agents that can learn inductively generalizable spatial concepts in a <b>continual</b> <b>manner,</b> e.g, constructing a tower of a given height. Existing work suffers from certain limitations (a) (Liang et al., 2023) and their <b>multi-modal</b> extensions, rely heavily on prior knowledge and are not grounded in the demonstrations (b) (Liu et al., 2023) lack the ability to generalize due to their purely neural approach. A key challenge is to achieve a fine balance between symbolic representations which have the capability to generalize, and neural representations that are physically grounded. In response, we propose a neuro-symbolic approach by expressing inductive concepts as symbolic compositions over grounded neural concepts. Our key insight is to decompose the concept learning problem into the following steps 1) Sketch: Getting a programmatic representation for the given instruction 2) Plan: Perform Model-Based RL over the sequence of grounded neural action concepts to learn a grounded plan 3) Generalize: Abstract out a generic (lifted) Python program to facilitate generalizability. <b>Continual</b> <b>learning</b> is achieved by interspersing learning of grounded neural concepts with higher level symbolic constructs. Our experiments demonstrate that our approach significantly outperforms existing baselines in terms of its ability to learn novel concepts and generalize inductively.</p></p class="citation"></blockquote><h3 id=1445--129276-anomaly-detection-in-power-grids-via-context-agnostic-learning-sangwoo-park-et-al-2024>(14/45 | 129/276) Anomaly Detection in Power Grids via Context-Agnostic Learning (SangWoo Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>SangWoo Park, Amritanshu Pandey. (2024)<br><strong>Anomaly Detection in Power Grids via Context-Agnostic Learning</strong><br><button class=copy-to-clipboard title="Anomaly Detection in Power Grids via Context-Agnostic Learning" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP<br>Keyword Score: 30<br>Keywords: Anomaly Detection, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07898v1.pdf filename=2404.07898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An important tool grid operators use to safeguard against failures, whether naturally occurring or malicious, involves detecting anomalies in the power system SCADA data. In this paper, we aim to solve a real-time <b>anomaly</b> <b>detection</b> problem. Given time-series measurement values coming from a fixed set of sensors on the grid, can we identify anomalies in the network topology or measurement data? Existing methods, primarily optimization-based, mostly use only a single snapshot of the measurement values and do not scale well with the network size. Recent data-driven ML techniques have shown promise by using a combination of current and historical data for <b>anomaly</b> <b>detection</b> but generally do not consider physical attributes like the impact of topology or load/generation changes on sensor measurements and thus cannot accommodate regular context-variability in the historical data. To address this gap, we propose a novel context-aware <b>anomaly</b> <b>detection</b> algorithm, GridCAL, that considers the effect of regular topology and load/generation changes. This algorithm converts the real-time power flow measurements to context-agnostic values, which allows us to analyze measurement coming from different grid contexts in an aggregate fashion, enabling us to derive a unified statistical model that becomes the basis of <b>anomaly</b> <b>detection.</b> Through numerical <b>simulations</b> on networks up to 2383 nodes, we show that our approach is accurate, outperforming state-of-the-art approaches, and is computationally efficient.</p></p class="citation"></blockquote><h3 id=1545--130276-an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization-minshuo-chen-et-al-2024>(15/45 | 130/276) An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization (Minshuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minshuo Chen, Song Mei, Jianqing Fan, Mengdi Wang. (2024)<br><strong>An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization</strong><br><button class=copy-to-clipboard title="An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Diffusion Model, Generative AI, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07771v1.pdf filename=2404.07771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models,</b> a powerful and universal <b>generative</b> <b>AI</b> technology, have achieved tremendous success in computer vision, audio, <b>reinforcement</b> <b>learning,</b> and computational biology. In these applications, <b>diffusion</b> <b>models</b> provide flexible high-dimensional data modeling, and act as a sampler for generating new samples under active guidance towards task-desired properties. Despite the significant empirical success, theory of <b>diffusion</b> <b>models</b> is very limited, potentially slowing down principled methodological innovations for further harnessing and improving <b>diffusion</b> <b>models.</b> In this paper, we review emerging applications of <b>diffusion</b> <b>models,</b> understanding their sample generation under various controls. Next, we overview the existing theories of <b>diffusion</b> <b>models,</b> covering their statistical properties and sampling capabilities. We adopt a progressive routine, beginning with unconditional <b>diffusion</b> <b>models</b> and connecting to conditional counterparts. Further, we review a new avenue in high-dimensional structured optimization through conditional <b>diffusion</b> <b>models,</b> where searching for solutions is reformulated as a conditional sampling problem and solved by <b>diffusion</b> <b>models.</b> Lastly, we discuss future directions about <b>diffusion</b> <b>models.</b> The purpose of this paper is to provide a well-rounded theoretical exposure for stimulating forward-looking theories and methods of <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1645--131276-pinnacle-pinn-adaptive-collocation-and-experimental-points-selection-gregory-kang-ruey-lau-et-al-2024>(16/45 | 131/276) PINNACLE: PINN Adaptive ColLocation and Experimental points selection (Gregory Kang Ruey Lau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregory Kang Ruey Lau, Apivich Hemachandra, See-Kiong Ng, Bryan Kian Hsiang Low. (2024)<br><strong>PINNACLE: PINN Adaptive ColLocation and Experimental points selection</strong><br><button class=copy-to-clipboard title="PINNACLE: PINN Adaptive ColLocation and Experimental points selection" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-comp-ph, physics-data-an, stat-ML<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07662v1.pdf filename=2404.07662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or <b>simulations.</b> Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interaction among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and <b>transfer</b> <b>learning</b> problems.</p></p class="citation"></blockquote><h3 id=1745--132276-differentially-private-reinforcement-learning-with-self-play-dan-qiao-et-al-2024>(17/45 | 132/276) Differentially Private Reinforcement Learning with Self-Play (Dan Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Qiao, Yu-Xiang Wang. (2024)<br><strong>Differentially Private Reinforcement Learning with Self-Play</strong><br><button class=copy-to-clipboard title="Differentially Private Reinforcement Learning with Self-Play" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs-MA, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Markov Game, Reinforcement Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07559v1.pdf filename=2404.07559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of multi-agent <b>reinforcement</b> <b>learning</b> (multi-agent RL) with <b>differential</b> <b>privacy</b> (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users&rsquo; private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic <b>Markov</b> <b>Games,</b> where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first line of results towards understanding trajectory-wise privacy protection in multi-agent RL.</p></p class="citation"></blockquote><h3 id=1845--133276-protein-intrinsic-disorder-prediction-using-attention-u-net-and-prottrans-protein-language-model-krzysztof-kotowski-et-al-2024>(18/45 | 133/276) Protein intrinsic disorder prediction using Attention U-Net and ProtTrans protein language model (Krzysztof Kotowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Krzysztof Kotowski, Irena Roterman, Katarzyna Stapor. (2024)<br><strong>Protein intrinsic disorder prediction using Attention U-Net and ProtTrans protein language model</strong><br><button class=copy-to-clipboard title="Protein intrinsic disorder prediction using Attention U-Net and ProtTrans protein language model" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 28<br>Keywords: Convolutional Neural Network, Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08108v1.pdf filename=2404.08108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prediction of intrinsic disorder regions has significant implications for understanding protein function, structure, and dynamics. It can help to discover novel functions or protein-protein interactions essential to designing new drugs, therapies, or enzymes. Recently, a new generation of predictors based on protein language models is emerging. These algorithms reach state-of-the-art accuracy without calculating time-consuming multiple sequence alignments (MSAs). The article pre-sents a new protein intrinsic disorder predictor DisorderUnetLM based on the Attention U-Net <b>convolutional</b> <b>neural</b> <b>network</b> using features from the protein language model ProtTrans. DisorderUnetLM shows top results in the direct comparison with flDPnn and IDP-CRF predictors using MSAs and with the SETH predictor using features from the same ProtTrans model. Moreover, among 41 predictors from the latest Critical Assessment of Protein Intrinsic Disorder Prediction (CAID-2) <b>benchmark,</b> it ranks 9th for the Disorder-PDB subset (with ROC-AUC of 0.924) and 1st for the Disorder-NOX subset (with ROC-AUC of 0.844) which confirms its potential to perform well in the upcoming CAID-3 challenge for which Disor-derUnetLM was submitted.</p></p class="citation"></blockquote><h3 id=1945--134276-on-the-sample-efficiency-of-abstractions-and-potential-based-reward-shaping-in-reinforcement-learning-giuseppe-canonaco-et-al-2024>(19/45 | 134/276) On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning (Giuseppe Canonaco et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Canonaco, Leo Ardon, Alberto Pozanco, Daniel Borrajo. (2024)<br><strong>On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Convolutional Neural Network, Convolutional Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07826v1.pdf filename=2404.07826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of Potential Based Reward Shaping (PBRS) has shown great promise in the ongoing research effort to tackle sample inefficiency in <b>Reinforcement</b> <b>Learning</b> (RL). However, the choice of the potential function is critical for this technique to be effective. Additionally, RL techniques are usually constrained to use a finite horizon for computational limitations. This introduces a bias when using PBRS, thus adding an additional layer of complexity. In this paper, we leverage abstractions to automatically produce a &ldquo;good&rdquo; potential function. We analyse the bias induced by finite horizons in the context of PBRS producing novel insights. Finally, to asses sample efficiency and performance impact, we evaluate our approach on four environments including a goal-oriented navigation task and three Arcade Learning Environments (ALE) games demonstrating that we can reach the same level of performance as <b>CNN-based</b> solutions with a simple fully-connected network.</p></p class="citation"></blockquote><h3 id=2045--135276-flatness-improves-backbone-generalisation-in-few-shot-classification-rui-li-et-al-2024>(20/45 | 135/276) Flatness Improves Backbone Generalisation in Few-shot Classification (Rui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Li, Martin Trapp, Marcus Klasson, Arno Solin. (2024)<br><strong>Flatness Improves Backbone Generalisation in Few-shot Classification</strong><br><button class=copy-to-clipboard title="Flatness Improves Backbone Generalisation in Few-shot Classification" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Deep Neural Network, Few-shot, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07696v1.pdf filename=2404.07696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deployment of <b>deep</b> <b>neural</b> <b>networks</b> in real-world settings typically requires adaptation to new tasks with few examples. <b>Few-shot</b> classification (FSC) provides a solution to this problem by leveraging pre-trained backbones for fast adaptation to new classes. Surprisingly, most efforts have only focused on developing architectures for easing the adaptation to the target domain without considering the importance of backbone training for good generalisation. We show that flatness-aware backbone training with vanilla <b>fine-tuning</b> results in a simpler yet competitive baseline compared to the state-of-the-art. Our results indicate that for in- and cross-domain FSC, backbone training is crucial to achieving good generalisation across different adaptation methods. We advocate more care should be taken when training these models.</p></p class="citation"></blockquote><h3 id=2145--136276-physics-enhanced-graph-neural-networks-for-soft-sensing-in-industrial-internet-of-things-keivan-faghih-niresi-et-al-2024>(21/45 | 136/276) Physics-Enhanced Graph Neural Networks For Soft Sensing in Industrial Internet of Things (Keivan Faghih Niresi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keivan Faghih Niresi, Hugo Bissig, Henri Baumann, Olga Fink. (2024)<br><strong>Physics-Enhanced Graph Neural Networks For Soft Sensing in Industrial Internet of Things</strong><br><button class=copy-to-clipboard title="Physics-Enhanced Graph Neural Networks For Soft Sensing in Industrial Internet of Things" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08061v1.pdf filename=2404.08061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Industrial Internet of Things (IIoT) is reshaping manufacturing, industrial processes, and infrastructure management. By fostering new levels of automation, efficiency, and predictive maintenance, IIoT is transforming traditional industries into intelligent, seamlessly interconnected ecosystems. However, achieving highly reliable IIoT can be hindered by factors such as the cost of installing large numbers of sensors, limitations in retrofitting existing systems with sensors, or harsh environmental conditions that may make sensor installation impractical. Soft (virtual) sensing leverages mathematical models to estimate variables from physical sensor data, offering a solution to these challenges. Data-driven and physics-based modeling are the two main methodologies widely used for soft sensing. The choice between these strategies depends on the complexity of the underlying system, with the data-driven approach often being preferred when the physics-based inference models are intricate and present challenges for state estimation. However, conventional deep learning models are typically hindered by their inability to explicitly represent the complex interactions among various sensors. To address this limitation, we adopt <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> renowned for their ability to effectively capture the complex relationships between sensor measurements. In this research, we propose physics-enhanced <b>GNNs,</b> which integrate principles of physics into <b>graph-based</b> <b>methodologies.</b> <b>This</b> is achieved by augmenting additional nodes in the input <b>graph</b> <b>derived</b> <b>from</b> the underlying characteristics of the physical processes. Our evaluation of the proposed methodology on the case study of district heating networks reveals significant improvements over purely data-driven <b>GNNs,</b> even in the presence of noise and parameter inaccuracies.</p></p class="citation"></blockquote><h3 id=2245--137276-characterizing-the-influence-of-topology-on-graph-learning-tasks-kailong-wu-et-al-2024>(22/45 | 137/276) Characterizing the Influence of Topology on Graph Learning Tasks (Kailong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailong Wu, Yule Xie, Jiaxin Ding, Yuxiang Ren, Luoyi Fu, Xinbing Wang, Chenghu Zhou. (2024)<br><strong>Characterizing the Influence of Topology on Graph Learning Tasks</strong><br><button class=copy-to-clipboard title="Characterizing the Influence of Topology on Graph Learning Tasks" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07493v1.pdf filename=2404.07493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNN)</b> have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how <b>graph</b> <b>topology</b> <b>influences</b> the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of <b>graph</b> <b>topology</b> <b>by</b> measuring the level of compatibility between the topological information of <b>graph</b> <b>data</b> <b>and</b> downstream task objectives. We provide analysis based on the decoupled <b>GNNs</b> on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance <b>graph</b> learning.</p></p class="citation"></blockquote><h3 id=2345--138276-eliminating-catastrophic-overfitting-via-abnormal-adversarial-examples-regularization-runqi-lin-et-al-2024>(23/45 | 138/276) Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization (Runqi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runqi Lin, Chaojian Yu, Tongliang Liu. (2024)<br><strong>Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization</strong><br><button class=copy-to-clipboard title="Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08154v1.pdf filename=2404.08154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-step <b>adversarial</b> <b>training</b> (SSAT) has demonstrated the potential to achieve both efficiency and robustness. However, SSAT suffers from catastrophic overfitting (CO), a phenomenon that leads to a severely distorted classifier, making it vulnerable to multi-step <b>adversarial</b> <b>attacks.</b> In this work, we observe that some <b>adversarial</b> <b>examples</b> generated on the SSAT-trained network exhibit anomalous behaviour, that is, although these training samples are generated by the inner maximization process, their associated loss decreases instead, which we named abnormal <b>adversarial</b> <b>examples</b> (AAEs). Upon further analysis, we discover a close relationship between AAEs and classifier distortion, as both the number and outputs of AAEs undergo a significant variation with the onset of CO. Given this observation, we re-examine the SSAT process and uncover that before the occurrence of CO, the classifier already displayed a slight distortion, indicated by the presence of few AAEs. Furthermore, the classifier directly optimizing these AAEs will accelerate its distortion, and correspondingly, the variation of AAEs will sharply increase as a result. In such a vicious circle, the classifier rapidly becomes highly distorted and manifests as CO within a few iterations. These observations motivate us to eliminate CO by hindering the generation of AAEs. Specifically, we design a novel method, termed Abnormal <b>Adversarial</b> <b>Examples</b> Regularization (AAER), which explicitly regularizes the variation of AAEs to hinder the classifier from becoming distorted. Extensive experiments demonstrate that our method can effectively eliminate CO and further boost <b>adversarial</b> <b>robustness</b> with negligible additional computational overhead.</p></p class="citation"></blockquote><h3 id=2445--139276-frame-quantization-of-neural-networks-wojciech-czaja-et-al-2024>(24/45 | 139/276) Frame Quantization of Neural Networks (Wojciech Czaja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wojciech Czaja, Sanghoon Na. (2024)<br><strong>Frame Quantization of Neural Networks</strong><br><button class=copy-to-clipboard title="Frame Quantization of Neural Networks" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08131v1.pdf filename=2404.08131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a post-training <b>quantization</b> algorithm with error estimates relying on ideas originating from frame theory. Specifically, we use first-order Sigma-Delta ($\Sigma\Delta$) <b>quantization</b> for finite unit-norm tight frames to <b>quantize</b> weight matrices and biases in a neural network. In our scenario, we derive an error bound between the original neural network and the <b>quantized</b> neural network in terms of step size and the number of frame elements. We also demonstrate how to leverage the redundancy of frames to achieve a <b>quantized</b> neural network with higher accuracy.</p></p class="citation"></blockquote><h3 id=2545--140276-learning-hamiltonian-dynamics-with-reproducing-kernel-hilbert-spaces-and-random-features-torbjørn-smith-et-al-2024>(25/45 | 140/276) Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and Random Features (Torbjørn Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Torbjørn Smith, Olav Egeland. (2024)<br><strong>Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and Random Features</strong><br><button class=copy-to-clipboard title="Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and Random Features" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07703v1.pdf filename=2404.07703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A method for learning Hamiltonian dynamics from a limited and noisy dataset is proposed. The method learns a Hamiltonian vector field on a reproducing kernel Hilbert space (RKHS) of inherently Hamiltonian vector fields, and in particular, odd Hamiltonian vector fields. This is done with a symplectic kernel, and it is shown how the kernel can be modified to an odd symplectic kernel to impose the odd symmetry. A random feature approximation is developed for the proposed kernel to reduce the problem size. This includes random feature approximations for odd kernels. The performance of the method is validated in <b>simulations</b> for three Hamiltonian systems. It is demonstrated that the use of an odd symplectic kernel improves prediction accuracy, and that the learned vector fields are Hamiltonian and exhibit the imposed odd symmetry characteristics.</p></p class="citation"></blockquote><h3 id=2645--141276-remembering-transformer-for-continual-learning-yuwei-sun-et-al-2024>(26/45 | 141/276) Remembering Transformer for Continual Learning (Yuwei Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Sun, Jun Sakuma, Ryota Kanai. (2024)<br><strong>Remembering Transformer for Continual Learning</strong><br><button class=copy-to-clipboard title="Remembering Transformer for Continual Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continual Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07518v1.pdf filename=2404.07518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks encounter the challenge of Catastrophic Forgetting (CF) in <b>continual</b> <b>learning,</b> where new task knowledge interferes with previously learned knowledge. We propose Remembering <b>Transformer,</b> inspired by the brain&rsquo;s Complementary Learning Systems (CLS), to tackle this issue. Remembering <b>Transformer</b> employs a mixture-of-adapters and a generative model-based routing mechanism to alleviate CF by dynamically routing task data to relevant adapters. Our approach demonstrated a new SOTA performance in various vision <b>continual</b> <b>learning</b> tasks and great parameter efficiency.</p></p class="citation"></blockquote><h3 id=2745--142276-generating-counterfactual-explanations-using-cardinality-constraints-rubén-ruiz-torrubiano-2024>(27/45 | 142/276) Generating Counterfactual Explanations Using Cardinality Constraints (Rubén Ruiz-Torrubiano, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rubén Ruiz-Torrubiano. (2024)<br><strong>Generating Counterfactual Explanations Using Cardinality Constraints</strong><br><button class=copy-to-clipboard title="Generating Counterfactual Explanations Using Cardinality Constraints" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Counter-factual, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07502v1.pdf filename=2404.07502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Providing explanations about how machine learning algorithms work and/or make particular predictions is one of the main tools that can be used to improve their trusworthiness, <b>fairness</b> and robustness. Among the most intuitive type of explanations are <b>counterfactuals,</b> which are examples that differ from a given point only in the prediction target and some set of features, presenting which features need to be changed in the original example to flip the prediction for that example. However, such <b>counterfactuals</b> can have many different features than the original example, making their interpretation difficult. In this paper, we propose to explicitly add a cardinality constraint to <b>counterfactual</b> generation limiting how many features can be different from the original example, thus providing more interpretable and easily understantable <b>counterfactuals.</b></p></p class="citation"></blockquote><h3 id=2845--143276-leveraging-domain-unlabeled-data-in-offline-reinforcement-learning-across-two-domains-soichiro-nishimori-et-al-2024>(28/45 | 143/276) Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning across Two Domains (Soichiro Nishimori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soichiro Nishimori, Xin-Qiang Cai, Johannes Ackermann, Masashi Sugiyama. (2024)<br><strong>Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning across Two Domains</strong><br><button class=copy-to-clipboard title="Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning across Two Domains" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07465v1.pdf filename=2404.07465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate an <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL) problem where datasets are collected from two domains. In this scenario, having datasets with domain labels facilitates efficient policy training. However, in practice, the task of assigning domain labels can be resource-intensive or infeasible at a large scale, leading to a prevalence of domain-unlabeled data. To formalize this challenge, we introduce a novel <b>offline</b> <b>RL</b> <b>problem</b> setting named Positive-Unlabeled <b>Offline</b> <b>RL</b> <b>(PUORL),</b> which incorporates domain-unlabeled data. To address PUORL, we develop an <b>offline</b> <b>RL</b> <b>algorithm</b> utilizing positive-unlabeled learning to predict the domain labels of domain-unlabeled data, enabling the integration of this data into policy training. Our experiments show the effectiveness of our method in accurately identifying domains and learning policies that outperform baselines in the PUORL setting, highlighting its capability to leverage domain-unlabeled data effectively.</p></p class="citation"></blockquote><h3 id=2945--144276-persistent-classification-a-new-approach-to-stability-of-data-and-adversarial-examples-brian-bell-et-al-2024>(29/45 | 144/276) Persistent Classification: A New Approach to Stability of Data and Adversarial Examples (Brian Bell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian Bell, Michael Geyer, David Glickenstein, Keaton Hamm, Carlos Scheidegger, Amanda Fernandez, Juston Moore. (2024)<br><strong>Persistent Classification: A New Approach to Stability of Data and Adversarial Examples</strong><br><button class=copy-to-clipboard title="Persistent Classification: A New Approach to Stability of Data and Adversarial Examples" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: MNIST, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08069v1.pdf filename=2404.08069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are a number of hypotheses underlying the existence of adversarial examples for classification problems. These include the high-dimensionality of the data, high codimension in the ambient space of the data manifolds of interest, and that the structure of machine learning models may encourage classifiers to develop decision boundaries close to data points. This article proposes a new framework for studying adversarial examples that does not depend directly on the distance to the decision boundary. Similarly to the smoothed classifier literature, we define a (natural or adversarial) data point to be $(\gamma,\sigma)$-stable if the probability of the same classification is at least $\gamma$ for points sampled in a Gaussian neighborhood of the point with a given standard deviation $\sigma$. We focus on studying the differences between persistence metrics along interpolants of natural and adversarial points. We show that adversarial examples have significantly lower persistence than natural examples for large neural networks in the context of the <b>MNIST</b> and ImageNet datasets. We connect this lack of persistence with decision boundary <b>geometry</b> by measuring angles of interpolants with respect to decision boundaries. Finally, we connect this approach with robustness by developing a manifold alignment gradient metric and demonstrating the increase in robustness that can be achieved when training with the addition of this metric.</p></p class="citation"></blockquote><h3 id=3045--145276-calibration-of-continual-learning-models-lanpei-li-et-al-2024>(30/45 | 145/276) Calibration of Continual Learning Models (Lanpei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanpei Li, Elia Piccoli, Andrea Cossu, Davide Bacciu, Vincenzo Lomonaco. (2024)<br><strong>Calibration of Continual Learning Models</strong><br><button class=copy-to-clipboard title="Calibration of Continual Learning Models" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07817v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07817v2.pdf filename=2404.07817v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>Learning</b> (CL) focuses on maximizing the predictive performance of a model across a non-stationary stream of data. Unfortunately, CL models tend to forget previous knowledge, thus often underperforming when compared with an offline model trained jointly on the entire data stream. Given that any CL model will eventually make mistakes, it is of crucial importance to build calibrated CL models: models that can reliably tell their confidence when making a prediction. Model calibration is an active research topic in machine learning, yet to be properly investigated in CL. We provide the first empirical study of the behavior of calibration approaches in CL, showing that CL strategies do not inherently learn calibrated models. To mitigate this issue, we design a <b>continual</b> <b>calibration</b> approach that improves the performance of post-processing calibration methods over a wide range of different <b>benchmarks</b> and CL strategies. CL does not necessarily need perfect predictive models, but rather it can benefit from reliable predictive models. We believe our study on <b>continual</b> <b>calibration</b> represents a first step towards this direction.</p></p class="citation"></blockquote><h3 id=3145--146276-realistic-continual-learning-approach-using-pre-trained-models-nadia-nasri-et-al-2024>(31/45 | 146/276) Realistic Continual Learning Approach using Pre-trained Models (Nadia Nasri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadia Nasri, Carlos Gutiérrez-Álvarez, Sergio Lafuente-Arroyo, Saturnino Maldonado-Bascón, Roberto J. López-Sastre. (2024)<br><strong>Realistic Continual Learning Approach using Pre-trained Models</strong><br><button class=copy-to-clipboard title="Realistic Continual Learning Approach using Pre-trained Models" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07729v1.pdf filename=2404.07729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> (CL) is crucial for evaluating adaptability in learning solutions to retain knowledge. Our research addresses the challenge of catastrophic forgetting, where models lose proficiency in previously learned tasks as they acquire new ones. While numerous solutions have been proposed, existing experimental setups often rely on idealized class-incremental learning scenarios. We introduce Realistic <b>Continual</b> <b>Learning</b> (RealCL), a novel CL paradigm where class distributions across tasks are random, departing from structured setups. We also present CLARE <b>(Continual</b> <b>Learning</b> Approach with pRE-trained models for RealCL scenarios), a pre-trained model-based solution designed to integrate new knowledge while preserving past learning. Our contributions include pioneering RealCL as a generalization of traditional CL setups, proposing CLARE as an adaptable approach for RealCL tasks, and conducting extensive experiments demonstrating its effectiveness across various RealCL scenarios. Notably, CLARE outperforms existing models on RealCL <b>benchmarks,</b> highlighting its versatility and robustness in unpredictable learning environments.</p></p class="citation"></blockquote><h3 id=3245--147276-fedauxhmtl-federated-auxiliary-hard-parameter-sharing-multi-task-learning-for-network-edge-traffic-classification-faisal-ahmed-et-al-2024>(32/45 | 147/276) FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task Learning for Network Edge Traffic Classification (Faisal Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faisal Ahmed, Myungjin Lee, Suresh Subramaniam, Motoharu Matsuura, Hiroshi Hasegawa, Shih-Chun Lin. (2024)<br><strong>FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task Learning for Network Edge Traffic Classification</strong><br><button class=copy-to-clipboard title="FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task Learning for Network Edge Traffic Classification" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08028v1.pdf filename=2404.08028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has garnered significant interest recently due to its potential as an effective solution for tackling many challenges in diverse application scenarios, for example, data privacy in network edge traffic classification. Despite its recognized advantages, FL encounters obstacles linked to statistical data heterogeneity and labeled data scarcity during the training of single-task models for machine learning-based traffic classification, leading to hindered learning performance. In response to these challenges, adopting a hard-parameter sharing multi-task learning model with auxiliary tasks proves to be a suitable approach. Such a model has the capability to reduce communication and computation costs, navigate statistical complexities inherent in FL contexts, and overcome labeled data scarcity by leveraging knowledge derived from interconnected auxiliary tasks. This paper introduces a new framework for <b>federated</b> <b>auxiliary</b> hard-parameter sharing multi-task learning, namely, FedAuxHMTL. The introduced framework incorporates model parameter exchanges between edge server and base stations, enabling base stations from distributed areas to participate in the FedAuxHMTL process and enhance the learning performance of the main task-network edge traffic classification. Empirical experiments are conducted to validate and demonstrate the FedAuxHMTL&rsquo;s effectiveness in terms of accuracy, total global loss, communication costs, computing time, and energy consumption compared to its counterparts.</p></p class="citation"></blockquote><h3 id=3345--148276-recurrentgemma-moving-past-transformers-for-efficient-open-language-models-aleksandar-botev-et-al-2024>(33/45 | 148/276) RecurrentGemma: Moving Past Transformers for Efficient Open Language Models (Aleksandar Botev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksandar Botev, Soham De, Samuel L Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Clément Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, Nando de Frietas. (2024)<br><strong>RecurrentGemma: Moving Past Transformers for Efficient Open Language Models</strong><br><button class=copy-to-clipboard title="RecurrentGemma: Moving Past Transformers for Efficient Open Language Models" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07839v1.pdf filename=2404.07839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce RecurrentGemma, an open language model which uses Google&rsquo;s novel Griffin architecture. Griffin combines linear recurrences with local attention to achieve excellent performance on language. It has a fixed-sized state, which reduces memory use and enables efficient inference on long sequences. We provide a pre-trained model with 2B non-embedding parameters, and an instruction tuned variant. Both models achieve comparable performance to Gemma-2B despite being trained on fewer tokens.</p></p class="citation"></blockquote><h3 id=3445--149276-unsupervised-concept-drift-detection-based-on-parallel-activations-of-neural-network-joanna-komorniczak-et-al-2024>(34/45 | 149/276) Unsupervised Concept Drift Detection based on Parallel Activations of Neural Network (Joanna Komorniczak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joanna Komorniczak, Paweł Ksieniewicz. (2024)<br><strong>Unsupervised Concept Drift Detection based on Parallel Activations of Neural Network</strong><br><button class=copy-to-clipboard title="Unsupervised Concept Drift Detection based on Parallel Activations of Neural Network" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07776v1.pdf filename=2404.07776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Practical applications of artificial intelligence increasingly often have to deal with the streaming properties of real data, which, considering the time factor, are subject to phenomena such as periodicity and more or less chaotic degeneration - resulting directly in the concept drifts. The modern concept drift detectors almost always assume immediate access to labels, which due to their cost, limited availability and possible delay has been shown to be unrealistic. This work proposes an <b>unsupervised</b> Parallel Activations Drift Detector, utilizing the outputs of an untrained neural network, presenting its key design elements, intuitions about processing properties, and a pool of computer experiments demonstrating its competitiveness with state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3545--150276-enhancing-policy-gradient-with-the-polyak-step-size-adaption-yunxiang-li-et-al-2024>(35/45 | 150/276) Enhancing Policy Gradient with the Polyak Step-Size Adaption (Yunxiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunxiang Li, Rui Yuan, Chen Fan, Mark Schmidt, Samuel Horváth, Robert M. Gower, Martin Takáč. (2024)<br><strong>Enhancing Policy Gradient with the Polyak Step-Size Adaption</strong><br><button class=copy-to-clipboard title="Enhancing Policy Gradient with the Polyak Step-Size Adaption" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07525v1.pdf filename=2404.07525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Policy gradient is a widely utilized and foundational algorithm in the field of <b>reinforcement</b> <b>learning</b> (RL). Renowned for its convergence guarantees and stability compared to other RL algorithms, its practical application is often hindered by sensitivity to hyper-parameters, particularly the step-size. In this paper, we introduce the integration of the Polyak step-size in RL, which automatically adjusts the step-size without prior knowledge. To adapt this method to RL settings, we address several issues, including unknown f* in the Polyak step-size. Additionally, we showcase the performance of the Polyak step-size in RL through experiments, demonstrating faster convergence and the attainment of more stable policies.</p></p class="citation"></blockquote><h3 id=3645--151276-predictive-modelling-of-air-quality-index-aqi-across-diverse-cities-and-states-of-india-using-machine-learning-investigating-the-influence-of-punjabs-stubble-burning-on-aqi-variability-kamaljeet-kaur-sidhu-et-al-2024>(36/45 | 151/276) Predictive Modelling of Air Quality Index (AQI) Across Diverse Cities and States of India using Machine Learning: Investigating the Influence of Punjab&rsquo;s Stubble Burning on AQI Variability (Kamaljeet Kaur Sidhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kamaljeet Kaur Sidhu, Habeeb Balogun, Kazeem Oluwakemi Oseni. (2024)<br><strong>Predictive Modelling of Air Quality Index (AQI) Across Diverse Cities and States of India using Machine Learning: Investigating the Influence of Punjab&rsquo;s Stubble Burning on AQI Variability</strong><br><button class=copy-to-clipboard title="Predictive Modelling of Air Quality Index (AQI) Across Diverse Cities and States of India using Machine Learning: Investigating the Influence of Punjab's Stubble Burning on AQI Variability" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08702v1.pdf filename=2404.08702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Air pollution is a common and serious problem nowadays and it cannot be ignored as it has harmful impacts on human health. To address this issue proactively, people should be aware of their surroundings, which means the environment where they survive. With this motive, this research has predicted the AQI based on different air pollutant concentrations in the atmosphere. The dataset used for this research has been taken from the official website of CPCB. The dataset has the air pollutant concentration from 22 different monitoring stations in different cities of Delhi, Haryana, and Punjab. This data is checked for null values and outliers. But, the most important thing to note is the correct understanding and imputation of such values rather than ignoring or doing wrong imputation. The time series data has been used in this research which is tested for stationarity using The Dickey-Fuller test. Further different ML models like CatBoost, XGBoost, Random Forest, SVM regressor, time series model SARIMAX, and deep learning model <b>LSTM</b> have been used to predict AQI. For the performance evaluation of different models, I used MSE, RMSE, MAE, and R2. It is observed that Random Forest performed better as compared to other models.</p></p class="citation"></blockquote><h3 id=3745--152276-data-driven-portfolio-management-for-motion-pictures-industry-a-new-data-driven-optimization-methodology-using-a-large-language-model-as-the-expert-mohammad-alipour-vaezi-et-al-2024>(37/45 | 152/276) Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert (Mohammad Alipour-Vaezi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Alipour-Vaezi, Kwok-Leung Tsui. (2024)<br><strong>Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert</strong><br><button class=copy-to-clipboard title="Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07434v1.pdf filename=2404.07434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Portfolio management is one of the unresponded problems of the Motion Pictures Industry (MPI). To design an optimal portfolio for an MPI distributor, it is essential to predict the box office of each project. Moreover, for an accurate box office prediction, it is critical to consider the effect of the celebrities involved in each MPI project, which was impossible with any precedent expert-based method. Additionally, the asymmetric characteristic of MPI data decreases the performance of any predictive algorithm. In this paper, firstly, the fame score of the celebrities is determined using a <b>large</b> <b>language</b> <b>model.</b> Then, to tackle the asymmetric character of MPI&rsquo;s data, projects are classified. Furthermore, the box office prediction takes place for each class of projects. Finally, using a hybrid multi-attribute decision-making technique, the preferability of each project for the distributor is calculated, and benefiting from a bi-objective optimization model, the optimal portfolio is designed.</p></p class="citation"></blockquote><h3 id=3845--153276-wildgraph-realistic-graph-based-trajectory-generation-for-wildlife-ali-al-lawati-et-al-2024>(38/45 | 153/276) WildGraph: Realistic Graph-based Trajectory Generation for Wildlife (Ali Al-Lawati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Al-Lawati, Elsayed Eshra, Prasenjit Mitra. (2024)<br><strong>WildGraph: Realistic Graph-based Trajectory Generation for Wildlife</strong><br><button class=copy-to-clipboard title="WildGraph: Realistic Graph-based Trajectory Generation for Wildlife" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08068v1.pdf filename=2404.08068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory generation is an important task in movement studies; it circumvents the privacy, ethical, and technical challenges of collecting real trajectories from the target population. In particular, real trajectories in the wildlife domain are scarce as a result of ethical and environmental constraints of the collection process. In this paper, we consider the problem of generating long-horizon trajectories, akin to wildlife migration, based on a small set of real <b>samples.</b> <b>We</b> propose a hierarchical approach to learn the global movement characteristics of the real dataset and recursively refine localized regions. Our solution, WildGraph, discretizes the geographic path into a prototype network of H3 (<a href=https://www.uber.com/blog/h3/>https://www.uber.com/blog/h3/</a>) regions and leverages a recurrent variational auto-encoder to probabilistically generate paths over the regions, based on occupancy. WildGraph successfully generates realistic months-long trajectories using a <b>sample</b> <b>size</b> as small as 60. Experiments performed on two wildlife migration datasets demonstrate that our proposed method improves the generalization of the generated trajectories in comparison to existing work while achieving superior or comparable performance in several <b>benchmark</b> metrics. Our code is published on the following repository: \url{https://github.com/aliwister/wildgraph}.</p></p class="citation"></blockquote><h3 id=3945--154276-streaming-detection-of-significant-delay-changes-in-public-transport-systems-przemysław-wrona-et-al-2024>(39/45 | 154/276) Streaming detection of significant delay changes in public transport systems (Przemysław Wrona et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Przemysław Wrona, Maciej Grzenda, Marcin Luckner. (2024)<br><strong>Streaming detection of significant delay changes in public transport systems</strong><br><button class=copy-to-clipboard title="Streaming detection of significant delay changes in public transport systems" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-soc-ph<br>Keyword Score: 9<br>Keywords: Graph, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07860v1.pdf filename=2404.07860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Public transport systems are expected to reduce pollution and contribute to sustainable development. However, disruptions in public transport such as delays may negatively affect mobility choices. To quantify delays, aggregated data from vehicle locations systems are frequently used. However, delays observed at individual stops are caused inter alia by fluctuations in running times and propagation of delays occurring in other locations. Hence, in this work, we propose both the method detecting significant delays and reference architecture, relying on stream processing engines, in which the method is implemented. The method can complement the calculation of delays defined as deviation from schedules. This provides both online rather than batch identification of significant and repetitive delays, and resilience to the limited quality of location data. The method we propose can be used with different change detectors, such as ADWIN, applied to location data stream shuffled to individual edges of a transport <b>graph.</b> It can detect in an online manner at which edges statistically significant delays are observed and at which edges delays arise and are reduced. Detections can be used to model mobility choices and quantify the impact of repetitive rather than random disruptions on feasible trips with <b>multimodal</b> trip modelling engines. The evaluation performed with the public transport data of over 2000 vehicles confirms the merits of the method and reveals that a limited-size subgraph of a transport system <b>graph</b> causes statistically significant delays</p></p class="citation"></blockquote><h3 id=4045--155276-the-oxmat-dataset-a-multimodal-resource-for-the-development-of-ai-driven-technologies-in-maternal-and-newborn-child-health-m-jaleed-khan-et-al-2024>(40/45 | 155/276) The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health (M. Jaleed Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M. Jaleed Khan, Ioana Duta, Beth Albert, William Cooke, Manu Vatish, Gabriel Davis Jones. (2024)<br><strong>The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health</strong><br><button class=copy-to-clipboard title="The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08024v1.pdf filename=2404.08024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of Artificial Intelligence (AI) in healthcare presents a unique opportunity for advancements in obstetric care, particularly through the analysis of cardiotocography (CTG) for fetal monitoring. However, the effectiveness of such technologies depends upon the availability of large, high-quality datasets that are suitable for machine learning. This paper introduces the Oxford Maternity (OxMat) dataset, the world&rsquo;s largest curated dataset of CTGs, featuring raw time series CTG data and extensive clinical data for both mothers and babies, which is ideally placed for machine learning. The OxMat dataset addresses the critical gap in women&rsquo;s health data by providing over 177,211 unique CTG recordings from 51,036 pregnancies, carefully curated and reviewed since 1991. The dataset also comprises over 200 antepartum, intrapartum and postpartum clinical variables, ensuring near-complete data for crucial outcomes such as stillbirth and acidaemia. While this dataset also covers the intrapartum stage, around 94% of the constituent CTGS are antepartum. This allows for a unique focus on the underserved antepartum period, in which early detection of at-risk fetuses can significantly improve health outcomes. Our comprehensive review of existing datasets reveals the limitations of current datasets: primarily, their lack of sufficient volume, detailed clinical data and antepartum data. The OxMat dataset lays a foundation for future AI-driven prenatal care, offering a robust resource for developing and testing algorithms aimed at improving maternal and fetal health outcomes.</p></p class="citation"></blockquote><h3 id=4145--156276-iitp-vdland-a-comprehensive-dataset-on-decentraland-parcels-ankit-k-bhagat-et-al-2024>(41/45 | 156/276) IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels (Ankit K. Bhagat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankit K. Bhagat, Dipika Jha, Raju Halder, Rajendra N. Paramanik, Chandra M. Kumar. (2024)<br><strong>IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels</strong><br><button class=copy-to-clipboard title="IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-ET, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07533v1.pdf filename=2404.07533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents IITP-VDLand, a comprehensive dataset of Decentraland parcels sourced from diverse platforms. Unlike existing datasets which have limited attributes and records, IITP-VDLand offers a rich array of attributes, encompassing parcel characteristics, trading history, past activities, transactions, and social media interactions. Alongside, we introduce a key attribute in the dataset, namely Rarity score, which measures the uniqueness of each parcel within the virtual world. Addressing the significant challenge posed by the dispersed nature of this data across various sources, we employ a systematic approach, utilizing both available APIs and custom scripts, to gather it. Subsequently, we meticulously curate and organize the information into four distinct segments: (1) Characteristics Data-Fragment, (2) OpenSea Trading History Data-Fragment, (3) Ethereum Activity Transactions Data-Fragment, and (4) Social Media Data-Fragment. We envisage that this dataset would serve as a robust resource for training machine- and deep-learning models specifically designed to address real-world challenges within the domain of Decentraland parcels. The performance <b>benchmarking</b> of more than 20 state-of-the-art price prediction models on our dataset yields promising results, achieving a maximum R2 score of 0.8251 and an accuracy of 74.23% in case of Extra Trees Regressor and Classifier. The key findings reveal that the ensemble models performs better than both deep learning and linear models for our dataset. We observe a significant impact of coordinates, geographical proximity, rarity score, and few other economic indicators on the prediction of parcel prices.</p></p class="citation"></blockquote><h3 id=4245--157276-point-cloud-geometry-scalable-coding-with-a-quality-conditioned-latents-probability-estimator-daniele-mari-et-al-2024>(42/45 | 157/276) Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator (Daniele Mari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniele Mari, André F. R. Guarda, Nuno M. M. Rodrigues, Simone Milani, Fernando Pereira. (2024)<br><strong>Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator</strong><br><button class=copy-to-clipboard title="Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07698v1.pdf filename=2404.07698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread usage of point clouds (PC) for immersive visual applications has resulted in the use of very heterogeneous receiving conditions and devices, notably in terms of network, hardware, and display capabilities. In this scenario, quality scalability, i.e., the ability to reconstruct a signal at different qualities by progressively decoding a single bitstream, is a major requirement that has yet to be conveniently addressed, notably in most learning-based PC coding solutions. This paper proposes a quality scalability scheme, named Scalable Quality Hyperprior (SQH), adaptable to learning-based static point cloud <b>geometry</b> codecs, which uses a Quality-conditioned Latents Probability Estimator (QuLPE) to decode a high-quality version of a PC learning-based representation, based on an available lower quality base layer. SQH is integrated in the future JPEG PC coding standard, allowing to create a layered bitstream that can be used to progressively decode the PC <b>geometry</b> with increasing quality and fidelity. Experimental results show that SQH offers the quality scalability feature with very limited or no compression performance penalty at all when compared with the corresponding non-scalable solution, thus preserving the significant compression gains over other state-of-the-art PC codecs.</p></p class="citation"></blockquote><h3 id=4345--158276-representation-learning-of-tangled-key-value-sequence-data-for-early-classification-tao-duan-et-al-2024>(43/45 | 158/276) Representation Learning of Tangled Key-Value Sequence Data for Early Classification (Tao Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Duan, Junzhou Zhao, Shuo Zhang, Jing Tao, Pinghui Wang. (2024)<br><strong>Representation Learning of Tangled Key-Value Sequence Data for Early Classification</strong><br><button class=copy-to-clipboard title="Representation Learning of Tangled Key-Value Sequence Data for Early Classification" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07454v1.pdf filename=2404.07454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Key-value sequence data has become ubiquitous and naturally appears in a variety of real-world applications, ranging from the user-product purchasing sequences in e-commerce, to network packet sequences forwarded by routers in networking. Classifying these key-value sequences is important in many scenarios such as user profiling and malicious applications identification. In many time-sensitive scenarios, besides the requirement of classifying a key-value sequence accurately, it is also desired to classify a key-value sequence early, in order to respond fast. However, these two goals are conflicting in nature, and it is challenging to achieve them simultaneously. In this work, we formulate a novel tangled key-value sequence early classification problem, where a tangled key-value sequence is a mixture of several concurrent key-value sequences with different keys. The goal is to classify each individual key-value sequence sharing a same key both accurately and early. To address this problem, we propose a novel method, i.e., Key-Value sequence Early Co-classification (KVEC), which leverages both inner- and inter-correlations of items in a tangled key-value sequence through key correlation and value correlation to learn a better sequence <b>representation.</b> <b>Meanwhile,</b> a time-aware halting policy decides when to stop the ongoing key-value sequence and classify it based on current sequence <b>representation.</b> <b>Experiments</b> on both real-world and synthetic datasets demonstrate that our method outperforms the state-of-the-art baselines significantly. KVEC improves the prediction accuracy by up to $4.7 - 17.5%$ under the same prediction earliness condition, and improves the harmonic mean of accuracy and earliness by up to $3.7 - 14.0%$.</p></p class="citation"></blockquote><h3 id=4445--159276-f_β-plot----a-visual-tool-for-evaluating-imbalanced-data-classifiers-szymon-wojciechowski-et-al-2024>(44/45 | 159/276) $F_β$-plot &ndash; a visual tool for evaluating imbalanced data classifiers (Szymon Wojciechowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Szymon Wojciechowski, Michał Woźniak. (2024)<br><strong>$F_β$-plot &ndash; a visual tool for evaluating imbalanced data classifiers</strong><br><button class=copy-to-clipboard title="$F_β$-plot -- a visual tool for evaluating imbalanced data classifiers" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08709v1.pdf filename=2404.08709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the significant problems associated with imbalanced data classification is the lack of reliable metrics. This runs primarily from the fact that for most real-life (as well as commonly used <b>benchmark)</b> problems, we do not have information from the user on the actual form of the loss function that should be minimized. Although it is pretty common to have metrics indicating the classification quality within each class, for the end user, the analysis of several such metrics is then required, which in practice causes difficulty in interpreting the usefulness of a given classifier. Hence, many aggregate metrics have been proposed or adopted for the imbalanced data classification problem, but there is still no consensus on which should be used. An additional disadvantage is their ambiguity and systematic bias toward one class. Moreover, their use in analyzing experimental results in recognition of those classification models that perform well for the chosen aggregated metrics is burdened with the drawbacks mentioned above. Hence, the paper proposes a simple approach to analyzing the popular parametric metric $F_\beta$. We point out that it is possible to indicate for a given pool of analyzed classifiers when a given model should be preferred depending on user requirements.</p></p class="citation"></blockquote><h3 id=4545--160276-global-versus-local-evaluating-alexnet-architectures-for-tropical-cyclone-intensity-estimation-vikas-dwivedi-2024>(45/45 | 160/276) Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation (Vikas Dwivedi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vikas Dwivedi. (2024)<br><strong>Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation</strong><br><button class=copy-to-clipboard title="Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, physics-ao-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07395v1.pdf filename=2404.07395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the destructive impacts of tropical cyclones, it is critical to have a reliable system for cyclone intensity detection. Various techniques are available for this purpose, each with differing levels of accuracy. In this paper, we introduce two ensemble-based models based on AlexNet architecture to estimate tropical cyclone intensity using visible satellite images. The first model, trained on the entire dataset, is called the global AlexNet model. The second model is a distributed version of AlexNet in which multiple AlexNets are trained separately on subsets of the training data categorized according to the Saffir-Simpson wind speed scale prescribed by the meterologists. We evaluated the performance of both models against a deep learning <b>benchmark</b> model called \textit{Deepti} using a publicly available cyclone image dataset. Results indicate that both the global model (with a root mean square error (RMSE) of 9.03 knots) and the distributed model (with a RMSE of 9.3 knots) outperform the <b>benchmark</b> model (with a RMSE of 13.62 knots). We provide a thorough discussion of our solution approach, including an explanantion of the AlexNet&rsquo;s performance using gradient class activation maps (grad-CAM). Our proposed solution strategy allows future experimentation with various deep learning models in both single and multi-channel settings.</p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=115--161276-reflectance-estimation-for-proximity-sensing-by-vision-language-models-utilizing-distributional-semantics-for-low-level-cognition-in-robotics-masashi-osada-et-al-2024>(1/15 | 161/276) Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics (Masashi Osada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masashi Osada, Gustavo A. Garcia Ricardez, Yosuke Suzuki, Tadahiro Taniguchi. (2024)<br><strong>Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics</strong><br><button class=copy-to-clipboard title="Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07717v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07717v2.pdf filename=2404.07717v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and <b>vision-language</b> models (VLMs) have been increasingly used in robotics for high-level cognition, but their use for low-level cognition, such as interpreting sensor information, remains underexplored. In robotic grasping, estimating the reflectance of objects is crucial for successful grasping, as it significantly impacts the distance measured by proximity sensors. We investigate whether <b>LLMs</b> can estimate reflectance from object names alone, leveraging the embedded human knowledge in distributional semantics, and if the latent structure of language in VLMs positively affects image-based reflectance estimation. In this paper, we verify that 1) <b>LLMs</b> such as <b>GPT-3.5</b> and <b>GPT-4</b> can estimate an object&rsquo;s reflectance using only text as input; and 2) VLMs such as CLIP can increase their generalization capabilities in reflectance estimation from images. Our experiments show that <b>GPT-4</b> can estimate an object&rsquo;s reflectance using only text input with a mean error of 14.7%, lower than the image-only ResNet. Moreover, CLIP achieved the lowest mean error of 11.8%, while <b>GPT-3.5</b> obtained a competitive 19.9% compared to ResNet&rsquo;s 17.8%. These results suggest that the distributional semantics in <b>LLMs</b> and VLMs increases their generalization capabilities, and the knowledge acquired by VLMs benefits from the latent structure of language.</p></p class="citation"></blockquote><h3 id=215--162276-generating-consistent-pddl-domains-with-large-language-models-pavel-smirnov-et-al-2024>(2/15 | 162/276) Generating consistent PDDL domains with Large Language Models (Pavel Smirnov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavel Smirnov, Frank Joublin, Antonello Ceravola, Michael Gienger. (2024)<br><strong>Generating consistent PDDL domains with Large Language Models</strong><br><button class=copy-to-clipboard title="Generating consistent PDDL domains with Large Language Models" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Planning Domain Descrition Language, human-in-the-loop, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07751v1.pdf filename=2404.07751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are capable of transforming natural language domain descriptions into plausibly looking <b>PDDL</b> markup. However, ensuring that actions are consistent within domains still remains a challenging task. In this paper we present a novel concept to significantly improve the quality of <b>LLM-generated</b> <b>PDDL</b> models by performing automated consistency checking during the generation process. Although the proposed consistency checking strategies still can&rsquo;t guarantee absolute correctness of generated models, they can serve as valuable source of feedback reducing the amount of correction efforts expected from a human in the loop. We demonstrate the capabilities of our error detection approach on a number of classical and custom planning domains (logistics, gripper, tyreworld, household, pizza).</p></p class="citation"></blockquote><h3 id=315--163276-can-vehicle-motion-planning-generalize-to-realistic-long-tail-scenarios-marcel-hallgarten-et-al-2024>(3/15 | 163/276) Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios? (Marcel Hallgarten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Hallgarten, Julian Zapata, Martin Stoll, Katrin Renz, Andreas Zell. (2024)<br><strong>Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?</strong><br><button class=copy-to-clipboard title="Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07569v1.pdf filename=2404.07569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner&rsquo;s capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop <b>benchmark</b> interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our <b>benchmark</b> and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios. A recently evolving direction is the usage of <b>foundation</b> <b>models</b> like <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to handle generalization. We evaluate an <b>LLM-only</b> planner and introduce a novel hybrid planner that combines an <b>LLM-based</b> behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=415--164276-quasisim-parameterized-quasi-physical-simulators-for-dexterous-manipulations-transfer-xueyi-liu-et-al-2024>(4/15 | 164/276) QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer (Xueyi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueyi Liu, Kangbo Lyu, Jieqiong Zhang, Tao Du, Li Yi. (2024)<br><strong>QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer</strong><br><button class=copy-to-clipboard title="QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-GR, cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07988v1.pdf filename=2404.07988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore the dexterous manipulation transfer problem by designing simulators. The task wishes to transfer human manipulations to dexterous robot hand <b>simulations</b> and is inherently difficult due to its intricate, highly-constrained, and discontinuous dynamics and the need to control a dexterous hand with a DoF to accurately replicate human manipulations. Previous approaches that optimize in high-fidelity <b>black-box</b> <b>simulators</b> or a modified one with relaxed constraints only demonstrate limited capabilities or are restricted by insufficient <b>simulation</b> fidelity. We introduce parameterized quasi-physical simulators and a physics curriculum to overcome these limitations. The key ideas are 1) balancing between fidelity and optimizability of the <b>simulation</b> via a curriculum of parameterized simulators, and 2) solving the problem in each of the simulators from the curriculum, with properties ranging from high task optimizability to high fidelity. We successfully enable a dexterous hand to track complex and diverse manipulations in high-fidelity simulated environments, boosting the success rate by 11%+ from the best-performed baseline. The project website is available at <a href=https://meowuu7.github.io/QuasiSim/>https://meowuu7.github.io/QuasiSim/</a>.</p></p class="citation"></blockquote><h3 id=515--165276-multi-robot-target-tracking-with-sensing-and-communication-danger-zones-jiazhen-li-et-al-2024>(5/15 | 165/276) Multi-Robot Target Tracking with Sensing and Communication Danger Zones (Jiazhen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazhen Li, Peihan Li, Yuwei Wu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou. (2024)<br><strong>Multi-Robot Target Tracking with Sensing and Communication Danger Zones</strong><br><button class=copy-to-clipboard title="Multi-Robot Target Tracking with Sensing and Communication Danger Zones" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07880v1.pdf filename=2404.07880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot target tracking finds extensive applications in different scenarios, such as environmental surveillance and wildfire management, which require the robustness of the practical deployment of multi-robot systems in uncertain and dangerous environments. Traditional approaches often focus on the performance of tracking accuracy with no modeling and assumption of the environments, neglecting potential environmental hazards which result in system failures in real-world deployments. To address this challenge, we investigate multi-robot target tracking in the adversarial environment considering sensing and communication attacks with uncertainty. We design specific strategies to avoid different danger zones and proposed a multi-agent tracking framework under the perilous environment. We approximate the probabilistic constraints and formulate practical optimization strategies to address computational challenges efficiently. We evaluate the performance of our proposed methods in <b>simulations</b> to demonstrate the ability of robots to adjust their risk-aware behaviors under different levels of environmental uncertainty and risk confidence. The proposed method is further validated via real-world robot experiments where a team of drones successfully track dynamic ground robots while being risk-aware of the sensing and/or communication danger zones.</p></p class="citation"></blockquote><h3 id=615--166276-dual-quaternion-control-of-uavs-with-cable-suspended-load-yuxia-yuan-et-al-2024>(6/15 | 166/276) Dual Quaternion Control of UAVs with Cable-suspended Load (Yuxia Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxia Yuan, Markus Ryll. (2024)<br><strong>Dual Quaternion Control of UAVs with Cable-suspended Load</strong><br><button class=copy-to-clipboard title="Dual Quaternion Control of UAVs with Cable-suspended Load" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07635v1.pdf filename=2404.07635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling the kinematics and dynamics of robotics systems with suspended loads using dual quaternions has not been explored so far. This paper introduces a new innovative control strategy using dual quaternions for UAVs with cable-suspended loads, focusing on the sling load lifting and tracking problems. By utilizing the mathematical efficiency and compactness of dual quaternions, a unified representation of the UAV and its suspended load&rsquo;s dynamics and kinematics is achieved, facilitating the realization of load lifting and trajectory tracking. The <b>simulation</b> results have tested the proposed strategy&rsquo;s accuracy, efficiency, and robustness. This study makes a substantial contribution to present this novel control strategy that harnesses the benefits of dual quaternions for cargo UAVs. Our work also holds promise for inspiring future innovations in under-actuated systems control using dual quaternions.</p></p class="citation"></blockquote><h3 id=715--167276-differentiable-rendering-as-a-way-to-program-cable-driven-soft-robots-kasra-arnavaz-et-al-2024>(7/15 | 167/276) Differentiable Rendering as a Way to Program Cable-Driven Soft Robots (Kasra Arnavaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasra Arnavaz, Kenny Erleben. (2024)<br><strong>Differentiable Rendering as a Way to Program Cable-Driven Soft Robots</strong><br><button class=copy-to-clipboard title="Differentiable Rendering as a Way to Program Cable-Driven Soft Robots" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: I-3-6; I-3-7, cs-GR, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07590v1.pdf filename=2404.07590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soft robots have gained increased popularity in recent years due to their adaptability and compliance. In this paper, we use a digital twin model of cable-driven soft robots to learn control parameters in <b>simulation.</b> In doing so, we take advantage of differentiable rendering as a way to instruct robots to complete tasks such as point reach, gripping an object, and obstacle avoidance. This approach simplifies the mathematical description of such complicated tasks and removes the need for landmark points and their tracking. Our experiments demonstrate the applicability of our method.</p></p class="citation"></blockquote><h3 id=815--168276-adademo-data-efficient-demonstration-expansion-for-generalist-robotic-agent-tongzhou-mu-et-al-2024>(8/15 | 168/276) AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent (Tongzhou Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongzhou Mu, Yijie Guo, Jie Xu, Ankit Goyal, Hao Su, Dieter Fox, Animesh Garg. (2024)<br><strong>AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent</strong><br><button class=copy-to-clipboard title="AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07428v1.pdf filename=2404.07428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Encouraged by the remarkable achievements of language and vision <b>foundation</b> <b>models,</b> developing generalist robotic agents through imitation learning, using large demonstration datasets, has become a prominent area of interest in robot learning. The efficacy of imitation learning is heavily reliant on the quantity and quality of the demonstration datasets. In this study, we aim to scale up demonstrations in a data-efficient way to facilitate the learning of generalist robotic agents. We introduce AdaDemo (Adaptive Online Demonstration Expansion), a general framework designed to improve multi-task policy learning by actively and continually expanding the demonstration dataset. AdaDemo strategically collects new demonstrations to address the identified weakness in the existing policy, ensuring data efficiency is maximized. Through a comprehensive evaluation on a total of 22 tasks across two robotic manipulation <b>benchmarks</b> (RLBench and Adroit), we demonstrate AdaDemo&rsquo;s capability to progressively improve policy performance by guiding the generation of high-quality demonstration datasets in a data-efficient manner.</p></p class="citation"></blockquote><h3 id=915--169276-data-driven-system-identification-of-quadrotors-subject-to-motor-delays-jonas-eschmann-et-al-2024>(9/15 | 169/276) Data-Driven System Identification of Quadrotors Subject to Motor Delays (Jonas Eschmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Eschmann, Dario Albani, Giuseppe Loianno. (2024)<br><strong>Data-Driven System Identification of Quadrotors Subject to Motor Delays</strong><br><button class=copy-to-clipboard title="Data-Driven System Identification of Quadrotors Subject to Motor Delays" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07837v1.pdf filename=2404.07837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently non-linear control methods like Model Predictive Control (MPC) and <b>Reinforcement</b> <b>Learning</b> (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotor&rsquo;s inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions.</p></p class="citation"></blockquote><h3 id=1015--170276-diffusing-in-someone-elses-shoes-robotic-perspective-taking-with-diffusion-josua-spisak-et-al-2024>(10/15 | 170/276) Diffusing in Someone Else&rsquo;s Shoes: Robotic Perspective Taking with Diffusion (Josua Spisak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josua Spisak, Matthias Kerzel, Stefan Wermter. (2024)<br><strong>Diffusing in Someone Else&rsquo;s Shoes: Robotic Perspective Taking with Diffusion</strong><br><button class=copy-to-clipboard title="Diffusing in Someone Else's Shoes: Robotic Perspective Taking with Diffusion" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07735v1.pdf filename=2404.07735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humanoid robots can benefit from their similarity to the human shape by learning from humans. When humans teach other humans how to perform actions, they often demonstrate the actions and the learning human can try to imitate the demonstration. Being able to mentally transfer from a demonstration seen from a third-person perspective to how it should look from a first-person perspective is fundamental for this ability in humans. As this is a challenging task, it is often simplified for robots by creating a demonstration in the first-person perspective. Creating these demonstrations requires more effort but allows for an easier imitation. We introduce a novel <b>diffusion</b> <b>model</b> aimed at enabling the robot to directly learn from the third-person demonstrations. Our model is capable of learning and generating the first-person perspective from the third-person perspective by translating the size and rotations of objects and the environment between two perspectives. This allows us to utilise the benefits of easy-to-produce third-person demonstrations and easy-to-imitate first-person demonstrations. The model can either represent the first-person perspective in an RGB image or calculate the joint values. Our approach significantly outperforms other image-to-image models in this task.</p></p class="citation"></blockquote><h3 id=1115--171276-model-predictive-trajectory-planning-for-human-robot-handovers-thies-oelerich-et-al-2024>(11/15 | 171/276) Model Predictive Trajectory Planning for Human-Robot Handovers (Thies Oelerich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thies Oelerich, Christian Hartl-Nesic, Andreas Kugi. (2024)<br><strong>Model Predictive Trajectory Planning for Human-Robot Handovers</strong><br><button class=copy-to-clipboard title="Model Predictive Trajectory Planning for Human-Robot Handovers" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07505v1.pdf filename=2404.07505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work develops a novel trajectory planner for human-robot handovers. The handover requirements can naturally be handled by a path-following-based model predictive controller, where the path progress serves as a progress measure of the handover. Moreover, the deviations from the path are used to follow human motion by adapting the path deviation bounds with a handover location prediction. A <b>Gaussian</b> <b>process</b> regression model, which is trained on known handover trajectories, is employed for this prediction. Experiments with a collaborative 7-DoF robotic manipulator show the effectiveness and versatility of the proposed approach.</p></p class="citation"></blockquote><h3 id=1215--172276-one-shot-transfer-of-long-horizon-extrinsic-manipulation-through-contact-retargeting-albert-wu-et-al-2024>(12/15 | 172/276) One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting (Albert Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert Wu, Ruocheng Wang, Sirui Chen, Clemens Eppner, C. Karen Liu. (2024)<br><strong>One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting</strong><br><button class=copy-to-clipboard title="One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07468v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07468v1.pdf filename=2404.07468v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints <b>stemming</b> from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation. Code and additional details are available at stanford-tml.github.io/extrinsic-manipulation.</p></p class="citation"></blockquote><h3 id=1315--173276-socially-pertinent-robots-in-gerontological-healthcare-xavier-alameda-pineda-et-al-2024>(13/15 | 173/276) Socially Pertinent Robots in Gerontological Healthcare (Xavier Alameda-Pineda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xavier Alameda-Pineda, Angus Addlesee, Daniel Hernández García, Chris Reinke, Soraya Arias, Federica Arrigoni, Alex Auternaud, Lauriane Blavette, Cigdem Beyan, Luis Gomez Camara, Ohad Cohen, Alessandro Conti, Sébastien Dacunha, Christian Dondrup, Yoav Ellinson, Francesco Ferro, Sharon Gannot, Florian Gras, Nancie Gunson, Radu Horaud, Moreno D&rsquo;Incà, Imad Kimouche, Séverin Lemaignan, Oliver Lemon, Cyril Liotard, Luca Marchionni, Mordehay Moradi, Tomas Pajdla, Maribel Pino, Michal Polic, Matthieu Py, Ariel Rado, Bin Ren, Elisa Ricci, Anne-Sophie Rigaud, Paolo Rota, Marta Romeo, Nicu Sebe, Weronika Sieińska, Pinchas Tandeitnik, Francesco Tonini, Nicolas Turro, Timothée Wintz, Yanchao Yu. (2024)<br><strong>Socially Pertinent Robots in Gerontological Healthcare</strong><br><button class=copy-to-clipboard title="Socially Pertinent Robots in Gerontological Healthcare" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07560v1.pdf filename=2404.07560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the many recent achievements in developing and deploying social robotics, there are still many underexplored environments and applications for which systematic evaluation of such systems by end-users is necessary. While several robotic platforms have been used in gerontological healthcare, the question of whether or not a social interactive robot with <b>multi-modal</b> conversational capabilities will be useful and accepted in real-life facilities is yet to be answered. This paper is an attempt to partially answer this question, via two waves of experiments with patients and companions in a day-care gerontological facility in Paris with a full-sized humanoid robot endowed with social and conversational interaction capabilities. The software architecture, developed during the H2020 SPRING project, together with the experimental protocol, allowed us to evaluate the acceptability (AES) and usability (SUS) with more than 60 end-users. Overall, the users are receptive to this technology, especially when the robot perception and action skills are robust to environmental clutter and flexible to handle a plethora of different interactions.</p></p class="citation"></blockquote><h3 id=1415--174276-interactive-farinteractive-fast-and-adaptable-routing-for-navigation-among-movable-obstacles-in-complex-unknown-environments-botao-he-et-al-2024>(14/15 | 174/276) Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments (Botao He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botao He, Guofei Chen, Wenshan Wang, Ji Zhang, Cornelia Fermuller, Yiannis Aloimonos. (2024)<br><strong>Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments</strong><br><button class=copy-to-clipboard title="Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07447v1.pdf filename=2404.07447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles. Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback. The main contributions include an improved dynamic Directed Visibility <b>Graph</b> (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments. Our algorithm is capable of replanning the global path in several milliseconds. It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly. Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments. It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity. We also open-source our code in the docker demo to facilitate future research.</p></p class="citation"></blockquote><h3 id=1515--175276-parameterized-fast-and-safe-tracking-fastrack-using-deepreach-hyun-joe-jeong-et-al-2024>(15/15 | 175/276) Parameterized Fast and Safe Tracking (FaSTrack) using Deepreach (Hyun Joe Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyun Joe Jeong, Zheng Gong, Somil Bansal, Sylvia Herbert. (2024)<br><strong>Parameterized Fast and Safe Tracking (FaSTrack) using Deepreach</strong><br><button class=copy-to-clipboard title="Parameterized Fast and Safe Tracking (FaSTrack) using Deepreach" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07431v1.pdf filename=2404.07431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fast and Safe Tracking (FaSTrack) is a modular framework that provides safety guarantees while planning and executing trajectories in real time via value functions of Hamilton-Jacobi (HJ) reachability. These value functions are computed through dynamic programming, which is notorious for being computationally inefficient. Moreover, the resulting trajectory does not adapt online to the environment, such as sudden disturbances or obstacles. DeepReach is a scalable deep learning method to HJ reachability that allows parameterization of states, which opens up possibilities for online adaptation to various controls and disturbances. In this paper, we propose Parametric FaSTrack, which uses DeepReach to approximate a value function that parameterizes the control bounds of the planning model. The new framework can smoothly trade off between the navigation speed and the tracking error (therefore maneuverability) while guaranteeing obstacle avoidance in a priori unknown environments. We demonstrate our method through two examples and a <b>benchmark</b> comparison with existing methods, showing the safety, efficiency, and faster solution times of the framework.</p></p class="citation"></blockquote><h2 id=csai-10>cs.AI (10)</h2><h3 id=110--176276-generative-probabilistic-planning-for-optimizing-supply-chain-networks-hyung-il-ahn-et-al-2024>(1/10 | 176/276) Generative Probabilistic Planning for Optimizing Supply Chain Networks (Hyung-il Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyung-il Ahn, Santiago Olivar, Hershel Mehta, Young Chol Song. (2024)<br><strong>Generative Probabilistic Planning for Optimizing Supply Chain Networks</strong><br><button class=copy-to-clipboard title="Generative Probabilistic Planning for Optimizing Supply Chain Networks" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Generative AI, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07511v1.pdf filename=2404.07511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Supply chain networks in enterprises are typically composed of complex topological <b>graphs</b> <b>involving</b> <b>various</b> types of nodes and edges, accommodating numerous products with considerable demand and supply variability. However, as supply chain networks expand in size and complexity, traditional supply chain planning methods (e.g., those found in heuristic rule-based and operations research-based systems) tend to become locally optimal or lack computational scalability, resulting in substantial imbalances between supply and demand across nodes in the network. This paper introduces a novel <b>Generative</b> <b>AI</b> technique, which we call <b>Generative</b> <b>Probabilistic</b> Planning (GPP). GPP generates dynamic supply action plans that are globally optimized across all network nodes over the time horizon for changing objectives like maximizing profits or service levels, factoring in time-varying probabilistic demand, lead time, and production conditions. GPP leverages attention-based <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNN),</b> offline deep <b>reinforcement</b> <b>learning</b> (Offline RL), and policy <b>simulations</b> to train <b>generative</b> <b>policy</b> models and create optimal plans through probabilistic <b>simulations,</b> effectively accounting for various uncertainties. Our experiments using historical data from a global consumer goods company with complex supply chain networks demonstrate that GPP accomplishes objective-adaptable, probabilistically resilient, and dynamic planning for supply chain networks, leading to significant improvements in performance and profitability for enterprises. Our work plays a pivotal role in shaping the trajectory of AI adoption within the supply chain domain.</p></p class="citation"></blockquote><h3 id=210--177276-wese-weak-exploration-to-strong-exploitation-for-llm-agents-xu-huang-et-al-2024>(2/10 | 177/276) WESE: Weak Exploration to Strong Exploitation for LLM Agents (Xu Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen. (2024)<br><strong>WESE: Weak Exploration to Strong Exploitation for LLM Agents</strong><br><button class=copy-to-clipboard title="WESE: Weak Exploration to Strong Exploitation for LLM Agents" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 61<br>Keywords: Graph, Benchmarking, Fine-tuning, Knowledge Graph, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07456v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07456v1.pdf filename=2404.07456v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent&rsquo;s <b>reasoning</b> or decision-making abilities through well-designed <b>prompt</b> engineering or task-specific <b>fine-tuning,</b> ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance <b>LLM</b> agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global <b>knowledge.</b> <b>A</b> <b>knowledge</b> <b>graph-based</b> strategy is then introduced to store the acquired <b>knowledge</b> <b>and</b> extract task-relevant <b>knowledge,</b> <b>enhancing</b> the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=310--178276-augmenting-knowledge-graph-hierarchies-using-neural-transformers-sanat-sharma-et-al-2024>(3/10 | 178/276) Augmenting Knowledge Graph Hierarchies Using Neural Transformers (Sanat Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanat Sharma, Mayank Poddar, Jayant Kumar, Kosta Blank, Tracy King. (2024)<br><strong>Augmenting Knowledge Graph Hierarchies Using Neural Transformers</strong><br><button class=copy-to-clipboard title="Augmenting Knowledge Graph Hierarchies Using Neural Transformers" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-DL, cs-IR, cs-LG, cs.AI<br>Keyword Score: 53<br>Keywords: Graph, Few-shot, Knowledge Graph, Knowledge Graph, Transformer, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08020v1.pdf filename=2404.08020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graphs</b> are useful tools to organize, recommend and sort data. Hierarchies in <b>knowledge</b> <b>graphs</b> provide significant benefit in improving understanding and compartmentalization of the data within a <b>knowledge</b> <b>graph.</b> This work leverages <b>large</b> <b>language</b> <b>models</b> to generate and augment hierarchies in an existing <b>knowledge</b> <b>graph.</b> For small (&lt;100,000 node) domain-specific <b>KGs,</b> we find that a combination of <b>few-shot</b> <b>prompting</b> with one-shot generation works well, while larger <b>KG</b> may require cyclical generation. We present techniques for augmenting hierarchies, which led to coverage increase by 98% for intents and 99% for colors in our <b>knowledge</b> <b>graph.</b></p></p class="citation"></blockquote><h3 id=410--179276-designqa-a-multimodal-benchmark-for-evaluating-large-language-models-understanding-of-engineering-documentation-anna-c-doris-et-al-2024>(4/10 | 179/276) DesignQA: A Multimodal Benchmark for Evaluating Large Language Models&rsquo; Understanding of Engineering Documentation (Anna C. Doris et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna C. Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Hyunmin Cheong, Faez Ahmed. (2024)<br><strong>DesignQA: A Multimodal Benchmark for Evaluating Large Language Models&rsquo; Understanding of Engineering Documentation</strong><br><button class=copy-to-clipboard title="DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 39<br>Keywords: Automatic Evaluation, Benchmarking, Multi-modal, Multi-modal, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07917v1.pdf filename=2404.07917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research introduces DesignQA, a novel <b>benchmark</b> aimed at evaluating the proficiency of <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) in comprehending and applying engineering requirements in technical documentation. Developed with a focus on real-world engineering challenges, DesignQA uniquely combines <b>multimodal</b> data-including textual design requirements, CAD images, and engineering drawings-derived from the Formula SAE student competition. Different from many existing MLLM <b>benchmarks,</b> DesignQA contains document-grounded visual questions where the input image and input document come from different sources. The <b>benchmark</b> features <b>automatic</b> <b>evaluation</b> metrics and is divided into segments-Rule Comprehension, Rule Compliance, and Rule Extraction-based on tasks that engineers perform when designing according to requirements. We evaluate state-of-the-art models like <b>GPT4</b> and LLaVA against the <b>benchmark,</b> and our study uncovers the existing gaps in MLLMs&rsquo; abilities to interpret complex engineering documentation. Key findings suggest that while MLLMs demonstrate potential in navigating technical documents, substantial limitations exist, particularly in accurately extracting and applying detailed requirements to engineering designs. This <b>benchmark</b> sets a foundation for future advancements in AI-supported engineering design processes. DesignQA is publicly available at: <a href=https://github.com/anniedoris/design_qa/>https://github.com/anniedoris/design_qa/</a>.</p></p class="citation"></blockquote><h3 id=510--180276-gnn-based-probabilistic-supply-and-inventory-predictions-in-supply-chain-networks-hyung-il-ahn-et-al-2024>(5/10 | 180/276) GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks (Hyung-il Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyung-il Ahn, Young Chol Song, Santiago Olivar, Hershel Mehta, Naveen Tewari. (2024)<br><strong>GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks</strong><br><button class=copy-to-clipboard title="GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07523v1.pdf filename=2404.07523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Successful supply chain optimization must mitigate imbalances between supply and demand over time. While accurate demand prediction is essential for supply planning, it alone does not suffice. The key to successful supply planning for optimal and viable execution lies in maximizing predictability for both demand and supply throughout an execution horizon. Therefore, enhancing the accuracy of supply predictions is imperative to create an attainable supply plan that matches demand without overstocking or understocking. However, in complex supply chain networks with numerous nodes and edges, accurate supply predictions are challenging due to dynamic node interactions, cascading supply delays, resource availability, production and logistic capabilities. Consequently, supply executions often deviate from their initial plans. To address this, we present the <b>Graph-based</b> <b>Supply</b> <b>Prediction</b> (GSP) <b>probabilistic</b> <b>model.</b> Our attention-based <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> model predicts supplies, inventory, and imbalances using <b>graph-structured</b> <b>historical</b> <b>data,</b> demand forecasting, and original supply plan inputs. The experiments, conducted using historical data from a global consumer goods company&rsquo;s large-scale supply chain, demonstrate that GSP significantly improves supply and inventory prediction accuracy, potentially offering supply plan corrections to optimize executions.</p></p class="citation"></blockquote><h3 id=610--181276-osworld-benchmarking-multimodal-agents-for-open-ended-tasks-in-real-computer-environments-tianbao-xie-et-al-2024>(6/10 | 181/276) OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments (Tianbao Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu. (2024)<br><strong>OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</strong><br><button class=copy-to-clipboard title="OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 32<br>Keywords: Benchmarking, Benchmarking, Human Intervention, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07972v1.pdf filename=2404.07972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous agents that accomplish complex computer tasks with minimal <b>human</b> <b>interventions</b> have the potential to transform <b>human-computer</b> <b>interaction,</b> significantly enhancing accessibility and productivity. However, existing <b>benchmarks</b> either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for <b>multimodal</b> agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a <b>benchmark</b> of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While <b>humans</b> <b>can</b> accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI <b>grounding</b> and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing <b>multimodal</b> generalist agents that were not possible with previous <b>benchmarks.</b> Our code, environment, baseline models, and data are publicly available at <a href=https://os-world.github.io>https://os-world.github.io</a>.</p></p class="citation"></blockquote><h3 id=710--182276-generating-games-via-llms-an-investigation-with-video-game-description-language-chengpeng-hu-et-al-2024>(7/10 | 182/276) Generating Games via LLMs: An Investigation with Video Game Description Language (Chengpeng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengpeng Hu, Yunlong Zhao, Jialin Liu. (2024)<br><strong>Generating Games via LLMs: An Investigation with Video Game Description Language</strong><br><button class=copy-to-clipboard title="Generating Games via LLMs: An Investigation with Video Game Description Language" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08706v1.pdf filename=2404.08706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has unlocked new opportunities for procedural content generation. However, recent attempts mainly focus on level generation for specific games with defined game rules such as Super Mario Bros. and Zelda. This paper investigates the game generation via <b>LLMs.</b> Based on video game description language, this paper proposes an <b>LLM-based</b> framework to generate game rules and levels simultaneously. Experiments demonstrate how the framework works with <b>prompts</b> considering different combinations of context. Our findings extend the current applications of <b>LLMs</b> and offer new insights for generating new games in the area of procedural content generation.</p></p class="citation"></blockquote><h3 id=810--183276-behavior-trees-enable-structured-programming-of-language-model-agents-richard-kelley-2024>(8/10 | 183/276) Behavior Trees Enable Structured Programming of Language Model Agents (Richard Kelley, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Kelley. (2024)<br><strong>Behavior Trees Enable Structured Programming of Language Model Agents</strong><br><button class=copy-to-clipboard title="Behavior Trees Enable Structured Programming of Language Model Agents" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Reinforcement Learning from Human Feedback, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07439v1.pdf filename=2404.07439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise &ldquo;language-model agents.&rdquo; In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through <b>instruction</b> <b>tuning</b> or <b>RLHF.</b></p></p class="citation"></blockquote><h3 id=910--184276-goal-recognition-via-linear-programming-felipe-meneguzzi-et-al-2024>(9/10 | 184/276) Goal Recognition via Linear Programming (Felipe Meneguzzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe Meneguzzi, Luísa R. de A. Santos, Ramon Fraga Pereira, André G. Pereira. (2024)<br><strong>Goal Recognition via Linear Programming</strong><br><button class=copy-to-clipboard title="Goal Recognition via Linear Programming" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07934v1.pdf filename=2404.07934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Goal Recognition is the task by which an observer aims to discern the goals that correspond to plans that comply with the perceived behavior of subject agents given as a sequence of observations. Research on Goal Recognition as Planning encompasses <b>reasoning</b> about the model of a planning task, the observations, and the goals using planning techniques, resulting in very efficient recognition approaches. In this article, we design novel recognition approaches that rely on the Operator-Counting framework, proposing new constraints, and analyze their constraints&rsquo; properties both theoretically and empirically. The Operator-Counting framework is a technique that efficiently computes heuristic estimates of cost-to-goal using Integer/Linear Programming (IP/LP). In the realm of theory, we prove that the new constraints provide lower bounds on the cost of plans that comply with observations. We also provide an extensive empirical evaluation to assess how the new constraints improve the quality of the solution, and we found that they are especially informed in deciding which goals are unlikely to be part of the solution. Our novel recognition approaches have two pivotal advantages: first, they employ new IP/LP constraints for efficiently recognizing goals; second, we show how the new IP/LP constraints can improve the recognition of goals under both partial and noisy observability.</p></p class="citation"></blockquote><h3 id=1010--185276-monte-carlo-tree-search-with-boltzmann-exploration-michael-painter-et-al-2024>(10/10 | 185/276) Monte Carlo Tree Search with Boltzmann Exploration (Michael Painter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Painter, Mohamed Baioumy, Nick Hawes, Bruno Lacerda. (2024)<br><strong>Monte Carlo Tree Search with Boltzmann Exploration</strong><br><button class=copy-to-clipboard title="Monte Carlo Tree Search with Boltzmann Exploration" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07732v1.pdf filename=2404.07732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monte-Carlo Tree Search (MCTS) methods, such as Upper Confidence Bound applied to Trees (UCT), are instrumental to automated planning techniques. However, UCT can be slow to explore an optimal action when it initially appears inferior to other actions. Maximum ENtropy Tree-Search (MENTS) incorporates the maximum entropy principle into an MCTS approach, utilising Boltzmann policies to sample actions, naturally encouraging more exploration. In this paper, we highlight a major limitation of MENTS: optimal actions for the maximum entropy objective do not necessarily correspond to optimal actions for the original objective. We introduce two algorithms, Boltzmann Tree Search (BTS) and Decaying ENtropy Tree-Search (DENTS), that address these limitations and preserve the benefits of Boltzmann policies, such as allowing actions to be sampled faster by using the Alias method. Our empirical analysis shows that our algorithms show consistent high performance across several <b>benchmark</b> domains, including the game of Go.</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=17--186276-generative-information-retrieval-evaluation-marwah-alaofi-et-al-2024>(1/7 | 186/276) Generative Information Retrieval Evaluation (Marwah Alaofi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, Mark Sanderson. (2024)<br><strong>Generative Information Retrieval Evaluation</strong><br><button class=copy-to-clipboard title="Generative Information Retrieval Evaluation" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08137v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08137v2.pdf filename=2404.08137v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is a draft of a chapter intended to appear in a forthcoming book on generative <b>information</b> <b>retrieval,</b> <b>co-edited</b> <b>by</b> Chirag Shah and Ryen White. In this chapter, we consider generative <b>information</b> <b>retrieval</b> <b>evaluation</b> <b>from</b> two distinct but interrelated perspectives. First, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> themselves are rapidly becoming tools for evaluation, with current research indicating that <b>LLMs</b> may be superior to crowdsource workers and other paid assessors on basic relevance judgement tasks. We review past and ongoing related research, including speculation on the future of shared task initiatives, such as TREC, and a discussion on the continuing need for human assessments. Second, we consider the evaluation of emerging <b>LLM-based</b> generative <b>information</b> <b>retrieval</b> <b>(GenIR)</b> <b>systems,</b> including <b>retrieval</b> <b>augmented</b> <b>generation</b> <b>(RAG)</b> systems. We consider approaches that focus both on the end-to-end evaluation of GenIR systems and on the evaluation of a <b>retrieval</b> <b>component</b> <b>as</b> an element in a <b>RAG</b> system. Going forward, we expect the evaluation of GenIR systems to be at least partially based on <b>LLM-based</b> assessment, creating an apparent circularity, with a system seemingly evaluating its own output. We resolve this apparent circularity in two ways: 1) by viewing <b>LLM-based</b> assessment as a form of &ldquo;slow search&rdquo;, where a slower IR system is used for evaluation and training of a faster production IR system; and 2) by recognizing a continuing need to ground evaluation in human assessment, even if the characteristics of that human assessment must change.</p></p class="citation"></blockquote><h3 id=27--187276-can-large-language-models-assess-serendipity-in-recommender-systems-yu-tokutake-et-al-2024>(2/7 | 187/276) Can Large Language Models Assess Serendipity in Recommender Systems? (Yu Tokutake et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Tokutake, Kazushi Okamoto. (2024)<br><strong>Can Large Language Models Assess Serendipity in Recommender Systems?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Assess Serendipity in Recommender Systems?" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 43<br>Keywords: Benchmarking, Recommender System, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07499v1.pdf filename=2404.07499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Serendipity-oriented <b>recommender</b> <b>systems</b> aim to counteract over-specialization in user preferences. However, evaluating a user&rsquo;s serendipitous response towards a recommended item can be challenging because of its emotional nature. In this study, we address this issue by leveraging the rich knowledge of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which can perform a variety of tasks. First, this study explored the alignment between serendipitous evaluations made by <b>LLMs</b> and those made by humans. In this investigation, a binary classification task was given to the <b>LLMs</b> to predict whether a user would find the recommended item serendipitously. The predictive performances of three <b>LLMs</b> on a <b>benchmark</b> dataset in which humans assigned the ground truth of serendipitous items were measured. The experimental findings reveal that <b>LLM-based</b> assessment methods did not have a very high agreement rate with human assessments. However, they performed as well as or better than the baseline methods. Further validation results indicate that the number of user rating histories provided to <b>LLM</b> <b>prompts</b> should be carefully chosen to avoid both insufficient and excessive inputs and that the output of <b>LLMs</b> that show high classification performance is difficult to interpret.</p></p class="citation"></blockquote><h3 id=37--188276-manipulating-large-language-models-to-increase-product-visibility-aounon-kumar-et-al-2024>(3/7 | 188/276) Manipulating Large Language Models to Increase Product Visibility (Aounon Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aounon Kumar, Himabindu Lakkaraju. (2024)<br><strong>Manipulating Large Language Models to Increase Product Visibility</strong><br><button class=copy-to-clipboard title="Manipulating Large Language Models to Increase Product Visibility" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07981v1.pdf filename=2404.07981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly being integrated into search engines to provide natural language responses tailored to user queries. Customers and end-users are also becoming more dependent on these models for quick and easy purchase decisions. In this work, we investigate whether <b>recommendations</b> from <b>LLMs</b> can be manipulated to enhance a product&rsquo;s visibility. We demonstrate that adding a strategic text sequence (STS) &ndash; a carefully crafted message &ndash; to a product&rsquo;s information page can significantly increase its likelihood of being listed as the <b>LLM&rsquo;s</b> top <b>recommendation.</b> To understand the impact of STS, we use a catalog of fictitious coffee machines and analyze its effect on two target products: one that seldom appears in the <b>LLM&rsquo;s</b> <b>recommendations</b> and another that usually ranks second. We observe that the strategic text sequence significantly enhances the visibility of both products by increasing their chances of appearing as the top <b>recommendation.</b> This ability to manipulate <b>LLM-generated</b> search responses provides vendors with a considerable competitive advantage and has the potential to disrupt fair market competition. Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing <b>LLM</b> <b>recommendations</b> could profoundly impact content optimization for AI-driven search services. Code for our experiments is available at <a href=https://github.com/aounon/llm-rank-optimizer>https://github.com/aounon/llm-rank-optimizer</a>.</p></p class="citation"></blockquote><h3 id=47--189276-adaptive-fair-representation-learning-for-personalized-fairness-in-recommendations-via-information-alignment-xinyu-zhu-et-al-2024>(4/7 | 189/276) Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment (Xinyu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Zhu, Lilin Zhang, Ning Yang. (2024)<br><strong>Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment</strong><br><button class=copy-to-clipboard title="Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 25<br>Keywords: Fairness, Recommendation, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07494v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07494v2.pdf filename=2404.07494v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personalized <b>fairness</b> in <b>recommendations</b> has been attracting increasing attention from researchers. The existing works often treat a <b>fairness</b> requirement, represented as a collection of sensitive attributes, as a hyper-parameter, and pursue extreme <b>fairness</b> by completely removing information of sensitive attributes from the learned fair embedding, which suffer from two challenges: huge training cost incurred by the explosion of attribute combinations, and the suboptimal trade-off between <b>fairness</b> and accuracy. In this paper, we propose a novel Adaptive Fair <b>Representation</b> <b>Learning</b> (AFRL) model, which achieves a real personalized <b>fairness</b> due to its advantage of training only one model to adaptively serve different <b>fairness</b> requirements during inference phase. Particularly, AFRL treats <b>fairness</b> requirements as inputs and can learn an attribute-specific embedding for each attribute from the unfair user embedding, which endows AFRL with the adaptability during inference phase to determine the non-sensitive attributes under the guidance of the user&rsquo;s unique <b>fairness</b> requirement. To achieve a better trade-off between <b>fairness</b> and accuracy in <b>recommendations,</b> AFRL conducts a novel Information Alignment to exactly preserve discriminative information of non-sensitive attributes and incorporate a debiased collaborative embedding into the fair embedding to capture attribute-independent collaborative signals, without loss of <b>fairness.</b> Finally, the extensive experiments conducted on real datasets together with the sound theoretical analysis demonstrate the superiority of AFRL.</p></p class="citation"></blockquote><h3 id=57--190276-m-scan-a-multi-scenario-causal-driven-adaptive-network-for-recommendation-jiachen-zhu-et-al-2024>(5/7 | 190/276) M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation (Jiachen Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu. (2024)<br><strong>M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation</strong><br><button class=copy-to-clipboard title="M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Counter-factual, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07581v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07581v2.pdf filename=2404.07581v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We primarily focus on the field of multi-scenario <b>recommendation,</b> which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal <b>graph</b> analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal <b>counterfactual</b> inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.</p></p class="citation"></blockquote><h3 id=67--191276-extending-translate-train-for-colbert-x-to-african-language-clir-eugene-yang-et-al-2024>(6/7 | 191/276) Extending Translate-Train for ColBERT-X to African Language CLIR (Eugene Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eugene Yang, Dawn J. Lawrie, Paul McNamee, James Mayfield. (2024)<br><strong>Extending Translate-Train for ColBERT-X to African Language CLIR</strong><br><button class=copy-to-clipboard title="Extending Translate-Train for ColBERT-X to African Language CLIR" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08134v1.pdf filename=2404.08134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the submission runs from the HLTCOE team at the CIRAL CLIR tasks for African languages at FIRE 2023. Our submissions use <b>machine</b> <b>translation</b> models to translate the documents and the training passages, and ColBERT-X as the retrieval model. Additionally, we present a set of unofficial runs that use an alternative training procedure with a similar training setting.</p></p class="citation"></blockquote><h3 id=77--192276-overview-of-the-trec-2023-neuclir-track-dawn-lawrie-et-al-2024>(7/7 | 192/276) Overview of the TREC 2023 NeuCLIR Track (Dawn Lawrie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas W. Oard, Luca Soldaini, Eugene Yang. (2024)<br><strong>Overview of the TREC 2023 NeuCLIR Track</strong><br><button class=copy-to-clipboard title="Overview of the TREC 2023 NeuCLIR Track" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08071v1.pdf filename=2404.08071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The principal goal of the TREC Neural Cross-Language <b>Information</b> <b>Retrieval</b> (NeuCLIR) track is to study the impact of neural approaches to cross-language <b>information</b> <b>retrieval.</b> The track has created four collections, large collections of Chinese, Persian, and Russian newswire and a smaller collection of Chinese scientific abstracts. The principal tasks are ranked retrieval of news in one of the three languages, using English topics. Results for a multilingual task, also with English topics but with documents from all three newswire collections, are also reported. New in this second year of the track is a pilot technical documents CLIR task for ranked retrieval of Chinese technical documents using English topics. A total of 220 runs across all tasks were submitted by six participating teams and, as baselines, by track coordinators. Task descriptions and results are presented.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--193276-measuring-geographic-diversity-of-foundation-models-with-a-natural-language--based-geo-guessing-experiment-on-gpt-4-zilong-liu-et-al-2024>(1/3 | 193/276) Measuring Geographic Diversity of Foundation Models with a Natural Language&ndash;based Geo-guessing Experiment on GPT-4 (Zilong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilong Liu, Krzysztof Janowicz, Kitty Currier, Meilin Shi. (2024)<br><strong>Measuring Geographic Diversity of Foundation Models with a Natural Language&ndash;based Geo-guessing Experiment on GPT-4</strong><br><button class=copy-to-clipboard title="Measuring Geographic Diversity of Foundation Models with a Natural Language--based Geo-guessing Experiment on GPT-4" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 56<br>Keywords: Foundation Model, Generative AI, Multi-modal, Multi-modal, GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07612v1.pdf filename=2404.07612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> based on <b>foundation</b> <b>models</b> provides a first glimpse into the world represented by machines trained on vast amounts of <b>multimodal</b> data ingested by these models during training. If we consider the resulting models as knowledge bases in their own right, this may open up new avenues for understanding places through the lens of machines. In this work, we adopt this thinking and select <b>GPT-4,</b> a state-of-the-art representative in the family of <b>multimodal</b> <b>large</b> <b>language</b> <b>models,</b> to study its geographic diversity regarding how well geographic features are represented. Using DBpedia abstracts as a ground-truth corpus for probing, our natural language&ndash;based geo-guessing experiment shows that <b>GPT-4</b> may currently encode insufficient knowledge about several geographic feature types on a global level. On a local level, we observe not only this insufficiency but also inter-regional disparities in <b>GPT-4&rsquo;s</b> geo-guessing performance on UNESCO World Heritage Sites that carry significance to both local and global populations, and the inter-regional disparities may become smaller as the geographic scale increases. Morever, whether assessing the geo-guessing performance on a global or local level, we find inter-model disparities in <b>GPT-4&rsquo;s</b> geo-guessing performance when comparing its unimodal and <b>multimodal</b> variants. We hope this work can initiate a discussion on geographic diversity as an ethical principle within the GIScience community in the face of global socio-technical challenges.</p></p class="citation"></blockquote><h3 id=23--194276-toxic-synergy-between-hate-speech-and-fake-news-exposure-munjung-kim-et-al-2024>(2/3 | 194/276) Toxic Synergy Between Hate Speech and Fake News Exposure (Munjung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Munjung Kim, Tuğrulcan Elmas, Filippo Menczer. (2024)<br><strong>Toxic Synergy Between Hate Speech and Fake News Exposure</strong><br><button class=copy-to-clipboard title="Toxic Synergy Between Hate Speech and Fake News Exposure" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-SI, cs.CY<br>Keyword Score: 10<br>Keywords: Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08110v1.pdf filename=2404.08110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hate speech on social media is a pressing concern. Understanding the factors associated with hate speech may help mitigate it. Here we explore the association between hate speech and exposure to <b>fake</b> <b>news</b> by studying the correlation between exposure to news from low-credibility sources through following connections and the use of hate speech on Twitter. Using news source credibility labels and a dataset of posts with hate speech targeting various populations, we find that hate speakers are exposed to lower percentages of posts linking to credible news sources. When taking the target population into account, we find that this association is mainly driven by anti-semitic and anti-Muslim content. We also observe that hate speakers are more likely to be exposed to low-credibility news with low popularity. Finally, while hate speech is associated with low-credibility news from partisan sources, we find that those sources tend to skew to the political left for antisemitic content and to the political right for hate speech targeting Muslim and Latino populations. Our results suggest that mitigating <b>fake</b> <b>news</b> and hate speech may have synergistic effects.</p></p class="citation"></blockquote><h3 id=33--195276-the-survey-on-multi-source-data-fusion-in-cyber-physical-social-systemsfoundational-infrastructure-for-industrial-metaverses-and-industries-50-xiao-wang-et-al-2024>(3/3 | 195/276) The Survey on Multi-Source Data Fusion in Cyber-Physical-Social Systems:Foundational Infrastructure for Industrial Metaverses and Industries 5.0 (Xiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Wang, Yutong Wang, Jing Yang, Xiaofeng Jia, Lijun Li, Weiping Ding, Fei-Yue Wang. (2024)<br><strong>The Survey on Multi-Source Data Fusion in Cyber-Physical-Social Systems:Foundational Infrastructure for Industrial Metaverses and Industries 5.0</strong><br><button class=copy-to-clipboard title="The Survey on Multi-Source Data Fusion in Cyber-Physical-Social Systems:Foundational Infrastructure for Industrial Metaverses and Industries 5.0" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07476v1.pdf filename=2404.07476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the concept of Industries 5.0 develops, industrial metaverses are expected to operate in parallel with the actual industrial processes to offer <code>Human-Centric" Safe, Secure, Sustainable, Sensitive, Service, and Smartness </code>6S" manufacturing solutions. Industrial metaverses not only visualize the process of productivity in a dynamic and evolutional way, but also provide an immersive laboratory experimental environment for optimizing and remodeling the process. Besides, the customized user needs that are hidden in social media data can be discovered by social computing technologies, which introduces an input channel for building the whole social manufacturing process including industrial metaverses. This makes the fusion of multi-source data cross Cyber-Physical-Social Systems (CPSS) the foundational and key challenge. This work firstly proposes a multi-source-data-fusion-driven operational architecture for industrial metaverses on the basis of conducting a comprehensive literature review on the state-of-the-art multi-source data fusion methods. The advantages and disadvantages of each type of method are analyzed by considering the fusion mechanisms and application scenarios. Especially, we combine the strengths of deep learning and <b>knowledge</b> <b>graphs</b> in scalability and parallel computation to enable our proposed framework the ability of prescriptive optimization and evolution. This integration can address the shortcomings of deep learning in terms of explainability and fact fabrication, as well as overcoming the incompleteness and the challenges of construction and maintenance inherent in <b>knowledge</b> <b>graphs.</b> The effectiveness of the proposed architecture is validated through a parallel weaving case study. In the end, we discuss the challenges and future directions of multi-source data fusion cross CPSS for industrial metaverses and social manufacturing in Industries 5.0.</p></p class="citation"></blockquote><h2 id=q-finrm-1>q-fin.RM (1)</h2><h3 id=11--196276-risklabs-predicting-financial-risk-using-large-language-model-based-on-multi-sources-data-yupeng-cao-et-al-2024>(1/1 | 196/276) RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data (Yupeng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yupeng Cao, Zhi Chen, Qingyun Pei, Fabrizio Dimino, Lorenzo Ausiello, Prashant Kumar, K. P. Subbalakshmi, Papa Momar Ndiaye. (2024)<br><strong>RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data</strong><br><button class=copy-to-clipboard title="RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.RM<br>Categories: cs-AI, cs-CE, cs-LG, q-fin-PM, q-fin-RM, q-fin.RM<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Question Answering, Text Summarization, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07452v1.pdf filename=2404.07452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of Artificial Intelligence (AI) techniques, particularly <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial <b>text</b> <b>summarization,</b> <b>question-answering</b> <b>(Q$&$A),</b> and stock movement prediction (binary classification), with a notable gap in the application of <b>LLMs</b> for financial risk prediction. Addressing this gap, in this paper, we introduce \textbf{RiskLabs}, a novel framework that leverages <b>LLMs</b> to analyze and predict financial risks. RiskLabs uniquely combines different types of financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data surrounding ECC release dates. Our approach involves a multi-stage process: initially extracting and analyzing ECC data using <b>LLMs,</b> followed by gathering and processing time-series data before the ECC dates to model and understand risk over different timeframes. Using <b>multimodal</b> fusion techniques, RiskLabs amalgamates these varied data features for comprehensive multi-task financial risk prediction. Empirical experiment results demonstrate RiskLab&rsquo;s effectiveness in forecasting both volatility and variance in financial markets. Through comparative experiments, we demonstrate how different data sources contribute to financial risk assessment and discuss the critical role of <b>LLMs</b> in this context. Our findings not only contribute to the AI in finance application but also open new avenues for applying <b>LLMs</b> in financial risk assessment.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--197276-an-effective-automated-speaking-assessment-approach-to-mitigating-data-scarcity-and-imbalanced-distribution-tien-hong-lo-et-al-2024>(1/1 | 197/276) An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution (Tien-Hong Lo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tien-Hong Lo, Fu-An Chao, Tzu-I Wu, Yao-Ting Sung, Berlin Chen. (2024)<br><strong>An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution</strong><br><button class=copy-to-clipboard title="An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 53<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Learning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07575v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07575v2.pdf filename=2404.07575v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated speaking assessment (ASA) typically involves <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> and hand-crafted feature extraction from the <b>ASR</b> transcript of a learner&rsquo;s <b>speech.</b> <b>Recently,</b> <b>self-supervised</b> <b>learning</b> (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss reweighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE <b>benchmark</b> dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.</p></p class="citation"></blockquote><h2 id=cscr-12>cs.CR (12)</h2><h3 id=112--198276-llm-agents-can-autonomously-exploit-one-day-vulnerabilities-richard-fang-et-al-2024>(1/12 | 198/276) LLM Agents can Autonomously Exploit One-day Vulnerabilities (Richard Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang. (2024)<br><strong>LLM Agents can Autonomously Exploit One-day Vulnerabilities</strong><br><button class=copy-to-clipboard title="LLM Agents can Autonomously Exploit One-day Vulnerabilities" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08144v1.pdf filename=2404.08144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of <b>LLM</b> agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities. In this work, we show that <b>LLM</b> agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, <b>GPT-4</b> is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test <b>(GPT-3.5,</b> open-source <b>LLMs)</b> and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our <b>GPT-4</b> agent requires the CVE description for high performance: without the description, <b>GPT-4</b> can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable <b>LLM</b> agents.</p></p class="citation"></blockquote><h3 id=212--199276-backdoor-contrastive-learning-via-bi-level-trigger-optimization-weiyu-sun-et-al-2024>(2/12 | 199/276) Backdoor Contrastive Learning via Bi-level Trigger Optimization (Weiyu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiyu Sun, Xinyu Zhang, Hao Lu, Yingcong Chen, Ting Wang, Jinghui Chen, Lu Lin. (2024)<br><strong>Backdoor Contrastive Learning via Bi-level Trigger Optimization</strong><br><button class=copy-to-clipboard title="Backdoor Contrastive Learning via Bi-level Trigger Optimization" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Representation Learning, Unsupervised Learning, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07863v1.pdf filename=2404.07863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>Learning</b> (CL) has attracted enormous attention due to its remarkable capability in <b>unsupervised</b> <b>representation</b> <b>learning.</b> However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate <b>(ASR).</b> This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., $99%$ <b>ASR</b> on ImageNet-100) with a very low poisoning rate ($1%$). Besides, our attack can effectively evade existing state-of-the-art defenses. Code is available at: <a href=https://github.com/SWY666/SSL-backdoor-BLTO>https://github.com/SWY666/SSL-backdoor-BLTO</a>.</p></p class="citation"></blockquote><h3 id=312--200276-enhancing-network-intrusion-detection-performance-using-generative-adversarial-networks-xinxing-zhao-et-al-2024>(3/12 | 200/276) Enhancing Network Intrusion Detection Performance using Generative Adversarial Networks (Xinxing Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinxing Zhao, Kar Wai Fok, Vrizlynn L. L. Thing. (2024)<br><strong>Enhancing Network Intrusion Detection Performance using Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="Enhancing Network Intrusion Detection Performance using Generative Adversarial Networks" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 23<br>Keywords: Benchmarking, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07464v1.pdf filename=2404.07464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network intrusion detection systems (NIDS) play a pivotal role in safeguarding critical digital infrastructures against cyber threats. Machine learning-based detection models applied in NIDS are prevalent today. However, the effectiveness of these machine learning-based models is often limited by the evolving and sophisticated nature of intrusion techniques as well as the lack of diverse and updated training samples. In this research, a novel approach for enhancing the performance of an NIDS through the integration of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> is proposed. By harnessing the power of <b>GANs</b> in generating synthetic network traffic data that closely mimics real-world network behavior, we address a key challenge associated with NIDS training datasets, which is the data scarcity. Three distinct <b>GAN</b> models (Vanilla <b>GAN,</b> Wasserstein <b>GAN</b> and Conditional Tabular <b>GAN)</b> are implemented in this work to generate authentic network traffic patterns specifically tailored to represent the anomalous activity. We demonstrate how this synthetic data resampling technique can significantly improve the performance of the NIDS model for detecting such activity. By conducting comprehensive experiments using the CIC-IDS2017 <b>benchmark</b> dataset, augmented with <b>GAN-generated</b> data, we offer empirical evidence that shows the effectiveness of our proposed approach. Our findings show that the integration of <b>GANs</b> into NIDS can lead to enhancements in intrusion detection performance for attacks with limited training data, making it a promising avenue for bolstering the cybersecurity posture of organizations in an increasingly interconnected and vulnerable digital landscape.</p></p class="citation"></blockquote><h3 id=412--201276-privacy-preserving-layer-partitioning-for-deep-neural-network-models-kishore-rajasekar-et-al-2024>(4/12 | 201/276) Privacy preserving layer partitioning for Deep Neural Network models (Kishore Rajasekar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kishore Rajasekar, Randolph Loh, Kar Wai Fok, Vrizlynn L. L. Thing. (2024)<br><strong>Privacy preserving layer partitioning for Deep Neural Network models</strong><br><button class=copy-to-clipboard title="Privacy preserving layer partitioning for Deep Neural Network models" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 15<br>Keywords: Deep Neural Network, Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07437v1.pdf filename=2404.07437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>MLaaS (Machine Learning as a Service) has become popular in the cloud computing domain, allowing users to leverage cloud resources for running private inference of ML models on their data. However, ensuring user input privacy and secure inference execution is essential. One of the approaches to protect data privacy and integrity is to use Trusted Execution Environments (TEEs) by enabling execution of programs in secure hardware enclave. Using TEEs can introduce significant performance overhead due to the additional layers of encryption, decryption, <b>security</b> and integrity checks. This can lead to slower inference times compared to running on unprotected hardware. In our work, we enhance the runtime performance of ML models by introducing layer partitioning technique and offloading computations to GPU. The technique comprises two distinct partitions: one executed within the TEE, and the other carried out using a GPU accelerator. Layer partitioning exposes intermediate feature maps in the clear which can lead to reconstruction attacks to recover the input. We conduct experiments to demonstrate the effectiveness of our approach in protecting against input reconstruction attacks developed using trained conditional Generative Adversarial Network(c-GAN). The evaluation is performed on widely used models such as VGG-16, ResNet-50, and EfficientNetB0, using two datasets: ImageNet for Image classification and TON IoT dataset for cybersecurity attack detection.</p></p class="citation"></blockquote><h3 id=512--202276-a-survey-on-security-of-ultrahyper-reliable-low-latency-communication-recent-advancements-challenges-and-future-directions-annapurna-pradhan-et-al-2024>(5/12 | 202/276) A Survey on Security of Ultra/Hyper Reliable Low Latency Communication: Recent Advancements, Challenges, and Future Directions (Annapurna Pradhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Annapurna Pradhan, Susmita Das, Md. Jalil Piran, Zhu Han. (2024)<br><strong>A Survey on Security of Ultra/Hyper Reliable Low Latency Communication: Recent Advancements, Challenges, and Future Directions</strong><br><button class=copy-to-clipboard title="A Survey on Security of Ultra/Hyper Reliable Low Latency Communication: Recent Advancements, Challenges, and Future Directions" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08160v1.pdf filename=2404.08160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ultra-reliable low latency communication (URLLC) is an innovative service offered by fifth-generation (5G) wireless systems. URLLC enables various mission-critical applications by facilitating reliable and low-latency signal transmission to support extreme Quality of Service (QoS) requirements. Apart from reliability and latency, ensuring secure data transmission for URLLC has been a prominent issue for researchers in recent years. Using finite blocklength signals to achieve the stringent reliability and latency criteria in URLLC eliminates the possibility of using conventional complex cryptographic <b>security</b> enhancement techniques based on encoding and decoding of secret keys. Thus, the development of lightweight <b>security</b> mechanisms is of paramount importance for URLLC. Recently, Physical-Layer <b>Security</b> (PLS) techniques have emerged as a powerful alternative to the complex cryptography-based <b>security</b> approaches for facilitating secure URLLC by exploiting the randomness of the wireless channel. Therefore, in this survey, we present a comprehensive and in-depth review of the state-of-the-art PLS enhancements utilized to unleash secure URLLC while analyzing the impact of various system design parameters on its performance. Moreover, the survey incorporates a detailed overview of the recent advancements in ensuring secure URLLC using PLS in various mission-critical applications, and 5G URLLC enabling technologies like non-orthogonal multiple access (NOMA), multi-antenna systems, cooperative communication using unmanned aerial vehicles (UAV), and intelligent reflective surfaces (IRS). Apart from this, we briefly discuss the role of advanced Machine Learning (ML) techniques in designing robust and intelligent PLS schemes for URLLC service.</p></p class="citation"></blockquote><h3 id=612--203276-leapfrog-the-rowhammer-instruction-skip-attack-andrew-adiletta-et-al-2024>(6/12 | 203/276) LeapFrog: The Rowhammer Instruction Skip Attack (Andrew Adiletta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Adiletta, Caner Tol, Berk Sunar. (2024)<br><strong>LeapFrog: The Rowhammer Instruction Skip Attack</strong><br><button class=copy-to-clipboard title="LeapFrog: The Rowhammer Instruction Skip Attack" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AR, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07878v1.pdf filename=2404.07878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats not only compromising data integrity but also the control flow integrity of victim processes. Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results. In this paper, we present a new type of Rowhammer gadget, called a LeapFrog gadget, which, when present in the victim code, allows an adversary to subvert code execution to bypass a critical piece of code (e.g., authentication check logic, encryption rounds, padding in <b>security</b> protocols). The Leapfrog gadget manifests when the victim code stores the Program Counter (PC) value in the user or kernel stack (e.g., a return address during a function call) which, when tampered with, re-positions the return address to a location that bypasses a <b>security-critical</b> code pattern. This research also presents a systematic process to identify Leapfrog gadgets. This methodology enables the automated detection of susceptible targets and the determination of optimal attack parameters. We first showcase this new attack vector through a practical demonstration on a TLS handshake client/server scenario, successfully inducing an instruction skip in a client application. We then demonstrate the attack on real-world code found in the wild, implementing an attack on OpenSSL. Our findings extend the impact of Rowhammer attacks on control flow and contribute to the development of more robust defenses against these increasingly sophisticated threats.</p></p class="citation"></blockquote><h3 id=712--204276-protected-qr-code-based-anti-counterfeit-system-for-pharmaceutical-manufacturing-nitol-saha-et-al-2024>(7/12 | 204/276) Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing (Nitol Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nitol Saha, Md Masruk Aulia, Md. Mostafizur Rahman, Mohammed Shafiul Alam Khan. (2024)<br><strong>Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing</strong><br><button class=copy-to-clipboard title="Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07831v1.pdf filename=2404.07831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pharmaceutical manufacturing faces critical challenges due to the global threat of counterfeit drugs. This paper proposes a new approach of protected QR <b>codes</b> <b>to</b> secure unique product information for safeguarding the pharmaceutical supply chain. The proposed solution integrates secure QR <b>code</b> <b>generation</b> and encrypted data transmission to establish a comprehensive anti-counterfeit ecosystem. The protected QR <b>codes</b> <b>encapsulate</b> product information that cannot be identified using traditional QR <b>code</b> <b>scanners</b> which protect the information against replication and tampering. The system is developed with scalability in mind, which can be easily implemented without introducing any additional modification in the traditional supply chain.</p></p class="citation"></blockquote><h3 id=812--205276-opportunistic-sensor-based-multi-factor-authentication-in-and-for-the-internet-of-things-marc-saideh-et-al-2024>(8/12 | 205/276) Opportunistic Sensor-Based Multi-Factor Authentication in and for the Internet of Things (Marc Saideh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Saideh, Jean-Paul Jamont, Laurent Vercouter. (2024)<br><strong>Opportunistic Sensor-Based Multi-Factor Authentication in and for the Internet of Things</strong><br><button class=copy-to-clipboard title="Opportunistic Sensor-Based Multi-Factor Authentication in and for the Internet of Things" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07675v1.pdf filename=2404.07675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Communication between connected objects often requires secure and reliable authentication mechanisms. These mechanisms are essential for verifying the identities of objects and preventing unauthorized access. The IoT offers several advantages and opportunities that are not necessarily found in other domains. For instance, IoT sensors collect real-time data about their environment and other objects which contain valuable information that, if used, can reinforce authentication. In this paper, we propose a novel idea for building opportunistic sensor-based authentication factors between IoT objects by leveraging the sensors already present in the systems where they interact. We claim that sensors can be utilized to build factors that reinforce object-to-object authentication mechanisms. Through the integration of these opportunistic sensor-based authentication factors into multi-factor authentication mechanisms, authentication in IoT can achieve a higher level of <b>security.</b> We provide illustrative experiments on two types of vehicles : mobile robots and cars.</p></p class="citation"></blockquote><h3 id=912--206276-towards-secure-and-reliable-heterogeneous-real-time-telemetry-communication-in-autonomous-uav-swarms-pavlo-mykytyn-et-al-2024>(9/12 | 206/276) Towards Secure and Reliable Heterogeneous Real-time Telemetry Communication in Autonomous UAV Swarms (Pavlo Mykytyn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavlo Mykytyn, Marcin Brzozowski, Zoya Dyka, Peter Langendörfer. (2024)<br><strong>Towards Secure and Reliable Heterogeneous Real-time Telemetry Communication in Autonomous UAV Swarms</strong><br><button class=copy-to-clipboard title="Towards Secure and Reliable Heterogeneous Real-time Telemetry Communication in Autonomous UAV Swarms" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-RO, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07557v1.pdf filename=2404.07557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of cutting-edge autonomous systems, Unmanned Aerial Vehicles (UAVs) are becoming an essential part of the solutions for numerous complex challenges. This paper evaluates UAV peer-to-peer telemetry communication, highlighting its <b>security</b> vulnerabilities and explores a transition to a het-erogeneous multi-hop mesh all-to-all communication architecture to increase inter-swarm connectivity and reliability. Additionally, we suggest a symmetric key agreement and data encryption mechanism implementation for inter - swarm communication, to ensure data integrity and confidentiality without compromising performance.</p></p class="citation"></blockquote><h3 id=1012--207276-security-modelling-for-cyber-physical-systems-a-systematic-literature-review-shaofei-huang-et-al-2024>(10/12 | 207/276) Security Modelling for Cyber-Physical Systems: A Systematic Literature Review (Shaofei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaofei Huang, Christopher M. Poskitt, Lwin Khin Shar. (2024)<br><strong>Security Modelling for Cyber-Physical Systems: A Systematic Literature Review</strong><br><button class=copy-to-clipboard title="Security Modelling for Cyber-Physical Systems: A Systematic Literature Review" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07527v1.pdf filename=2404.07527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cyber-physical systems (CPS) are at the intersection of digital technology and engineering domains, rendering them high-value targets of sophisticated and well-funded cybersecurity threat actors. Prominent cybersecurity attacks on CPS have brought attention to the vulnerability of these systems, and the soft underbelly of critical infrastructure reliant on CPS. <b>Security</b> modelling for CPS is an important mechanism to systematically identify and assess vulnerabilities, threats, and risks throughout system lifecycles, and to ultimately ensure system resilience, safety, and reliability. This literature review delves into state-of-the-art research in CPS <b>security</b> modelling, encompassing both threat and attack modelling. While these terms are sometimes used interchangeably, they are different concepts. This article elaborates on the differences between threat and attack modelling, examining their implications for CPS <b>security.</b> A systematic search yielded 428 articles, from which 15 were selected and categorised into three clusters: those focused on threat modelling methods, attack modelling methods, and literature reviews. Specifically, we sought to examine what <b>security</b> modelling methods exist today, and how they address real-world cybersecurity threats and CPS-specific attacker capabilities throughout the lifecycle of CPS, which typically span longer durations compared to traditional IT systems. This article also highlights several limitations in existing research, wherein <b>security</b> models adopt simplistic approaches that do not adequately consider the dynamic, multi-layer, multi-path, and multi-agent characteristics of real-world cyber-physical attacks.</p></p class="citation"></blockquote><h3 id=1112--208276-rtl-interconnect-obfuscation-by-polymorphic-switch-boxes-for-secure-hardware-generation-haimanti-chakraborty-et-al-2024>(11/12 | 208/276) RTL Interconnect Obfuscation By Polymorphic Switch Boxes For Secure Hardware Generation (Haimanti Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haimanti Chakraborty, Ranga Vemuri. (2024)<br><strong>RTL Interconnect Obfuscation By Polymorphic Switch Boxes For Secure Hardware Generation</strong><br><button class=copy-to-clipboard title="RTL Interconnect Obfuscation By Polymorphic Switch Boxes For Secure Hardware Generation" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07426v1.pdf filename=2404.07426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logic Obfuscation is a well renowned design-for-trust solution to protect an Integrated Circuit (IC) from unauthorized use and illegal overproduction by including key-gates to lock the design. This is particularly necessary for ICs manufactured at untrusted third-party foundries getting exposed to <b>security</b> threats. In the past, several logic obfuscation methodologies have been proposed that are vulnerable to attacks such as the Boolean Satisfiability Attack. Many of these techniques are implemented at the gate level that may involve expensive re-synthesis cycles. In this paper, we present an interconnect obfuscation scheme at the Register-Transfer Level (RTL) using Switch Boxes (SBs) constructed of Polymorphic Transistors. A polymorphic SB can be designed using the same transistor count as its Complementary-Metal-Oxide-Semiconductor based counterpart, thereby no increased area in comparison, but serving as an advantage in having more key-bit combinations for an attacker to correctly identify and unlock each polymorphic SB. <b>Security-aware</b> high-level synthesis algorithms have also been presented to increase RTL interconnects to Functional Units impacting multiple outputs such that when a polymorphic SB is strategically inserted, those outputs would be corrupted upon incorrect key-bit identification. Finally, we run the SMT (Satisfiability Modulo Theories)-based RTL Logic Attack on the obfuscated design to examine its robustness.</p></p class="citation"></blockquote><h3 id=1212--209276-fragile-model-watermark-for-integrity-protection-leveraging-boundary-volatility-and-sensitive-sample-pairing-zhenzhe-gao-et-al-2024>(12/12 | 209/276) Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing (ZhenZhe Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu. (2024)<br><strong>Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing</strong><br><button class=copy-to-clipboard title="Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 5<br>Keywords: Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07572v1.pdf filename=2404.07572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks have increasingly influenced people&rsquo;s lives. Ensuring the faithful deployment of neural networks as designed by their model owners is crucial, as they may be susceptible to various malicious or unintentional modifications, such as backdooring and poisoning attacks. Fragile model watermarks aim to prevent unexpected tampering that could lead <b>DNN</b> models to make incorrect decisions. They ensure the detection of any tampering with the model as sensitively as possible.However, prior watermarking methods suffered from inefficient sample generation and insufficient sensitivity, limiting their practical applicability. Our approach employs a sample-pairing technique, placing the model boundaries between pairs of samples, while simultaneously maximizing logits. This ensures that the model&rsquo;s decision results of sensitive samples change as much as possible and the Top-1 labels easily alter regardless of the direction it moves.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--210276-neural-fault-injection-generating-software-faults-from-natural-language-domenico-cotroneo-et-al-2024>(1/5 | 210/276) Neural Fault Injection: Generating Software Faults from Natural Language (Domenico Cotroneo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domenico Cotroneo, Pietro Liguori. (2024)<br><strong>Neural Fault Injection: Generating Software Faults from Natural Language</strong><br><button class=copy-to-clipboard title="Neural Fault Injection: Generating Software Faults from Natural Language" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07491v1.pdf filename=2404.07491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional software fault injection methods, while foundational, face limitations in adequately representing real-world faults, offering customization, and requiring significant manual effort and expertise. This paper introduces a novel methodology that harnesses the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> augmented with <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> to overcome these challenges. The usage of <b>RLHF</b> emphasizes an iterative refinement process, allowing testers to provide feedback on generated faults, which is then used to enhance the <b>LLM&rsquo;s</b> fault generation capabilities, ensuring the generation of fault scenarios that closely mirror actual operational risks. This innovative methodology aims to significantly reduce the manual effort involved in crafting fault scenarios as it allows testers to focus on higher-level testing strategies, hence paving the way to new possibilities for enhancing the dependability of software systems.</p></p class="citation"></blockquote><h3 id=25--211276-on-unified-prompt-tuning-for-request-quality-assurance-in-public-code-review-xinyu-chen-et-al-2024>(2/5 | 211/276) On Unified Prompt Tuning for Request Quality Assurance in Public Code Review (Xinyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Chen, Lin Li, Rui Zhang, Peng Liang. (2024)<br><strong>On Unified Prompt Tuning for Request Quality Assurance in Public Code Review</strong><br><button class=copy-to-clipboard title="On Unified Prompt Tuning for Request Quality Assurance in Public Code Review" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Question Answering, Masked Language Model, Masked Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07942v1.pdf filename=2404.07942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Public Code Review (PCR) can be implemented through a Software <b>Question</b> <b>Answering</b> (SQA) community, which facilitates high knowledge dissemination. Current methods mainly focus on the reviewer&rsquo;s perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. Our intuition is that satisfying review necessity requests can increase their visibility, which in turn is a prerequisite for better review responses. To this end, we propose a unified framework called UniPCR to complete developer-based request quality assurance (i.e., predicting request necessity and recommending tags subtask) under a <b>Masked</b> <b>Language</b> <b>Model</b> <b>(MLM).</b> Specifically, we reformulate both subtasks via 1) text <b>prompt</b> tuning, which converts two subtasks into <b>MLM</b> by constructing <b>prompt</b> templates using hard <b>prompt;</b> 2) code prefix tuning, which optimizes a small segment of generated continuous vectors as the prefix of the code representation using soft <b>prompt.</b> Experimental results on the Public Code Review dataset for the time span 2011-2022 demonstrate that our UniPCR framework adapts to the two subtasks and outperforms comparable accuracy-based results with state-of-the-art methods for request quality assurance. These conclusions highlight the effectiveness of our unified framework from the developer&rsquo;s perspective in public code review.</p></p class="citation"></blockquote><h3 id=35--212276-structure-aware-fine-tuning-for-code-pre-trained-models-jiayi-wu-et-al-2024>(3/5 | 212/276) Structure-aware Fine-tuning for Code Pre-trained Models (Jiayi Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Wu, Renyu Zhu, Nuo Chen, Qiushi Sun, Xiang Li, Ming Gao. (2024)<br><strong>Structure-aware Fine-tuning for Code Pre-trained Models</strong><br><button class=copy-to-clipboard title="Structure-aware Fine-tuning for Code Pre-trained Models" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07471v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07471v1.pdf filename=2404.07471v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, we have witnessed remarkable advancements in Code Pre-trained Models (CodePTMs). These models achieved excellent representation capabilities by designing structure-based pre-training tasks for code. However, how to enhance the absorption of structural knowledge when <b>fine-tuning</b> CodePTMs still remains a significant challenge. To fill this gap, in this paper, we present Structure-aware <b>Fine-tuning</b> (SAT), a novel structure-enhanced and plug-and-play <b>fine-tuning</b> method for CodePTMs. We first propose a structure loss to quantify the difference between the information learned by CodePTMs and the knowledge extracted from code structure. Specifically, we use the attention scores extracted from <b>Transformer</b> layer as the learned structural information, and the shortest path length between leaves in abstract syntax trees as the structural knowledge. Subsequently, multi-task learning is introduced to improve the performance of <b>fine-tuning.</b> Experiments conducted on four pre-trained models and two generation tasks demonstrate the effectiveness of our proposed method as a plug-and-play solution. Furthermore, we observed that SAT can benefit CodePTMs more with limited training data.</p></p class="citation"></blockquote><h3 id=45--213276-decentralized-faas-over-multi-clouds-with-blockchain-based-management-for-supporting-emerging-applications-rabimba-karanjai-et-al-2024>(4/5 | 213/276) Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications (Rabimba Karanjai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rabimba Karanjai, Lei Xu, Lin Chen, Nour Diallo, Weidong Shi. (2024)<br><strong>Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications</strong><br><button class=copy-to-clipboard title="Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08151v1.pdf filename=2404.08151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Function-as-a-Service (FaaS) offers a streamlined cloud computing paradigm, but existing centralized systems suffer from vendor lock-in and single points of failure. We propose DeFaaS, a decentralized FaaS system leveraging blockchain technology and decentralized API management. DeFaaS addresses these limitations by establishing a secure, transparent registry of functions on a blockchain and enabling applications to discover and invoke them. This approach fosters scalability, flexibility, enhanced <b>security,</b> and improved reliability. Furthermore, DeFaaS&rsquo;s architecture extends beyond decentralized FaaS, supporting other distributed computing scenarios like dApps, volunteer computing, and multi-cloud service meshes. DeFaaS represents a significant advancement in decentralized computing with the potential to unlock a multitude of novel applications and use cases.</p></p class="citation"></blockquote><h3 id=55--214276-devaic-a-tool-for-security-assessment-of-ai-generated-code-domenico-cotroneo-et-al-2024>(5/5 | 214/276) DeVAIC: A Tool for Security Assessment of AI-generated Code (Domenico Cotroneo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domenico Cotroneo, Roberta De Luca, Pietro Liguori. (2024)<br><strong>DeVAIC: A Tool for Security Assessment of AI-generated Code</strong><br><button class=copy-to-clipboard title="DeVAIC: A Tool for Security Assessment of AI-generated Code" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07548v1.pdf filename=2404.07548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: AI code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises <b>security</b> concerns. Furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions. Objective: This research work introduces DeVAIC (Detection of Vulnerabilities in AI-generated Code), a tool to evaluate the <b>security</b> of AI-generated Python code, which overcomes the challenge of examining incomplete code. Method: We followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool. The implementation of DeVAIC includes a set of detection rules based on regular expressions that cover 35 Common Weakness Enumerations (CWEs) falling under the OWASP Top 10 vulnerability categories. Results: We utilized four popular AI models to generate Python code, which we then used as a foundation to evaluate the effectiveness of our tool. DeVAIC demonstrated a statistically significant difference in its ability to detect <b>security</b> vulnerabilities compared to the state-of-the-art solutions, showing an F1 Score and Accuracy of 94% while maintaining a low computational cost of 0.14 seconds per code snippet, on average. Conclusions: The proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--215276-synthetic-brain-images-bridging-the-gap-in-brain-mapping-with-generative-adversarial-model-drici-mourad-et-al-2024>(1/7 | 215/276) Synthetic Brain Images: Bridging the Gap in Brain Mapping With Generative Adversarial Model (Drici Mourad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Drici Mourad, Kazeem Oluwakemi Oseni. (2024)<br><strong>Synthetic Brain Images: Bridging the Gap in Brain Mapping With Generative Adversarial Model</strong><br><button class=copy-to-clipboard title="Synthetic Brain Images: Bridging the Gap in Brain Mapping With Generative Adversarial Model" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, q-bio-NC<br>Keyword Score: 50<br>Keywords: Adversarial Learning, Convolution, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08703v1.pdf filename=2404.08703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Magnetic Resonance Imaging (MRI) is a vital modality for gaining precise anatomical information, and it plays a significant role in medical imaging for diagnosis and therapy planning. Image synthesis problems have seen a revolution in recent years due to the introduction of deep learning techniques, specifically <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs).</b> This work investigates the use of Deep <b>Convolutional</b> <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (DCGAN) for producing high-fidelity and realistic MRI image slices. The suggested approach uses a dataset with a variety of brain MRI scans to train a DCGAN architecture. While the discriminator network discerns between created and real slices, the generator network learns to synthesise realistic MRI image slices. The generator refines its capacity to generate slices that closely mimic real MRI <b>data</b> <b>through</b> an <b>adversarial</b> <b>training</b> approach. The outcomes demonstrate that the DCGAN promise for a range of uses in medical imaging research, since they show that it can effectively produce MRI image slices if we train them for a consequent number of epochs. This work adds to the expanding corpus of research on the application of deep learning techniques for medical image synthesis. The slices that are could be produced possess the capability to enhance datasets, provide <b>data</b> <b>augmentation</b> in the training of deep learning models, as well as a number of functions are made available to make MRI <b>data</b> <b>cleaning</b> easier, and a three ready to use and clean dataset on the major anatomical plans.</p></p class="citation"></blockquote><h3 id=27--216276-survival-prediction-across-diverse-cancer-types-using-neural-networks-xu-yan-et-al-2024>(2/7 | 216/276) Survival Prediction Across Diverse Cancer Types Using Neural Networks (Xu Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Yan, Weimin Wang, MingXuan Xiao, Yufeng Li, Min Gao. (2024)<br><strong>Survival Prediction Across Diverse Cancer Types Using Neural Networks</strong><br><button class=copy-to-clipboard title="Survival Prediction Across Diverse Cancer Types Using Neural Networks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 38<br>Keywords: Convolutional Neural Network, Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08713v1.pdf filename=2404.08713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gastric cancer and Colon adenocarcinoma represent widespread and challenging malignancies with high mortality rates and complex treatment landscapes. In response to the critical need for accurate prognosis in cancer patients, the medical community has embraced the 5-year survival rate as a vital metric for estimating patient outcomes. This study introduces a pioneering approach to enhance survival prediction models for gastric and Colon adenocarcinoma patients. Leveraging advanced image analysis techniques, we sliced whole slide images (WSI) of these cancers, extracting comprehensive features to capture nuanced tumor characteristics. Subsequently, we constructed patient-level <b>graphs,</b> encapsulating intricate spatial relationships within tumor tissues. These <b>graphs</b> served as inputs for a sophisticated 4-layer <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network</b> <b>(GCN),</b> designed to exploit the inherent connectivity of the data for comprehensive analysis and prediction. By integrating patients&rsquo; total survival time and survival status, we computed C-index values for gastric cancer and Colon adenocarcinoma, yielding 0.57 and 0.64, respectively. Significantly surpassing previous <b>convolutional</b> <b>neural</b> <b>network</b> models, these results underscore the efficacy of our approach in accurately predicting patient survival outcomes. This research holds profound implications for both the medical and AI communities, offering insights into cancer biology and progression while advancing personalized treatment strategies. Ultimately, our study represents a significant stride in leveraging AI-driven methodologies to revolutionize cancer prognosis and improve patient outcomes on a global scale.</p></p class="citation"></blockquote><h3 id=37--217276-lucf-net-lightweight-u-shaped-cascade-fusion-network-for-medical-image-segmentation-songkai-sun-et-al-2024>(3/7 | 217/276) LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation (Songkai Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Songkai Sun, Qingshan She, Yuliang Ma, Rihui Li, Yingchun Zhang. (2024)<br><strong>LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 35<br>Keywords: Convolutional Neural Network, Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07473v1.pdf filename=2404.07473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, the performance of existing U-shaped neural network architectures was enhanced for medical image segmentation by adding <b>Transformer.</b> Although <b>Transformer</b> architectures are powerful at extracting global information, its ability to capture local information is limited due to its high complexity. To address this challenge, we proposed a new lightweight U-shaped cascade fusion network (LUCF-Net) for medical image segmentation. It utilized an asymmetrical structural design and incorporated both local and global modules to enhance its capacity for local and global modeling. Additionally, a multi-layer cascade fusion decoding network was designed to further bolster the network&rsquo;s information fusion capabilities. Validation results achieved on multi-organ datasets in CT format, cardiac segmentation datasets in MRI format, and dermatology datasets in image format demonstrated that the proposed model outperformed other state-of-the-art methods in handling local-global information, achieving an improvement of 1.54% in Dice coefficient and 2.6 mm in Hausdorff distance on multi-organ segmentation. Furthermore, as a network that combines <b>Convolutional</b> <b>Neural</b> <b>Network</b> and <b>Transformer</b> architectures, it achieves competitive segmentation performance with only 6.93 million parameters and 6.6 gigabytes of floating point operations, without the need of pre-training. In summary, the proposed method demonstrated enhanced performance while retaining a simpler model design compared to other <b>Transformer-based</b> segmentation networks.</p></p class="citation"></blockquote><h3 id=47--218276-latte-low-precision-approximate-attention-with-head-wise-trainable-threshold-for-efficient-transformer-jiing-ping-wang-et-al-2024>(4/7 | 218/276) LATTE: Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer (Jiing-Ping Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiing-Ping Wang, Ming-Guang Lin, An-Yeu, Wu. (2024)<br><strong>LATTE: Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer</strong><br><button class=copy-to-clipboard title="LATTE: Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Transformer, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07519v1.pdf filename=2404.07519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of <b>Transformer</b> models in NLP and CV domain, Multi-Head Attention has been proven to be a game-changer. However, its expensive computation poses challenges to the model throughput and efficiency, especially for the long sequence tasks. Exploiting the sparsity in attention has been proven to be an effective way to reduce computation. Nevertheless, prior works do not consider the various distributions among different heads and lack a systematic method to determine the threshold. To address these challenges, we propose Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient <b>Transformer</b> (LATTE). LATTE employs a headwise threshold-based filter with the low-precision dot product and computation reuse mechanism to reduce the computation of MHA. Moreover, the trainable threshold is introduced to provide a systematic method for adjusting the thresholds and enable end-to-end optimization. Experimental results indicate LATTE can smoothly adapt to both NLP and CV tasks, offering significant computation savings with only a minor compromise in performance. Also, the trainable threshold is shown to be essential for the leverage between the performance and the computation. As a result, LATTE filters up to 85.16% keys with only a 0.87% accuracy drop in the CV task and 89.91% keys with a 0.86 <b>perplexity</b> increase in the NLP task.</p></p class="citation"></blockquote><h3 id=57--219276-diffusion-probabilistic-multi-cue-level-set-for-reducing-edge-uncertainty-in-pancreas-segmentation-yue-gou-et-al-2024>(5/7 | 219/276) Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation (Yue Gou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Gou, Yuming Xing, Shengzhu Shi, Zhichang Guo. (2024)<br><strong>Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation</strong><br><button class=copy-to-clipboard title="Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07620v1.pdf filename=2404.07620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurately segmenting the pancreas remains a huge challenge. Traditional methods encounter difficulties in semantic localization due to the small volume and distorted structure of the pancreas, while deep learning methods encounter challenges in obtaining accurate edges because of low contrast and organ overlapping. To overcome these issues, we propose a multi-cue level set method based on the diffusion <b>probabilistic</b> <b>model,</b> namely Diff-mcs. Our method adopts a coarse-to-fine segmentation strategy. We use the diffusion <b>probabilistic</b> <b>model</b> in the coarse segmentation stage, with the obtained probability distribution serving as both the initial localization and prior cues for the level set method. In the fine segmentation stage, we combine the prior cues with grayscale cues and texture cues to refine the edge by maximizing the difference between probability distributions of the cues inside and outside the level set curve. The method is validated on three public datasets and achieves state-of-the-art performance, which can obtain more accurate segmentation results with lower uncertainty segmentation edges. In addition, we conduct ablation studies and uncertainty analysis to verify that the diffusion probability model provides a more appropriate initialization for the level set method. Furthermore, when combined with multiple cues, the level set method can better obtain edges and improve the overall accuracy. Our code is available at <a href=https://github.com/GOUYUEE/Diff-mcs>https://github.com/GOUYUEE/Diff-mcs</a>.</p></p class="citation"></blockquote><h3 id=67--220276-event-enhanced-snapshot-compressive-videography-at-10k-fps-bo-zhang-et-al-2024>(6/7 | 220/276) Event-Enhanced Snapshot Compressive Videography at 10K FPS (Bo Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Zhang, Jinli Suo, Qionghai Dai. (2024)<br><strong>Event-Enhanced Snapshot Compressive Videography at 10K FPS</strong><br><button class=copy-to-clipboard title="Event-Enhanced Snapshot Compressive Videography at 10K FPS" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07551v1.pdf filename=2404.07551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video snapshot compressive imaging (SCI) encodes the target dynamic scene compactly into a snapshot and reconstructs its high-speed frame sequence afterward, greatly reducing the required data footprint and transmission bandwidth as well as enabling high-speed imaging with a low frame rate intensity camera. In implementation, high-speed dynamics are encoded via temporally varying patterns, and only frames at corresponding temporal intervals can be reconstructed, while the dynamics occurring between consecutive frames are lost. To unlock the potential of conventional snapshot compressive videography, we propose a novel hybrid &ldquo;intensity+event&rdquo; imaging scheme by incorporating an event camera into a video SCI setup. Our proposed system consists of a dual-path optical setup to record the coded intensity measurement and intermediate event signals simultaneously, which is compact and photon-efficient by collecting the half photons discarded in conventional video SCI. Correspondingly, we developed a dual-branch <b>Transformer</b> utilizing the reciprocal relationship between two data modes to decode dense video frames. Extensive experiments on both simulated and real-captured data demonstrate our superiority to state-of-the-art video SCI and video frame interpolation (VFI) methods. Benefiting from the new hybrid design leveraging both intrinsic redundancy in videos and the unique feature of event cameras, we achieve high-quality videography at 0.1ms time intervals with a low-cost CMOS image sensor working at 24 FPS.</p></p class="citation"></blockquote><h3 id=77--221276-learning-to-classify-new-foods-incrementally-via-compressed-exemplars-justin-yang-et-al-2024>(7/7 | 221/276) Learning to Classify New Foods Incrementally Via Compressed Exemplars (Justin Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justin Yang, Zhihao Duan, Jiangpeng He, Fengqing Zhu. (2024)<br><strong>Learning to Classify New Foods Incrementally Via Compressed Exemplars</strong><br><button class=copy-to-clipboard title="Learning to Classify New Foods Incrementally Via Compressed Exemplars" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07507v1.pdf filename=2404.07507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Food image classification systems play a crucial role in health monitoring and diet tracking through image-based dietary assessment techniques. However, existing food recognition systems rely on static datasets characterized by a pre-defined fixed number of food classes. This contrasts drastically with the reality of food consumption, which features constantly changing data. Therefore, food image classification systems should adapt to and manage data that continuously evolves. This is where <b>continual</b> <b>learning</b> plays an important role. A challenge in <b>continual</b> <b>learning</b> is catastrophic forgetting, where ML models tend to discard old knowledge upon learning new information. While memory-replay algorithms have shown promise in mitigating this problem by storing old data as exemplars, they are hampered by the limited capacity of memory buffers, leading to an imbalance between new and previously learned data. To address this, our work explores the use of neural image compression to extend buffer size and enhance data diversity. We introduced the concept of continuously learning a neural compression model to adaptively improve the quality of compressed data and optimize the bitrates per pixel (bpp) to store more exemplars. Our extensive experiments, including evaluations on food-specific datasets including Food-101 and VFN-74, as well as the general dataset ImageNet-100, demonstrate improvements in classification accuracy. This progress is pivotal in advancing more realistic food recognition systems that are capable of adapting to continually evolving data. Moreover, the principles and methodologies we&rsquo;ve developed hold promise for broader applications, extending their benefits to other domains of <b>continual</b> <b>machine</b> learning systems.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--222276-1-bit-quantized-on-chip-hybrid-diffraction-neural-network-enabled-by-authentic-all-optical-fully-connected-architecture-yu-shao-et-al-2024>(1/1 | 222/276) 1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by Authentic All-optical Fully-connected Architecture (Yu Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Shao, Haiqi Gao, Yipeng Chen, Yujie liu, Junren Wen, Haidong He, Yuchuan Shao, Yueguang Zhang, Weidong Shen, Chenying Yang. (2024)<br><strong>1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by Authentic All-optical Fully-connected Architecture</strong><br><button class=copy-to-clipboard title="1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by Authentic All-optical Fully-connected Architecture" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-ET, cs-LG, physics-optics, physics.optics<br>Keyword Score: 45<br>Keywords: Deep Neural Network, Quantization, Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07443v1.pdf filename=2404.07443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical Diffraction Neural Networks <b>(DNNs),</b> a subset of Optical Neural Networks (ONNs), show promise in mirroring the prowess of electronic networks. This study introduces the Hybrid Diffraction Neural Network (HDNN), a novel architecture that incorporates matrix multiplication into <b>DNNs,</b> synergizing the benefits of conventional ONNs with those of <b>DNNs</b> to surmount the modulation limitations inherent in optical diffraction neural networks. Utilizing a singular phase modulation layer and an amplitude modulation layer, the trained neural network demonstrated remarkable accuracies of 96.39% and 89% in digit recognition tasks in <b>simulation</b> and experiment, respectively. Additionally, we develop the Binning Design (BD) method, which effectively mitigates the constraints imposed by sampling intervals on diffraction units, substantially streamlining experimental procedures. Furthermore, we propose an on-chip HDNN that not only employs a beam-splitting phase modulation layer for enhanced integration level but also significantly relaxes device fabrication requirements, replacing metasurfaces with relief surfaces designed by 1-bit <b>quantization.</b> Besides, we conceptualized an all-optical HDNN-assisted lesion detection network, achieving detection outcomes that were 100% aligned with <b>simulation</b> predictions. This work not only advances the performance of <b>DNNs</b> but also streamlines the path towards industrial optical neural network production.</p></p class="citation"></blockquote><h2 id=csni-8>cs.NI (8)</h2><h3 id=18--223276-collaborative-ground-space-communications-via-evolutionary-multi-objective-deep-reinforcement-learning-jiahui-li-et-al-2024>(1/8 | 223/276) Collaborative Ground-Space Communications via Evolutionary Multi-objective Deep Reinforcement Learning (Jiahui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Li, Geng Sun, Qingqing Wu, Dusit Niyato, Jiawen Kang, Abbas Jamalipour, Victor C. M. Leung. (2024)<br><strong>Collaborative Ground-Space Communications via Evolutionary Multi-objective Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Collaborative Ground-Space Communications via Evolutionary Multi-objective Deep Reinforcement Learning" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NE, cs-NI, cs.NI<br>Keyword Score: 40<br>Keywords: Markov Decision Process, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07450v1.pdf filename=2404.07450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a distributed collaborative beamforming (DCB)-based uplink communication paradigm for enabling ground-space direct communications. Specifically, DCB treats the terminals that are unable to establish efficient direct connections with the low Earth orbit (LEO) satellites as distributed antennas, forming a virtual antenna array to enhance the terminal-to-satellite uplink achievable rates and durations. However, such systems need multiple trade-off policies that variously balance the terminal-satellite uplink achievable rate, energy consumption of terminals, and satellite switching frequency to satisfy the scenario requirement changes. Thus, we perform a multi-objective optimization analysis and formulate a long-term optimization problem. To address availability in different terminal cluster scales, we reformulate this problem into an action space-reduced and universal multi-objective <b>Markov</b> <b>decision</b> <b>process.</b> Then, we propose an evolutionary multi-objective deep <b>reinforcement</b> <b>learning</b> algorithm to obtain the desirable policies, in which the low-value actions are masked to speed up the training process. As such, the applicability of a one-time trained model can cover more changing terminal-satellite uplink scenarios. <b>Simulation</b> results show that the proposed algorithm outmatches various baselines, and draw some useful insights. Specifically, it is found that DCB enables terminals that cannot reach the uplink achievable threshold to achieve efficient direct uplink transmission, which thus reveals that DCB is an effective solution for enabling direct ground-space communications. Moreover, it reveals that the proposed algorithm achieves multiple policies favoring different objectives and achieving near-optimal uplink achievable rates with low switching frequency.</p></p class="citation"></blockquote><h3 id=28--224276-uav-enabled-collaborative-beamforming-via-multi-agent-deep-reinforcement-learning-saichao-liu-et-al-2024>(2/8 | 224/276) UAV-enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement Learning (Saichao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saichao Liu, Geng Sun, Jiahui Li, Shuang Liang, Qingqing Wu, Pengfei Wang, Dusit Niyato. (2024)<br><strong>UAV-enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="UAV-enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement Learning" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NE, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07453v1.pdf filename=2404.07453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate an unmanned aerial vehicle (UAV)-assistant air-to-ground communication system, where multiple UAVs form a UAV-enabled virtual antenna array (UVAA) to communicate with remote base stations by utilizing collaborative beamforming. To improve the work efficiency of the UVAA, we formulate a UAV-enabled collaborative beamforming multi-objective optimization problem (UCBMOP) to simultaneously maximize the transmission rate of the UVAA and minimize the energy consumption of all UAVs by optimizing the positions and excitation current weights of all UAVs. This problem is challenging because these two optimization objectives conflict with each other, and they are non-concave to the optimization variables. Moreover, the system is dynamic, and the cooperation among UAVs is complex, making traditional methods take much time to compute the optimization solution for a single task. In addition, as the task changes, the previously obtained solution will become obsolete and invalid. To handle these issues, we leverage the multi-agent deep <b>reinforcement</b> <b>learning</b> (MADRL) to address the UCBMOP. Specifically, we use the heterogeneous-agent trust region policy optimization (HATRPO) as the basic framework, and then propose an improved HATRPO algorithm, namely HATRPO-UCB, where three techniques are introduced to enhance the performance. <b>Simulation</b> results demonstrate that the proposed algorithm can learn a better strategy compared with other methods. Moreover, extensive experiments also demonstrate the effectiveness of the proposed techniques.</p></p class="citation"></blockquote><h3 id=38--225276-an-application-layer-multi-hop-collective-perception-service-for-vehicular-adhoc-networks-vincent-albert-wolff-et-al-2024>(3/8 | 225/276) An Application Layer Multi-Hop Collective Perception Service for Vehicular Adhoc Networks (Vincent Albert Wolff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Albert Wolff, Edmir Xhoxhi, Felix Tautz. (2024)<br><strong>An Application Layer Multi-Hop Collective Perception Service for Vehicular Adhoc Networks</strong><br><button class=copy-to-clipboard title="An Application Layer Multi-Hop Collective Perception Service for Vehicular Adhoc Networks" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07761v1.pdf filename=2404.07761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collective Perception will play a crucial role for ensuring vehicular safety in the near future, enabling the sharing of local perceived objects with other Intelligent Transport System Stations (ITS-Ss). However, at the beginning of the roll-out, low market penetration rates are expected. This paper proposes and evaluates an application layer multi-hop Collective Perception Service (CPS) for vehicular ad-hoc networks. The goal is to improve the environmental awareness ratio in scenarios with low CPS market penetration. In such scenarios, the CPS service without forwarding enabled struggles to achieve complete awareness. A decentralized application layer forwarding algorithm is presented that shares perceived object information across multiple hops while maintaining a low age of information. The proposed approach is compared against standard CPS with no forwarding and CPS with geographically-scoped (GBC) multi-hop forwarding. <b>Simulations</b> according to standards of the European Telecommunications Standards Institute (ETSI) demonstrate that the application layer forwarding achieves near 100% awareness at 10% penetration rate versus 92% for standard CPS. The awareness improvement comes with moderate channel load, unlike GBC forwarding which quickly saturates the channel. The median age of information remains below 80 ms for the proposed scheme, enabling real-time CPS operation. Our application layer multi-hop approach effectively improves environmental awareness during initial CPS deployment while aligning with latency and channel load requirements.</p></p class="citation"></blockquote><h3 id=48--226276-swi-feed-smart-water-iot-framework-for-evaluation-of-energy-and-data-in-massive-scenarios-antonino-pagano-et-al-2024>(4/8 | 226/276) SWI-FEED: Smart Water IoT Framework for Evaluation of Energy and Data in Massive Scenarios (Antonino Pagano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonino Pagano, Domenico Garlisi, Fabrizio Giuliano, Tiziana Cattai, Francesca Cuomo. (2024)<br><strong>SWI-FEED: Smart Water IoT Framework for Evaluation of Energy and Data in Massive Scenarios</strong><br><button class=copy-to-clipboard title="SWI-FEED: Smart Water IoT Framework for Evaluation of Energy and Data in Massive Scenarios" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07692v1.pdf filename=2404.07692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a comprehensive framework designed to facilitate the widespread deployment of the Internet of Things (IoT) for enhanced monitoring and optimization of Water Distribution Systems (WDSs). The framework aims to investigate the utilization of massive IoT in monitoring and optimizing WDSs, with a particular focus on leakage detection, energy consumption and wireless network performance assessment in real-world water networks. The framework integrates <b>simulation</b> environments at both the application level (using EPANET) and the radio level (using NS-3) within the LoRaWAN network. The paper culminates with a practical use case, alongside evaluation results concerning power consumption in a large-scale LoRaWAN network and strategies for optimal gateway positioning.</p></p class="citation"></blockquote><h3 id=58--227276-two-way-aerial-secure-communications-via-distributed-collaborative-beamforming-under-eavesdropper-collusion-jiahui-li-et-al-2024>(5/8 | 227/276) Two-Way Aerial Secure Communications via Distributed Collaborative Beamforming under Eavesdropper Collusion (Jiahui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Li, Geng Sun, Qingqing Wu, Shuang Liang, Pengfei Wang, Dusit Niyato. (2024)<br><strong>Two-Way Aerial Secure Communications via Distributed Collaborative Beamforming under Eavesdropper Collusion</strong><br><button class=copy-to-clipboard title="Two-Way Aerial Secure Communications via Distributed Collaborative Beamforming under Eavesdropper Collusion" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07444v1.pdf filename=2404.07444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned aerial vehicles (UAVs)-enabled aerial communication provides a flexible, reliable, and cost-effective solution for a range of wireless applications. However, due to the high line-of-sight (LoS) probability, aerial communications between UAVs are vulnerable to eavesdropping attacks, particularly when multiple eavesdroppers collude. In this work, we aim to introduce distributed collaborative beamforming (DCB) into UAV swarms and handle the eavesdropper collusion by controlling the corresponding signal distributions. Specifically, we consider a two-way DCB-enabled aerial communication between two UAV swarms and construct these swarms as two UAV virtual antenna arrays. Then, we minimize the two-way known secrecy capacity and the maximum sidelobe level to avoid information leakage from the known and unknown eavesdroppers, respectively. Simultaneously, we also minimize the energy consumption of UAVs for constructing virtual antenna arrays. Due to the conflicting relationships between secure performance and energy efficiency, we consider these objectives as a multi-objective optimization problem. Following this, we propose an enhanced multi-objective swarm intelligence algorithm via the characterized properties of the problem. <b>Simulation</b> results show that our proposed algorithm can obtain a set of informative solutions and outperform other state-of-the-art baseline algorithms. Experimental tests demonstrate that our method can be deployed in limited computing power platforms of UAVs and is beneficial for saving computational resources.</p></p class="citation"></blockquote><h3 id=68--228276-resource-aware-deployment-of-dynamic-dnns-over-multi-tiered-interconnected-systems-chetna-singhal-et-al-2024>(6/8 | 228/276) Resource-aware Deployment of Dynamic DNNs over Multi-tiered Interconnected Systems (Chetna Singhal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chetna Singhal, Yashuo Wu, Francesco Malandrino, Marco Levorato, Carla Fabiana Chiasserini. (2024)<br><strong>Resource-aware Deployment of Dynamic DNNs over Multi-tiered Interconnected Systems</strong><br><button class=copy-to-clipboard title="Resource-aware Deployment of Dynamic DNNs over Multi-tiered Interconnected Systems" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 13<br>Keywords: Graph, Deep Neural Network, Deep Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08060v1.pdf filename=2404.08060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing pervasiveness of intelligent mobile applications requires to exploit the full range of resources offered by the mobile-edge-cloud network for the execution of inference tasks. However, due to the heterogeneity of such multi-tiered networks, it is essential to make the applications&rsquo; demand amenable to the available resources while minimizing energy consumption. Modern dynamic <b>deep</b> <b>neural</b> <b>networks</b> <b>(DNN)</b> achieve this goal by designing multi-branched architectures where early exits enable sample-based adaptation of the model depth. In this paper, we tackle the problem of allocating sections of <b>DNNs</b> with early exits to the nodes of the mobile-edge-cloud system. By envisioning a 3-stage <b>graph-modeling</b> approach, we represent the possible options for splitting the <b>DNN</b> and deploying the <b>DNN</b> blocks on the multi-tiered network, embedding both the system constraints and the application requirements in a convenient and efficient way. Our framework &ndash; named Feasible Inference <b>Graph</b> (FIN) &ndash; can identify the solution that minimizes the overall inference energy consumption while enabling distributed inference over the multi-tiered network with the target quality and latency. Our results, obtained for <b>DNNs</b> with different levels of complexity, show that FIN matches the optimum and yields over 65% energy savings relative to a state-of-the-art technique for cost minimization.</p></p class="citation"></blockquote><h3 id=78--229276-predictive-handover-strategy-in-6g-and-beyond-a-deep-and-transfer-learning-approach-ioannis-panitsas-et-al-2024>(7/8 | 229/276) Predictive Handover Strategy in 6G and Beyond: A Deep and Transfer Learning Approach (Ioannis Panitsas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ioannis Panitsas, Akrit Mudvari, Ali Maatouk, Leandros Tassiulas. (2024)<br><strong>Predictive Handover Strategy in 6G and Beyond: A Deep and Transfer Learning Approach</strong><br><button class=copy-to-clipboard title="Predictive Handover Strategy in 6G and Beyond: A Deep and Transfer Learning Approach" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08113v1.pdf filename=2404.08113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-generation cellular networks will evolve into more complex and virtualized systems, employing machine learning for enhanced optimization and leveraging higher frequency bands and denser deployments to meet varied service demands. This evolution, while bringing numerous advantages, will also pose challenges, especially in mobility management, as it will increase the overall number of handovers due to smaller coverage areas and the higher signal attenuation. To address these challenges, we propose a deep learning based algorithm for predicting the future serving cell utilizing sequential user equipment measurements to minimize the handover failures and interruption time. Our algorithm enables network operators to dynamically adjust handover triggering events or incorporate UAV base stations for enhanced coverage and capacity, optimizing network objectives like load balancing and energy efficiency through <b>transfer</b> <b>learning</b> techniques. Our framework complies with the O-RAN specifications and can be deployed in a Near-Real-Time RAN Intelligent Controller as an xApp leveraging the E2SM-KPM service model. The evaluation results demonstrate that our algorithm achieves a 92% accuracy in predicting future serving cells with high probability. Finally, by utilizing <b>transfer</b> <b>learning,</b> our algorithm significantly reduces the retraining time by 91% and 77% when new handover trigger decisions or UAV base stations are introduced to the network dynamically.</p></p class="citation"></blockquote><h3 id=88--230276-konnektor-connection-protocol-for-ensuring-peer-uniqueness-in-decentralized-p2p-networks-onur-ozkan-2024>(8/8 | 230/276) Konnektor: Connection Protocol for Ensuring Peer Uniqueness in Decentralized P2P Networks (Onur Ozkan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Onur Ozkan. (2024)<br><strong>Konnektor: Connection Protocol for Ensuring Peer Uniqueness in Decentralized P2P Networks</strong><br><button class=copy-to-clipboard title="Konnektor: Connection Protocol for Ensuring Peer Uniqueness in Decentralized P2P Networks" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: C-2-1; C-2-4, cs-DC, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07861v1.pdf filename=2404.07861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Konnektor is a connection protocol designed to solve the challenge of managing unique peers within distributed peer-to-peer networks. By prioritizing network integrity and efficiency, Konnektor offers a comprehensive solution that safeguards against the spread of duplicate peers while optimizing resource utilization. This paper provides a detailed explanation of the protocol&rsquo;s key components, including peer addressing, connection initialization, detecting peer duplications and mitigation strategies against potential <b>security</b> threats.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--231276-multimodal-emotion-recognition-by-fusing-video-semantic-in-mooc-learning-scenarios-yuan-zhang-et-al-2024>(1/1 | 231/276) Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios (Yuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Zhang, Xiaomei Tao, Hanxu Ai, Tao Chen, Yanling Gan. (2024)<br><strong>Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios</strong><br><button class=copy-to-clipboard title="Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-MM, cs.MM<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Emotion Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07484v1.pdf filename=2404.07484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Massive Open Online Courses (MOOC) learning scenario, the semantic information of instructional videos has a crucial impact on learners&rsquo; <b>emotional</b> <b>state.</b> Learners mainly acquire knowledge by watching instructional videos, and the semantic information in the videos directly affects learners&rsquo; <b>emotional</b> <b>states.</b> However, few studies have paid attention to the potential influence of the semantic information of instructional videos on learners&rsquo; <b>emotional</b> <b>states.</b> To deeply explore the impact of video semantic information on learners&rsquo; <b>emotions,</b> <b>this</b> paper innovatively proposes a <b>multimodal</b> <b>emotion</b> <b>recognition</b> method by fusing video semantic information and physiological signals. We generate video descriptions through a pre-trained <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to obtain high-level semantic information about instructional videos. Using the cross-attention mechanism for modal interaction, the semantic information is fused with the eye movement and PhotoPlethysmoGraphy (PPG) signals to obtain the features containing the critical information of the three modes. The accurate recognition of learners&rsquo; <b>emotional</b> <b>states</b> is realized through the <b>emotion</b> <b>classifier.</b> The experimental results show that our method has significantly improved <b>emotion</b> <b>recognition</b> performance, providing a new perspective and efficient method for <b>emotion</b> <b>recognition</b> research in MOOC learning scenarios. The method proposed in this paper not only contributes to a deeper understanding of the impact of instructional videos on learners&rsquo; <b>emotional</b> <b>states</b> but also provides a beneficial reference for future research on <b>emotion</b> <b>recognition</b> in MOOC learning scenarios.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=12--232276-do-large-language-models-learn-human-like-strategic-preferences-jesse-roberts-et-al-2024>(1/2 | 232/276) Do Large Language Models Learn Human-Like Strategic Preferences? (Jesse Roberts et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesse Roberts, Kyle Moore, Doug Fisher. (2024)<br><strong>Do Large Language Models Learn Human-Like Strategic Preferences?</strong><br><button class=copy-to-clipboard title="Do Large Language Models Learn Human-Like Strategic Preferences?" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs.GT<br>Keyword Score: 30<br>Keywords: Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08710v1.pdf filename=2404.08710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We evaluate whether <b>LLMs</b> learn to make human-like preference judgements in strategic scenarios as compared with known empirical results. We show that Solar and <b>Mistral</b> exhibit stable value-based preference consistent with human in the prisoner&rsquo;s dilemma, including stake-size effect, and traveler&rsquo;s dilemma, including penalty-size effect. We establish a relationship between model size, value based preference, and superficiality. Finally, we find that models that tend to be less brittle were trained with sliding window attention. Additionally, we contribute a novel method for constructing preference relations from arbitrary <b>LLMs</b> and support for a hypothesis regarding human behavior in the traveler&rsquo;s dilemma.</p></p class="citation"></blockquote><h3 id=22--233276-auctions-with-llm-summaries-kumar-avinava-dubey-et-al-2024>(2/2 | 233/276) Auctions with LLM Summaries (Kumar Avinava Dubey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kumar Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, Di Wang. (2024)<br><strong>Auctions with LLM Summaries</strong><br><button class=copy-to-clipboard title="Auctions with LLM Summaries" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08126v1.pdf filename=2404.08126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study an auction setting in which bidders bid for placement of their content within a summary generated by a <b>large</b> <b>language</b> <b>model</b> <b>(LLM),</b> e.g., an ad auction in which the display is a summary paragraph of multiple ads. This generalizes the classic ad settings such as position auctions to an <b>LLM</b> generated setting, which allows us to handle general display formats. We propose a novel factorized framework in which an auction module and an <b>LLM</b> module work together via a prediction model to provide welfare maximizing summary outputs in an incentive compatible manner. We provide a theoretical analysis of this framework and synthetic experiments to demonstrate the feasibility and validity of the system together with welfare comparisons.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--234276-unraveling-the-dilemma-of-ai-errors-exploring-the-effectiveness-of-human-and-machine-explanations-for-large-language-models-marvin-pafla-et-al-2024>(1/4 | 234/276) Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models (Marvin Pafla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marvin Pafla, Kate Larson, Mark Hancock. (2024)<br><strong>Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models</strong><br><button class=copy-to-clipboard title="Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: ChatGPT, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07725v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07725v1.pdf filename=2404.07725v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies <b>question</b> <b>the</b> efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a <b>question-answering</b> <b>task</b> (N=40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and <b>ChatGPT)</b> in a human-participant study (N=136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.</p></p class="citation"></blockquote><h3 id=24--235276-leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation-jinkyung-park-et-al-2024>(2/4 | 235/276) Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation (Jinkyung Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinkyung Park, Pamela Wisniewski, Vivek Singh. (2024)<br><strong>Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07926v1.pdf filename=2404.07926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this position paper, we discuss the potential for leveraging <b>LLMs</b> as interactive research tools to facilitate collaboration between human coders and AI to effectively annotate online risk data at scale. Collaborative human-AI labeling is a promising approach to annotating <b>large-scale</b> <b>and</b> <b>complex</b> data for various tasks. Yet, tools and methods to support effective human-AI collaboration for data annotation are under-studied. This gap is pertinent because co-labeling tasks need to support a two-way interactive discussion that can add nuance and context, particularly in the context of online risk, which is highly subjective and contextualized. Therefore, we provide some of the early benefits and challenges of using <b>LLMs-based</b> tools for risk annotation and suggest future directions for the HCI research community to leverage <b>LLMs</b> as research tools to facilitate human-AI collaboration in contextualized online data annotation. Our research interests align very well with the purposes of the <b>LLMs</b> as Research Tools workshop to identify ongoing applications and challenges of using <b>LLMs</b> to work with data in HCI research. We anticipate learning valuable insights from organizers and participants into how <b>LLMs</b> can help reshape the HCI community&rsquo;s methods for working with data.</p></p class="citation"></blockquote><h3 id=34--236276-rassar-room-accessibility-and-safety-scanning-in-augmented-reality-xia-su-et-al-2024>(3/4 | 236/276) RASSAR: Room Accessibility and Safety Scanning in Augmented Reality (Xia Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xia Su, Han Zhang, Kaiming Cheng, Jaewook Lee, Qiaochu Liu, Wyatt Olson, Jon Froehlich. (2024)<br><strong>RASSAR: Room Accessibility and Safety Scanning in Augmented Reality</strong><br><button class=copy-to-clipboard title="RASSAR: Room Accessibility and Safety Scanning in Augmented Reality" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Augmented Reality (AR), Augmented Reality (AR)<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07479v1.pdf filename=2404.07479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, <b>augmented</b> <b>reality</b> <b>(AR),</b> and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile <b>AR</b> application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR&rsquo;s extensibility, and key application scenarios.</p></p class="citation"></blockquote><h3 id=44--237276-apprentice-tutor-builder-a-platform-for-users-to-create-and-personalize-intelligent-tutors-glen-smith-et-al-2024>(4/4 | 237/276) Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors (Glen Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Glen Smith, Adit Gupta, Christopher MacLellan. (2024)<br><strong>Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors</strong><br><button class=copy-to-clipboard title="Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07883v1.pdf filename=2404.07883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent tutoring systems (ITS) are effective for improving students&rsquo; learning outcomes. However, their development is often complex, time-consuming, and requires specialized programming and tutor design knowledge, thus hindering their widespread application and personalization. We present the Apprentice Tutor Builder (ATB) , a platform that simplifies tutor creation and personalization. Instructors can utilize ATB&rsquo;s drag-and-drop tool to build tutor interfaces. Instructors can then interactively train the tutors&rsquo; underlying AI agent to produce expert models that can solve problems. Training is achieved via using multiple interaction modalities including demonstrations, feedback, and user labels. We conducted a user study with 14 instructors to evaluate the effectiveness of ATB&rsquo;s design with end users. We found that users enjoyed the flexibility of the interface builder and ease and speed of agent teaching, but often desired additional time-saving features. With these insights, we identified a set of design <b>recommendations</b> for our platform and others that utilize interactive AI agents for tutor creation and customization.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--238276-robust-performance-metrics-for-imbalanced-classification-problems-hajo-holzmann-et-al-2024>(1/3 | 238/276) Robust performance metrics for imbalanced classification problems (Hajo Holzmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hajo Holzmann, Bernhard Klar. (2024)<br><strong>Robust performance metrics for imbalanced classification problems</strong><br><button class=copy-to-clipboard title="Robust performance metrics for imbalanced classification problems" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07661v1.pdf filename=2404.07661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that established performance metrics in binary classification, such as the F-score, the Jaccard similarity coefficient or Matthews&rsquo; correlation coefficient (MCC), are not robust to class imbalance in the sense that if the proportion of the minority class tends to $0$, the true positive rate (TPR) of the Bayes classifier under these metrics tends to $0$ as well. Thus, in imbalanced classification problems, these metrics favour classifiers which ignore the minority class. To alleviate this issue we introduce robust modifications of the F-score and the MCC for which, even in strongly imbalanced settings, the TPR is bounded away from $0$. We numerically illustrate the behaviour of the various performance metrics in <b>simulations</b> as well as on a credit default data set. We also discuss connections to the ROC and precision-recall curves and give <b>recommendations</b> on how to combine their usage with performance metrics.</p></p class="citation"></blockquote><h3 id=23--239276-diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings-julia-linhart-et-al-2024>(2/3 | 239/276) Diffusion posterior sampling for simulation-based inference in tall data settings (Julia Linhart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues. (2024)<br><strong>Diffusion posterior sampling for simulation-based inference in tall data settings</strong><br><button class=copy-to-clipboard title="Diffusion posterior sampling for simulation-based inference in tall data settings" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07593v1.pdf filename=2404.07593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Determining which parameters of a non-linear model could best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators (a.k.a. <b>black-box</b> <b>simulators).</b> The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. <b>Simulation-based</b> inference (SBI) stands out in this context by only requiring a dataset of <b>simulations</b> to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available and one wishes to leverage their shared information to better infer the parameters of the model. The method we propose is built upon recent developments from the flourishing score-based diffusion literature and allows us to estimate the tall data posterior distribution simply using information from the score network trained on individual observations. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</p></p class="citation"></blockquote><h3 id=33--240276-inferring-change-points-in-high-dimensional-linear-regression-via-approximate-message-passing-gabriel-arpino-et-al-2024>(3/3 | 240/276) Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing (Gabriel Arpino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel Arpino, Xiaoqi Liu, Ramji Venkataramanan. (2024)<br><strong>Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing</strong><br><button class=copy-to-clipboard title="Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07864v1.pdf filename=2404.07864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of localizing change points in high-dimensional linear regression. We propose an Approximate Message Passing (AMP) algorithm for estimating both the signals and the change point locations. Assuming Gaussian covariates, we give an exact asymptotic characterization of its estimation performance in the limit where the number of samples grows proportionally to the signal dimension. Our algorithm can be tailored to exploit any prior information on the signal, noise, and change points. It also enables uncertainty quantification in the form of an efficiently computable approximate posterior distribution, whose asymptotic form we characterize exactly. We validate our theory via numerical experiments, and demonstrate the favorable performance of our estimators on both synthetic data and images.</p></p class="citation"></blockquote><h2 id=mathna-3>math.NA (3)</h2><h3 id=13--241276-gan-based-iterative-motion-estimation-in-haste-mri-mathias-s-feinler-et-al-2024>(1/3 | 241/276) GAN-based iterative motion estimation in HASTE MRI (Mathias S. Feinler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mathias S. Feinler, Bernadette N. Hahn. (2024)<br><strong>GAN-based iterative motion estimation in HASTE MRI</strong><br><button class=copy-to-clipboard title="GAN-based iterative motion estimation in HASTE MRI" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65R32, 68T07, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07576v1.pdf filename=2404.07576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs),</b> and explicit motion estimation. In this paper we propose an iterative approach which uses <b>GAN</b> predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed <b>supervised</b> network on motion corrupted HASTE acquisitions of brain and abdomen.</p></p class="citation"></blockquote><h3 id=23--242276-adaptive-hyperbolic-cross-space-mapped-jacobi-method-on-unbounded-domains-with-applications-to-solving-multidimensional-spatiotemporal-integrodifferential-equations-yunhong-deng-et-al-2024>(2/3 | 242/276) Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations (Yunhong Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhong Deng, Sihong Shao, Alex Mogilner, Mingtao Xia. (2024)<br><strong>Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations</strong><br><button class=copy-to-clipboard title="Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07844v1.pdf filename=2404.07844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we develop a new adaptive hyperbolic-cross-space mapped Jacobi (AHMJ) method for solving multidimensional spatiotemporal integrodifferential equations in unbounded domains. By devising adaptive techniques for sparse mapped Jacobi spectral expansions defined in a hyperbolic cross space, our proposed AHMJ method can efficiently solve various spatiotemporal integrodifferential equations such as the anomalous <b>diffusion</b> <b>model</b> with reduced numbers of basis functions. Our analysis of the AHMJ method gives a uniform upper error bound for solving a class of spatiotemporal integrodifferential equations, leading to effective error control.</p></p class="citation"></blockquote><h3 id=33--243276-high-performance-matrix-free-unfitted-finite-element-operator-evaluation-maximilian-bergbauer-et-al-2024>(3/3 | 243/276) High-performance matrix-free unfitted finite element operator evaluation (Maximilian Bergbauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Bergbauer, Peter Munch, Wolfgang A. Wall, Martin Kronbichler. (2024)<br><strong>High-performance matrix-free unfitted finite element operator evaluation</strong><br><button class=copy-to-clipboard title="High-performance matrix-free unfitted finite element operator evaluation" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N30, 65Y20, 65Y05, 68W10, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07911v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07911v2.pdf filename=2404.07911v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unfitted finite element methods, like CutFEM, have traditionally been implemented in a matrix-based fashion, where a sparse matrix is assembled and later applied to vectors while solving the resulting linear system. With the goal of increasing performance and enabling algorithms with polynomial spaces of higher degrees, this contribution chooses a more abstract approach by matrix-free evaluation of the operator action on vectors instead. The proposed method loops over cells and locally evaluates the cell, face, and interface integrals, including the contributions from cut cells and the different means of stabilization. The main challenge is the efficient numerical evaluation of terms in the weak form with unstructured quadrature points arising from the unfitted discretization in cells cut by the interface. We present design choices and performance optimizations for tensor-product elements and demonstrate the performance by means of <b>benchmarks</b> and application examples. We demonstrate a speedup of more than one order of magnitude for the operator evaluation of a discontinuous Galerkin discretization with polynomial degree three compared to a sparse matrix-vector product and develop performance models to quantify the performance properties over a wide range of polynomial degrees.</p></p class="citation"></blockquote><h2 id=econgn-2>econ.GN (2)</h2><h3 id=12--244276-chatgpt-can-predict-the-future-when-it-tells-stories-set-in-the-future-about-the-past-van-pham-et-al-2024>(1/2 | 244/276) ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past (Van Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Van Pham, Scott Cunningham. (2024)<br><strong>ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past</strong><br><button class=copy-to-clipboard title="ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-AI, econ-GN, econ.GN, q-fin-EC<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07396v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07396v2.pdf filename=2404.07396v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates whether OpenAI&rsquo;s <b>ChatGPT-3.5</b> and <b>ChatGPT-4</b> can accurately forecast future events using two distinct <b>prompting</b> strategies. To evaluate the accuracy of the predictions, we take advantage of the fact that the training data at the time of experiment stopped at September 2021, and ask about events that happened in 2022 using <b>ChatGPT-3.5</b> and <b>ChatGPT-4.</b> We employed two <b>prompting</b> strategies: direct prediction and what we call future narratives which ask <b>ChatGPT</b> to tell fictional stories set in the future with characters that share events that have happened to them, but after <b>ChatGPT&rsquo;s</b> training data had been collected. Concentrating on events in 2022, we <b>prompted</b> <b>ChatGPT</b> to engage in storytelling, particularly within economic contexts. After analyzing 100 <b>prompts,</b> we discovered that future narrative <b>prompts</b> significantly enhanced <b>ChatGPT-4&rsquo;s</b> forecasting accuracy. This was especially evident in its predictions of major Academy Award winners as well as economic trends, the latter inferred from scenarios where the model impersonated public figures like the Federal Reserve Chair, Jerome Powell. These findings indicate that narrative <b>prompts</b> leverage the models&rsquo; capacity for hallucinatory narrative construction, facilitating more effective data synthesis and extrapolation than straightforward predictions. Our research reveals new aspects of <b>LLMs&rsquo;</b> predictive capabilities and suggests potential future applications in analytical contexts.</p></p class="citation"></blockquote><h3 id=22--245276-machine-learning-and-economic-forecasting-the-role-of-international-trade-networks-thiago-c-silva-et-al-2024>(2/2 | 245/276) Machine learning and economic forecasting: the role of international trade networks (Thiago C. Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thiago C. Silva, Paulo V. B. Wilhelm, Diego R. Amancio. (2024)<br><strong>Machine learning and economic forecasting: the role of international trade networks</strong><br><button class=copy-to-clipboard title="Machine learning and economic forecasting: the role of international trade networks" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-LG, econ-GN, econ.GN, physics-soc-ph, q-fin-EC<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08712v1.pdf filename=2404.08712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study examines the effects of de-globalization trends on international trade networks and their role in improving forecasts for economic growth. Using section-level trade data from nearly 200 countries from 2010 to 2022, we identify significant shifts in the network topology driven by rising trade policy uncertainty. Our analysis highlights key global players through centrality rankings, with the United States, China, and Germany maintaining consistent dominance. Using a horse race of <b>supervised</b> regressors, we find that network topology descriptors evaluated from section-specific trade networks substantially enhance the quality of a country&rsquo;s GDP growth forecast. We also find that non-linear models, such as Random Forest, XGBoost, and LightGBM, outperform traditional linear models used in the economics literature. Using SHAP values to interpret these non-linear model&rsquo;s predictions, we find that about half of most important features originate from the network descriptors, underscoring their vital role in refining forecasts. Moreover, this study emphasizes the significance of recent economic performance, population growth, and the primary sector&rsquo;s influence in shaping economic growth predictions, offering novel insights into the intricacies of economic growth forecasting.</p></p class="citation"></blockquote><h2 id=q-bioqm-2>q-bio.QM (2)</h2><h3 id=12--246276-pathology-genomic-fusion-via-biologically-informed-cross-modality-graph-learning-for-survival-analysis-zeyu-zhang-et-al-2024>(1/2 | 246/276) Pathology-genomic fusion via biologically informed cross-modality graph learning for survival analysis (Zeyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Zhang, Yuanshen Zhao, Jingxian Duan, Yaou Liu, Hairong Zheng, Dong Liang, Zhenyu Zhang, Zhi-Cheng Li. (2024)<br><strong>Pathology-genomic fusion via biologically informed cross-modality graph learning for survival analysis</strong><br><button class=copy-to-clipboard title="Pathology-genomic fusion via biologically informed cross-modality graph learning for survival analysis" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 21<br>Keywords: Graph, Graph Neural Network, Multi-modal, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08023v1.pdf filename=2404.08023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The diagnosis and prognosis of cancer are typically based on <b>multi-modal</b> clinical data, including histology images and genomic data, due to the complex pathogenesis and high heterogeneity. Despite the advancements in digital pathology and high-throughput genome sequencing, establishing effective <b>multi-modal</b> fusion models for survival prediction and revealing the potential association between histopathology and transcriptomics remains challenging. In this paper, we propose Pathology-Genome Heterogeneous <b>Graph</b> <b>(PGHG)</b> <b>that</b> integrates whole slide images (WSI) and bulk RNA-Seq expression data with heterogeneous <b>graph</b> <b>neural</b> <b>network</b> for cancer survival analysis. The PGHG consists of biological knowledge-guided <b>representation</b> <b>learning</b> network and pathology-genome heterogeneous <b>graph.</b> <b>The</b> <b>representation</b> <b>learning</b> network utilizes the biological prior knowledge of intra-modal and inter-modal data associations to guide the feature extraction. The node features of each modality are updated through attention-based <b>graph</b> <b>learning</b> <b>strategy.</b> Unimodal features and bi-modal fused features are extracted via attention pooling module and then used for survival prediction. We evaluate the model on low-grade gliomas, glioblastoma, and kidney renal papillary cell carcinoma datasets from the Cancer Genome Atlas (TCGA) and the First Affiliated Hospital of Zhengzhou University (FAHZU). Extensive experimental results demonstrate that the proposed method outperforms both unimodal and other <b>multi-modal</b> fusion models. For demonstrating the model interpretability, we also visualize the attention heatmap of pathological images and utilize integrated gradient algorithm to identify important tissue structure, biological pathways and key genes.</p></p class="citation"></blockquote><h3 id=22--247276-learning-chemotherapy-drug-action-via-universal-physics-informed-neural-networks-lena-podina-et-al-2024>(2/2 | 247/276) Learning Chemotherapy Drug Action via Universal Physics-Informed Neural Networks (Lena Podina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lena Podina, Ali Ghodsi, Mohammad Kohandel. (2024)<br><strong>Learning Chemotherapy Drug Action via Universal Physics-Informed Neural Networks</strong><br><button class=copy-to-clipboard title="Learning Chemotherapy Drug Action via Universal Physics-Informed Neural Networks" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, physics-chem-ph, q-bio-QM, q-bio.QM<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08019v1.pdf filename=2404.08019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantitative systems pharmacology (QSP) is widely used to assess drug effects and toxicity before the drug goes to clinical trial. However, significant manual <b>distillation</b> of the literature is needed in order to construct a QSP model. Parameters may need to be fit, and simplifying assumptions of the model need to be made. In this work, we apply Universal Physics-Informed Neural Networks (UPINNs) to learn unknown components of various differential equations that model chemotherapy pharmacodynamics. We learn three commonly employed chemotherapeutic drug actions (log-kill, Norton-Simon, and E_max) from synthetic data. Then, we use the UPINN method to fit the parameters for several synthetic datasets simultaneously. Finally, we learn the net proliferation rate in a model of doxorubicin (a chemotherapeutic) pharmacodynamics. As these are only toy examples, we highlight the usefulness of UPINNs in learning unknown terms in pharmacodynamic and pharmacokinetic models.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=14--248276-iprefer-an-intelligent-parameter-extractor-based-on-features-for-bsim-cmg-models-zhiliang-peng-et-al-2024>(1/4 | 248/276) iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models (Zhiliang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang. (2024)<br><strong>iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models</strong><br><button class=copy-to-clipboard title="iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07827v1.pdf filename=2404.07827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor&rsquo;s remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate <b>simulations</b> for semiconductor device performance prediction.</p></p class="citation"></blockquote><h3 id=24--249276-dynamic-modeling-and-simulation-of-a-flash-clay-calciner-nicola-cantisani-et-al-2024>(2/4 | 249/276) Dynamic modeling and simulation of a flash clay calciner (Nicola Cantisani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Cantisani, Jan Lorenz Svensen, Ole Fink Hansen, John Bagterp Jørgensen. (2024)<br><strong>Dynamic modeling and simulation of a flash clay calciner</strong><br><button class=copy-to-clipboard title="Dynamic modeling and simulation of a flash clay calciner" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07674v1.pdf filename=2404.07674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel dynamic model of a flash clay calciner. The model consists of thermophysical properties, reaction kinetics and stoichiometry, transport, mass and energy balances, and algebraic constraints. This gives rise to a system of partial differential-algebraic equations (PDAE). Spatial discretization is performed to convert the PDAEs into a system of differential-algebraic equations (DAE). The model can be used, for example, to perform dynamic <b>simulations</b> with changing inputs, and process design and optimization. Moreover, it can be used to develop model-based control, which is relevant for flexible operation of a clay calcination plant for green cement production.</p></p class="citation"></blockquote><h3 id=34--250276-leveraging-eclipse-mosaic-for-modeling-and-analyzing-ride-hailing-services-karl-schrab-et-al-2024>(3/4 | 250/276) Leveraging Eclipse MOSAIC for Modeling and Analyzing Ride-Hailing Services (Karl Schrab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karl Schrab, Moritz Schweppenhäuser, Robert Protzmann, Kay Massow, Ilja Radusch. (2024)<br><strong>Leveraging Eclipse MOSAIC for Modeling and Analyzing Ride-Hailing Services</strong><br><button class=copy-to-clipboard title="Leveraging Eclipse MOSAIC for Modeling and Analyzing Ride-Hailing Services" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07547v1.pdf filename=2404.07547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ride-hailing services enjoy a large popularity in the sector of individualized mobility. Due to broad availability, ease of use, and competitive pricing strategies, these services have established themselves throughout the last decades. With the increased popularity, ride-hailing providers aimed to consistently improve the efficiency of their services, leading to the inception of novel research questions. Many of which can be effectively tackled using <b>simulation.</b> In this paper, we present such a <b>simulation-based</b> approach using Eclipse MOSAIC in-hand with a large-scale traffic scenario of Berlin. We analyze real-world logbook data including detailed shifts of drivers and discuss how to integrate them with the <b>simulation</b> scenario. Moreover, we present extensions to MOSAIC required for the modeling of the ride-hailing services, utilizing the powerful Application Simulator. Accordingly, as the primary result of this paper, we managed to extend the Eclipse MOSAIC framework to be able to answer research questions in the domain of ride-hailing and ride-sharing. Additionally, in an initial exemplary study, we analyze the traffic and environmental impacts of different, yet basic, rebalancing strategies, finding non-negligible differences in mileages and pollutant emissions. We, furthermore, applied our findings to the entire ride-hailing fleet in the city of Berlin for one year, showcasing the impacts different rebalancing strategies could have on environment and general traffic. To our knowledge, the consideration of environmental factors on a city-wide scale is a novel contribution of this paper, not addressed in previous research.</p></p class="citation"></blockquote><h3 id=44--251276-saturation-informed-current-limiting-control-for-grid-forming-converters-maitraya-avadhut-desai-et-al-2024>(4/4 | 251/276) Saturation-Informed Current-Limiting Control for Grid-Forming Converters (Maitraya Avadhut Desai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maitraya Avadhut Desai, Xiuqiang He, Linbin Huang, Florian Dörfler. (2024)<br><strong>Saturation-Informed Current-Limiting Control for Grid-Forming Converters</strong><br><button class=copy-to-clipboard title="Saturation-Informed Current-Limiting Control for Grid-Forming Converters" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: DoS<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07682v1.pdf filename=2404.07682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the transient stability of a state-of-the-art grid-forming complex-droop control (i.e., dispatchable virtual oscillator control, dVOC) under current saturation. We quantify the saturation level of a converter by introducing the concept of degree of saturation <b>(DoS),</b> and we propose a provably stable current-limiting control with saturation-informed feedback, which feeds the degree of saturation back to the inner voltage-control loop and the outer grid-forming loop. As a result, although the output current is saturated, the voltage phase angle can still be generated from an internal virtual voltage-source node that is governed by an equivalent complex-droop control. We prove that the proposed control achieves transient stability during current saturation under grid faults. We also provide parametric stability conditions for multi-converter systems under grid-connected and islanded scenarios. The stability performance of the current-limiting control is validated with various case studies.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--252276-trainable-joint-channel-estimation-detection-and-decoding-for-mimo-urllc-systems-yi-sun-et-al-2024>(1/2 | 252/276) Trainable Joint Channel Estimation, Detection and Decoding for MIMO URLLC Systems (Yi Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Sun, Hong Shen, Bingqing Li, Wei Xu, Pengcheng Zhu, Nan Hu, Chunming Zhao. (2024)<br><strong>Trainable Joint Channel Estimation, Detection and Decoding for MIMO URLLC Systems</strong><br><button class=copy-to-clipboard title="Trainable Joint Channel Estimation, Detection and Decoding for MIMO URLLC Systems" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07721v1.pdf filename=2404.07721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The receiver design for multi-input multi-output (MIMO) ultra-reliable and low-latency communication (URLLC) systems can be a tough task due to the use of short channel codes and few pilot symbols. Consequently, error propagation can occur in traditional turbo receivers, leading to performance degradation. Moreover, the processing delay induced by information exchange between different modules may also be undesirable for URLLC. To address the issues, we advocate to perform joint channel estimation, detection, and decoding (JCDD) for MIMO URLLC systems encoded by short low-density parity-check (LDPC) codes. Specifically, we develop two novel JCDD problem formulations based on the maximum a posteriori (MAP) criterion for Gaussian MIMO channels and sparse mmWave MIMO channels, respectively, which integrate the pilots, the bit-to-symbol mapping, the LDPC code constraints, as well as the channel statistical information. Both the challenging large-scale non-convex problems are then solved based on the alternating direction method of multipliers (ADMM) algorithms, where closed-form solutions are achieved in each ADMM iteration. Furthermore, two JCDD neural networks, called JCDDNet-G and JCDDNet-S, are built by unfolding the derived ADMM algorithms and introducing trainable parameters. It is interesting to find via <b>simulations</b> that the proposed trainable JCDD receivers can outperform the turbo receivers with affordable computational complexities.</p></p class="citation"></blockquote><h3 id=22--253276-precoder-design-for-user-centric-network-massive-mimo-with-matrix-manifold-optimization-rui-sun-et-al-2024>(2/2 | 253/276) Precoder Design for User-Centric Network Massive MIMO with Matrix Manifold Optimization (Rui Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Sun, Li You, An-An Lu, Chen Sun, Xiqi Gao, Xiang-Gen Xia. (2024)<br><strong>Precoder Design for User-Centric Network Massive MIMO with Matrix Manifold Optimization</strong><br><button class=copy-to-clipboard title="Precoder Design for User-Centric Network Massive MIMO with Matrix Manifold Optimization" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07425v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07425v1.pdf filename=2404.07425v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the precoder design for user-centric network (UCN) massive multiple-input multiple-output (mMIMO) downlink with matrix manifold optimization. In UCN mMIMO systems, each user terminal (UT) is served by a subset of base stations (BSs) instead of all the BSs, facilitating the implementation of the system and lowering the dimension of the precoders to be designed. By proving that the precoder set satisfying the per-BS power constraints forms a Riemannian submanifold of a linear product manifold, we transform the constrained precoder design problem in Euclidean space to an unconstrained one on the Riemannian submanifold. Riemannian ingredients, including orthogonal projection, Riemannian gradient, retraction and vector transport, of the problem on the Riemannian submanifold are further derived, with which the Riemannian conjugate gradient (RCG) design method is proposed for solving the unconstrained problem. The proposed method avoids the inverses of large dimensional matrices, which is beneficial in practice. The complexity analyses show the high computational efficiency of RCG precoder design. <b>Simulation</b> results demonstrate the numerical superiority of the proposed precoder design and the high efficiency of the UCN mMIMO system.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--254276-interactive-ontology-matching-with-cost-efficient-learning-bin-cheng-et-al-2024>(1/1 | 254/276) Interactive Ontology Matching with Cost-Efficient Learning (Bin Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Cheng, Jonathan Fürst, Tobias Jacobs, Celia Garrido-Hidalgo. (2024)<br><strong>Interactive Ontology Matching with Cost-Efficient Learning</strong><br><button class=copy-to-clipboard title="Interactive Ontology Matching with Cost-Efficient Learning" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-LG, cs.DB<br>Keyword Score: 20<br>Keywords: Active Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07663v1.pdf filename=2404.07663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of high-quality ontologies is crucial for data integration and knowledge-based <b>reasoning,</b> specifically in the context of the rising data economy. However, automatic ontology matchers are often bound to the heuristics they are based on, leaving many matches unidentified. Interactive ontology matching systems involving human experts have been introduced, but they do not solve the fundamental issue of flexibly finding additional matches outside the scope of the implemented heuristics, even though this is highly demanded in industrial settings. <b>Active</b> <b>machine</b> learning methods appear to be a promising path towards a flexible interactive ontology matcher. However, off-the-shelf <b>active</b> <b>learning</b> mechanisms suffer from low query efficiency due to extreme class imbalance, resulting in a last-mile problem where high human effort is required to identify the remaining matches. To address the last-mile problem, this work introduces DualLoop, an <b>active</b> <b>learning</b> method tailored to ontology matching. DualLoop offers three main contributions: (1) an ensemble of tunable heuristic matchers, (2) a short-term learner with a novel query strategy adapted to highly imbalanced data, and (3) long-term learners to explore potential matches by creating and tuning new heuristics. We evaluated DualLoop on three datasets of varying sizes and domains. Compared to existing <b>active</b> <b>learning</b> methods, we consistently achieved better F1 scores and recall, reducing the expected query cost spent on finding 90% of all matches by over 50%. Compared to traditional interactive ontology matchers, we are able to find additional, last-mile matches. Finally, we detail the successful deployment of our approach within an actual product and report its operational performance results within the Architecture, Engineering, and Construction (AEC) industry sector, showcasing its practical value and efficiency.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--255276-r2-indicator-and-deep-reinforcement-learning-enhanced-adaptive-multi-objective-evolutionary-algorithm-farajollah-tahernezhad-javazm-et-al-2024>(1/2 | 255/276) R2 Indicator and Deep Reinforcement Learning Enhanced Adaptive Multi-Objective Evolutionary Algorithm (Farajollah Tahernezhad-Javazm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farajollah Tahernezhad-Javazm, Debbie Rankin, Naomi Du Bois, Alice E. Smith, Damien Coyle. (2024)<br><strong>R2 Indicator and Deep Reinforcement Learning Enhanced Adaptive Multi-Objective Evolutionary Algorithm</strong><br><button class=copy-to-clipboard title="R2 Indicator and Deep Reinforcement Learning Enhanced Adaptive Multi-Objective Evolutionary Algorithm" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08161v1.pdf filename=2404.08161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Choosing an appropriate optimization algorithm is essential to achieving success in optimization challenges. Here we present a new evolutionary algorithm structure that utilizes a <b>reinforcement</b> <b>learning-based</b> agent aimed at addressing these issues. The agent employs a double deep q-network to choose a specific evolutionary operator based on feedback it receives from the environment during optimization. The algorithm&rsquo;s structure contains five single-objective evolutionary algorithm operators. This single-objective structure is transformed into a multi-objective one using the R2 indicator. This indicator serves two purposes within our structure: first, it renders the algorithm multi-objective, and second, provides a means to evaluate each algorithm&rsquo;s performance in each generation to facilitate constructing the <b>reinforcement</b> <b>learning-based</b> reward function. The proposed R2-reinforcement learning multi-objective evolutionary algorithm (R2-RLMOEA) is compared with six other multi-objective algorithms that are based on R2 indicators. These six algorithms include the operators used in R2-RLMOEA as well as an R2 indicator-based algorithm that randomly selects operators during optimization. We <b>benchmark</b> performance using the CEC09 functions, with performance measured by inverted generational distance and spacing. The R2-RLMOEA algorithm outperforms all other algorithms with strong statistical significance (p&lt;0.001) when compared with the average spacing metric across all ten <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=22--256276-impact-of-training-instance-selection-on-automated-algorithm-selection-models-for-numerical-black-box-optimization-konstantin-dietrich-et-al-2024>(2/2 | 256/276) Impact of Training Instance Selection on Automated Algorithm Selection Models for Numerical Black-box Optimization (Konstantin Dietrich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konstantin Dietrich, Diederick Vermetten, Carola Doerr, Pascal Kerschke. (2024)<br><strong>Impact of Training Instance Selection on Automated Algorithm Selection Models for Numerical Black-box Optimization</strong><br><button class=copy-to-clipboard title="Impact of Training Instance Selection on Automated Algorithm Selection Models for Numerical Black-box Optimization" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 8<br>Keywords: Benchmarking, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07539v1.pdf filename=2404.07539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently proposed MA-BBOB function generator provides a way to create numerical <b>black-box</b> <b>benchmark</b> problems based on the well-established BBOB suite. Initial studies on this generator highlighted its ability to smoothly transition between the component functions, both from a low-level landscape feature perspective, as well as with regard to algorithm performance. This suggests that MA-BBOB-generated functions can be an ideal testbed for automated machine learning methods, such as automated algorithm selection (AAS). In this paper, we generate 11800 functions in dimensions $d=2$ and $d=5$, respectively, and analyze the potential gains from AAS by studying performance complementarity within a set of eight algorithms. We combine this performance data with exploratory landscape features to create an AAS pipeline that we use to investigate how to efficiently select training sets within this space. We show that simply using the BBOB component functions for training yields poor test performance, while the ranking between uniformly chosen and diversity-based training sets strongly depends on the distribution of the test set.</p></p class="citation"></blockquote><h2 id=cspl-2>cs.PL (2)</h2><h3 id=12--257276-kestrel-relational-verification-using-e-graphs-for-program-alignment-robert-dickerson-et-al-2024>(1/2 | 257/276) KestRel: Relational Verification Using E-Graphs for Program Alignment (Robert Dickerson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Dickerson, Prasita Mukherjee, Benjamin Delaware. (2024)<br><strong>KestRel: Relational Verification Using E-Graphs for Program Alignment</strong><br><button class=copy-to-clipboard title="KestRel: Relational Verification Using E-Graphs for Program Alignment" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08106v1.pdf filename=2404.08106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many interesting program properties involve the execution of multiple programs, including observational equivalence, noninterference, co-termination, monotonicity, and idempotency. One popular approach to <b>reasoning</b> about these sorts of relational properties is to construct and verify a product program: a program whose correctness implies that the individual programs exhibit the desired relational property. A key challenge in product program construction is finding a good alignment of the original programs. An alignment puts subparts of the original programs into correspondence so that their similarities can be exploited in order to simplify verification. We propose an approach to product program construction that uses e-graphs, equality saturation, and algebraic realignment rules to efficiently represent and build verifiable product programs. A key ingredient of our solution is a novel data-driven extraction technique that uses execution traces of product programs to identify candidate solutions that are semantically well-aligned. We have implemented a relational verification engine based on our proposed approach, called KestRel, and use it to evaluate our approach over a suite of <b>benchmarks</b> taken from the relational verification literature.</p></p class="citation"></blockquote><h3 id=22--258276-vicar-visualizing-categories-with-automated-rewriting-in-coq-bhakti-shah-et-al-2024>(2/2 | 258/276) ViCAR: Visualizing Categories with Automated Rewriting in Coq (Bhakti Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhakti Shah, William Spencer, Laura Zielinski, Ben Caldwell, Adrian Lehmann, Robert Rand. (2024)<br><strong>ViCAR: Visualizing Categories with Automated Rewriting in Coq</strong><br><button class=copy-to-clipboard title="ViCAR: Visualizing Categories with Automated Rewriting in Coq" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL, math-CT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08163v1.pdf filename=2404.08163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ViCAR, a library for working with monoidal categories in the Coq proof assistant. ViCAR provides definitions for categorical structures that users can instantiate with their own verification projects. Upon verifying relevant coherence conditions, ViCAR gives a set of lemmas and tactics for manipulating categorical structures. We also provide a visualizer that can display any composition and tensor product of morphisms as a string diagram, showing its categorical structure. This enables graphical <b>reasoning</b> and automated rewriting for Coq projects with monoidal structures.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--259276-diagram-analysis-of-iterative-algorithms-chris-jones-et-al-2024>(1/1 | 259/276) Diagram Analysis of Iterative Algorithms (Chris Jones et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Jones, Lucas Pesenti. (2024)<br><strong>Diagram Analysis of Iterative Algorithms</strong><br><button class=copy-to-clipboard title="Diagram Analysis of Iterative Algorithms" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DS, cs.CC, math-CO<br>Keyword Score: 13<br>Keywords: Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07881v1.pdf filename=2404.07881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small <b>graph,</b> and the operations of the algorithm correspond to simple combinatorial operations on these <b>graphs.</b> We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors. The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by &ldquo;implementing&rdquo; heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP. These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--260276-linguaquanta-towards-a-quantum-transpiler-between-openqasm-and-quipper-extended-scott-wesley-2024>(1/1 | 260/276) LinguaQuanta: Towards a Quantum Transpiler Between OpenQASM and Quipper (Extended) (Scott Wesley, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scott Wesley. (2024)<br><strong>LinguaQuanta: Towards a Quantum Transpiler Between OpenQASM and Quipper (Extended)</strong><br><button class=copy-to-clipboard title="LinguaQuanta: Towards a Quantum Transpiler Between OpenQASM and Quipper (Extended)" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-SE, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08147v1.pdf filename=2404.08147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As quantum computing evolves, many important questions emerge, such as how best to represent quantum programs, and how to promote interoperability between quantum program analysis tools. These questions arise naturally in the design of quantum transpilers, which translate between quantum programming languages. In this paper, we take a step towards answering these questions by identifying challenges and best practices in quantum transpiler design. We base these <b>recommendations</b> on our experience designing LinguaQuanta, a quantum transpiler between Quipper and OpenQASM. First, we provide categorical specifications for quantum transpilers, which aim to encapsulate the core principles of the UNIX philosophy. We then identify quantum circuit decompositions which we expect to be useful in quantum transpilation. With these foundations in place, we then discuss challenges faced during the implementation of LinguaQuanta, such as ancilla management and stability under round translation. To show that LinguaQuanta works in practice, a short tutorial is given for the example of quantum phase estimation. We conclude with <b>recommendations</b> for the future of LinguaQuanta, and for quantum software development tools more broadly.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--261276-the-impact-of-speech-anonymization-on-pathology-and-its-limits-soroosh-tayebi-arasteh-et-al-2024>(1/1 | 261/276) The Impact of Speech Anonymization on Pathology and Its Limits (Soroosh Tayebi Arasteh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroosh Tayebi Arasteh, Tomas Arias-Vergara, Paula Andrea Perez-Toro, Tobias Weise, Kai Packhaeuser, Maria Schuster, Elmar Noeth, Andreas Maier, Seung Hee Yang. (2024)<br><strong>The Impact of Speech Anonymization on Pathology and Its Limits</strong><br><button class=copy-to-clipboard title="The Impact of Speech Anonymization on Pathology and Its Limits" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CR, cs-LG, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08064v1.pdf filename=2404.08064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integration of speech into healthcare has intensified privacy concerns due to its potential as a non-invasive biomarker containing individual biometric information. In response, speaker anonymization aims to conceal personally identifiable information while retaining crucial linguistic content. However, the application of anonymization techniques to pathological speech, a critical area where privacy is especially vital, has not been extensively examined. This study investigates anonymization&rsquo;s impact on pathological speech across over 2,700 speakers from multiple German institutions, focusing on privacy, pathological utility, and demographic <b>fairness.</b> We explore both training-based and signal processing-based anonymization methods, and document substantial privacy improvements across disorders-evidenced by equal error rate increases up to 1933%, with minimal overall impact on utility. Specific disorders such as Dysarthria, Dysphonia, and Cleft Lip and Palate experienced minimal utility changes, while Dysglossia showed slight improvements. Our findings underscore that the impact of anonymization varies substantially across different disorders. This necessitates disorder-specific anonymization strategies to optimally balance privacy with diagnostic utility. Additionally, our <b>fairness</b> analysis revealed consistent anonymization effects across most of the demographics. This study demonstrates the effectiveness of anonymization in pathological speech for enhancing privacy, while also highlighting the importance of customized approaches to account for inversion attacks.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--262276-auditing-health-related-recommendations-in-social-media-a-case-study-of-abortion-on-youtube-mohammed-lahsaini-et-al-2024>(1/1 | 262/276) Auditing health-related recommendations in social media: A Case Study of Abortion on YouTube (Mohammed Lahsaini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Lahsaini, Mohamed Lechiakh, Alexandre Maurer. (2024)<br><strong>Auditing health-related recommendations in social media: A Case Study of Abortion on YouTube</strong><br><button class=copy-to-clipboard title="Auditing health-related recommendations in social media: A Case Study of Abortion on YouTube" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-IR, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07896v1.pdf filename=2404.07896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> algorithms (RS) used by social media, like YouTube, significantly shape our information consumption across various domains, especially in healthcare. Hence, algorithmic auditing becomes crucial to uncover their potential bias and misinformation, particularly in the context of controversial topics like abortion. We introduce a simple yet effective sock puppet auditing approach to investigate how YouTube recommends abortion-related videos to individuals with different backgrounds. Our framework allows for efficient auditing of RS, regardless of the complexity of the underlying algorithms</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--263276-learning-deterministic-multi-clock-timed-automata-yu-teng-et-al-2024>(1/1 | 263/276) Learning Deterministic Multi-Clock Timed Automata (Yu Teng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Teng, Miaomiao Zhang, Jie An. (2024)<br><strong>Learning Deterministic Multi-Clock Timed Automata</strong><br><button class=copy-to-clipboard title="Learning Deterministic Multi-Clock Timed Automata" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs.FL<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07823v1.pdf filename=2404.07823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an algorithm for <b>active</b> <b>learning</b> of deterministic timed automata with multiple clocks. The algorithm is within the querying framework of Angluin&rsquo;s $L^*$ algorithm and follows the idea proposed in existing work on the <b>active</b> <b>learning</b> of deterministic one-clock timed automata. We introduce an equivalence relation over the reset-clocked language of a timed automaton and then transform the learning problem into learning the corresponding reset-clocked language of the target automaton. Since a reset-clocked language includes the clock reset information which is not observable, we first present the approach of learning from a powerful teacher who can provide reset information by answering reset information queries from the learner. Then we extend the algorithm in a normal teacher situation in which the learner can only ask standard membership query and equivalence query while the learner guesses the reset information. We prove that the learning algorithm terminates and returns a correct deterministic timed automaton. Due to the need of guessing whether the clocks reset at the transitions, the algorithm is of exponential complexity in the size of the target automaton.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--264276-an-efficient-uniqueness-theorem-for-overcomplete-tensor-decomposition-pascal-koiran-2024>(1/3 | 264/276) An efficient uniqueness theorem for overcomplete tensor decomposition (Pascal Koiran, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pascal Koiran. (2024)<br><strong>An efficient uniqueness theorem for overcomplete tensor decomposition</strong><br><button class=copy-to-clipboard title="An efficient uniqueness theorem for overcomplete tensor decomposition" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 10<br>Keywords: Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07801v1.pdf filename=2404.07801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give a new, constructive uniqueness theorem for <b>tensor</b> <b>decomposition.</b> It applies to order 3 <b>tensors</b> <b>of</b> format $n \times n \times p$ and can prove uniqueness of decomposition for generic <b>tensors</b> <b>up</b> to rank $r=4n/3$ as soon as $p \geq 4$. One major advantage over Kruskal&rsquo;s uniqueness theorem is that our theorem has an algorithmic proof, and the resulting algorithm is efficient. Like the uniqueness theorem, it applies in the range $n \leq r \leq 4n/3$. As a result, we obtain the first efficient algorithm for overcomplete decomposition of generic <b>tensors</b> <b>of</b> order 3. For instance, prior to this work it was not known how to efficiently decompose generic <b>tensors</b> <b>of</b> format $n \times n \times n$ and rank $r=1.01n$ (or rank $r \leq (1+\epsilon) n$, for some constant $\epsilon >0$). Efficient overcomplete decomposition of generic <b>tensors</b> <b>of</b> format $n \times n \times 3$ remains an open problem. Our results are based on the method of commuting extensions pioneered by Strassen for the proof of his $3n/2$ lower bound on <b>tensor</b> <b>rank</b> and border rank. In particular, we rely on an algorithm for the computation of commuting extensions recently proposed in a companion paper, and on the classical diagonalization-based &ldquo;Jennrich algorithm&rdquo; for undercomplete <b>tensor</b> <b>decomposition.</b></p></p class="citation"></blockquote><h3 id=23--265276-an-improvement-of-degree-based-hashing-dbh-graph-partition-method-using-a-novel-metric-anna-mastikhina-et-al-2024>(2/3 | 265/276) An improvement of degree-based hashing (DBH) graph partition method, using a novel metric (Anna Mastikhina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Mastikhina, Oleg Senkevich, Dmitry Sirotkin, Danila Demin, Stanislav Moiseev. (2024)<br><strong>An improvement of degree-based hashing (DBH) graph partition method, using a novel metric</strong><br><button class=copy-to-clipboard title="An improvement of degree-based hashing (DBH) graph partition method, using a novel metric" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07624v1.pdf filename=2404.07624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper examines the <b>graph</b> partition problem and introduces a new metric, MSIDS (maximal sum of inner degrees squared). We establish its connection to the replication factor (RF) optimization, which has been the main focus of theoretical work in this field. Additionally, we propose a new partition algorithm, DBH-X, based on the DBH partitioner. We demonstrate that DBH-X significantly improves both the RF and MSIDS, compared to the baseline DBH algorithm. In addition, we provide test results that show the runtime acceleration of GraphX-based PageRank and Label propagation algorithms.</p></p class="citation"></blockquote><h3 id=33--266276-parameterized-complexity-of-submodular-minimization-under-uncertainty-naonori-kakimura-et-al-2024>(3/3 | 266/276) Parameterized Complexity of Submodular Minimization under Uncertainty (Naonori Kakimura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naonori Kakimura, Ildikó Schlotter. (2024)<br><strong>Parameterized Complexity of Submodular Minimization under Uncertainty</strong><br><button class=copy-to-clipboard title="Parameterized Complexity of Submodular Minimization under Uncertainty" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07516v1.pdf filename=2404.07516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the computational complexity of a robust variant of a two-stage submodular minimization problem that we call Robust Submodular Minimizer. In this problem, we are given $k$ submodular functions $f_1,\dots,f_k$ over a set family $2^V$, which represent $k$ possible scenarios in the future when we will need to find an optimal solution for one of these scenarios, i.e., a minimizer for one of the functions. The present task is to find a set $X \subseteq V$ that is close to some optimal solution for each $f_i$ in the sense that some minimizer of $f_i$ can be obtained from $X$ by adding/removing at most $d$ elements for a given integer $d$. The main contribution of this paper is to provide a complete computational map of this problem with respect to parameters $k$ and $d$, which reveals a tight complexity threshold for both parameters: (1) Robust Submodular Minimizer can be solved in polynomial time when $k \leq 2$, but is NP-hard if $k$ is a constant with $k \geq 3$. (2) Robust Submodular Minimizer can be solved in polynomial time when $d=0$, but is NP-hard if $d$ is a constant with $d \geq 1$. (3) Robust Submodular Minimizer is fixed-parameter tractable when parameterized by $(k,d)$. We also show that if some submodular function $f_i$ has a polynomial number of minimizers, then the problem becomes fixed-parameter tractable when parameterized by $d$. We remark that all our hardness results hold even if each submodular function is given by a cut function of a directed <b>graph.</b></p></p class="citation"></blockquote><h2 id=mathoc-2>math.OC (2)</h2><h3 id=12--267276-a-continuous-time-violation-free-multi-agent-optimization-algorithm-and-its-applications-to-safe-distributed-control-xiao-tan-et-al-2024>(1/2 | 267/276) A continuous-time violation-free multi-agent optimization algorithm and its applications to safe distributed control (Xiao Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Tan, Changxin Liu, Karl H. Johansson, Dimos V. Dimarogonas. (2024)<br><strong>A continuous-time violation-free multi-agent optimization algorithm and its applications to safe distributed control</strong><br><button class=copy-to-clipboard title="A continuous-time violation-free multi-agent optimization algorithm and its applications to safe distributed control" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07571v1.pdf filename=2404.07571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose a <b>continuous-time</b> <b>distributed</b> optimization algorithm with guaranteed zero coupling constraint violation and apply it to safe distributed control in the presence of multiple control barrier functions (CBF). The optimization problem is defined over a network that collectively minimizes a separable cost function with coupled linear constraints. An equivalent optimization problem with auxiliary decision variables and a decoupling structure is proposed. A sensitivity analysis demonstrates that the subgradient information can be computed using local information. This then leads to a subgradient algorithm for updating the auxiliary variables. A case with sparse coupling constraints is further considered, and it is shown to have better memory and communication efficiency. For the specific case of a CBF-induced time-varying quadratic program (QP), an update law is proposed that achieves finite-time convergence. Numerical results involving a static resource allocation problem and a safe coordination problem for a multi-agent system demonstrate the efficiency and effectiveness of our proposed algorithms.</p></p class="citation"></blockquote><h3 id=22--268276-equitable-routing---rethinking-the-multiple-traveling-salesman-problem-abhay-singh-bhadoriya-et-al-2024>(2/2 | 268/276) Equitable Routing - Rethinking the Multiple Traveling Salesman Problem (Abhay Singh Bhadoriya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhay Singh Bhadoriya, Deepjyoti Deka, Kaarthik Sundar. (2024)<br><strong>Equitable Routing - Rethinking the Multiple Traveling Salesman Problem</strong><br><button class=copy-to-clipboard title="Equitable Routing - Rethinking the Multiple Traveling Salesman Problem" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 90B06, 90C27, 90C57, 90C90, cs-RO, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08157v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08157v3.pdf filename=2404.08157v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Multiple Traveling Salesman Problem (MTSP) with a single depot is a generalization of the well-known Traveling Salesman Problem (TSP) that involves an additional parameter, namely, the number of salesmen. In the MTSP, several salesmen at the depot need to visit a set of interconnected targets, such that each target is visited precisely once by at most one salesman while minimizing the total length of their tours. An equally important variant of the MTSP, the min-max MTSP, aims to distribute the workload (length of the individual tours) among salesmen by requiring the longest tour of all the salesmen to be as short as possible, i.e., minimizing the maximum tour length among all salesmen. The min-max MTSP appears in real-life applications to ensure a good balance of workloads for the salesmen. It is known in the literature that the min-max MTSP is notoriously difficult to solve to optimality due to the poor lower bounds its linear relaxations provide. In this paper, we formulate two novel parametric variants of the MTSP called the &ldquo;fair-MTSP&rdquo;. One variant is formulated as a Mixed-Integer Second Order Cone Program (MISOCP), and the other as a Mixed Integer Linear Program (MILP). Both focus on enforcing the workloads for the salesmen to be equitable, i.e., the distribution of tour lengths for the salesmen to be fair while minimizing the total cost of their tours. We present algorithms to solve the two variants of the fair-MTSP to global optimality and computational results on <b>benchmark</b> and real-world test instances that make a case for fair-MTSP as a viable alternative to the min-max MTSP.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--269276-goppa-codes-key-to-high-efficiency-and-reliability-in-communications-behrooz-mosallaei-et-al-2024>(1/2 | 269/276) Goppa Codes: Key to High Efficiency and Reliability in Communications (Behrooz Mosallaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Behrooz Mosallaei, Sepideh Farivar, Vahid Nourozi. (2024)<br><strong>Goppa Codes: Key to High Efficiency and Reliability in Communications</strong><br><button class=copy-to-clipboard title="Goppa Codes: Key to High Efficiency and Reliability in Communications" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-AG, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.08132v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.08132v2.pdf filename=2404.08132v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study some codes of algebraic <b>geometry</b> related to certain maximal curves. Quantum stabilizer codes obtained through the self orthogonality of Hermitian codes of this error correcting do not always have good parameters. However, appropriate parameters found that the Hermitian self-orthogonal code quantum stabilizer code has good parameters. Therefore, we investigated the quantum stabilizer code at a certain maximum curve and modified its parameters. Algebraic <b>geometry</b> codes show promise for enabling high data rate transmission over noisy power line communication channels.</p></p class="citation"></blockquote><h3 id=22--270276-ris-assisted-otfs-communications-phase-configuration-via-received-energy-maximization-mohamad-h-dinan-et-al-2024>(2/2 | 270/276) RIS-Assisted OTFS Communications: Phase Configuration via Received Energy Maximization (Mohamad H. Dinan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamad H. Dinan, Arman Farhang. (2024)<br><strong>RIS-Assisted OTFS Communications: Phase Configuration via Received Energy Maximization</strong><br><button class=copy-to-clipboard title="RIS-Assisted OTFS Communications: Phase Configuration via Received Energy Maximization" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07759v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07759v2.pdf filename=2404.07759v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the integration of two revolutionary technologies, reconfigurable intelligent surfaces (RISs) and orthogonal time frequency space (OTFS) modulation, to enhance high-speed wireless communications. We introduce a novel phase shift design algorithm for RIS-assisted OTFS, optimizing energy reception and channel gain in dynamic environments. The study evaluates the proposed approach in a downlink scenario, demonstrating significant performance improvements compared to <b>benchmark</b> schemes in the literature, particularly in terms of bit error rate (BER). Our results showcase the potential of RIS to enhance the system&rsquo;s performance. Specifically, our proposed phase shift design technique outperforms the <b>benchmark</b> solutions by over 4 dB. Furthermore, even greater gains can be obtained as the number of RIS elements increases.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--271276-q-itags-quality-optimized-spatio-temporal-heterogeneous-task-allocation-with-a-time-budget-glen-neville-et-al-2024>(1/1 | 271/276) Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget (Glen Neville et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Glen Neville, Jiazhen Liu, Sonia Chernova, Harish Ravichandar. (2024)<br><strong>Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget</strong><br><button class=copy-to-clipboard title="Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07902v1.pdf filename=2404.07902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex multi-objective missions require the coordination of heterogeneous robots at multiple inter-connected levels, such as coalition formation, scheduling, and motion planning. The associated challenges are exacerbated when solutions to these interconnected problems need to both maximize task performance and respect practical constraints on time and resources. In this work, we formulate a new class of spatio-temporal heterogeneous task allocation problems that consider these complexities. We contribute a novel framework, named Quality-Optimized Incremental Task Allocation <b>Graph</b> Search (Q-ITAGS), to solve such problems. Q-ITAGS builds upon our prior work in trait-based coordination and offers a flexible interleaved framework that i) explicitly models and optimizes the effect of collective capabilities on task performance via learnable trait-quality maps, and ii) respects both resource constraints and spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). In addition to algorithmic contributions, we derive theoretical suboptimality bounds in terms of task performance that varies as a function of a single hyperparameter. Our detailed experiments involving a simulated emergency response task and a real-world video game dataset reveal that i) Q-ITAGS results in superior team performance compared to a state-of-the-art method, while also respecting complex spatio-temporal and resource constraints, ii) Q-ITAGS efficiently learns trait-quality maps to enable effective trade-off between task performance and resource constraints, and iii) Q-ITAGS&rsquo; suboptimality bounds consistently hold in practice.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--272276-reflexive-graph-lenses-in-univalent-foundations-jonathan-sterling-2024>(1/1 | 272/276) Reflexive graph lenses in univalent foundations (Jonathan Sterling, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Sterling. (2024)<br><strong>Reflexive graph lenses in univalent foundations</strong><br><button class=copy-to-clipboard title="Reflexive graph lenses in univalent foundations" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO, math-CT, math-LO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07854v1.pdf filename=2404.07854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Martin-L"of&rsquo;s identity types provide a generic (albeit opaque) notion of identification or &ldquo;equality&rdquo; between any two elements of the same type, embodied in a canonical reflexive <b>graph</b> structure $(=_A, \mathbf{refl})$ on any type $A$. The miracle of Voevodsky&rsquo;s univalence principle is that it ensures, for essentially any naturally occurring structure in mathematics, that this the resultant notion of identification is equivalent to the type of isomorphisms in the category of such structures. Characterisations of this kind are not automatic and must be established one-by-one; to this end, several authors have employed reflexive <b>graphs</b> and displayed reflexive <b>graphs</b> to organise the characterisation of identity types. We contribute reflexive <b>graph</b> lenses, a new family of intermediate abstractions lying between families of reflexive <b>graphs</b> and displayed reflexive <b>graphs</b> that simplifies the characterisation of identity types for complex structures. Every reflexive <b>graph</b> lens gives rise to a (more complicated) displayed reflexive <b>graph,</b> and our experience suggests that many naturally occurring displayed reflexive <b>graphs</b> arise in this way. Evidence for the utility of reflexive <b>graph</b> lenses is given by means of several case studies, including the theory of reflexive <b>graphs</b> itself as well as that of polynomial type operators. Finally, we exhibit an equivalence between the type of reflexive <b>graph</b> fibrations and the type of univalent reflexive <b>graph</b> lenses.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--273276-beyond-recognizing-well-covered-graphs-carl-feghali-et-al-2024>(1/1 | 273/276) Beyond recognizing well-covered graphs (Carl Feghali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carl Feghali, Malory Marin, Rémi Watrigant. (2024)<br><strong>Beyond recognizing well-covered graphs</strong><br><button class=copy-to-clipboard title="Beyond recognizing well-covered graphs" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07853v1.pdf filename=2404.07853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We prove a number of results related to the computational complexity of recognizing well-covered <b>graphs.</b> Let $k$ and $s$ be positive integers and let $G$ be a <b>graph.</b> Then $G$ is said - $\mathbf{W_k}$ if for any $k$ pairwise disjoint independent vertex sets $A_1, \dots, A_k$ in $G$, there exist $k$ pairwise disjoint maximum independent sets $S_1, \dots,S_k$ in $G$ such that $A_i \subseteq S_i$ for $i \in [k]$. - $\mathbf{E_s}$ if every independent set in $G$ of size at most $s$ is contained in a maximum independent set in $G$. Chv'atal and Slater (1993) and Sankaranarayana and Stewart (1992) famously showed that recognizing $\mathbf{W_1}$ <b>graphs</b> or, equivalently, well-covered <b>graphs</b> is coNP-complete. We extend this result by showing that recognizing $\mathbf{W_{k+1}}$ <b>graphs</b> in either $\mathbf{W_k}$ or $\mathbf{E_s}$ <b>graphs</b> is coNP-complete. This answers a question of Levit and Tankus (2023) and strengthens a theorem of Feghali and Marin (2024). We also show that recognizing $\mathbf{E_{s+1}}$ <b>graphs</b> is $\Theta_2^p$-complete even in $\mathbf{E_s}$ <b>graphs,</b> where $\Theta_2^p = \text{P}^{\text{NP}[\log]}$ is the class of problems solvable in polynomial time using a logarithmic number of calls to a SAT oracle. This strengthens a theorem of Berg'e, Busson, Feghali and Watrigant (2023). We also obtain the complete picture of the complexity of recognizing chordal $\mathbf{W_k}$ and $\mathbf{E_s}$ <b>graphs</b> which, in particular, simplifies and generalizes a result of Dettlaff, Henning and Topp (2023).</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--274276-integrating-on-demand-ride-sharing-with-mass-transit-at-scale-danushka-edirimanna-et-al-2024>(1/1 | 274/276) Integrating On-demand Ride-sharing with Mass Transit at-Scale (Danushka Edirimanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danushka Edirimanna, Hins Hu, Samitha Samaranayake. (2024)<br><strong>Integrating On-demand Ride-sharing with Mass Transit at-Scale</strong><br><button class=copy-to-clipboard title="Integrating On-demand Ride-sharing with Mass Transit at-Scale" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07691v1.pdf filename=2404.07691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We are in the midst of a technology-driven transformation of the urban mobility landscape. However, unfortunately these new innovations are still dominated by car-centric personal mobility, which leads to concerns such as environmental sustainability, congestion, and equity. On the other hand, mass transit provides a means to move large amounts of travelers very efficiently, but is not very versatile and depends on an adequate concentration of demand. In this context, our overarching goal is to explore opportunities for new technologies such as ride-sharing to integrate with mass transit and provide a better service. More specifically, we envision a hybrid system that uses on-demand shuttles in conjunction with mass transit to move passengers efficiently, and provide an algorithmic framework for operational optimization. Our approach extends a state-of-the-art trip-vehicle assignment model to the <b>multi-modal</b> setting, where we develop a new integer-linear programming formulation to solve the problem efficiently. A comprehensive study covering five major cities in the United States based on real-world data is carried out to verify the advantages of such a system and the effectiveness of our algorithms. We show that our hybrid system provides significant improvements in comparison to a purely on-demand model by exploiting the efficiencies of the mass transit system.</p></p class="citation"></blockquote><h2 id=mathpr-1>math.PR (1)</h2><h3 id=11--275276-glauber-dynamics-for-the-hard-core-model-on-bounded-degree-h-free-graphs-mark-jerrum-2024>(1/1 | 275/276) Glauber dynamics for the hard-core model on bounded-degree $H$-free graphs (Mark Jerrum, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Jerrum. (2024)<br><strong>Glauber dynamics for the hard-core model on bounded-degree $H$-free graphs</strong><br><button class=copy-to-clipboard title="Glauber dynamics for the hard-core model on bounded-degree $H$-free graphs" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.PR<br>Categories: 05C69 (Primary) 60J10, 68W20, 82B20 (Secondary), F-2-2, cs-DS, math-CO, math-PR, math.PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07615v1.pdf filename=2404.07615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The hard-core model has as its configurations the independent sets of some <b>graph</b> instance $G$. The probability distribution on independent sets is controlled by a `fugacity&rsquo; $\lambda>0$, with higher $\lambda$ leading to denser configurations. We investigate the mixing time of Glauber (single-site) dynamics for the hard-core model on restricted classes of bounded-degree <b>graphs</b> in which a particular <b>graph</b> $H$ is excluded as an induced subgraph. If $H$ is a subdivided claw then, for all $\lambda$, the mixing time is $O(n\log n)$, where $n$ is the order of $G$. This extends a result of Chen and Gu for claw-free <b>graphs.</b> When $H$ is a path, the set of possible instances is finite. For all other $H$, the mixing time is exponential in $n$ for sufficiently large $\lambda$, depending on $H$ and the maximum degree of $G$.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--276276-approximating-shortest-paths-in-weighted-square-and-hexagonal-meshes-prosenjit-bose-et-al-2024>(1/1 | 276/276) Approximating shortest paths in weighted square and hexagonal meshes (Prosenjit Bose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prosenjit Bose, Guillermo Esteban, David Orden, Rodrigo I. Silveira. (2024)<br><strong>Approximating shortest paths in weighted square and hexagonal meshes</strong><br><button class=copy-to-clipboard title="Approximating shortest paths in weighted square and hexagonal meshes" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2404.07562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2404.07562v1.pdf filename=2404.07562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continuous 2-dimensional space is often discretized by considering a mesh of weighted cells. In this work we study how well a weighted mesh approximates the space, with respect to shortest paths. We consider a shortest path $ \mathit{SP_w}(s,t) $ from $ s $ to $ t $ in the continuous 2-dimensional space, a shortest vertex path $ \mathit{SVP_w}(s,t) $ (or any-angle path), which is a shortest path where the vertices of the path are vertices of the mesh, and a shortest grid path $ \mathit{SGP_w}(s,t) $, which is a shortest path in a <b>graph</b> associated to the weighted mesh. We provide upper and lower bounds on the ratios $ \frac{\lVert \mathit{SGP_w}(s,t)\rVert}{\lVert \mathit{SP_w}(s,t)\rVert} $, $ \frac{\lVert \mathit{SVP_w}(s,t)\rVert}{\lVert \mathit{SP_w}(s,t)\rVert} $, $ \frac{\lVert \mathit{SGP_w}(s,t)\rVert}{\lVert \mathit{SVP_w}(s,t)\rVert} $ in square and hexagonal meshes, extending previous results for triangular grids. These ratios determine the effectiveness of existing algorithms that compute shortest paths on the <b>graphs</b> obtained from the grids. Our main results are that the ratio $ \frac{\lVert \mathit{SGP_w}(s,t)\rVert}{\lVert \mathit{SP_w}(s,t)\rVert} $ is at most $ \frac{2}{\sqrt{2+\sqrt{2}}} \approx 1.08 $ and $ \frac{2}{\sqrt{2+\sqrt{3}}} \approx 1.04 $ in a square and a hexagonal mesh, respectively.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.04.12</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202404/20240414000000/ title="arXiv @ 2024.04.14" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.04.14</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-38>cs.CL (38)</a><ul><li><a href=#138--1276-mm-phyqa-multimodal-physics-question-answering-with-multi-image-cot-prompting-avinash-anand-et-al-2024>(1/38 | 1/276) MM-PhyQA: Multimodal Physics Question-Answering With Multi-Image CoT Prompting (Avinash Anand et al., 2024)</a></li><li><a href=#238--2276-guiding-large-language-models-to-post-edit-machine-translation-with-error-annotations-dayeon-ki-et-al-2024>(2/38 | 2/276) Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations (Dayeon Ki et al., 2024)</a></li><li><a href=#338--3276-mscinli-a-diverse-benchmark-for-scientific-natural-language-inference-mobashir-sadat-et-al-2024>(3/38 | 3/276) MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference (Mobashir Sadat et al., 2024)</a></li><li><a href=#438--4276-amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbreaking-both-open-and-closed-llms-zeyi-liao-et-al-2024>(4/38 | 4/276) AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs (Zeyi Liao et al., 2024)</a></li><li><a href=#538--5276-data-augmentation-based-dialectal-adaptation-for-llms-fahim-faisal-et-al-2024>(5/38 | 5/276) Data-Augmentation-Based Dialectal Adaptation for LLMs (Fahim Faisal et al., 2024)</a></li><li><a href=#638--6276-lloco-learning-long-contexts-offline-sijun-tan-et-al-2024>(6/38 | 6/276) LLoCO: Learning Long Contexts Offline (Sijun Tan et al., 2024)</a></li><li><a href=#738--7276-automatic-generation-and-evaluation-of-reading-comprehension-test-items-with-large-language-models-andreas-säuberli-et-al-2024>(7/38 | 7/276) Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models (Andreas Säuberli et al., 2024)</a></li><li><a href=#838--8276-comments-as-natural-logic-pivots-improve-code-generation-via-comment-perspective-yijie-chen-et-al-2024>(8/38 | 8/276) Comments as Natural Logic Pivots: Improve Code Generation via Comment Perspective (Yijie Chen et al., 2024)</a></li><li><a href=#938--9276-from-words-to-numbers-your-large-language-model-is-secretly-a-capable-regressor-when-given-in-context-examples-robert-vacareanu-et-al-2024>(9/38 | 9/276) From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples (Robert Vacareanu et al., 2024)</a></li><li><a href=#1038--10276-distilling-algorithmic-reasoning-from-llms-via-explaining-solution-programs-jierui-li-et-al-2024>(10/38 | 10/276) Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs (Jierui Li et al., 2024)</a></li><li><a href=#1138--11276-on-training-data-influence-of-gpt-models-qingyi-liu-et-al-2024>(11/38 | 11/276) On Training Data Influence of GPT Models (Qingyi Liu et al., 2024)</a></li><li><a href=#1238--12276-decomposing-label-space-format-and-discrimination-rethinking-how-llms-respond-and-solve-tasks-via-in-context-learning-quanyu-long-et-al-2024>(12/38 | 12/276) Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning (Quanyu Long et al., 2024)</a></li><li><a href=#1338--13276-sqbc-active-learning-using-llm-generated-synthetic-data-for-stance-detection-in-online-political-discussions-stefan-sylvius-wagner-et-al-2024>(13/38 | 13/276) SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions (Stefan Sylvius Wagner et al., 2024)</a></li><li><a href=#1438--14276-high-dimension-human-value-representation-in-large-language-models-samuel-cahyawijaya-et-al-2024>(14/38 | 14/276) High-Dimension Human Value Representation in Large Language Models (Samuel Cahyawijaya et al., 2024)</a></li><li><a href=#1538--15276-discourse-aware-in-context-learning-for-temporal-expression-normalization-akash-kumar-gautam-et-al-2024>(15/38 | 15/276) Discourse-Aware In-Context Learning for Temporal Expression Normalization (Akash Kumar Gautam et al., 2024)</a></li><li><a href=#1638--16276-introducing-l2m3-a-multilingual-medical-large-language-model-to-advance-health-equity-in-low-resource-regions-agasthya-gangavarapu-2024>(16/38 | 16/276) Introducing L2M3, A Multilingual Medical Large Language Model to Advance Health Equity in Low-Resource Regions (Agasthya Gangavarapu, 2024)</a></li><li><a href=#1738--17276-interactive-prompt-debugging-with-sequence-salience-ian-tenney-et-al-2024>(17/38 | 17/276) Interactive Prompt Debugging with Sequence Salience (Ian Tenney et al., 2024)</a></li><li><a href=#1838--18276-oda-observation-driven-agent-for-integrating-llms-and-knowledge-graphs-lei-sun-et-al-2024>(18/38 | 18/276) ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs (Lei Sun et al., 2024)</a></li><li><a href=#1938--19276-scalable-language-model-with-generalized-continual-learning-bohao-peng-et-al-2024>(19/38 | 19/276) Scalable Language Model with Generalized Continual Learning (Bohao Peng et al., 2024)</a></li><li><a href=#2038--20276-hgrn2-gated-linear-rnns-with-state-expansion-zhen-qin-et-al-2024>(20/38 | 20/276) HGRN2: Gated Linear RNNs with State Expansion (Zhen Qin et al., 2024)</a></li><li><a href=#2138--21276-nostra-domina-at-evalatin-2024-improving-latin-polarity-detection-through-data-augmentation-stephen-bothwell-et-al-2024>(21/38 | 21/276) Nostra Domina at EvaLatin 2024: Improving Latin Polarity Detection through Data Augmentation (Stephen Bothwell et al., 2024)</a></li><li><a href=#2238--22276-audio-dialogues-dialogues-dataset-for-audio-and-music-understanding-arushi-goel-et-al-2024>(22/38 | 22/276) Audio Dialogues: Dialogues dataset for audio and music understanding (Arushi Goel et al., 2024)</a></li><li><a href=#2338--23276-lavy-vietnamese-multimodal-large-language-model-chi-tran-et-al-2024>(23/38 | 23/276) LaVy: Vietnamese Multimodal Large Language Model (Chi Tran et al., 2024)</a></li><li><a href=#2438--24276-medical-mt5-an-open-source-multilingual-text-to-text-llm-for-the-medical-domain-iker-garcía-ferrero-et-al-2024>(24/38 | 24/276) Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain (Iker García-Ferrero et al., 2024)</a></li><li><a href=#2538--25276-noticia-a-clickbait-article-summarization-dataset-in-spanish-iker-garcía-ferrero-et-al-2024>(25/38 | 25/276) NoticIA: A Clickbait Article Summarization Dataset in Spanish (Iker García-Ferrero et al., 2024)</a></li><li><a href=#2638--26276-ultraeval-a-lightweight-platform-for-flexible-and-comprehensive-evaluation-for-llms-chaoqun-he-et-al-2024>(26/38 | 26/276) UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs (Chaoqun He et al., 2024)</a></li><li><a href=#2738--27276-leveraging-data-augmentation-for-process-information-extraction-julian-neuberger-et-al-2024>(27/38 | 27/276) Leveraging Data Augmentation for Process Information Extraction (Julian Neuberger et al., 2024)</a></li><li><a href=#2838--28276-jetmoe-reaching-llama2-performance-with-01m-dollars-yikang-shen-et-al-2024>(28/38 | 28/276) JetMoE: Reaching Llama2 Performance with 0.1M Dollars (Yikang Shen et al., 2024)</a></li><li><a href=#2938--29276-rho-1-not-all-tokens-are-what-you-need-zhenghao-lin-et-al-2024>(29/38 | 29/276) Rho-1: Not All Tokens Are What You Need (Zhenghao Lin et al., 2024)</a></li><li><a href=#3038--30276-annoctr-a-dataset-for-detecting-and-linking-entities-tactics-and-techniques-in-cyber-threat-reports-lukas-lange-et-al-2024>(30/38 | 30/276) AnnoCTR: A Dataset for Detecting and Linking Entities, Tactics, and Techniques in Cyber Threat Reports (Lukas Lange et al., 2024)</a></li><li><a href=#3138--31276-curated-datasets-and-neural-models-for-machine-translation-of-informal-registers-between-mayan-and-spanish-vernaculars-andrés-lou-et-al-2024>(31/38 | 31/276) Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars (Andrés Lou et al., 2024)</a></li><li><a href=#3238--32276-rollama-an-r-package-for-using-generative-large-language-models-through-ollama-johannes-b-gruber-et-al-2024>(32/38 | 32/276) rollama: An R package for using generative large language models through Ollama (Johannes B. Gruber et al., 2024)</a></li><li><a href=#3338--33276-confidently-nonsensical-a-critical-survey-on-the-perspectives-and-challenges-of-hallucinations-in-nlp-pranav-narayanan-venkit-et-al-2024>(33/38 | 33/276) &lsquo;Confidently Nonsensical?&rsquo;&rsquo;: A Critical Survey on the Perspectives and Challenges of &lsquo;Hallucinations&rsquo; in NLP (Pranav Narayanan Venkit et al., 2024)</a></li><li><a href=#3438--34276-graph-integrated-language-transformers-for-next-action-prediction-in-complex-phone-calls-amin-hosseiny-marani-et-al-2024>(34/38 | 34/276) Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls (Amin Hosseiny Marani et al., 2024)</a></li><li><a href=#3538--35276-researchagent-iterative-research-idea-generation-over-scientific-literature-with-large-language-models-jinheon-baek-et-al-2024>(35/38 | 35/276) ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models (Jinheon Baek et al., 2024)</a></li><li><a href=#3638--36276-hltcoe-at-trec-2023-neuclir-track-eugene-yang-et-al-2024>(36/38 | 36/276) HLTCOE at TREC 2023 NeuCLIR Track (Eugene Yang et al., 2024)</a></li><li><a href=#3738--37276-laissez-faire-harms-algorithmic-biases-in-generative-language-models-evan-shieh-et-al-2024>(37/38 | 37/276) Laissez-Faire Harms: Algorithmic Biases in Generative Language Models (Evan Shieh et al., 2024)</a></li><li><a href=#3838--38276-multimodal-contextual-dialogue-breakdown-detection-for-conversational-ai-models-md-messal-monem-miah-et-al-2024>(38/38 | 38/276) Multimodal Contextual Dialogue Breakdown Detection for Conversational AI Models (Md Messal Monem Miah et al., 2024)</a></li></ul></li><li><a href=#cscv-77>cs.CV (77)</a><ul><li><a href=#177--39276-learning-to-localize-objects-improves-spatial-reasoning-in-visual-llms-kanchana-ranasinghe-et-al-2024>(1/77 | 39/276) Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs (Kanchana Ranasinghe et al., 2024)</a></li><li><a href=#277--40276-tbsn-transformer-based-blind-spot-network-for-self-supervised-image-denoising-junyi-li-et-al-2024>(2/77 | 40/276) TBSN: Transformer-Based Blind-Spot Network for Self-Supervised Image Denoising (Junyi Li et al., 2024)</a></li><li><a href=#377--41276-glid-pre-training-a-generalist-encoder-decoder-vision-model-jihao-liu-et-al-2024>(3/77 | 41/276) GLID: Pre-training a Generalist Encoder-Decoder Vision Model (Jihao Liu et al., 2024)</a></li><li><a href=#477--42276-dgmamba-domain-generalization-via-generalized-state-space-model-shaocong-long-et-al-2024>(4/77 | 42/276) DGMamba: Domain Generalization via Generalized State Space Model (Shaocong Long et al., 2024)</a></li><li><a href=#577--43276-progressive-semantic-guided-vision-transformer-for-zero-shot-learning-shiming-chen-et-al-2024>(5/77 | 43/276) Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning (Shiming Chen et al., 2024)</a></li><li><a href=#677--44276-finding-dino-a-plug-and-play-framework-for-unsupervised-detection-of-out-of-distribution-objects-using-prototypes-poulami-sinhamahapatra-et-al-2024>(6/77 | 44/276) Finding Dino: A plug-and-play framework for unsupervised detection of out-of-distribution objects using prototypes (Poulami Sinhamahapatra et al., 2024)</a></li><li><a href=#777--45276-self-supervised-dataset-distillation-a-good-compression-is-all-you-need-muxin-zhou-et-al-2024>(7/77 | 45/276) Self-supervised Dataset Distillation: A Good Compression Is All You Need (Muxin Zhou et al., 2024)</a></li><li><a href=#877--46276-simba-mamba-augmented-u-shiftgcn-for-skeletal-action-recognition-in-videos-soumyabrata-chaudhuri-et-al-2024>(8/77 | 46/276) Simba: Mamba augmented U-ShiftGCN for Skeletal Action Recognition in Videos (Soumyabrata Chaudhuri et al., 2024)</a></li><li><a href=#977--47276-voice-assisted-real-time-traffic-sign-recognition-system-using-convolutional-neural-network-mayura-manawadu-et-al-2024>(9/77 | 47/276) Voice-Assisted Real-Time Traffic Sign Recognition System Using Convolutional Neural Network (Mayura Manawadu et al., 2024)</a></li><li><a href=#1077--48276-attention-based-end-to-end-network-for-offline-writer-identification-on-word-level-data-vineet-kumar-et-al-2024>(10/77 | 48/276) Attention based End to end network for Offline Writer Identification on Word level data (Vineet Kumar et al., 2024)</a></li><li><a href=#1177--49276-openbias-open-set-bias-detection-in-text-to-image-generative-models-moreno-dincà-et-al-2024>(11/77 | 49/276) OpenBias: Open-set Bias Detection in Text-to-Image Generative Models (Moreno D&rsquo;Incà et al., 2024)</a></li><li><a href=#1277--50276-implicit-and-explicit-language-guidance-for-diffusion-based-visual-perception-hefeng-wang-et-al-2024>(12/77 | 50/276) Implicit and Explicit Language Guidance for Diffusion-based Visual Perception (Hefeng Wang et al., 2024)</a></li><li><a href=#1377--51276-encoding-urban-ecologies-automated-building-archetype-generation-through-self-supervised-learning-for-energy-modeling-xinwei-zhuang-et-al-2024>(13/77 | 51/276) Encoding Urban Ecologies: Automated Building Archetype Generation through Self-Supervised Learning for Energy Modeling (Xinwei Zhuang et al., 2024)</a></li><li><a href=#1477--52276-boosting-self-supervision-for-single-view-scene-completion-via-knowledge-distillation-keonhee-han-et-al-2024>(14/77 | 52/276) Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation (Keonhee Han et al., 2024)</a></li><li><a href=#1577--53276-promptsync-bridging-domain-gaps-in-vision-language-models-through-class-aware-prototype-alignment-and-discrimination-anant-khandelwal-2024>(15/77 | 53/276) PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination (Anant Khandelwal, 2024)</a></li><li><a href=#1677--54276-transferable-and-principled-efficiency-for-open-vocabulary-segmentation-jingxuan-xu-et-al-2024>(16/77 | 54/276) Transferable and Principled Efficiency for Open-Vocabulary Segmentation (Jingxuan Xu et al., 2024)</a></li><li><a href=#1777--55276-improving-shift-invariance-in-convolutional-neural-networks-with-translation-invariant-polyphase-sampling-sourajit-saha-et-al-2024>(17/77 | 55/276) Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling (Sourajit Saha et al., 2024)</a></li><li><a href=#1877--56276-latent-guard-a-safety-framework-for-text-to-image-generation-runtao-liu-et-al-2024>(18/77 | 56/276) Latent Guard: a Safety Framework for Text-to-image Generation (Runtao Liu et al., 2024)</a></li><li><a href=#1977--57276-controlnet-improving-conditional-controls-with-efficient-consistency-feedback-ming-li-et-al-2024>(19/77 | 57/276) ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback (Ming Li et al., 2024)</a></li><li><a href=#2077--58276-view-selection-for-3d-captioning-via-diffusion-ranking-tiange-luo-et-al-2024>(20/77 | 58/276) View Selection for 3D Captioning via Diffusion Ranking (Tiange Luo et al., 2024)</a></li><li><a href=#2177--59276-ferret-v2-an-improved-baseline-for-referring-and-grounding-with-large-language-models-haotian-zhang-et-al-2024>(21/77 | 59/276) Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models (Haotian Zhang et al., 2024)</a></li><li><a href=#2277--60276-multi-image-visual-question-answering-for-unsupervised-anomaly-detection-jun-li-et-al-2024>(22/77 | 60/276) Multi-Image Visual Question Answering for Unsupervised Anomaly Detection (Jun Li et al., 2024)</a></li><li><a href=#2377--61276-heron-bench-a-benchmark-for-evaluating-vision-language-models-in-japanese-yuichi-inoue-et-al-2024>(23/77 | 61/276) Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese (Yuichi Inoue et al., 2024)</a></li><li><a href=#2477--62276-any2point-empowering-any-modality-large-models-for-efficient-3d-understanding-yiwen-tang-et-al-2024>(24/77 | 62/276) Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding (Yiwen Tang et al., 2024)</a></li><li><a href=#2577--63276-contrastive-based-deep-embeddings-for-label-noise-resilient-histopathology-image-classification-lucas-dedieu-et-al-2024>(25/77 | 63/276) Contrastive-Based Deep Embeddings for Label Noise-Resilient Histopathology Image Classification (Lucas Dedieu et al., 2024)</a></li><li><a href=#2677--64276-context-aware-video-anomaly-detection-in-long-term-datasets-zhengye-yang-et-al-2024>(26/77 | 64/276) Context-aware Video Anomaly Detection in Long-Term Datasets (Zhengye Yang et al., 2024)</a></li><li><a href=#2777--65276-aug-a-new-dataset-and-an-efficient-model-for-aerial-image-urban-scene-graph-generation-yansheng-li-et-al-2024>(27/77 | 65/276) AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation (Yansheng Li et al., 2024)</a></li><li><a href=#2877--66276-weakly-supervised-learning-via-multi-lateral-decoder-branching-for-guidewire-segmentation-in-robot-assisted-cardiovascular-catheterization-olatunji-mumini-omisore-et-al-2024>(28/77 | 66/276) Weakly-Supervised Learning via Multi-Lateral Decoder Branching for Guidewire Segmentation in Robot-Assisted Cardiovascular Catheterization (Olatunji Mumini Omisore et al., 2024)</a></li><li><a href=#2977--67276-multi-rater-prompting-for-ambiguous-medical-image-segmentation-jinhong-wang-et-al-2024>(29/77 | 67/276) Multi-rater Prompting for Ambiguous Medical Image Segmentation (Jinhong Wang et al., 2024)</a></li><li><a href=#3077--68276-objblur-a-curriculum-learning-approach-with-progressive-object-level-blurring-for-improved-layout-to-image-generation-stanislav-frolov-et-al-2024>(30/77 | 68/276) ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring for Improved Layout-to-Image Generation (Stanislav Frolov et al., 2024)</a></li><li><a href=#3177--69276-copilotcad-empowering-radiologists-with-report-completion-models-and-quantitative-evidence-from-medical-image-foundation-models-sheng-wang-et-al-2024>(31/77 | 69/276) CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models (Sheng Wang et al., 2024)</a></li><li><a href=#3277--70276-vifnet-an-end-to-end-visible-infrared-fusion-network-for-image-dehazing-meng-yu-et-al-2024>(32/77 | 70/276) VIFNet: An End-to-end Visible-Infrared Fusion Network for Image Dehazing (Meng Yu et al., 2024)</a></li><li><a href=#3377--71276-two-effects-one-trigger-on-the-modality-gap-object-bias-and-information-imbalance-in-contrastive-vision-language-representation-learning-simon-schrodi-et-al-2024>(33/77 | 71/276) Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning (Simon Schrodi et al., 2024)</a></li><li><a href=#3477--72276-fusionmamba-efficient-image-fusion-with-state-space-model-siran-peng-et-al-2024>(34/77 | 72/276) FusionMamba: Efficient Image Fusion with State Space Model (Siran Peng et al., 2024)</a></li><li><a href=#3577--73276-the-power-of-properties-uncovering-the-influential-factors-in-emotion-classification-tim-büchner-et-al-2024>(35/77 | 73/276) The Power of Properties: Uncovering the Influential Factors in Emotion Classification (Tim Büchner et al., 2024)</a></li><li><a href=#3677--74276-pram-place-recognition-anywhere-model-for-efficient-visual-localization-fei-xue-et-al-2024>(36/77 | 74/276) PRAM: Place Recognition Anywhere Model for Efficient Visual Localization (Fei Xue et al., 2024)</a></li><li><a href=#3777--75276-vim-unet-vision-mamba-for-biomedical-segmentation-anwai-archit-et-al-2024>(37/77 | 75/276) ViM-UNet: Vision Mamba for Biomedical Segmentation (Anwai Archit et al., 2024)</a></li><li><a href=#3877--76276-automatic-detection-of-dark-ship-to-ship-transfers-using-deep-learning-and-satellite-imagery-ollie-ballinger-2024>(38/77 | 76/276) Automatic Detection of Dark Ship-to-Ship Transfers using Deep Learning and Satellite Imagery (Ollie Ballinger, 2024)</a></li><li><a href=#3977--77276-consistencydet-robust-object-detector-with-denoising-paradigm-of-consistency-model-lifan-jiang-et-al-2024>(39/77 | 77/276) ConsistencyDet: Robust Object Detector with Denoising Paradigm of Consistency Model (Lifan Jiang et al., 2024)</a></li><li><a href=#4077--78276-opentrench3d-a-photogrammetric-3d-point-cloud-dataset-for-semantic-segmentation-of-underground-utilities-lasse-h-hansen-et-al-2024>(40/77 | 78/276) OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities (Lasse H. Hansen et al., 2024)</a></li><li><a href=#4177--79276-dealing-with-subject-similarity-in-differential-morphing-attack-detection-nicolò-di-domenico-et-al-2024>(41/77 | 79/276) Dealing with Subject Similarity in Differential Morphing Attack Detection (Nicolò Di Domenico et al., 2024)</a></li><li><a href=#4277--80276-separated-attention-an-improved-cycle-gan-based-under-water-image-enhancement-method-tashmoy-ghosh-2024>(42/77 | 80/276) Separated Attention: An Improved Cycle GAN Based Under Water Image Enhancement Method (Tashmoy Ghosh, 2024)</a></li><li><a href=#4377--81276-connecting-nerfs-images-and-text-francesco-ballerini-et-al-2024>(43/77 | 81/276) Connecting NeRFs, Images, and Text (Francesco Ballerini et al., 2024)</a></li><li><a href=#4477--82276-post-hurricane-building-damage-assessment-using-street-view-imagery-and-structured-data-a-multi-modal-deep-learning-approach-zhuoqun-xue-et-al-2024>(44/77 | 82/276) Post-hurricane building damage assessment using street-view imagery and structured data: A multi-modal deep learning approach (Zhuoqun Xue et al., 2024)</a></li><li><a href=#4577--83276-self-supervised-learning-of-color-constancy-markus-r-ernst-et-al-2024>(45/77 | 83/276) Self-Supervised Learning of Color Constancy (Markus R. Ernst et al., 2024)</a></li><li><a href=#4677--84276-rethinking-artistic-copyright-infringements-in-the-era-of-text-to-image-generative-models-mazda-moayeri-et-al-2024>(46/77 | 84/276) Rethinking Artistic Copyright Infringements in the Era of Text-to-Image Generative Models (Mazda Moayeri et al., 2024)</a></li><li><a href=#4777--85276-taming-stable-diffusion-for-text-to-360-panorama-image-generation-cheng-zhang-et-al-2024>(47/77 | 85/276) Taming Stable Diffusion for Text to 360° Panorama Image Generation (Cheng Zhang et al., 2024)</a></li><li><a href=#4877--86276-resolve-domain-conflicts-for-generalizable-remote-physiological-measurement-weiyu-sun-et-al-2024>(48/77 | 86/276) Resolve Domain Conflicts for Generalizable Remote Physiological Measurement (Weiyu Sun et al., 2024)</a></li><li><a href=#4977--87276-streamlined-photoacoustic-image-processing-with-foundation-models-a-training-free-solution-handi-deng-et-al-2024>(49/77 | 87/276) Streamlined Photoacoustic Image Processing with Foundation Models: A Training-Free Solution (Handi Deng et al., 2024)</a></li><li><a href=#5077--88276-depth-estimation-using-weighted-loss-and-transfer-learning-muhammad-adeel-hafeez-et-al-2024>(50/77 | 88/276) Depth Estimation using Weighted-loss and Transfer Learning (Muhammad Adeel Hafeez et al., 2024)</a></li><li><a href=#5177--89276-run-time-monitoring-of-3d-object-detection-in-automated-driving-systems-using-early-layer-neural-activation-patterns-hakan-yekta-yatbaz-et-al-2024>(51/77 | 89/276) Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns (Hakan Yekta Yatbaz et al., 2024)</a></li><li><a href=#5277--90276-model-based-cleaning-of-the-quilt-1m-pathology-dataset-for-text-conditional-image-synthesis-marc-aubreville-et-al-2024>(52/77 | 90/276) Model-based Cleaning of the QUILT-1M Pathology Dataset for Text-Conditional Image Synthesis (Marc Aubreville et al., 2024)</a></li><li><a href=#5377--91276-sfsort-scene-features-based-simple-online-real-time-tracker-m-m-morsali-et-al-2024>(53/77 | 91/276) SFSORT: Scene Features-based Simple Online Real-Time Tracker (M. M. Morsali et al., 2024)</a></li><li><a href=#5477--92276-mitigating-object-dependencies-improving-point-cloud-self-supervised-learning-through-object-exchange-yanhao-wu-et-al-2024>(54/77 | 92/276) Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange (Yanhao Wu et al., 2024)</a></li><li><a href=#5577--93276-fine-grained-side-information-guided-dual-prompts-for-zero-shot-skeleton-action-recognition-yang-chen-et-al-2024>(55/77 | 93/276) Fine-Grained Side Information Guided Dual-Prompts for Zero-Shot Skeleton Action Recognition (Yang Chen et al., 2024)</a></li><li><a href=#5677--94276-trashbusters-deep-learning-approach-for-litter-detection-and-tracking-kashish-jain-et-al-2024>(56/77 | 94/276) Trashbusters: Deep Learning Approach for Litter Detection and Tracking (Kashish Jain et al., 2024)</a></li><li><a href=#5777--95276-simplifying-two-stage-detectors-for-on-device-inference-in-remote-sensing-jaemin-kang-et-al-2024>(57/77 | 95/276) Simplifying Two-Stage Detectors for On-Device Inference in Remote Sensing (Jaemin Kang et al., 2024)</a></li><li><a href=#5877--96276-gomvs-geometrically-consistent-cost-aggregation-for-multi-view-stereo-jiang-wu-et-al-2024>(58/77 | 96/276) GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo (Jiang Wu et al., 2024)</a></li><li><a href=#5977--97276-rmaff-psn-a-residual-multi-scale-attention-feature-fusion-photometric-stereo-network-kai-luo-et-al-2024>(59/77 | 97/276) RMAFF-PSN: A Residual Multi-Scale Attention Feature Fusion Photometric Stereo Network (Kai Luo et al., 2024)</a></li><li><a href=#6077--98276-mindbridge-a-cross-subject-brain-decoding-framework-shizun-wang-et-al-2024>(60/77 | 98/276) MindBridge: A Cross-Subject Brain Decoding Framework (Shizun Wang et al., 2024)</a></li><li><a href=#6177--99276-g-nerf-geometry-enhanced-novel-view-synthesis-from-single-view-images-zixiong-huang-et-al-2024>(61/77 | 99/276) G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images (Zixiong Huang et al., 2024)</a></li><li><a href=#6277--100276-3d-csad-untrained-3d-anomaly-detection-for-complex-manufacturing-surfaces-xuanming-cao-et-al-2024>(62/77 | 100/276) 3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces (Xuanming Cao et al., 2024)</a></li><li><a href=#6377--101276-exploiting-object-based-and-segmentation-based-semantic-features-for-deep-learning-based-indoor-scene-classification-ricardo-pereira-et-al-2024>(63/77 | 101/276) Exploiting Object-based and Segmentation-based Semantic Features for Deep Learning-based Indoor Scene Classification (Ricardo Pereira et al., 2024)</a></li><li><a href=#6477--102276-how-is-visual-attention-influenced-by-text-guidance-database-and-model-yinan-sun-et-al-2024>(64/77 | 102/276) How is Visual Attention Influenced by Text Guidance? Database and Model (Yinan Sun et al., 2024)</a></li><li><a href=#6577--103276-gaga-group-any-gaussians-via-3d-aware-memory-bank-weijie-lyu-et-al-2024>(65/77 | 103/276) Gaga: Group Any Gaussians via 3D-aware Memory Bank (Weijie Lyu et al., 2024)</a></li><li><a href=#6677--104276-sparse-laneformer-ji-liu-et-al-2024>(66/77 | 104/276) Sparse Laneformer (Ji Liu et al., 2024)</a></li><li><a href=#6777--105276-joint-conditional-diffusion-model-for-image-restoration-with-mixed-degradations-yufeng-yue-et-al-2024>(67/77 | 105/276) Joint Conditional Diffusion Model for Image Restoration with Mixed Degradations (Yufeng Yue et al., 2024)</a></li><li><a href=#6877--106276-generating-synthetic-satellite-imagery-with-deep-learning-text-to-image-models----technical-challenges-and-implications-for-monitoring-and-verification-tuong-vy-nguyen-et-al-2024>(68/77 | 106/276) Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models &ndash; Technical Challenges and Implications for Monitoring and Verification (Tuong Vy Nguyen et al., 2024)</a></li><li><a href=#6977--107276-applying-guidance-in-a-limited-interval-improves-sample-and-distribution-quality-in-diffusion-models-tuomas-kynkäänniemi-et-al-2024>(69/77 | 107/276) Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models (Tuomas Kynkäänniemi et al., 2024)</a></li><li><a href=#7077--108276-cat-contrastive-adapter-training-for-personalized-image-generation-jae-wan-park-et-al-2024>(70/77 | 108/276) CAT: Contrastive Adapter Training for Personalized Image Generation (Jae Wan Park et al., 2024)</a></li><li><a href=#7177--109276-content-adaptive-non-local-convolution-for-remote-sensing-pansharpening-yule-duan-et-al-2024>(71/77 | 109/276) Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening (Yule Duan et al., 2024)</a></li><li><a href=#7277--110276-generalization-gap-in-data-augmentation-insights-from-illumination-jianqiang-xiao-et-al-2024>(72/77 | 110/276) Generalization Gap in Data Augmentation: Insights from Illumination (Jianqiang Xiao et al., 2024)</a></li><li><a href=#7377--111276-pillartrack-redesigning-pillar-based-transformer-network-for-single-object-tracking-on-point-clouds-weisheng-xu-et-al-2024>(73/77 | 111/276) PillarTrack: Redesigning Pillar-based Transformer Network for Single Object Tracking on Point Clouds (Weisheng Xu et al., 2024)</a></li><li><a href=#7477--112276-gomavatar-efficient-animatable-human-modeling-from-monocular-video-using-gaussians-on-mesh-jing-wen-et-al-2024>(74/77 | 112/276) GoMAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh (Jing Wen et al., 2024)</a></li><li><a href=#7577--113276-real-time-detection-and-analysis-of-vehicles-and-pedestrians-using-deep-learning-md-nahid-sadik-et-al-2024>(75/77 | 113/276) Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep Learning (Md Nahid Sadik et al., 2024)</a></li><li><a href=#7677--114276-survmamba-state-space-model-with-multi-grained-multi-modal-interaction-for-survival-prediction-ying-chen-et-al-2024>(76/77 | 114/276) SurvMamba: State Space Model with Multi-grained Multi-modal Interaction for Survival Prediction (Ying Chen et al., 2024)</a></li><li><a href=#7777--115276-stereo-lidar-depth-estimation-with-deformable-propagation-and-learned-disparity-depth-conversion-ang-li-et-al-2024>(77/77 | 115/276) Stereo-LiDAR Depth Estimation with Deformable Propagation and Learned Disparity-Depth Conversion (Ang Li et al., 2024)</a></li></ul></li><li><a href=#cslg-45>cs.LG (45)</a><ul><li><a href=#145--116276-variance-reduced-zeroth-order-methods-for-fine-tuning-language-models-tanmay-gautam-et-al-2024>(1/45 | 116/276) Variance-reduced Zeroth-Order Methods for Fine-Tuning Language Models (Tanmay Gautam et al., 2024)</a></li><li><a href=#245--117276-bayesian-federated-model-compression-for-communication-and-computation-efficiency-chengyu-xia-et-al-2024>(2/45 | 117/276) Bayesian Federated Model Compression for Communication and Computation Efficiency (Chengyu Xia et al., 2024)</a></li><li><a href=#345--118276-continual-learning-of-range-dependent-transmission-loss-for-underwater-acoustic-using-conditional-convolutional-neural-net-indu-kant-deo-et-al-2024>(3/45 | 118/276) Continual Learning of Range-Dependent Transmission Loss for Underwater Acoustic using Conditional Convolutional Neural Net (Indu Kant Deo et al., 2024)</a></li><li><a href=#445--119276-vetrass-vehicle-trajectory-similarity-search-through-graph-modeling-and-representation-learning-ming-cheng-et-al-2024>(4/45 | 119/276) VeTraSS: Vehicle Trajectory Similarity Search Through Graph Modeling and Representation Learning (Ming Cheng et al., 2024)</a></li><li><a href=#545--120276-can-contrastive-learning-refine-embeddings-lihui-liu-et-al-2024>(5/45 | 120/276) Can Contrastive Learning Refine Embeddings (Lihui Liu et al., 2024)</a></li><li><a href=#645--121276-graph-attention-network-for-lane-wise-and-topology-invariant-intersection-traffic-simulation-nooshin-yousefzadeh-et-al-2024>(6/45 | 121/276) Graph Attention Network for Lane-Wise and Topology-Invariant Intersection Traffic Simulation (Nooshin Yousefzadeh et al., 2024)</a></li><li><a href=#745--122276-efficient-duple-perturbation-robustness-in-low-rank-mdps-yang-hu-et-al-2024>(7/45 | 122/276) Efficient Duple Perturbation Robustness in Low-rank MDPs (Yang Hu et al., 2024)</a></li><li><a href=#845--123276-large-language-model-can-continue-evolving-from-mistakes-haokun-zhao-et-al-2024>(8/45 | 123/276) Large Language Model Can Continue Evolving From Mistakes (Haokun Zhao et al., 2024)</a></li><li><a href=#945--124276-a-multi-expert-large-language-model-architecture-for-verilog-code-generation-bardia-nadimi-et-al-2024>(9/45 | 124/276) A Multi-Expert Large Language Model Architecture for Verilog Code Generation (Bardia Nadimi et al., 2024)</a></li><li><a href=#1045--125276-generating-comprehensive-lithium-battery-charging-data-with-generative-ai-lidang-jiang-et-al-2024>(10/45 | 125/276) Generating Comprehensive Lithium Battery Charging Data with Generative AI (Lidang Jiang et al., 2024)</a></li><li><a href=#1145--126276-a-parsimonious-setup-for-streamflow-forecasting-using-cnn-lstm-sudan-pokharel-et-al-2024>(11/45 | 126/276) A Parsimonious Setup for Streamflow Forecasting using CNN-LSTM (Sudan Pokharel et al., 2024)</a></li><li><a href=#1245--127276-post-hoc-reversal-are-we-selecting-models-prematurely-rishabh-ranjan-et-al-2024>(12/45 | 127/276) Post-Hoc Reversal: Are We Selecting Models Prematurely? (Rishabh Ranjan et al., 2024)</a></li><li><a href=#1345--128276-sketch-plan-generalize-continual-few-shot-learning-of-inductively-generalizable-spatial-concepts-for-language-guided-robot-manipulation-namasivayam-kalithasan-et-al-2024>(13/45 | 128/276) Sketch-Plan-Generalize: Continual Few-Shot Learning of Inductively Generalizable Spatial Concepts for Language-Guided Robot Manipulation (Namasivayam Kalithasan et al., 2024)</a></li><li><a href=#1445--129276-anomaly-detection-in-power-grids-via-context-agnostic-learning-sangwoo-park-et-al-2024>(14/45 | 129/276) Anomaly Detection in Power Grids via Context-Agnostic Learning (SangWoo Park et al., 2024)</a></li><li><a href=#1545--130276-an-overview-of-diffusion-models-applications-guided-generation-statistical-rates-and-optimization-minshuo-chen-et-al-2024>(15/45 | 130/276) An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization (Minshuo Chen et al., 2024)</a></li><li><a href=#1645--131276-pinnacle-pinn-adaptive-collocation-and-experimental-points-selection-gregory-kang-ruey-lau-et-al-2024>(16/45 | 131/276) PINNACLE: PINN Adaptive ColLocation and Experimental points selection (Gregory Kang Ruey Lau et al., 2024)</a></li><li><a href=#1745--132276-differentially-private-reinforcement-learning-with-self-play-dan-qiao-et-al-2024>(17/45 | 132/276) Differentially Private Reinforcement Learning with Self-Play (Dan Qiao et al., 2024)</a></li><li><a href=#1845--133276-protein-intrinsic-disorder-prediction-using-attention-u-net-and-prottrans-protein-language-model-krzysztof-kotowski-et-al-2024>(18/45 | 133/276) Protein intrinsic disorder prediction using Attention U-Net and ProtTrans protein language model (Krzysztof Kotowski et al., 2024)</a></li><li><a href=#1945--134276-on-the-sample-efficiency-of-abstractions-and-potential-based-reward-shaping-in-reinforcement-learning-giuseppe-canonaco-et-al-2024>(19/45 | 134/276) On the Sample Efficiency of Abstractions and Potential-Based Reward Shaping in Reinforcement Learning (Giuseppe Canonaco et al., 2024)</a></li><li><a href=#2045--135276-flatness-improves-backbone-generalisation-in-few-shot-classification-rui-li-et-al-2024>(20/45 | 135/276) Flatness Improves Backbone Generalisation in Few-shot Classification (Rui Li et al., 2024)</a></li><li><a href=#2145--136276-physics-enhanced-graph-neural-networks-for-soft-sensing-in-industrial-internet-of-things-keivan-faghih-niresi-et-al-2024>(21/45 | 136/276) Physics-Enhanced Graph Neural Networks For Soft Sensing in Industrial Internet of Things (Keivan Faghih Niresi et al., 2024)</a></li><li><a href=#2245--137276-characterizing-the-influence-of-topology-on-graph-learning-tasks-kailong-wu-et-al-2024>(22/45 | 137/276) Characterizing the Influence of Topology on Graph Learning Tasks (Kailong Wu et al., 2024)</a></li><li><a href=#2345--138276-eliminating-catastrophic-overfitting-via-abnormal-adversarial-examples-regularization-runqi-lin-et-al-2024>(23/45 | 138/276) Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization (Runqi Lin et al., 2024)</a></li><li><a href=#2445--139276-frame-quantization-of-neural-networks-wojciech-czaja-et-al-2024>(24/45 | 139/276) Frame Quantization of Neural Networks (Wojciech Czaja et al., 2024)</a></li><li><a href=#2545--140276-learning-hamiltonian-dynamics-with-reproducing-kernel-hilbert-spaces-and-random-features-torbjørn-smith-et-al-2024>(25/45 | 140/276) Learning Hamiltonian Dynamics with Reproducing Kernel Hilbert Spaces and Random Features (Torbjørn Smith et al., 2024)</a></li><li><a href=#2645--141276-remembering-transformer-for-continual-learning-yuwei-sun-et-al-2024>(26/45 | 141/276) Remembering Transformer for Continual Learning (Yuwei Sun et al., 2024)</a></li><li><a href=#2745--142276-generating-counterfactual-explanations-using-cardinality-constraints-rubén-ruiz-torrubiano-2024>(27/45 | 142/276) Generating Counterfactual Explanations Using Cardinality Constraints (Rubén Ruiz-Torrubiano, 2024)</a></li><li><a href=#2845--143276-leveraging-domain-unlabeled-data-in-offline-reinforcement-learning-across-two-domains-soichiro-nishimori-et-al-2024>(28/45 | 143/276) Leveraging Domain-Unlabeled Data in Offline Reinforcement Learning across Two Domains (Soichiro Nishimori et al., 2024)</a></li><li><a href=#2945--144276-persistent-classification-a-new-approach-to-stability-of-data-and-adversarial-examples-brian-bell-et-al-2024>(29/45 | 144/276) Persistent Classification: A New Approach to Stability of Data and Adversarial Examples (Brian Bell et al., 2024)</a></li><li><a href=#3045--145276-calibration-of-continual-learning-models-lanpei-li-et-al-2024>(30/45 | 145/276) Calibration of Continual Learning Models (Lanpei Li et al., 2024)</a></li><li><a href=#3145--146276-realistic-continual-learning-approach-using-pre-trained-models-nadia-nasri-et-al-2024>(31/45 | 146/276) Realistic Continual Learning Approach using Pre-trained Models (Nadia Nasri et al., 2024)</a></li><li><a href=#3245--147276-fedauxhmtl-federated-auxiliary-hard-parameter-sharing-multi-task-learning-for-network-edge-traffic-classification-faisal-ahmed-et-al-2024>(32/45 | 147/276) FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task Learning for Network Edge Traffic Classification (Faisal Ahmed et al., 2024)</a></li><li><a href=#3345--148276-recurrentgemma-moving-past-transformers-for-efficient-open-language-models-aleksandar-botev-et-al-2024>(33/45 | 148/276) RecurrentGemma: Moving Past Transformers for Efficient Open Language Models (Aleksandar Botev et al., 2024)</a></li><li><a href=#3445--149276-unsupervised-concept-drift-detection-based-on-parallel-activations-of-neural-network-joanna-komorniczak-et-al-2024>(34/45 | 149/276) Unsupervised Concept Drift Detection based on Parallel Activations of Neural Network (Joanna Komorniczak et al., 2024)</a></li><li><a href=#3545--150276-enhancing-policy-gradient-with-the-polyak-step-size-adaption-yunxiang-li-et-al-2024>(35/45 | 150/276) Enhancing Policy Gradient with the Polyak Step-Size Adaption (Yunxiang Li et al., 2024)</a></li><li><a href=#3645--151276-predictive-modelling-of-air-quality-index-aqi-across-diverse-cities-and-states-of-india-using-machine-learning-investigating-the-influence-of-punjabs-stubble-burning-on-aqi-variability-kamaljeet-kaur-sidhu-et-al-2024>(36/45 | 151/276) Predictive Modelling of Air Quality Index (AQI) Across Diverse Cities and States of India using Machine Learning: Investigating the Influence of Punjab&rsquo;s Stubble Burning on AQI Variability (Kamaljeet Kaur Sidhu et al., 2024)</a></li><li><a href=#3745--152276-data-driven-portfolio-management-for-motion-pictures-industry-a-new-data-driven-optimization-methodology-using-a-large-language-model-as-the-expert-mohammad-alipour-vaezi-et-al-2024>(37/45 | 152/276) Data-Driven Portfolio Management for Motion Pictures Industry: A New Data-Driven Optimization Methodology Using a Large Language Model as the Expert (Mohammad Alipour-Vaezi et al., 2024)</a></li><li><a href=#3845--153276-wildgraph-realistic-graph-based-trajectory-generation-for-wildlife-ali-al-lawati-et-al-2024>(38/45 | 153/276) WildGraph: Realistic Graph-based Trajectory Generation for Wildlife (Ali Al-Lawati et al., 2024)</a></li><li><a href=#3945--154276-streaming-detection-of-significant-delay-changes-in-public-transport-systems-przemysław-wrona-et-al-2024>(39/45 | 154/276) Streaming detection of significant delay changes in public transport systems (Przemysław Wrona et al., 2024)</a></li><li><a href=#4045--155276-the-oxmat-dataset-a-multimodal-resource-for-the-development-of-ai-driven-technologies-in-maternal-and-newborn-child-health-m-jaleed-khan-et-al-2024>(40/45 | 155/276) The OxMat dataset: a multimodal resource for the development of AI-driven technologies in maternal and newborn child health (M. Jaleed Khan et al., 2024)</a></li><li><a href=#4145--156276-iitp-vdland-a-comprehensive-dataset-on-decentraland-parcels-ankit-k-bhagat-et-al-2024>(41/45 | 156/276) IITP-VDLand: A Comprehensive Dataset on Decentraland Parcels (Ankit K. Bhagat et al., 2024)</a></li><li><a href=#4245--157276-point-cloud-geometry-scalable-coding-with-a-quality-conditioned-latents-probability-estimator-daniele-mari-et-al-2024>(42/45 | 157/276) Point Cloud Geometry Scalable Coding with a Quality-Conditioned Latents Probability Estimator (Daniele Mari et al., 2024)</a></li><li><a href=#4345--158276-representation-learning-of-tangled-key-value-sequence-data-for-early-classification-tao-duan-et-al-2024>(43/45 | 158/276) Representation Learning of Tangled Key-Value Sequence Data for Early Classification (Tao Duan et al., 2024)</a></li><li><a href=#4445--159276-f_β-plot----a-visual-tool-for-evaluating-imbalanced-data-classifiers-szymon-wojciechowski-et-al-2024>(44/45 | 159/276) $F_β$-plot &ndash; a visual tool for evaluating imbalanced data classifiers (Szymon Wojciechowski et al., 2024)</a></li><li><a href=#4545--160276-global-versus-local-evaluating-alexnet-architectures-for-tropical-cyclone-intensity-estimation-vikas-dwivedi-2024>(45/45 | 160/276) Global versus Local: Evaluating AlexNet Architectures for Tropical Cyclone Intensity Estimation (Vikas Dwivedi, 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#115--161276-reflectance-estimation-for-proximity-sensing-by-vision-language-models-utilizing-distributional-semantics-for-low-level-cognition-in-robotics-masashi-osada-et-al-2024>(1/15 | 161/276) Reflectance Estimation for Proximity Sensing by Vision-Language Models: Utilizing Distributional Semantics for Low-Level Cognition in Robotics (Masashi Osada et al., 2024)</a></li><li><a href=#215--162276-generating-consistent-pddl-domains-with-large-language-models-pavel-smirnov-et-al-2024>(2/15 | 162/276) Generating consistent PDDL domains with Large Language Models (Pavel Smirnov et al., 2024)</a></li><li><a href=#315--163276-can-vehicle-motion-planning-generalize-to-realistic-long-tail-scenarios-marcel-hallgarten-et-al-2024>(3/15 | 163/276) Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios? (Marcel Hallgarten et al., 2024)</a></li><li><a href=#415--164276-quasisim-parameterized-quasi-physical-simulators-for-dexterous-manipulations-transfer-xueyi-liu-et-al-2024>(4/15 | 164/276) QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer (Xueyi Liu et al., 2024)</a></li><li><a href=#515--165276-multi-robot-target-tracking-with-sensing-and-communication-danger-zones-jiazhen-li-et-al-2024>(5/15 | 165/276) Multi-Robot Target Tracking with Sensing and Communication Danger Zones (Jiazhen Li et al., 2024)</a></li><li><a href=#615--166276-dual-quaternion-control-of-uavs-with-cable-suspended-load-yuxia-yuan-et-al-2024>(6/15 | 166/276) Dual Quaternion Control of UAVs with Cable-suspended Load (Yuxia Yuan et al., 2024)</a></li><li><a href=#715--167276-differentiable-rendering-as-a-way-to-program-cable-driven-soft-robots-kasra-arnavaz-et-al-2024>(7/15 | 167/276) Differentiable Rendering as a Way to Program Cable-Driven Soft Robots (Kasra Arnavaz et al., 2024)</a></li><li><a href=#815--168276-adademo-data-efficient-demonstration-expansion-for-generalist-robotic-agent-tongzhou-mu-et-al-2024>(8/15 | 168/276) AdaDemo: Data-Efficient Demonstration Expansion for Generalist Robotic Agent (Tongzhou Mu et al., 2024)</a></li><li><a href=#915--169276-data-driven-system-identification-of-quadrotors-subject-to-motor-delays-jonas-eschmann-et-al-2024>(9/15 | 169/276) Data-Driven System Identification of Quadrotors Subject to Motor Delays (Jonas Eschmann et al., 2024)</a></li><li><a href=#1015--170276-diffusing-in-someone-elses-shoes-robotic-perspective-taking-with-diffusion-josua-spisak-et-al-2024>(10/15 | 170/276) Diffusing in Someone Else&rsquo;s Shoes: Robotic Perspective Taking with Diffusion (Josua Spisak et al., 2024)</a></li><li><a href=#1115--171276-model-predictive-trajectory-planning-for-human-robot-handovers-thies-oelerich-et-al-2024>(11/15 | 171/276) Model Predictive Trajectory Planning for Human-Robot Handovers (Thies Oelerich et al., 2024)</a></li><li><a href=#1215--172276-one-shot-transfer-of-long-horizon-extrinsic-manipulation-through-contact-retargeting-albert-wu-et-al-2024>(12/15 | 172/276) One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting (Albert Wu et al., 2024)</a></li><li><a href=#1315--173276-socially-pertinent-robots-in-gerontological-healthcare-xavier-alameda-pineda-et-al-2024>(13/15 | 173/276) Socially Pertinent Robots in Gerontological Healthcare (Xavier Alameda-Pineda et al., 2024)</a></li><li><a href=#1415--174276-interactive-farinteractive-fast-and-adaptable-routing-for-navigation-among-movable-obstacles-in-complex-unknown-environments-botao-he-et-al-2024>(14/15 | 174/276) Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments (Botao He et al., 2024)</a></li><li><a href=#1515--175276-parameterized-fast-and-safe-tracking-fastrack-using-deepreach-hyun-joe-jeong-et-al-2024>(15/15 | 175/276) Parameterized Fast and Safe Tracking (FaSTrack) using Deepreach (Hyun Joe Jeong et al., 2024)</a></li></ul></li><li><a href=#csai-10>cs.AI (10)</a><ul><li><a href=#110--176276-generative-probabilistic-planning-for-optimizing-supply-chain-networks-hyung-il-ahn-et-al-2024>(1/10 | 176/276) Generative Probabilistic Planning for Optimizing Supply Chain Networks (Hyung-il Ahn et al., 2024)</a></li><li><a href=#210--177276-wese-weak-exploration-to-strong-exploitation-for-llm-agents-xu-huang-et-al-2024>(2/10 | 177/276) WESE: Weak Exploration to Strong Exploitation for LLM Agents (Xu Huang et al., 2024)</a></li><li><a href=#310--178276-augmenting-knowledge-graph-hierarchies-using-neural-transformers-sanat-sharma-et-al-2024>(3/10 | 178/276) Augmenting Knowledge Graph Hierarchies Using Neural Transformers (Sanat Sharma et al., 2024)</a></li><li><a href=#410--179276-designqa-a-multimodal-benchmark-for-evaluating-large-language-models-understanding-of-engineering-documentation-anna-c-doris-et-al-2024>(4/10 | 179/276) DesignQA: A Multimodal Benchmark for Evaluating Large Language Models&rsquo; Understanding of Engineering Documentation (Anna C. Doris et al., 2024)</a></li><li><a href=#510--180276-gnn-based-probabilistic-supply-and-inventory-predictions-in-supply-chain-networks-hyung-il-ahn-et-al-2024>(5/10 | 180/276) GNN-based Probabilistic Supply and Inventory Predictions in Supply Chain Networks (Hyung-il Ahn et al., 2024)</a></li><li><a href=#610--181276-osworld-benchmarking-multimodal-agents-for-open-ended-tasks-in-real-computer-environments-tianbao-xie-et-al-2024>(6/10 | 181/276) OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments (Tianbao Xie et al., 2024)</a></li><li><a href=#710--182276-generating-games-via-llms-an-investigation-with-video-game-description-language-chengpeng-hu-et-al-2024>(7/10 | 182/276) Generating Games via LLMs: An Investigation with Video Game Description Language (Chengpeng Hu et al., 2024)</a></li><li><a href=#810--183276-behavior-trees-enable-structured-programming-of-language-model-agents-richard-kelley-2024>(8/10 | 183/276) Behavior Trees Enable Structured Programming of Language Model Agents (Richard Kelley, 2024)</a></li><li><a href=#910--184276-goal-recognition-via-linear-programming-felipe-meneguzzi-et-al-2024>(9/10 | 184/276) Goal Recognition via Linear Programming (Felipe Meneguzzi et al., 2024)</a></li><li><a href=#1010--185276-monte-carlo-tree-search-with-boltzmann-exploration-michael-painter-et-al-2024>(10/10 | 185/276) Monte Carlo Tree Search with Boltzmann Exploration (Michael Painter et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#17--186276-generative-information-retrieval-evaluation-marwah-alaofi-et-al-2024>(1/7 | 186/276) Generative Information Retrieval Evaluation (Marwah Alaofi et al., 2024)</a></li><li><a href=#27--187276-can-large-language-models-assess-serendipity-in-recommender-systems-yu-tokutake-et-al-2024>(2/7 | 187/276) Can Large Language Models Assess Serendipity in Recommender Systems? (Yu Tokutake et al., 2024)</a></li><li><a href=#37--188276-manipulating-large-language-models-to-increase-product-visibility-aounon-kumar-et-al-2024>(3/7 | 188/276) Manipulating Large Language Models to Increase Product Visibility (Aounon Kumar et al., 2024)</a></li><li><a href=#47--189276-adaptive-fair-representation-learning-for-personalized-fairness-in-recommendations-via-information-alignment-xinyu-zhu-et-al-2024>(4/7 | 189/276) Adaptive Fair Representation Learning for Personalized Fairness in Recommendations via Information Alignment (Xinyu Zhu et al., 2024)</a></li><li><a href=#57--190276-m-scan-a-multi-scenario-causal-driven-adaptive-network-for-recommendation-jiachen-zhu-et-al-2024>(5/7 | 190/276) M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation (Jiachen Zhu et al., 2024)</a></li><li><a href=#67--191276-extending-translate-train-for-colbert-x-to-african-language-clir-eugene-yang-et-al-2024>(6/7 | 191/276) Extending Translate-Train for ColBERT-X to African Language CLIR (Eugene Yang et al., 2024)</a></li><li><a href=#77--192276-overview-of-the-trec-2023-neuclir-track-dawn-lawrie-et-al-2024>(7/7 | 192/276) Overview of the TREC 2023 NeuCLIR Track (Dawn Lawrie et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--193276-measuring-geographic-diversity-of-foundation-models-with-a-natural-language--based-geo-guessing-experiment-on-gpt-4-zilong-liu-et-al-2024>(1/3 | 193/276) Measuring Geographic Diversity of Foundation Models with a Natural Language&ndash;based Geo-guessing Experiment on GPT-4 (Zilong Liu et al., 2024)</a></li><li><a href=#23--194276-toxic-synergy-between-hate-speech-and-fake-news-exposure-munjung-kim-et-al-2024>(2/3 | 194/276) Toxic Synergy Between Hate Speech and Fake News Exposure (Munjung Kim et al., 2024)</a></li><li><a href=#33--195276-the-survey-on-multi-source-data-fusion-in-cyber-physical-social-systemsfoundational-infrastructure-for-industrial-metaverses-and-industries-50-xiao-wang-et-al-2024>(3/3 | 195/276) The Survey on Multi-Source Data Fusion in Cyber-Physical-Social Systems:Foundational Infrastructure for Industrial Metaverses and Industries 5.0 (Xiao Wang et al., 2024)</a></li></ul></li><li><a href=#q-finrm-1>q-fin.RM (1)</a><ul><li><a href=#11--196276-risklabs-predicting-financial-risk-using-large-language-model-based-on-multi-sources-data-yupeng-cao-et-al-2024>(1/1 | 196/276) RiskLabs: Predicting Financial Risk Using Large Language Model Based on Multi-Sources Data (Yupeng Cao et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--197276-an-effective-automated-speaking-assessment-approach-to-mitigating-data-scarcity-and-imbalanced-distribution-tien-hong-lo-et-al-2024>(1/1 | 197/276) An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution (Tien-Hong Lo et al., 2024)</a></li></ul></li><li><a href=#cscr-12>cs.CR (12)</a><ul><li><a href=#112--198276-llm-agents-can-autonomously-exploit-one-day-vulnerabilities-richard-fang-et-al-2024>(1/12 | 198/276) LLM Agents can Autonomously Exploit One-day Vulnerabilities (Richard Fang et al., 2024)</a></li><li><a href=#212--199276-backdoor-contrastive-learning-via-bi-level-trigger-optimization-weiyu-sun-et-al-2024>(2/12 | 199/276) Backdoor Contrastive Learning via Bi-level Trigger Optimization (Weiyu Sun et al., 2024)</a></li><li><a href=#312--200276-enhancing-network-intrusion-detection-performance-using-generative-adversarial-networks-xinxing-zhao-et-al-2024>(3/12 | 200/276) Enhancing Network Intrusion Detection Performance using Generative Adversarial Networks (Xinxing Zhao et al., 2024)</a></li><li><a href=#412--201276-privacy-preserving-layer-partitioning-for-deep-neural-network-models-kishore-rajasekar-et-al-2024>(4/12 | 201/276) Privacy preserving layer partitioning for Deep Neural Network models (Kishore Rajasekar et al., 2024)</a></li><li><a href=#512--202276-a-survey-on-security-of-ultrahyper-reliable-low-latency-communication-recent-advancements-challenges-and-future-directions-annapurna-pradhan-et-al-2024>(5/12 | 202/276) A Survey on Security of Ultra/Hyper Reliable Low Latency Communication: Recent Advancements, Challenges, and Future Directions (Annapurna Pradhan et al., 2024)</a></li><li><a href=#612--203276-leapfrog-the-rowhammer-instruction-skip-attack-andrew-adiletta-et-al-2024>(6/12 | 203/276) LeapFrog: The Rowhammer Instruction Skip Attack (Andrew Adiletta et al., 2024)</a></li><li><a href=#712--204276-protected-qr-code-based-anti-counterfeit-system-for-pharmaceutical-manufacturing-nitol-saha-et-al-2024>(7/12 | 204/276) Protected QR Code-based Anti-counterfeit System for Pharmaceutical Manufacturing (Nitol Saha et al., 2024)</a></li><li><a href=#812--205276-opportunistic-sensor-based-multi-factor-authentication-in-and-for-the-internet-of-things-marc-saideh-et-al-2024>(8/12 | 205/276) Opportunistic Sensor-Based Multi-Factor Authentication in and for the Internet of Things (Marc Saideh et al., 2024)</a></li><li><a href=#912--206276-towards-secure-and-reliable-heterogeneous-real-time-telemetry-communication-in-autonomous-uav-swarms-pavlo-mykytyn-et-al-2024>(9/12 | 206/276) Towards Secure and Reliable Heterogeneous Real-time Telemetry Communication in Autonomous UAV Swarms (Pavlo Mykytyn et al., 2024)</a></li><li><a href=#1012--207276-security-modelling-for-cyber-physical-systems-a-systematic-literature-review-shaofei-huang-et-al-2024>(10/12 | 207/276) Security Modelling for Cyber-Physical Systems: A Systematic Literature Review (Shaofei Huang et al., 2024)</a></li><li><a href=#1112--208276-rtl-interconnect-obfuscation-by-polymorphic-switch-boxes-for-secure-hardware-generation-haimanti-chakraborty-et-al-2024>(11/12 | 208/276) RTL Interconnect Obfuscation By Polymorphic Switch Boxes For Secure Hardware Generation (Haimanti Chakraborty et al., 2024)</a></li><li><a href=#1212--209276-fragile-model-watermark-for-integrity-protection-leveraging-boundary-volatility-and-sensitive-sample-pairing-zhenzhe-gao-et-al-2024>(12/12 | 209/276) Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing (ZhenZhe Gao et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--210276-neural-fault-injection-generating-software-faults-from-natural-language-domenico-cotroneo-et-al-2024>(1/5 | 210/276) Neural Fault Injection: Generating Software Faults from Natural Language (Domenico Cotroneo et al., 2024)</a></li><li><a href=#25--211276-on-unified-prompt-tuning-for-request-quality-assurance-in-public-code-review-xinyu-chen-et-al-2024>(2/5 | 211/276) On Unified Prompt Tuning for Request Quality Assurance in Public Code Review (Xinyu Chen et al., 2024)</a></li><li><a href=#35--212276-structure-aware-fine-tuning-for-code-pre-trained-models-jiayi-wu-et-al-2024>(3/5 | 212/276) Structure-aware Fine-tuning for Code Pre-trained Models (Jiayi Wu et al., 2024)</a></li><li><a href=#45--213276-decentralized-faas-over-multi-clouds-with-blockchain-based-management-for-supporting-emerging-applications-rabimba-karanjai-et-al-2024>(4/5 | 213/276) Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications (Rabimba Karanjai et al., 2024)</a></li><li><a href=#55--214276-devaic-a-tool-for-security-assessment-of-ai-generated-code-domenico-cotroneo-et-al-2024>(5/5 | 214/276) DeVAIC: A Tool for Security Assessment of AI-generated Code (Domenico Cotroneo et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--215276-synthetic-brain-images-bridging-the-gap-in-brain-mapping-with-generative-adversarial-model-drici-mourad-et-al-2024>(1/7 | 215/276) Synthetic Brain Images: Bridging the Gap in Brain Mapping With Generative Adversarial Model (Drici Mourad et al., 2024)</a></li><li><a href=#27--216276-survival-prediction-across-diverse-cancer-types-using-neural-networks-xu-yan-et-al-2024>(2/7 | 216/276) Survival Prediction Across Diverse Cancer Types Using Neural Networks (Xu Yan et al., 2024)</a></li><li><a href=#37--217276-lucf-net-lightweight-u-shaped-cascade-fusion-network-for-medical-image-segmentation-songkai-sun-et-al-2024>(3/7 | 217/276) LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation (Songkai Sun et al., 2024)</a></li><li><a href=#47--218276-latte-low-precision-approximate-attention-with-head-wise-trainable-threshold-for-efficient-transformer-jiing-ping-wang-et-al-2024>(4/7 | 218/276) LATTE: Low-Precision Approximate Attention with Head-wise Trainable Threshold for Efficient Transformer (Jiing-Ping Wang et al., 2024)</a></li><li><a href=#57--219276-diffusion-probabilistic-multi-cue-level-set-for-reducing-edge-uncertainty-in-pancreas-segmentation-yue-gou-et-al-2024>(5/7 | 219/276) Diffusion Probabilistic Multi-cue Level Set for Reducing Edge Uncertainty in Pancreas Segmentation (Yue Gou et al., 2024)</a></li><li><a href=#67--220276-event-enhanced-snapshot-compressive-videography-at-10k-fps-bo-zhang-et-al-2024>(6/7 | 220/276) Event-Enhanced Snapshot Compressive Videography at 10K FPS (Bo Zhang et al., 2024)</a></li><li><a href=#77--221276-learning-to-classify-new-foods-incrementally-via-compressed-exemplars-justin-yang-et-al-2024>(7/7 | 221/276) Learning to Classify New Foods Incrementally Via Compressed Exemplars (Justin Yang et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--222276-1-bit-quantized-on-chip-hybrid-diffraction-neural-network-enabled-by-authentic-all-optical-fully-connected-architecture-yu-shao-et-al-2024>(1/1 | 222/276) 1-bit Quantized On-chip Hybrid Diffraction Neural Network Enabled by Authentic All-optical Fully-connected Architecture (Yu Shao et al., 2024)</a></li></ul></li><li><a href=#csni-8>cs.NI (8)</a><ul><li><a href=#18--223276-collaborative-ground-space-communications-via-evolutionary-multi-objective-deep-reinforcement-learning-jiahui-li-et-al-2024>(1/8 | 223/276) Collaborative Ground-Space Communications via Evolutionary Multi-objective Deep Reinforcement Learning (Jiahui Li et al., 2024)</a></li><li><a href=#28--224276-uav-enabled-collaborative-beamforming-via-multi-agent-deep-reinforcement-learning-saichao-liu-et-al-2024>(2/8 | 224/276) UAV-enabled Collaborative Beamforming via Multi-Agent Deep Reinforcement Learning (Saichao Liu et al., 2024)</a></li><li><a href=#38--225276-an-application-layer-multi-hop-collective-perception-service-for-vehicular-adhoc-networks-vincent-albert-wolff-et-al-2024>(3/8 | 225/276) An Application Layer Multi-Hop Collective Perception Service for Vehicular Adhoc Networks (Vincent Albert Wolff et al., 2024)</a></li><li><a href=#48--226276-swi-feed-smart-water-iot-framework-for-evaluation-of-energy-and-data-in-massive-scenarios-antonino-pagano-et-al-2024>(4/8 | 226/276) SWI-FEED: Smart Water IoT Framework for Evaluation of Energy and Data in Massive Scenarios (Antonino Pagano et al., 2024)</a></li><li><a href=#58--227276-two-way-aerial-secure-communications-via-distributed-collaborative-beamforming-under-eavesdropper-collusion-jiahui-li-et-al-2024>(5/8 | 227/276) Two-Way Aerial Secure Communications via Distributed Collaborative Beamforming under Eavesdropper Collusion (Jiahui Li et al., 2024)</a></li><li><a href=#68--228276-resource-aware-deployment-of-dynamic-dnns-over-multi-tiered-interconnected-systems-chetna-singhal-et-al-2024>(6/8 | 228/276) Resource-aware Deployment of Dynamic DNNs over Multi-tiered Interconnected Systems (Chetna Singhal et al., 2024)</a></li><li><a href=#78--229276-predictive-handover-strategy-in-6g-and-beyond-a-deep-and-transfer-learning-approach-ioannis-panitsas-et-al-2024>(7/8 | 229/276) Predictive Handover Strategy in 6G and Beyond: A Deep and Transfer Learning Approach (Ioannis Panitsas et al., 2024)</a></li><li><a href=#88--230276-konnektor-connection-protocol-for-ensuring-peer-uniqueness-in-decentralized-p2p-networks-onur-ozkan-2024>(8/8 | 230/276) Konnektor: Connection Protocol for Ensuring Peer Uniqueness in Decentralized P2P Networks (Onur Ozkan, 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--231276-multimodal-emotion-recognition-by-fusing-video-semantic-in-mooc-learning-scenarios-yuan-zhang-et-al-2024>(1/1 | 231/276) Multimodal Emotion Recognition by Fusing Video Semantic in MOOC Learning Scenarios (Yuan Zhang et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#12--232276-do-large-language-models-learn-human-like-strategic-preferences-jesse-roberts-et-al-2024>(1/2 | 232/276) Do Large Language Models Learn Human-Like Strategic Preferences? (Jesse Roberts et al., 2024)</a></li><li><a href=#22--233276-auctions-with-llm-summaries-kumar-avinava-dubey-et-al-2024>(2/2 | 233/276) Auctions with LLM Summaries (Kumar Avinava Dubey et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--234276-unraveling-the-dilemma-of-ai-errors-exploring-the-effectiveness-of-human-and-machine-explanations-for-large-language-models-marvin-pafla-et-al-2024>(1/4 | 234/276) Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models (Marvin Pafla et al., 2024)</a></li><li><a href=#24--235276-leveraging-large-language-models-llms-to-support-collaborative-human-ai-online-risk-data-annotation-jinkyung-park-et-al-2024>(2/4 | 235/276) Leveraging Large Language Models (LLMs) to Support Collaborative Human-AI Online Risk Data Annotation (Jinkyung Park et al., 2024)</a></li><li><a href=#34--236276-rassar-room-accessibility-and-safety-scanning-in-augmented-reality-xia-su-et-al-2024>(3/4 | 236/276) RASSAR: Room Accessibility and Safety Scanning in Augmented Reality (Xia Su et al., 2024)</a></li><li><a href=#44--237276-apprentice-tutor-builder-a-platform-for-users-to-create-and-personalize-intelligent-tutors-glen-smith-et-al-2024>(4/4 | 237/276) Apprentice Tutor Builder: A Platform For Users to Create and Personalize Intelligent Tutors (Glen Smith et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--238276-robust-performance-metrics-for-imbalanced-classification-problems-hajo-holzmann-et-al-2024>(1/3 | 238/276) Robust performance metrics for imbalanced classification problems (Hajo Holzmann et al., 2024)</a></li><li><a href=#23--239276-diffusion-posterior-sampling-for-simulation-based-inference-in-tall-data-settings-julia-linhart-et-al-2024>(2/3 | 239/276) Diffusion posterior sampling for simulation-based inference in tall data settings (Julia Linhart et al., 2024)</a></li><li><a href=#33--240276-inferring-change-points-in-high-dimensional-linear-regression-via-approximate-message-passing-gabriel-arpino-et-al-2024>(3/3 | 240/276) Inferring Change Points in High-Dimensional Linear Regression via Approximate Message Passing (Gabriel Arpino et al., 2024)</a></li></ul></li><li><a href=#mathna-3>math.NA (3)</a><ul><li><a href=#13--241276-gan-based-iterative-motion-estimation-in-haste-mri-mathias-s-feinler-et-al-2024>(1/3 | 241/276) GAN-based iterative motion estimation in HASTE MRI (Mathias S. Feinler et al., 2024)</a></li><li><a href=#23--242276-adaptive-hyperbolic-cross-space-mapped-jacobi-method-on-unbounded-domains-with-applications-to-solving-multidimensional-spatiotemporal-integrodifferential-equations-yunhong-deng-et-al-2024>(2/3 | 242/276) Adaptive Hyperbolic-cross-space Mapped Jacobi Method on Unbounded Domains with Applications to Solving Multidimensional Spatiotemporal Integrodifferential Equations (Yunhong Deng et al., 2024)</a></li><li><a href=#33--243276-high-performance-matrix-free-unfitted-finite-element-operator-evaluation-maximilian-bergbauer-et-al-2024>(3/3 | 243/276) High-performance matrix-free unfitted finite element operator evaluation (Maximilian Bergbauer et al., 2024)</a></li></ul></li><li><a href=#econgn-2>econ.GN (2)</a><ul><li><a href=#12--244276-chatgpt-can-predict-the-future-when-it-tells-stories-set-in-the-future-about-the-past-van-pham-et-al-2024>(1/2 | 244/276) ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past (Van Pham et al., 2024)</a></li><li><a href=#22--245276-machine-learning-and-economic-forecasting-the-role-of-international-trade-networks-thiago-c-silva-et-al-2024>(2/2 | 245/276) Machine learning and economic forecasting: the role of international trade networks (Thiago C. Silva et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-2>q-bio.QM (2)</a><ul><li><a href=#12--246276-pathology-genomic-fusion-via-biologically-informed-cross-modality-graph-learning-for-survival-analysis-zeyu-zhang-et-al-2024>(1/2 | 246/276) Pathology-genomic fusion via biologically informed cross-modality graph learning for survival analysis (Zeyu Zhang et al., 2024)</a></li><li><a href=#22--247276-learning-chemotherapy-drug-action-via-universal-physics-informed-neural-networks-lena-podina-et-al-2024>(2/2 | 247/276) Learning Chemotherapy Drug Action via Universal Physics-Informed Neural Networks (Lena Podina et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#14--248276-iprefer-an-intelligent-parameter-extractor-based-on-features-for-bsim-cmg-models-zhiliang-peng-et-al-2024>(1/4 | 248/276) iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models (Zhiliang Peng et al., 2024)</a></li><li><a href=#24--249276-dynamic-modeling-and-simulation-of-a-flash-clay-calciner-nicola-cantisani-et-al-2024>(2/4 | 249/276) Dynamic modeling and simulation of a flash clay calciner (Nicola Cantisani et al., 2024)</a></li><li><a href=#34--250276-leveraging-eclipse-mosaic-for-modeling-and-analyzing-ride-hailing-services-karl-schrab-et-al-2024>(3/4 | 250/276) Leveraging Eclipse MOSAIC for Modeling and Analyzing Ride-Hailing Services (Karl Schrab et al., 2024)</a></li><li><a href=#44--251276-saturation-informed-current-limiting-control-for-grid-forming-converters-maitraya-avadhut-desai-et-al-2024>(4/4 | 251/276) Saturation-Informed Current-Limiting Control for Grid-Forming Converters (Maitraya Avadhut Desai et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--252276-trainable-joint-channel-estimation-detection-and-decoding-for-mimo-urllc-systems-yi-sun-et-al-2024>(1/2 | 252/276) Trainable Joint Channel Estimation, Detection and Decoding for MIMO URLLC Systems (Yi Sun et al., 2024)</a></li><li><a href=#22--253276-precoder-design-for-user-centric-network-massive-mimo-with-matrix-manifold-optimization-rui-sun-et-al-2024>(2/2 | 253/276) Precoder Design for User-Centric Network Massive MIMO with Matrix Manifold Optimization (Rui Sun et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--254276-interactive-ontology-matching-with-cost-efficient-learning-bin-cheng-et-al-2024>(1/1 | 254/276) Interactive Ontology Matching with Cost-Efficient Learning (Bin Cheng et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--255276-r2-indicator-and-deep-reinforcement-learning-enhanced-adaptive-multi-objective-evolutionary-algorithm-farajollah-tahernezhad-javazm-et-al-2024>(1/2 | 255/276) R2 Indicator and Deep Reinforcement Learning Enhanced Adaptive Multi-Objective Evolutionary Algorithm (Farajollah Tahernezhad-Javazm et al., 2024)</a></li><li><a href=#22--256276-impact-of-training-instance-selection-on-automated-algorithm-selection-models-for-numerical-black-box-optimization-konstantin-dietrich-et-al-2024>(2/2 | 256/276) Impact of Training Instance Selection on Automated Algorithm Selection Models for Numerical Black-box Optimization (Konstantin Dietrich et al., 2024)</a></li></ul></li><li><a href=#cspl-2>cs.PL (2)</a><ul><li><a href=#12--257276-kestrel-relational-verification-using-e-graphs-for-program-alignment-robert-dickerson-et-al-2024>(1/2 | 257/276) KestRel: Relational Verification Using E-Graphs for Program Alignment (Robert Dickerson et al., 2024)</a></li><li><a href=#22--258276-vicar-visualizing-categories-with-automated-rewriting-in-coq-bhakti-shah-et-al-2024>(2/2 | 258/276) ViCAR: Visualizing Categories with Automated Rewriting in Coq (Bhakti Shah et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--259276-diagram-analysis-of-iterative-algorithms-chris-jones-et-al-2024>(1/1 | 259/276) Diagram Analysis of Iterative Algorithms (Chris Jones et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--260276-linguaquanta-towards-a-quantum-transpiler-between-openqasm-and-quipper-extended-scott-wesley-2024>(1/1 | 260/276) LinguaQuanta: Towards a Quantum Transpiler Between OpenQASM and Quipper (Extended) (Scott Wesley, 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--261276-the-impact-of-speech-anonymization-on-pathology-and-its-limits-soroosh-tayebi-arasteh-et-al-2024>(1/1 | 261/276) The Impact of Speech Anonymization on Pathology and Its Limits (Soroosh Tayebi Arasteh et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--262276-auditing-health-related-recommendations-in-social-media-a-case-study-of-abortion-on-youtube-mohammed-lahsaini-et-al-2024>(1/1 | 262/276) Auditing health-related recommendations in social media: A Case Study of Abortion on YouTube (Mohammed Lahsaini et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--263276-learning-deterministic-multi-clock-timed-automata-yu-teng-et-al-2024>(1/1 | 263/276) Learning Deterministic Multi-Clock Timed Automata (Yu Teng et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--264276-an-efficient-uniqueness-theorem-for-overcomplete-tensor-decomposition-pascal-koiran-2024>(1/3 | 264/276) An efficient uniqueness theorem for overcomplete tensor decomposition (Pascal Koiran, 2024)</a></li><li><a href=#23--265276-an-improvement-of-degree-based-hashing-dbh-graph-partition-method-using-a-novel-metric-anna-mastikhina-et-al-2024>(2/3 | 265/276) An improvement of degree-based hashing (DBH) graph partition method, using a novel metric (Anna Mastikhina et al., 2024)</a></li><li><a href=#33--266276-parameterized-complexity-of-submodular-minimization-under-uncertainty-naonori-kakimura-et-al-2024>(3/3 | 266/276) Parameterized Complexity of Submodular Minimization under Uncertainty (Naonori Kakimura et al., 2024)</a></li></ul></li><li><a href=#mathoc-2>math.OC (2)</a><ul><li><a href=#12--267276-a-continuous-time-violation-free-multi-agent-optimization-algorithm-and-its-applications-to-safe-distributed-control-xiao-tan-et-al-2024>(1/2 | 267/276) A continuous-time violation-free multi-agent optimization algorithm and its applications to safe distributed control (Xiao Tan et al., 2024)</a></li><li><a href=#22--268276-equitable-routing---rethinking-the-multiple-traveling-salesman-problem-abhay-singh-bhadoriya-et-al-2024>(2/2 | 268/276) Equitable Routing - Rethinking the Multiple Traveling Salesman Problem (Abhay Singh Bhadoriya et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--269276-goppa-codes-key-to-high-efficiency-and-reliability-in-communications-behrooz-mosallaei-et-al-2024>(1/2 | 269/276) Goppa Codes: Key to High Efficiency and Reliability in Communications (Behrooz Mosallaei et al., 2024)</a></li><li><a href=#22--270276-ris-assisted-otfs-communications-phase-configuration-via-received-energy-maximization-mohamad-h-dinan-et-al-2024>(2/2 | 270/276) RIS-Assisted OTFS Communications: Phase Configuration via Received Energy Maximization (Mohamad H. Dinan et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--271276-q-itags-quality-optimized-spatio-temporal-heterogeneous-task-allocation-with-a-time-budget-glen-neville-et-al-2024>(1/1 | 271/276) Q-ITAGS: Quality-Optimized Spatio-Temporal Heterogeneous Task Allocation with a Time Budget (Glen Neville et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--272276-reflexive-graph-lenses-in-univalent-foundations-jonathan-sterling-2024>(1/1 | 272/276) Reflexive graph lenses in univalent foundations (Jonathan Sterling, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--273276-beyond-recognizing-well-covered-graphs-carl-feghali-et-al-2024>(1/1 | 273/276) Beyond recognizing well-covered graphs (Carl Feghali et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--274276-integrating-on-demand-ride-sharing-with-mass-transit-at-scale-danushka-edirimanna-et-al-2024>(1/1 | 274/276) Integrating On-demand Ride-sharing with Mass Transit at-Scale (Danushka Edirimanna et al., 2024)</a></li></ul></li><li><a href=#mathpr-1>math.PR (1)</a><ul><li><a href=#11--275276-glauber-dynamics-for-the-hard-core-model-on-bounded-degree-h-free-graphs-mark-jerrum-2024>(1/1 | 275/276) Glauber dynamics for the hard-core model on bounded-degree $H$-free graphs (Mark Jerrum, 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--276276-approximating-shortest-paths-in-weighted-square-and-hexagonal-meshes-prosenjit-bose-et-al-2024>(1/1 | 276/276) Approximating shortest paths in weighted square and hexagonal meshes (Prosenjit Bose et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>