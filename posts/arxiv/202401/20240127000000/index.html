<!doctype html><html><head><title>arXiv @ 2024.01.27</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.01.27"><meta property="og:description" content="Primary Categories cs.CV (14) cs.CL (25) cs.HC (4) cs.LG (17) cs.NI (1) cs.RO (2) cs.CR (2) eess.AS (2) cs.SE (7) cs.DC (1) cs.CY (1) eess.IV (2) physics.optics (1) cs.SI (2) quant-ph (1) cs.AI (3) cs.DS (1) eess.SY (2) stat.ML (1) cs.IR (1) cs.CV (14) (1/90) Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities (Yiyuan Zhang et al., 2024) Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202401/20240127000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-27T00:00:00+00:00"><meta name=description content="arXiv @ 2024.01.27"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202401/20240127000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Jan 27, 2024</p></div><div class=title><h1>arXiv @ 2024.01.27</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=#cscv-14>cs.CV (14)</a></li><li><a href=#cscl-25>cs.CL (25)</a></li><li><a href=#cshc-4>cs.HC (4)</a></li><li><a href=#cslg-17>cs.LG (17)</a></li><li><a href=#csni-1>cs.NI (1)</a></li><li><a href=#csro-2>cs.RO (2)</a></li><li><a href=#cscr-2>cs.CR (2)</a></li><li><a href=#eessas-2>eess.AS (2)</a></li><li><a href=#csse-7>cs.SE (7)</a></li><li><a href=#csdc-1>cs.DC (1)</a></li><li><a href=#cscy-1>cs.CY (1)</a></li><li><a href=#eessiv-2>eess.IV (2)</a></li><li><a href=#physicsoptics-1>physics.optics (1)</a></li><li><a href=#cssi-2>cs.SI (2)</a></li><li><a href=#quant-ph-1>quant-ph (1)</a></li><li><a href=#csai-3>cs.AI (3)</a></li><li><a href=#csds-1>cs.DS (1)</a></li><li><a href=#eesssy-2>eess.SY (2)</a></li><li><a href=#statml-1>stat.ML (1)</a></li><li><a href=#csir-1>cs.IR (1)</a></li></ul><h2 id=cscv-14>cs.CV (14)</h2><h3 id=190-multimodal-pathway-improve-transformers-with-irrelevant-data-from-other-modalities-yiyuan-zhang-et-al-2024>(1/90) Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities (Yiyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue. (2024)<br><strong>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keywords: AI, ImageNet, Transformer, Transformers<br><a href=http://arxiv.org/abs/2401.14405v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. On the image, point cloud, video, and audio recognition tasks, we observe significant and consistent performance improvements with irrelevant data from other modalities. The code and models are available at <a href=https://github.com/AILab-CVC/M2PT>https://github.com/AILab-CVC/M2PT</a>.</p></p class="citation"></blockquote><h3 id=290-deconstructing-denoising-diffusion-models-for-self-supervised-learning-xinlei-chen-et-al-2024>(2/90) Deconstructing Denoising Diffusion Models for Self-Supervised Learning (Xinlei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He. (2024)<br><strong>Deconstructing Denoising Diffusion Models for Self-Supervised Learning</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keywords: Self-Supervised<br><a href=http://arxiv.org/abs/2401.14404v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore how various components of modern DDMs influence self-supervised representation learning. We observe that only a very few modern components are critical for learning good representations, while many others are nonessential. Our study ultimately arrives at an approach that is highly simplified and to a large extent resembles a classical DAE. We hope our study will rekindle interest in a family of classical methods within the realm of modern self-supervised learning.</p></p class="citation"></blockquote><h3 id=390-rethinking-patch-dependence-for-masked-autoencoders-letian-fu-et-al-2024>(3/90) Rethinking Patch Dependence for Masked Autoencoders (Letian Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Letian Fu, Long Lian, Renhao Wang, Baifeng Shi, Xudong Wang, Adam Yala, Trevor Darrell, Alexei A. Efros, Ken Goldberg. (2024)<br><strong>Rethinking Patch Dependence for Masked Autoencoders</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Attention, ImageNet<br><a href=http://arxiv.org/abs/2401.14391v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In this work, we re-examine inter-patch dependencies in the decoding mechanism of masked autoencoders (MAE). We decompose this decoding mechanism for masked patch reconstruction in MAE into self-attention and cross-attention. Our investigations suggest that self-attention between mask patches is not essential for learning good representations. To this end, we propose a novel pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE). CrossMAE&rsquo;s decoder leverages only cross-attention between masked and visible tokens, with no degradation in downstream performance. This design also enables decoding only a small subset of mask tokens, boosting efficiency. Furthermore, each decoder block can now leverage different encoder features, resulting in improved representation learning. CrossMAE matches MAE in performance with 2.5 to 3.7$\times$ less decoding compute. It also surpasses MAE on ImageNet classification and COCO instance segmentation under the same compute. Code and models: <a href=https://crossmae.github.io>https://crossmae.github.io</a></p></p class="citation"></blockquote><h3 id=490-urbangenai-reconstructing-urban-landscapes-using-panoptic-segmentation-and-diffusion-models-timo-kapsalis-2024>(4/90) UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models (Timo Kapsalis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timo Kapsalis. (2024)<br><strong>UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14379v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Intersection over Union (IoU) and CLIP scores across iterative evaluations for various categories of urban landscape features. Preliminary testing included utilising UrbanGenAI as an educational tool enhancing the learning experience in design pedagogy, and as a participatory instrument facilitating community-driven urban planning. Early results suggested that UrbanGenAI not only advances the technical frontiers of urban landscape reconstruction but also provides significant pedagogical and participatory planning benefits. The ongoing development of UrbanGenAI aims to further validate its effectiveness across broader contexts and integrate additional features such as real-time feedback mechanisms and 3D modelling capabilities. Keywords: generative AI; panoptic image segmentation; diffusion models; urban landscape design; design pedagogy; co-design</p></p class="citation"></blockquote><h3 id=590-unlocking-past-information-temporal-embeddings-in-cooperative-birds-eye-view-prediction-dominik-rößle-et-al-2024>(5/90) Unlocking Past Information: Temporal Embeddings in Cooperative Bird&rsquo;s Eye View Prediction (Dominik Rößle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Rößle, Jeremias Gerner, Klaus Bogenberger, Daniel Cremers, Stefanie Schmidtner, Torsten Schön. (2024)<br><strong>Unlocking Past Information: Temporal Embeddings in Cooperative Bird&rsquo;s Eye View Prediction</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Embedding<br><a href=http://arxiv.org/abs/2401.14325v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Accurate and comprehensive semantic segmentation of Bird&rsquo;s Eye View (BEV) is essential for ensuring safe and proactive navigation in autonomous driving. Although cooperative perception has exceeded the detection capabilities of single-agent systems, prevalent camera-based algorithms in cooperative perception neglect valuable information derived from historical observations. This limitation becomes critical during sensor failures or communication issues as cooperative perception reverts to single-agent perception, leading to degraded performance and incomplete BEV segmentation maps. This paper introduces TempCoBEV, a temporal module designed to incorporate historical cues into current observations, thereby improving the quality and reliability of BEV map segmentations. We propose an importance-guided attention architecture to effectively integrate temporal information that prioritizes relevant properties for BEV map segmentation. TempCoBEV is an independent temporal module that seamlessly integrates into state-of-the-art camera-based cooperative perception models. We demonstrate through extensive experiments on the OPV2V dataset that TempCoBEV performs better than non-temporal models in predicting current and future BEV map segmentations, particularly in scenarios involving communication failures. We show the efficacy of TempCoBEV and its capability to integrate historical cues into the current BEV map, improving predictions under optimal communication conditions by up to 2% and under communication failures by up to 19%. The code will be published on GitHub.</p></p class="citation"></blockquote><h3 id=690-sketch2nerf-multi-view-sketch-guided-text-to-3d-generation-minglin-chen-et-al-2024>(6/90) Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation (Minglin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo. (2024)<br><strong>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keywords: Sketch<br><a href=http://arxiv.org/abs/2401.14257v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents with fine-grained sketch control while being high-fidelity to text prompts. Extensive results show that our method achieves state-of-the-art performance in terms of sketch similarity and text alignment.</p></p class="citation"></blockquote><h3 id=790-exploring-the-unexplored-understanding-the-impact-of-layer-adjustments-on-image-classification-haixia-liu-et-al-2024>(7/90) Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification (Haixia Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haixia Liu, Tim Brailsford, James Goulding, Gavin Smith, Larry Bull. (2024)<br><strong>Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Image Classification<br><a href=http://arxiv.org/abs/2401.14236v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates how adjustments to deep learning architectures impact model performance in image classification. Small-scale experiments generate initial insights although the trends observed are not consistent with the entire dataset. Filtering operations in the image processing pipeline are crucial, with image filtering before pre-processing yielding better results. The choice and order of layers as well as filter placement significantly impact model performance. This study provides valuable insights into optimizing deep learning models, with potential avenues for future research including collaborative platforms.</p></p class="citation"></blockquote><h3 id=890-vivim-a-video-vision-mamba-for-medical-video-object-segmentation-yijun-yang-et-al-2024>(8/90) Vivim: a Video Vision Mamba for Medical Video Object Segmentation (Yijun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Yang, Zhaohu Xing, Lei Zhu. (2024)<br><strong>Vivim: a Video Vision Mamba for Medical Video Object Segmentation</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Transformer<br><a href=http://arxiv.org/abs/2401.14168v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Traditional convolutional neural networks have a limited receptive field while transformer-based networks are mediocre in constructing long-term dependency from the perspective of computational complexity. Such the bottleneck poses a significant challenge when processing long video sequences in video analysis tasks. Very recently, the state space models (SSMs) with efficient hardware-aware designs, famous by Mamba, have exhibited impressive achievements in long sequence modeling, which facilitates the development of deep neural networks on many vision tasks. To better capture available cues in video frames, this paper presents a generic Video Vision Mamba-based framework for medical video object segmentation tasks, named Vivim. Our Vivim can effectively compress the long-term spatiotemporal representation into sequences at varying scales by our designed Temporal Mamba Block. Compared to existing video-level Transformer-based methods, our model maintains excellent segmentation results with better speed performance. Extensive experiments on the breast US dataset demonstrate the effectiveness and efficiency of our Vivim. The code for Vivim is available at: <a href=https://github.com/scott-yjyang/Vivim>https://github.com/scott-yjyang/Vivim</a>.</p></p class="citation"></blockquote><h3 id=990-diffusion-based-data-augmentation-for-object-counting-problems-zhen-wang-et-al-2024>(9/90) Diffusion-based Data Augmentation for Object Counting Problems (Zhen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Wang, Yuelei Li, Jia Wan, Nuno Vasconcelos. (2024)<br><strong>Diffusion-based Data Augmentation for Object Counting Problems</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Augmentation, Transformer, Transformers<br><a href=http://arxiv.org/abs/2401.13992v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Crowd counting is an important problem in computer vision due to its wide range of applications in image understanding. Currently, this problem is typically addressed using deep learning approaches, such as Convolutional Neural Networks (CNNs) and Transformers. However, deep networks are data-driven and are prone to overfitting, especially when the available labeled crowd dataset is limited. To overcome this limitation, we have designed a pipeline that utilizes a diffusion model to generate extensive training data. We are the first to generate images conditioned on a location dot map (a binary dot map that specifies the location of human heads) with a diffusion model. We are also the first to use these diverse synthetic data to augment the crowd counting models. Our proposed smoothed density map input for ControlNet significantly improves ControlNet&rsquo;s performance in generating crowds in the correct locations. Also, Our proposed counting loss for the diffusion model effectively minimizes the discrepancies between the location dot map and the crowd images generated. Additionally, our innovative guidance sampling further directs the diffusion process toward regions where the generated crowd images align most accurately with the location dot map. Collectively, we have enhanced ControlNet&rsquo;s ability to generate specified objects from a location dot map, which can be used for data augmentation in various counting problems. Moreover, our framework is versatile and can be easily adapted to all kinds of counting problems. Extensive experiments demonstrate that our framework improves the counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS datasets, showcasing its effectiveness.</p></p class="citation"></blockquote><h3 id=1090-improving-pseudo-labelling-and-enhancing-robustness-for-semi-supervised-domain-generalization-adnan-khan-et-al-2024>(10/90) Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization (Adnan Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adnan Khan, Mai A. Shaaban, Muhammad Haris Khan. (2024)<br><strong>Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Semi-Supervised<br><a href=http://arxiv.org/abs/2401.13965v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Beyond attaining domain generalization (DG), visual recognition models should also be data-efficient during learning by leveraging limited labels. We study the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial for real-world applications like automated healthcare. SSDG requires learning a cross-domain generalizable model when the given training data is only partially labelled. Empirical investigations reveal that the DG methods tend to underperform in SSDG settings, likely because they are unable to exploit the unlabelled data. Semi-supervised learning (SSL) shows improved but still inferior results compared to fully-supervised learning. A key challenge, faced by the best-performing SSL-based SSDG methods, is selecting accurate pseudo-labels under multiple domain shifts and reducing overfitting to source domains under limited labels. In this work, we propose new SSDG approach, which utilizes a novel uncertainty-guided pseudo-labelling with model averaging (UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to improve pseudo-labelling selection, addressing poor model calibration under multi-source unlabelled data. The UPL technique, enhanced by our novel model averaging (MA) strategy, mitigates overfitting to source domains with limited labels. Extensive experiments on key representative DG datasets suggest that our method demonstrates effectiveness against existing methods. Our code and chosen labelled data seeds are available on GitHub: <a href=https://github.com/Adnan-Khan7/UPLM>https://github.com/Adnan-Khan7/UPLM</a></p></p class="citation"></blockquote><h3 id=1190-an-extensible-framework-for-open-heterogeneous-collaborative-perception-yifan-lu-et-al-2024>(11/90) An Extensible Framework for Open Heterogeneous Collaborative Perception (Yifan Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen, Yanfeng Wang. (2024)<br><strong>An Extensible Framework for Open Heterogeneous Collaborative Perception</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.13964v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. It also protects new agents&rsquo; model details from disclosure since the training can be conducted by the agent owner locally. To enrich agents&rsquo; data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. Code and data are available at: <a href=https://github.com/yifanlu0227/HEAL>https://github.com/yifanlu0227/HEAL</a>.</p></p class="citation"></blockquote><h3 id=1290-am-sort-adaptable-motion-predictor-with-historical-trajectory-embedding-for-multi-object-tracking-vitaliy-kim-et-al-2024>(12/90) AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking (Vitaliy Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vitaliy Kim, Gunho Jung, Seong-Whan Lee. (2024)<br><strong>AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Embedding<br><a href=http://arxiv.org/abs/2401.13950v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions.</p></p class="citation"></blockquote><h3 id=1390-self-supervised-video-object-segmentation-with-distillation-learning-of-deformable-attention-quang-trung-truong-et-al-2024>(13/90) Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention (Quang-Trung Truong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quang-Trung Truong, Duc Thanh Nguyen, Binh-Son Hua, Sai-Kit Yeung. (2024)<br><strong>Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Attention<br><a href=http://arxiv.org/abs/2401.13937v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Video object segmentation is a fundamental research problem in computer vision. Recent techniques have often applied attention mechanism to object representation learning from video sequences. However, due to temporal changes in the video data, attention maps may not well align with the objects of interest across video frames, causing accumulated errors in long-term video processing. In addition, existing techniques have utilised complex architectures, requiring highly computational complexity and hence limiting the ability to integrate video object segmentation into low-powered devices. To address these issues, we propose a new method for self-supervised video object segmentation based on distillation learning of deformable attention. Specifically, we devise a lightweight architecture for video object segmentation that is effectively adapted to temporal changes. This is enabled by deformable attention mechanism, where the keys and values capturing the memory of a video sequence in the attention module have flexible locations updated across frames. The learnt object representations are thus adaptive to both the spatial and temporal dimensions. We train the proposed architecture in a self-supervised fashion through a new knowledge distillation paradigm where deformable attention maps are integrated into the distillation loss. We qualitatively and quantitatively evaluate our method and compare it with existing methods on benchmark datasets including DAVIS 2016/2017 and YouTube-VOS 2018/2019. Experimental results verify the superiority of our method via its achieved state-of-the-art performance and optimal memory usage.</p></p class="citation"></blockquote><h3 id=1490-knowledge-graph-supported-benchmark-and-video-captioning-for-basketball-zeyu-xi-et-al-2024>(14/90) Knowledge Graph Supported Benchmark and Video Captioning for Basketball (Zeyu Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Xi, Ge Shi, Lifang Wu, Xuefen Li, Junchi Yan, Liang Wang, Zilin Liu. (2024)<br><strong>Knowledge Graph Supported Benchmark and Video Captioning for Basketball</strong></p><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keywords: Knowledge Graph<br><a href=http://arxiv.org/abs/2401.13888v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent emergence of video captioning models, how to generate the text description with specific entity names and fine-grained actions is far from being solved, which however has great applications such as basketball live text broadcast. In this paper, a new multimodal knowledge supported basketball benchmark for video captioning is proposed. Specifically, we construct a Multimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond videos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset that contains 9 types of fine-grained shooting events and 286 players&rsquo; knowledge (i.e., images and names) is constructed based on MbgKG. We develop a novel framework in the encoder-decoder form named Entity-Aware Captioner (EAC) for basketball live text broadcast. The temporal information in video is encoded by introducing the bi-directional GRU (Bi-GRU) module. And the multi-head self-attention module is utilized to model the relationships among the players and select the key players. Besides, we propose a new performance evaluation metric named Game Description Score (GDS), which measures not only the linguistic performance but also the accuracy of the names prediction. Extensive experiments on MbgVC dataset demonstrate that EAC effectively leverages external knowledge and outperforms advanced video captioning models. The proposed benchmark and corresponding codes will be publicly available soon.</p></p class="citation"></blockquote><h2 id=cscl-25>cs.CL (25)</h2><h3 id=1590-modular-adaptation-of-multilingual-encoders-to-written-swiss-german-dialect-jannis-vamvas-et-al-2024>(15/90) Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect (Jannis Vamvas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannis Vamvas, Noëmi Aepli, Rico Sennrich. (2024)<br><strong>Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: Multilingual, NLP<br><a href=http://arxiv.org/abs/2401.14400v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Creating neural text encoders for written Swiss German is challenging due to a dearth of training data combined with dialectal variation. In this paper, we build on several existing multilingual encoders and adapt them to Swiss German using continued pre-training. Evaluation on three diverse downstream tasks shows that simply adding a Swiss German adapter to a modular encoder achieves 97.5% of fully monolithic adaptation performance. We further find that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies. We release our code and the models trained for our experiments at <a href=https://github.com/ZurichNLP/swiss-german-text-encoders>https://github.com/ZurichNLP/swiss-german-text-encoders</a></p></p class="citation"></blockquote><h3 id=1690-turna-a-turkish-encoder-decoder-language-model-for-enhanced-understanding-and-generation-gökçe-uludoğan-et-al-2024>(16/90) TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation (Gökçe Uludoğan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gökçe Uludoğan, Zeynep Yirmibeşoğlu Balal, Furkan Akkurt, Melikşah Türker, Onur Güngör, Susan Üsküdarlı. (2024)<br><strong>TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14373v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at <a href=https://huggingface.co/boun-tabi-LMG/TURNA>https://huggingface.co/boun-tabi-LMG/TURNA</a> .</p></p class="citation"></blockquote><h3 id=1790-genie-achieving-human-parity-in-content-grounded-datasets-generation-asaf-yehudai-et-al-2024>(17/90) Genie: Achieving Human Parity in Content-Grounded Datasets Generation (Asaf Yehudai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, Leshem Choshen. (2024)<br><strong>Genie: Achieving Human Parity in Content-Grounded Datasets Generation</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keywords: QA, Summarization<br><a href=http://arxiv.org/abs/2401.14367v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data &ndash; ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained on human-generated data and consistently outperforming them in faithfulness. Finally, we applied our method to create LFQA data within the medical domain and compared a model trained on it with models trained on other domains.</p></p class="citation"></blockquote><h3 id=1890-a-comparative-analysis-of-noise-reduction-methods-in-sentiment-analysis-on-noisy-bengali-texts-kazi-toufique-elahi-et-al-2024>(18/90) A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts (Kazi Toufique Elahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar. (2024)<br><strong>A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts</strong></p><hr><p>Primary Category: cs.CL<br>Categories: 68T50 (Primary), I-2-7, cs-CL, cs.CL<br>Keywords: Sentiment Analysis<br><a href=http://arxiv.org/abs/2401.14360v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reduction methods in future research endeavors. We have made the implementation and dataset presented in this paper publicly available at <a href=https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts>https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts</a></p></p class="citation"></blockquote><h3 id=1990-topologies-of-reasoning-demystifying-chains-trees-and-graphs-of-thoughts-maciej-besta-et-al-2024>(19/90) Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts (Maciej Besta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, Torsten Hoefler. (2024)<br><strong>Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keywords: NLP, Reasoning<br><a href=http://arxiv.org/abs/2401.14295v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models&rsquo; (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM&rsquo;s capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and others parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.</p></p class="citation"></blockquote><h3 id=2090-romansetu-efficiently-unlocking-multilingual-capabilities-of-large-language-models-models-via-romanization-jaavid-aktar-husain-et-al-2024>(20/90) RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization (Jaavid Aktar Husain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Ratish Puduppully, Anoop Kunchukuttan. (2024)<br><strong>RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14280v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.</p></p class="citation"></blockquote><h3 id=2190-transformers-and-cortical-waves-encoders-for-pulling-in-context-across-time-lyle-muller-et-al-2024>(21/90) Transformers and Cortical Waves: Encoders for Pulling In Context Across Time (Lyle Muller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lyle Muller, Patricia S. Churchland, Terrence J. Sejnowski. (2024)<br><strong>Transformers and Cortical Waves: Encoders for Pulling In Context Across Time</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: ChatGPT, GPT, Language Model, Transformer, Transformers<br><a href=http://arxiv.org/abs/2401.14267v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world&rsquo;s attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long &ldquo;encoding vector&rdquo; - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, &ldquo;self-attention&rdquo; applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in transformers.</p></p class="citation"></blockquote><h3 id=2290-improving-natural-language-capability-of-code-large-language-model-wei-li-et-al-2024>(22/90) Improving Natural Language Capability of Code Large Language Model (Wei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Li, Daoguang Zan, Bei Guan, Ailun Yu, Xiaolin Chen, Yongji Wang. (2024)<br><strong>Improving Natural Language Capability of Code Large Language Model</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: Attention, Language Model<br><a href=http://arxiv.org/abs/2401.14242v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user&rsquo;s natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework.</p></p class="citation"></blockquote><h3 id=2390-bayesprompt-prompting-large-scale-pre-trained-language-models-on-few-shot-inference-via-debiased-domain-abstraction-jiangmeng-li-et-al-2024>(23/90) BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction (Jiangmeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng, Fuchun Sun, Hui Xiong. (2024)<br><strong>BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14166v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for PLMs. Guided by such an intuition, we propose a simple yet effective approach, namely BayesPrompt, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. BayesPrompt primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.</p></p class="citation"></blockquote><h3 id=2490-on-the-affinity-rationality-and-diversity-of-hierarchical-topic-modeling-xiaobao-wu-et-al-2024>(24/90) On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling (Xiaobao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobao Wu, Fengjun Pan, Thong Nguyen, Yichao Feng, Chaoqun Liu, Cong-Duy Nguyen, Anh Tuan Luu. (2024)<br><strong>On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: Topic Model, Topic Modeling<br><a href=http://arxiv.org/abs/2401.14113v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstrate that our method surpasses state-of-the-art baselines, effectively improving the affinity, rationality, and diversity of hierarchical topic modeling with better performance on downstream tasks.</p></p class="citation"></blockquote><h3 id=2590-compactifai-extreme-compression-of-large-language-models-using-quantum-inspired-tensor-networks-andrei-tomut-et-al-2024>(25/90) CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks (Andrei Tomut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Tomut, Saeed S. Jahromi, Sukhbinder Singh, Faysal Ishtiaq, Cesar Muñoz, Prabdeep Singh Bajaj, Ali Elborady, Gianni del Bimbo, Mehrazin Alizadeh, David Montero, Pablo Martin-Ramiro, Muhammad Ibrahim, Oussama Tahiri Alaoui, John Malcolm, Samuel Mugel, Roman Orus. (2024)<br><strong>CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL, quant-ph<br>Keywords: AI, ChatGPT, GPT, Language Model<br><a href=http://arxiv.org/abs/2401.14109v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there&rsquo;s no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model&rsquo;s correlation space instead, allowing for a more controlled, refined and interpretable model compression. Our method is versatile and can be implemented with - or on top of - other compression techniques. As a benchmark, we demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model to only $30%$ of its original size while recovering over $90%$ of the original accuracy after a brief distributed retraining.</p></p class="citation"></blockquote><h3 id=2690-takeed-the-first-generative-fact-checking-system-for-arabic-claims-saud-althabiti-et-al-2024>(26/90) Ta&rsquo;keed: The First Generative Fact-Checking System for Arabic Claims (Saud Althabiti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saud Althabiti, Mohammad Ammar Alsalka, Eric Atwell. (2024)<br><strong>Ta&rsquo;keed: The First Generative Fact-Checking System for Arabic Claims</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: Fact-Checking<br><a href=http://arxiv.org/abs/2401.14067v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Ta&rsquo;keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as &ldquo;True&rdquo; or &ldquo;False,&rdquo; there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta&rsquo;keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system&rsquo;s generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealing a potential correlation, with the model using the top seven hits outperforming others with an F1 score of 0.77.</p></p class="citation"></blockquote><h3 id=2790-towards-goal-oriented-large-language-model-prompting-a-survey-haochen-li-et-al-2024>(27/90) Towards Goal-oriented Large Language Model Prompting: A Survey (Haochen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochen Li, Jonathan Leung, Zhiqi Shen. (2024)<br><strong>Towards Goal-oriented Large Language Model Prompting: A Survey</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14043v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs&rsquo; performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.</p></p class="citation"></blockquote><h3 id=2890-chatgpt-v-bert-dawn-of-justice-for-semantic-change-detection-francesco-periti-et-al-2024>(28/90) (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection (Francesco Periti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Periti, Haim Dubossarsky, Nina Tahmasebi. (2024)<br><strong>(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: BERT, ChatGPT, GPT, Natural Language Processing, Transformer<br><a href=http://arxiv.org/abs/2401.14040v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in detecting short-term changes.</p></p class="citation"></blockquote><h3 id=2990-unitxt-flexible-shareable-and-reusable-data-preparation-and-evaluation-for-generative-ai-elron-bandel-et-al-2024>(29/90) Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI (Elron Bandel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz. (2024)<br><strong>Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: AI, Generative AI, NLP<br><a href=http://arxiv.org/abs/2401.14019v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively. Join the Unitxt community at <a href=https://github.com/IBM/unitxt>https://github.com/IBM/unitxt</a>!</p></p class="citation"></blockquote><h3 id=3090-towards-uncertainty-aware-language-agent-jiuzhou-han-et-al-2024>(30/90) Towards Uncertainty-Aware Language Agent (Jiuzhou Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiuzhou Han, Wray Buntine, Ehsan Shareghi. (2024)<br><strong>Towards Uncertainty-Aware Language Agent</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: Language Model, QA<br><a href=http://arxiv.org/abs/2401.14016v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.</p></p class="citation"></blockquote><h3 id=3190-cmmu-a-benchmark-for-chinese-multi-modal-multi-type-question-understanding-and-reasoning-zheqi-he-et-al-2024>(31/90) CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning (Zheqi He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, Hua Huang. (2024)<br><strong>CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-MM, cs.CL<br>Keywords: GPT, Reasoning<br><a href=http://arxiv.org/abs/2401.14011v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strategy aims to reduce position bias, minimize the influence of randomness on correctness, and perform a quantitative analysis of position bias. We evaluate seven open-source MLLMs along with GPT4-V, Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a significant challenge to the recent MLLMs.</p></p class="citation"></blockquote><h3 id=3290-constraintchecker-a-plugin-for-large-language-models-to-reason-on-commonsense-knowledge-bases-quyet-v-do-et-al-2024>(32/90) ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases (Quyet V. Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song. (2024)<br><strong>ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: Commonsense Knowledge, Language Model, Reasoning<br><a href=http://arxiv.org/abs/2401.14003v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed <strong>ConstraintChecker</strong>, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output. Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods. Codes and data are available at \url{https://github.com/HKUST-KnowComp/ConstraintChecker}.</p></p class="citation"></blockquote><h3 id=3390-investigate-consolidate-exploit-a-general-strategy-for-inter-task-agent-self-evolution-cheng-qian-et-al-2024>(33/90) Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution (Cheng Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: AI, GPT, GPT-3.5, GPT-4<br><a href=http://arxiv.org/abs/2401.13996v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE&rsquo;s effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model&rsquo;s capability. Specifically, when combined with GPT-3.5, ICE&rsquo;s performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy.</p></p class="citation"></blockquote><h3 id=3490-leeroo-orchestrator-elevating-llms-performance-through-model-integration-alireza-mohammadshahi-et-al-2024>(34/90) Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration (Alireza Mohammadshahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alireza Mohammadshahi, Ali Shaikh, Majid Yazdani. (2024)<br><strong>Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keywords: GPT<br><a href=http://arxiv.org/abs/2401.13979v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral&rsquo;s accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator nearly matches GPT4&rsquo;s performance at half the cost and even exceeds GPT4&rsquo;s results with a 25% cost reduction. These findings illustrate the potential of our architecture in creating state-of-the-art and cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve superior performance outcomes.</p></p class="citation"></blockquote><h3 id=3590-adaptive-text-watermark-for-large-language-models-yepeng-liu-et-al-2024>(35/90) Adaptive Text Watermark for Large Language Models (Yepeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yepeng Liu, Yuheng Bu. (2024)<br><strong>Adaptive Text Watermark for Large Language Models</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: AI, Language Model<br><a href=http://arxiv.org/abs/2401.13927v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark&rsquo;s impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \emph{un-watermarked} LLMs while maintaining security even under various attacks.</p></p class="citation"></blockquote><h3 id=3690-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models-hongliang-he-et-al-2024>(36/90) WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models (Hongliang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu. (2024)<br><strong>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keywords: GPT, GPT-4<br><a href=http://arxiv.org/abs/2401.13919v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager in practical applications. We found that our proposed automatic evaluation achieves 85.3% agreement with human judgment, paving the way for further development of web agents in a real-world setting.</p></p class="citation"></blockquote><h3 id=3790-no-more-distractions-an-adaptive-up-sampling-algorithm-to-reduce-data-artifacts-han-chen-2024>(37/90) No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts (Han Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Chen. (2024)<br><strong>No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: NLI<br><a href=http://arxiv.org/abs/2401.13907v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected.</p></p class="citation"></blockquote><h3 id=3890-a-comparative-study-of-zero-shot-inference-with-large-language-models-and-supervised-modeling-in-breast-cancer-pathology-classification-madhumita-sushil-et-al-2024>(38/90) A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification (Madhumita Sushil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Madhumita Sushil, Travis Zack, Divneet Mandair, Zhiwei Zheng, Ahmed Wali, Yan-Ning Yu, Yuwei Quan, Atul J. Butte. (2024)<br><strong>A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keywords: BERT, GPT, GPT-3.5, GPT-4, LSTM, NLP<br><a href=http://arxiv.org/abs/2401.13887v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the differences were more prominent. Frequent sources of GPT-4 errors included inferences from multiple samples and complex task design. On complex tasks where large annotated datasets cannot be easily collected, LLMs can reduce the burden of large-scale data labeling. However, if the use of LLMs is prohibitive, the use of simpler supervised models with large annotated datasets can provide comparable results. LLMs demonstrated the potential to speed up the execution of clinical NLP studies by reducing the need for curating large annotated datasets. This may result in an increase in the utilization of NLP-based variables and outcomes in observational clinical studies.</p></p class="citation"></blockquote><h3 id=3990-unmasking-and-quantifying-racial-bias-of-large-language-models-in-medical-report-generation-yifan-yang-et-al-2024>(39/90) Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation (Yifan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, Zhiyong Lu. (2024)<br><strong>Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation</strong></p><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keywords: Bias, GPT, GPT-3.5, GPT-4, Language Model<br><a href=http://arxiv.org/abs/2401.13867v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all patients.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=4090-the-typing-cure-experiences-with-large-language-model-chatbots-for-mental-health-support-inhwa-song-et-al-2024>(40/90) The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support (Inhwa Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury. (2024)<br><strong>The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support</strong></p><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs.HC<br>Keywords: AI, Language Model<br><a href=http://arxiv.org/abs/2401.14362v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care.</p></p class="citation"></blockquote><h3 id=4190-gptvoicetasker-llm-powered-virtual-assistant-for-smartphone-minh-duc-vu-et-al-2024>(41/90) GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone (Minh Duc Vu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Duc Vu, Han Wang, Zhuang Li, Jieshan Chen, Shengdong Zhao, Zhenchang Xing, Chunyang Chen. (2024)<br><strong>GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone</strong></p><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keywords: GPT, Language Model<br><a href=http://arxiv.org/abs/2401.14268v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Virtual assistants have the potential to play an important role in helping users achieves different tasks. However, these systems face challenges in their real-world usability, characterized by inefficiency and struggles in grasping user intentions. Leveraging recent advances in Large Language Models (LLMs), we introduce GptVoiceTasker, a virtual assistant poised to enhance user experiences and task efficiency on mobile devices. GptVoiceTasker excels at intelligently deciphering user commands and executing relevant device interactions to streamline task completion. The system continually learns from historical user commands to automate subsequent usages, further enhancing execution efficiency. Our experiments affirm GptVoiceTasker&rsquo;s exceptional command interpretation abilities and the precision of its task automation module. In our user study, GptVoiceTasker boosted task efficiency in real-world scenarios by 34.85%, accompanied by positive participant feedback. We made GptVoiceTasker open-source, inviting further research into LLMs utilization for diverse tasks through prompt engineering and leveraging user usage data to improve efficiency.</p></p class="citation"></blockquote><h3 id=4290-mapping-the-design-space-of-teachable-social-media-feed-experiences-k-j-kevin-feng-et-al-2024>(42/90) Mapping the Design Space of Teachable Social Media Feed Experiences (K. J. Kevin Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>K. J. Kevin Feng, Xander Koo, Lawrence Tan, Amy Bruckman, David W. McDonald, Amy X. Zhang. (2024)<br><strong>Mapping the Design Space of Teachable Social Media Feed Experiences</strong></p><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keywords: Social Media, Twitter<br><a href=http://arxiv.org/abs/2401.14000v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Social media feeds are deeply personal spaces that reflect individual values and preferences. However, top-down, platform-wide content algorithms can reduce users&rsquo; sense of agency and fail to account for nuanced experiences and values. Drawing on the paradigm of interactive machine teaching (IMT), an interaction framework for non-expert algorithmic adaptation, we map out a design space for teachable social media feed experiences to empower agential, personalized feed curation. To do so, we conducted a think-aloud study (N=24) featuring four social media platforms &ndash; Instagram, Mastodon, TikTok, and Twitter &ndash; to understand key signals users leveraged to determine the value of a post in their feed. We synthesized users&rsquo; signals into taxonomies that, when combined with user interviews, inform five design principles that extend IMT into the social media setting. We finally embodied our principles into three feed designs that we present as sensitizing concepts for teachable feed experiences moving forward.</p></p class="citation"></blockquote><h3 id=4390-cuichi-2024-building-trust-in-cuis-from-design-to-deployment-smit-desai-et-al-2024>(43/90) CUI@CHI 2024: Building Trust in CUIs-From Design to Deployment (Smit Desai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Smit Desai, Christina Wei, Jaisie Sin, Mateusz Dubiel, Nima Zargham, Shashank Ahire, Martin Porcheron, Anastasia Kuzminykh, Minha Lee, Heloisa Candello, Joel Fischer, Cosmin Munteanu, Benjamin R Cowan. (2024)<br><strong>CUI@CHI 2024: Building Trust in CUIs-From Design to Deployment</strong></p><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keywords: ChatGPT, GPT<br><a href=http://arxiv.org/abs/2401.13970v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Conversational user interfaces (CUIs) have become an everyday technology for people the world over, as well as a booming area of research. Advances in voice synthesis and the emergence of chatbots powered by large language models (LLMs), notably ChatGPT, have pushed CUIs to the forefront of human-computer interaction (HCI) research and practice. Now that these technologies enable an elemental level of usability and user experience (UX), we must turn our attention to higher-order human factors: trust and reliance. In this workshop, we aim to bring together a multidisciplinary group of researchers and practitioners invested in the next phase of CUI design. Through keynotes, presentations, and breakout sessions, we will share our knowledge, identify cutting-edge resources, and fortify an international network of CUI scholars. In particular, we will engage with the complexity of trust and reliance as attitudes and behaviours that emerge when people interact with conversational agents.</p></p class="citation"></blockquote><h2 id=cslg-17>cs.LG (17)</h2><h3 id=4490-serverlessllm-locality-enhanced-serverless-inference-for-large-language-models-yao-fu-et-al-2024>(44/90) ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models (Yao Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii Ustiugov, Yuvraj Patel, Luo Mai. (2024)<br><strong>ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14351v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This paper presents ServerlessLLM, a locality-enhanced serverless inference system for Large Language Models (LLMs). ServerlessLLM exploits the substantial capacity and bandwidth of storage and memory devices available on GPU servers, thereby reducing costly remote checkpoint downloads and achieving efficient checkpoint loading. ServerlessLLM achieves this through three main contributions: (i) fast LLM checkpoint loading via a novel loading-optimized checkpoint format design, coupled with an efficient multi-tier checkpoint loading system; (ii) locality-driven LLM inference with live migration, which allows ServerlessLLM to effectively achieve locality-driven server allocation while preserving the low latency of ongoing LLM inference; and (iii) locality-aware server allocation, enabling ServerlessLLM to evaluate the status of each server in a cluster and effectively schedule model startup time to capitalize on local checkpoint placement. Our comprehensive experiments, which include microbenchmarks and real-world traces, show that ServerlessLLM surpasses state-of-the-art systems by 10 - 200X in latency performance when running various LLM inference workloads.</p></p class="citation"></blockquote><h3 id=4590-interpretable-solutions-for-breast-cancer-diagnosis-with-grammatical-evolution-and-data-augmentation-yumnah-hasan-et-al-2024>(45/90) Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation (Yumnah Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumnah Hasan, Allan de Lima, Fatemeh Amerehi, Darian Reyes Fernandez de Bulnes, Patrick Healy, Conor Ryan. (2024)<br><strong>Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keywords: Augmentation<br><a href=http://arxiv.org/abs/2401.14255v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Medical imaging diagnosis increasingly relies on Machine Learning (ML) models. This is a task that is often hampered by severely imbalanced datasets, where positive cases can be quite rare. Their use is further compromised by their limited interpretability, which is becoming increasingly important. While post-hoc interpretability techniques such as SHAP and LIME have been used with some success on so-called black box models, the use of inherently understandable models makes such endeavors more fruitful. This paper addresses these issues by demonstrating how a relatively new synthetic data generation technique, STEM, can be used to produce data to train models produced by Grammatical Evolution (GE) that are inherently understandable. STEM is a recently introduced combination of the Synthetic Minority Oversampling Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously been successfully used to tackle both between class and within class imbalance issues. We test our technique on the Digital Database for Screening Mammography (DDSM) and the Wisconsin Breast Cancer (WBC) datasets and compare Area Under the Curve (AUC) results with an ensemble of the top three performing classifiers from a set of eight standard ML classifiers with varying degrees of interpretability. We demonstrate that the GE-derived models present the best AUC while still maintaining interpretable solutions.</p></p class="citation"></blockquote><h3 id=4690-sample-efficient-reinforcement-learning-by-automatically-learning-to-compose-subtasks-shuai-han-et-al-2024>(46/90) Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks (Shuai Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Han, Mehdi Dastani, Shihan Wang. (2024)<br><strong>Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keywords: Reinforcement Learning<br><a href=http://arxiv.org/abs/2401.14226v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our algorithm in a variety of sparse-reward environments. The experiment results show that our approach significantly outperforms the state-of-art baselines as the difficulty of the task increases.</p></p class="citation"></blockquote><h3 id=4790-how-can-large-language-models-understand-spatial-temporal-data-lei-liu-et-al-2024>(47/90) How Can Large Language Models Understand Spatial-Temporal Data? (Lei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, Yanming Shen. (2024)<br><strong>How Can Large Language Models Understand Spatial-Temporal Data?</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.14192v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diverse spatial-temporal benchmark datasets show that STG-LLM successfully unlocks LLM potential for spatial-temporal forecasting. Remarkably, our approach achieves competitive performance on par with dedicated SOTA methods.</p></p class="citation"></blockquote><h3 id=4890-alleviating-structural-distribution-shift-in-graph-anomaly-detection-yuan-gao-et-al-2024>(48/90) Alleviating Structural Distribution Shift in Graph Anomaly Detection (Yuan Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang. (2024)<br><strong>Alleviating Structural Distribution Shift in Graph Anomaly Detection</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keywords: Anomaly Detection, GNN<br><a href=http://arxiv.org/abs/2401.14155v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Graph anomaly detection (GAD) is a challenging binary classification problem due to its different structural distribution between anomalies and normal nodes &ndash; abnormal nodes are a minority, therefore holding high heterophily and low homophily compared to normal nodes. Furthermore, due to various time factors and the annotation preferences of human experts, the heterophily and homophily can change across training and testing data, which is called structural distribution shift (SDS) in this paper. The mainstream methods are built on graph neural networks (GNNs), benefiting the classification of normals from aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and suffering from poor generalization. This work solves the problem from a feature view. We observe that the degree of SDS varies between anomalies and normal nodes. Hence to address the issue, the key lies in resisting high heterophily for anomalies meanwhile benefiting the learning of normals from homophily. We tease out the anomaly features on which we constrain to mitigate the effect of heterophilous neighbors and make them invariant. We term our proposed framework as Graph Decomposition Network (GDN). Extensive experiments are conducted on two benchmark datasets, and the proposed framework achieves a remarkable performance boost in GAD, especially in an SDS environment where anomalies have largely different structural distribution across training and testing environments. Codes are open-sourced in <a href=https://github.com/blacksingular/wsdm_GDN>https://github.com/blacksingular/wsdm_GDN</a>.</p></p class="citation"></blockquote><h3 id=4990-true-knowledge-comes-from-practice-aligning-llms-with-embodied-environments-via-reinforcement-learning-weihao-tan-et-al-2024>(49/90) True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning (Weihao Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An. (2024)<br><strong>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keywords: Reinforcement Learning<br><a href=http://arxiv.org/abs/2401.14151v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs&rsquo; open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs&rsquo; original ability during online PPO finetuning.</p></p class="citation"></blockquote><h3 id=5090-fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design-haojun-xia-et-al-2024>(50/90) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design (Haojun Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024)<br><strong>FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-AR, cs-LG, cs.LG<br>Keywords: LLaMA, Language Model<br><a href=http://arxiv.org/abs/2401.14112v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline. The source code will be publicly available soon.</p></p class="citation"></blockquote><h3 id=5190-learning-under-label-noise-through-few-shot-human-in-the-loop-refinement-aaqib-saeed-et-al-2024>(51/90) Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement (Aaqib Saeed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaqib Saeed, Dimitris Spathis, Jungwoo Oh, Edward Choi, Ali Etemad. (2024)<br><strong>Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keywords: Few-Shot<br><a href=http://arxiv.org/abs/2401.14107v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels. We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19% accuracy improvement under symmetric and asymmetric noise. Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation. Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models.</p></p class="citation"></blockquote><h3 id=5290-sparse-and-transferable-universal-singular-vectors-attack-kseniia-kuvshinova-et-al-2024>(52/90) Sparse and Transferable Universal Singular Vectors Attack (Kseniia Kuvshinova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets. (2024)<br><strong>Sparse and Transferable Universal Singular Vectors Attack</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keywords: ImageNet<br><a href=http://arxiv.org/abs/2401.14031v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The research in the field of adversarial attacks and models&rsquo; vulnerability is one of the fundamental directions in modern machine learning. Recent studies reveal the vulnerability phenomenon, and understanding the mechanisms behind this is essential for improving neural network characteristics and interpretability. In this paper, we propose a novel sparse universal white-box adversarial attack. Our approach is based on truncated power iteration providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian matrices. Using the ImageNet benchmark validation subset, we analyze the proposed method in various settings, achieving results comparable to dense baselines with more than a 50% fooling rate while damaging only 5% of pixels and utilizing 256 samples for perturbation fitting. We also show that our algorithm admits higher attack magnitude without affecting the human ability to solve the task. Furthermore, we investigate that the constructed perturbations are highly transferable among different models without significantly decreasing the fooling rate. Our findings demonstrate the vulnerability of state-of-the-art models to sparse attacks and highlight the importance of developing robust machine learning systems.</p></p class="citation"></blockquote><h3 id=5390-accelerating-retrieval-augmented-language-model-serving-with-speculation-zhihao-zhang-et-al-2024>(53/90) Accelerating Retrieval-Augmented Language Model Serving with Speculation (Zhihao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, Zhihao Jia. (2024)<br><strong>Accelerating Retrieval-Augmented Language Model Serving with Speculation</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IR, cs-LG, cs.LG<br>Keywords: Language Model, NLP, QA<br><a href=http://arxiv.org/abs/2401.14021v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.</p></p class="citation"></blockquote><h3 id=5490-cross-domain-few-shot-learning-via-adaptive-transformer-networks-naeem-paeedeh-et-al-2024>(54/90) Cross-Domain Few-Shot Learning via Adaptive Transformer Networks (Naeem Paeedeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naeem Paeedeh, Mahardhika Pratama, Muhammad Anwar Ma&rsquo;sum, Wolfgang Mayer, Zehong Cao, Ryszard Kowlczyk. (2024)<br><strong>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keywords: Few-Shot, Transformer<br><a href=http://arxiv.org/abs/2401.13987v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.</p></p class="citation"></blockquote><h3 id=5590-dynamic-long-term-time-series-forecasting-via-meta-transformer-networks-muhammad-anwar-masum-et-al-2024>(55/90) Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks (Muhammad Anwar Ma&rsquo;sum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Anwar Ma&rsquo;sum, MD Rasel Sarkar, Mahardhika Pratama, Savitha Ramasamy, Sreenatha Anavatti, Lin Liu, Habibullah, Ryszard Kowalczyk. (2024)<br><strong>Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keywords: Transformer<br><a href=http://arxiv.org/abs/2401.13968v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANTRA are publicly available in \url{https://github.com/anwarmaxsum/MANTRA}.</p></p class="citation"></blockquote><h3 id=5690-reinforcement-learning-with-hidden-markov-models-for-discovering-decision-making-dynamics-xingche-guo-et-al-2024>(56/90) Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics (Xingche Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingche Guo, Donglin Zeng, Yuanjia Wang. (2024)<br><strong>Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP, stat-ME, stat-ML<br>Keywords: Reinforcement Learning<br><a href=http://arxiv.org/abs/2401.13929v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for analyzing reward-based decision-making. Our model accommodates learning strategy switching between two distinct approaches under a hidden Markov model (HMM): subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient EM algorithm for parameter estimation and employ a nonparametric bootstrap for inference. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.</p></p class="citation"></blockquote><h3 id=5790-towards-3d-molecule-text-interpretation-in-language-models-sihang-li-et-al-2024>(57/90) Towards 3D Molecule-Text Interpretation in Language Models (Sihang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, Qi Tian. (2024)<br><strong>Towards 3D Molecule-Text Interpretation in Language Models</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG, q-bio-BM<br>Keywords: Language Model, QA<br><a href=http://arxiv.org/abs/2401.13923v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze 3D molecules by equipping the LM with a 3D molecular encoder. This integration is achieved by a 3D molecule-text projector, bridging the 3D molecular encoder&rsquo;s representation space and the LM&rsquo;s input space. Moreover, to enhance 3D-MoLM&rsquo;s ability of cross-modal molecular understanding and instruction following, we meticulously curated a 3D molecule-centric instruction tuning dataset &ndash; 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder and LM. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular QA tasks, especially focusing on 3D-dependent properties.</p></p class="citation"></blockquote><h3 id=5890-locmoe-a-low-overhead-moe-for-large-language-model-training-jing-li-et-al-2024>(58/90) LocMoE: A Low-overhead MoE for Large Language Model Training (Jing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Li, Zhijie Sun, Xuan He, Li Zeng, Yi Lin, Entong Li, Binfan Zheng, Rongqian Zhao, Xin Chen. (2024)<br><strong>LocMoE: A Low-overhead MoE for Large Language Model Training</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keywords: Language Model<br><a href=http://arxiv.org/abs/2401.13920v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We port these modifications on the PanGu-Sigma model based on the MindSpore framework with multi-level routing and conduct experiments on Ascend clusters. The experiment results demonstrate that the proposed LocMoE reduces training time per epoch by 12.68% to 22.24% compared to classical routers, such as hash router and switch router, without impacting the model accuracy.</p></p class="citation"></blockquote><h3 id=5990-a-survey-of-deep-learning-and-foundation-models-for-time-series-forecasting-john-a-miller-et-al-2024>(59/90) A Survey of Deep Learning and Foundation Models for Time Series Forecasting (John A. Miller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu. (2024)<br><strong>A Survey of Deep Learning and Foundation Models for Time Series Forecasting</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keywords: Knowledge Graph, Language Model, Time Series<br><a href=http://arxiv.org/abs/2401.13912v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.</p></p class="citation"></blockquote><h3 id=6090-edge-conditional-node-update-graph-neural-network-for-multi-variate-time-series-anomaly-detection-hayoung-jo-et-al-2024>(60/90) Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection (Hayoung Jo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hayoung Jo, Seong-Whan Lee. (2024)<br><strong>Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection</strong></p><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keywords: Anomaly Detection, GNN, Graph Neural Network, Time Series<br><a href=http://arxiv.org/abs/2401.13872v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancement in cyber-physical systems, the increasing number of sensors has significantly complicated manual monitoring of system states. Consequently, graph-based time-series anomaly detection methods have gained attention due to their ability to explicitly represent relationships between sensors. However, these methods often apply a uniform source node representation across all connected target nodes, even when updating different target node representations. Moreover, the graph attention mechanism, commonly used to infer unknown graph structures, could constrain the diversity of source node representations. In this paper, we introduce the Edge Conditional Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge conditional node update module, dynamically transforms source node representations based on connected edges to represent target nodes aptly. We validate performance on three real-world datasets: SWaT, WADI, and PSM. Our model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively, compared to best F1 baseline models.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=6190-5g-network-security-practices-an-overview-and-survey-fatema-bannat-wala-et-al-2024>(61/90) 5G Network Security Practices: An Overview and Survey (Fatema Bannat Wala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatema Bannat Wala, Mariam Kiran. (2024)<br><strong>5G Network Security Practices: An Overview and Survey</strong></p><hr><p>Primary Category: cs.NI<br>Categories: cs-CR, cs-NI, cs.NI<br>Keywords: Network Security, Security<br><a href=http://arxiv.org/abs/2401.14350v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>This document provides an overview of 5G network security, describing various components of the 5G core network architecture and what kind of security services are offered by these 5G components. It also explores the potential security risks and vulnerabilities presented by the security architecture in 5G and recommends some of the best practices for the 5G network admins to consider while deploying a secure 5G network, based on the surveyed documents from the European government&rsquo;s efforts in commercializing the IoT devices and securing supply chain over 5G networks.</p></p class="citation"></blockquote><h2 id=csro-2>cs.RO (2)</h2><h3 id=6290-learning-to-navigate-efficiently-and-precisely-in-real-environments-guillaume-bono-et-al-2024>(62/90) Learning to navigate efficiently and precisely in real environments (Guillaume Bono et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillaume Bono, Hervé Poirier, Leonid Antsfeld, Gianluca Monaci, Boris Chidlovskii, Christian Wolf. (2024)<br><strong>Learning to navigate efficiently and precisely in real environments</strong></p><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14349v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.</p></p class="citation"></blockquote><h3 id=6390-concept-dynamic-risk-assessment-for-ai-controlled-robotic-systems-philipp-grimmeisen-et-al-2024>(63/90) Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems (Philipp Grimmeisen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Grimmeisen, Friedrich Sautter, Andrey Morozov. (2024)<br><strong>Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems</strong></p><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keywords: AI, Reinforcement Learning<br><a href=http://arxiv.org/abs/2401.14147v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>AI-controlled robotic systems pose a risk to human workers and the environment. Classical risk assessment methods cannot adequately describe such black box systems. Therefore, new methods for a dynamic risk assessment of such AI-controlled systems are required. In this paper, we introduce the concept of a new dynamic risk assessment approach for AI-controlled robotic systems. The approach pipelines five blocks: (i) a Data Logging that logs the data of the given simulation, (ii) a Skill Detection that automatically detects the executed skills with a deep learning technique, (iii) a Behavioral Analysis that creates the behavioral profile of the robotic systems, (iv) a Risk Model Generation that automatically transforms the behavioral profile and risk data containing the failure probabilities of robotic hardware components into advanced hybrid risk models, and (v) Risk Model Solvers for the numerical evaluation of the generated hybrid risk models. Keywords: Dynamic Risk Assessment, Hybrid Risk Models, M2M Transformation, ROS, AI-Controlled Robotic Systems, Deep Learning, Reinforcement Learning</p></p class="citation"></blockquote><h2 id=cscr-2>cs.CR (2)</h2><h3 id=6490-sunblock-cloudless-protection-for-iot-systems-vadim-safronov-et-al-2024>(64/90) SunBlock: Cloudless Protection for IoT Systems (Vadim Safronov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vadim Safronov, Anna Maria Mandalari, Daniel J. Dubois, David Choffnes, Hamed Haddadi. (2024)<br><strong>SunBlock: Cloudless Protection for IoT Systems</strong></p><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14332v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>With an increasing number of Internet of Things (IoT) devices present in homes, there is a rise in the number of potential information leakage channels and their associated security threats and privacy risks. Despite a long history of attacks on IoT devices in unprotected home networks, the problem of accurate, rapid detection and prevention of such attacks remains open. Many existing IoT protection solutions are cloud-based, sometimes ineffective, and might share consumer data with unknown third parties. This paper investigates the potential for effective IoT threat detection locally, on a home router, using AI tools combined with classic rule-based traffic-filtering algorithms. Our results show that with a slight rise of router hardware resources caused by machine learning and traffic filtering logic, a typical home router instrumented with our solution is able to effectively detect risks and protect a typical home IoT network, equaling or outperforming existing popular solutions, without any effects on benign IoT functionality, and without relying on cloud services and third parties.</p></p class="citation"></blockquote><h3 id=6590-cyber-twin-digital-twin-boosted-autonomous-attack-detection-for-vehicular-ad-hoc-networks-yagmur-yigit-et-al-2024>(65/90) Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for Vehicular Ad-Hoc Networks (Yagmur Yigit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yagmur Yigit, Ioannis Panitsas, Leandros Maglaras, Leandros Tassiulas, Berk Canberk. (2024)<br><strong>Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for Vehicular Ad-Hoc Networks</strong></p><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14005v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The rapid evolution of Vehicular Ad-hoc NETworks (VANETs) has ushered in a transformative era for intelligent transportation systems (ITS), significantly enhancing road safety and vehicular communication. However, the intricate and dynamic nature of VANETs presents formidable challenges, particularly in vehicle-to-infrastructure (V2I) communications. Roadside Units (RSUs), integral components of VANETs, are increasingly susceptible to cyberattacks, such as jamming and distributed denial-of-service (DDoS) attacks. These vulnerabilities pose grave risks to road safety, potentially leading to traffic congestion and vehicle malfunctions. Current approaches often struggle to effectively merge digital twin technology with Artificial Intelligence (AI) models to boost security and sustainability. Our study introduces an innovative cyber-twin framework tailored to enhance the security of RSUs in VANETs. This framework uniquely combines digital twin technology with cutting-edge AI to offer a real-time, dynamic representation of RSUs. This allows for detailed monitoring and efficient detection of threats, significantly strengthening RSU security in VANETs. Moreover, our framework makes a notable contribution to eco-friendly communication by improving the computational efficiency of RSUs, leading to increased energy efficiency and extended hardware durability. Our results show a considerable enhancement in resource management and attack detection, surpassing the performance of existing solutions. In particular, the cyber-twin framework showed a substantial reduction in RSU load and an optimal balance between resource consumption and high attack detection efficiency, with a defined twinning rate range of seventy-six to ninety per cent. These advancements underscore our commitment to developing sustainable, secure, and resilient vehicular communication systems for the future of smart cities.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=6690-vall-t-decoder-only-generative-transducer-for-robust-and-decoding-controllable-text-to-speech-chenpeng-du-et-al-2024>(66/90) VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech (Chenpeng Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenpeng Du, Yiwei Guo, Hankun Wang, Yifan Yang, Zhikang Niu, Shuai Wang, Hui Zhang, Xie Chen, Kai Yu. (2024)<br><strong>VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech</strong></p><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keywords: Transformer<br><a href=http://arxiv.org/abs/2401.14321v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and difficulty in stopping. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate. Furthermore, the controllability of alignment in VALL-T during decoding facilitates the use of untranscribed speech prompts, even in unknown languages. It also enables the synthesis of lengthy speech by utilizing an aligned context window.</p></p class="citation"></blockquote><h3 id=6790-intelli-z-toward-intelligible-zero-shot-tts-sunghee-jung-et-al-2024>(67/90) Intelli-Z: Toward Intelligible Zero-Shot TTS (Sunghee Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunghee Jung, Won Jang, Jaesam Yoon, Bongwan Kim. (2024)<br><strong>Intelli-Z: Toward Intelligible Zero-Shot TTS</strong></p><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keywords: Zero-Shot<br><a href=http://arxiv.org/abs/2401.13921v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Although numerous recent studies have suggested new frameworks for zero-shot TTS using large-scale, real-world data, studies that focus on the intelligibility of zero-shot TTS are relatively scarce. Zero-shot TTS demands additional efforts to ensure clear pronunciation and speech quality due to its inherent requirement of replacing a core parameter (speaker embedding or acoustic prompt) with a new one at the inference stage. In this study, we propose a zero-shot TTS model focused on intelligibility, which we refer to as Intelli-Z. Intelli-Z learns speaker embeddings by using multi-speaker TTS as its teacher and is trained with a cycle-consistency loss to include mismatched text-speech pairs for training. Additionally, it selectively aggregates speaker embeddings along the temporal dimension to minimize the interference of the text content of reference speech at the inference stage. We substantiate the effectiveness of the proposed methods with an ablation study. The Mean Opinion Score (MOS) increases by 9% for unseen speakers when the first two methods are ap- plied, and it further improves by 16% when selective temporal aggregation is applied.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=6890-multitest-physical-aware-object-insertion-for-testing-multi-sensor-fusion-perception-systems-xinyu-gao-et-al-2024>(68/90) MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems (Xinyu Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Gao, Zhijie Wang, Yang Feng, Lei Ma, Zhenyu Chen, Baowen Xu. (2024)<br><strong>MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14314v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Multi-sensor fusion stands as a pivotal technique in addressing numerous safety-critical tasks and applications, e.g., self-driving cars and automated robotic arms. With the continuous advancement in data-driven artificial intelligence (AI), MSF&rsquo;s potential for sensing and understanding intricate external environments has been further amplified, bringing a profound impact on intelligent systems and specifically on their perception systems. Similar to traditional software, adequate testing is also required for AI-enabled MSF systems. Yet, existing testing methods primarily concentrate on single-sensor perception systems (e.g., image-/point cloud-based object detection systems). There remains a lack of emphasis on generating multi-modal test cases for MSF systems. To address these limitations, we design and implement MultiTest, a fitness-guided metamorphic testing method for complex MSF perception systems. MultiTest employs a physical-aware approach to synthesize realistic multi-modal object instances and insert them into critical positions of background images and point clouds. A fitness metric is designed to guide and boost the test generation process. We conduct extensive experiments with five SOTA perception systems to evaluate MultiTest from the perspectives of: (1) generated test cases&rsquo; realism, (2) fault detection capabilities, and (3) performance improvement. The results show that MultiTest can generate realistic and modality-consistent test data and effectively detect hundreds of diverse faults of an MSF system under test. Moreover, retraining an MSF system on the test cases generated by MultiTest can improve the system&rsquo;s robustness.</p></p class="citation"></blockquote><h3 id=6990-zs4c-zero-shot-synthesis-of-compilable-code-for-incomplete-code-snippets-using-chatgpt-azmain-kabir-et-al-2024>(69/90) ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT (Azmain Kabir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azmain Kabir, Shaowei Wang, Yuan Tian, Tse-Hsun, Chen, Muhammad Asaduzzaman, Wenbin Zhang. (2024)<br><strong>ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keywords: ChatGPT, GPT, Language Model, Zero-Shot<br><a href=http://arxiv.org/abs/2401.14279v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Technical question and answering (Q&amp;A) sites such as Stack Overflow have become an important source for software developers to seek knowledge. However, code snippets on Q&amp;A sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze Q&amp;A code snippets. Prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. To address this problem, we propose ZS4C, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using Large Language Model (LLM). ZS4C operates in two stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. In the second stage, ZS4C fixes compilation errors caused by incorrect import statements and syntax errors through collaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C on a widely used benchmark called StatType-SO against the SOTA approach SnR. Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a 39.3% improvement. On average, ZS4C can infer more accurate import statements than SnR, with an improvement of 6.6% in the F1.</p></p class="citation"></blockquote><h3 id=7090-deepseek-coder-when-the-large-language-model-meets-programming----the-rise-of-code-intelligence-daya-guo-et-al-2024>(70/90) DeepSeek-Coder: When the Large Language Model Meets Programming &ndash; The Rise of Code Intelligence (Daya Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang. (2024)<br><strong>DeepSeek-Coder: When the Large Language Model Meets Programming &ndash; The Rise of Code Intelligence</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-SE, cs.SE<br>Keywords: GPT, GPT-3.5, Language Model<br><a href=http://arxiv.org/abs/2401.14196v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.</p></p class="citation"></blockquote><h3 id=7190-copilot-refinement-addressing-code-smells-in-copilot-generated-python-code-beiqi-zhang-et-al-2024>(71/90) Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code (Beiqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beiqi Zhang, Peng Liang, Qiong Feng, Yujia Fu, Zengyang Li. (2024)<br><strong>Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keywords: AI, Language Model<br><a href=http://arxiv.org/abs/2401.14176v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot&rsquo;s ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself. Besides, the effectiveness of Copilot Chat in fixing these smells can be improved with the provision of more detailed prompts. However, using Copilot Chat to fix these smells might introduce new code smells.</p></p class="citation"></blockquote><h3 id=7290-mcudi-model-centric-unsupervised-degradation-indicator-for-failure-prediction-aiops-solutions-lorena-poenaru-olaru-et-al-2024>(72/90) McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions (Lorena Poenaru-Olaru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorena Poenaru-Olaru, Luis Cruz, Jan Rellermeyer, Arie van Deursen. (2024)<br><strong>McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14093v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Due to the continuous change in operational data, AIOps solutions suffer from performance degradation over time. Although periodic retraining is the state-of-the-art technique to preserve the failure prediction AIOps models&rsquo; performance over time, this technique requires a considerable amount of labeled data to retrain. In AIOps obtaining label data is expensive since it requires the availability of domain experts to intensively annotate it. In this paper, we present McUDI, a model-centric unsupervised degradation indicator that is capable of detecting the exact moment the AIOps model requires retraining as a result of changes in data. We further show how employing McUDI in the maintenance pipeline of AIOps solutions can reduce the number of samples that require annotations with 30k for job failure prediction and 260k for disk failure prediction while achieving similar performance with periodic retraining.</p></p class="citation"></blockquote><h3 id=7390-from-requirements-to-architecture-an-ai-based-journey-to-semi-automatically-generate-software-architectures-tobias-eisenreich-et-al-2024>(73/90) From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures (Tobias Eisenreich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Eisenreich, Sandro Speth, Stefan Wagner. (2024)<br><strong>From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures</strong></p><hr><p>Primary Category: cs.SE<br>Categories: D-2-2, cs-AI, cs-SE, cs.SE<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14079v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system&rsquo;s quality of service. Due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. Existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. Therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. We further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. To evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies.</p></p class="citation"></blockquote><h3 id=7490-chatgpt-and-human-synergy-in-black-box-testing-a-comparative-analysis-hiroyuki-kirinuki-et-al-2024>(74/90) ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis (Hiroyuki Kirinuki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroyuki Kirinuki, Haruto Tanno. (2024)<br><strong>ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis</strong></p><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keywords: ChatGPT, GPT, GPT-4<br><a href=http://arxiv.org/abs/2401.13924v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In recent years, large language models (LLMs), such as ChatGPT, have been pivotal in advancing various artificial intelligence applications, including natural language processing and software engineering. A promising yet underexplored area is utilizing LLMs in software testing, particularly in black-box testing. This paper explores the test cases devised by ChatGPT in comparison to those created by human participants. In this study, ChatGPT (GPT-4) and four participants each created black-box test cases for three applications based on specifications written by the authors. The goal was to evaluate the real-world applicability of the proposed test cases, identify potential shortcomings, and comprehend how ChatGPT could enhance human testing strategies. ChatGPT can generate test cases that generally match or slightly surpass those created by human participants in terms of test viewpoint coverage. Additionally, our experiments demonstrated that when ChatGPT cooperates with humans, it can cover considerably more test viewpoints than each can achieve alone, suggesting that collaboration between humans and ChatGPT may be more effective than human pairs working together. Nevertheless, we noticed that the test cases generated by ChatGPT have certain issues that require addressing before use.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=7590-chiron-accelerating-node-synchronization-without-security-trade-offs-in-distributed-ledgers-ray-neiheiser-et-al-2024>(75/90) CHIRON: Accelerating Node Synchronization without Security Trade-offs in Distributed Ledgers (Ray Neiheiser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ray Neiheiser, Arman Babaei, Giannis Alexopoulos, Marios Kogias, Eleftherios Kokoris Kogias. (2024)<br><strong>CHIRON: Accelerating Node Synchronization without Security Trade-offs in Distributed Ledgers</strong></p><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keywords: Security<br><a href=http://arxiv.org/abs/2401.14278v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Blockchain performance has historically faced challenges posed by the throughput limitations of consensus algorithms. Recent breakthroughs in research have successfully alleviated these constraints by introducing a modular architecture that decouples consensus from execution. The move toward independent optimization of the consensus layer has shifted attention to the execution layer. While concurrent transaction execution is a promising solution for increasing throughput, practical challenges persist. Its effectiveness varies based on the workloads, and the associated increased hardware requirements raise concerns about undesirable centralization. This increased requirement results in full nodes and stragglers synchronizing from signed checkpoints, decreasing the trustless nature of blockchain systems. In response to these challenges, this paper introduces Chiron, a system designed to extract execution hints for the acceleration of straggling and full nodes. Notably, Chiron achieves this without compromising the security of the system or introducing overhead on the critical path of consensus. Evaluation results demonstrate a notable speedup of up to 30%, effectively addressing the gap between theoretical research and practical deployment. The quantification of this speedup is achieved through realistic blockchain benchmarks derived from a comprehensive analysis of Ethereum and Solana workloads, constituting an independent contribution.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=7690-on-mission-twitter-profiles-a-study-of-selective-toxic-behavior-hina-qayyum-et-al-2024>(76/90) On mission Twitter Profiles: A Study of Selective Toxic Behavior (Hina Qayyum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hina Qayyum, Muhammad Ikram, Benjamin Zi Hao Zhao, an D. Wood, Nicolas Kourtellis, Mohamed Ali Kaafar. (2024)<br><strong>On mission Twitter Profiles: A Study of Selective Toxic Behavior</strong></p><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keywords: Twitter<br><a href=http://arxiv.org/abs/2401.14252v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>The argument for persistent social media influence campaigns, often funded by malicious entities, is gaining traction. These entities utilize instrumented profiles to disseminate divisive content and disinformation, shaping public perception. Despite ample evidence of these instrumented profiles, few identification methods exist to locate them in the wild. To evade detection and appear genuine, small clusters of instrumented profiles engage in unrelated discussions, diverting attention from their true goals. This strategic thematic diversity conceals their selective polarity towards certain topics and fosters public trust. This study aims to characterize profiles potentially used for influence operations, termed &lsquo;on-mission profiles,&rsquo; relying solely on thematic content diversity within unlabeled data. Distinguishing this work is its focus on content volume and toxicity towards specific themes. Longitudinal data from 138K Twitter or X, profiles and 293M tweets enables profiling based on theme diversity. High thematic diversity groups predominantly produce toxic content concerning specific themes, like politics, health, and news classifying them as &lsquo;on-mission&rsquo; profiles. Using the identified <code>on-mission" profiles, we design a classifier for unseen, unlabeled data. Employing a linear SVM model, we train and test it on an 80/20% split of the most diverse profiles. The classifier achieves a flawless 100% accuracy, facilitating the discovery of previously unknown </code>on-mission" profiles in the wild.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=7790-clinical-melanoma-diagnosis-with-artificial-intelligence-insights-from-a-prospective-multicenter-study-lukas-heinlein-et-al-2024>(77/90) Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study (Lukas Heinlein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Heinlein, Roman C. Maron, Achim Hekler, Sarah Haggenmüller, Christoph Wies, Jochen S. Utikal, Friedegund Meier, Sarah Hobelsberger, Frank F. Gellrich, Mildred Sergon, Axel Hauschild, Lars E. French, Lucie Heinzerling, Justin G. Schlager, Kamran Ghoreschi, Max Schlaak, Franz J. Hilke, Gabriela Poch, Sören Korsing, Carola Berking, Markus V. Heppt, Michael Erdmann, Sebastian Haferkamp, Konstantin Drexler, Dirk Schadendorf, Wiebke Sondermann, Matthias Goebeler, Bastian Schilling, Eva Krieghoff-Henning, Titus J. Brinker. (2024)<br><strong>Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study</strong></p><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, stat-AP<br>Keywords: AI, Clinical<br><a href=http://arxiv.org/abs/2401.14193v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Early detection of melanoma, a potentially lethal type of skin cancer with high prevalence worldwide, improves patient prognosis. In retrospective studies, artificial intelligence (AI) has proven to be helpful for enhancing melanoma detection. However, there are few prospective studies confirming these promising results. Existing studies are limited by low sample sizes, too homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing a fair and thorough evaluation of AI and its generalizability, a crucial aspect for its application in the clinical setting. Therefore, we assessed &lsquo;All Data are Ext&rsquo; (ADAE), an established open-source ensemble algorithm for detecting melanomas, by comparing its diagnostic accuracy to that of dermatologists on a prospectively collected, external, heterogeneous test set comprising eight distinct hospitals, four different camera setups, rare melanoma subtypes, and special anatomical sites. We advanced the algorithm with real test-time augmentation (R-TTA, i.e. providing real photographs of lesions taken from multiple angles and averaging the predictions), and evaluated its generalization capabilities. Overall, the AI showed higher balanced accuracy than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781, 95% CI 0.760-0.802; p&lt;0.001), obtaining a higher sensitivity (0.921, 95% CI 0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p&lt;0.001) at the cost of a lower specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p&lt;0.001). As the algorithm exhibited a significant performance advantage on our heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI may offer the potential to support dermatologists particularly in diagnosing challenging cases.</p></p class="citation"></blockquote><h3 id=7890-attention-based-efficient-classification-for-3d-mri-image-of-alzheimers-disease-yihao-lin-et-al-2024>(78/90) Attention-based Efficient Classification for 3D MRI Image of Alzheimer&rsquo;s Disease (Yihao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Lin, Ximeng Li, Yan Zhang, Jinshan Tang. (2024)<br><strong>Attention-based Efficient Classification for 3D MRI Image of Alzheimer&rsquo;s Disease</strong></p><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keywords: Attention<br><a href=http://arxiv.org/abs/2401.14130v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to its subtle and complex clinical symptoms. Deep learning-assisted medical diagnosis using image recognition techniques has become an important research topic in this field. The features have to accurately capture main variations of anatomical brain structures. However, time-consuming is expensive for feature extraction by deep learning training. This study proposes a novel Alzheimer&rsquo;s disease detection model based on Convolutional Neural Networks. The model utilizes a pre-trained ResNet network as the backbone, incorporating post-fusion algorithm for 3D medical images and attention mechanisms. The experimental results indicate that the employed 2D fusion algorithm effectively improves the model&rsquo;s training expense. And the introduced attention mechanism accurately weights important regions in images, further enhancing the model&rsquo;s diagnostic accuracy.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=7990-multicasting-optical-reconfigurable-switch-niyazi-ulas-dinc-et-al-2024>(79/90) Multicasting Optical Reconfigurable Switch (Niyazi Ulas Dinc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niyazi Ulas Dinc, Mustafa Yildirim, Christophe Moser, Demetri Psaltis. (2024)<br><strong>Multicasting Optical Reconfigurable Switch</strong></p><hr><p>Primary Category: physics.optics<br>Categories: cs-NI, physics-optics, physics.optics<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14173v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) demands large data flows within datacenters, heavily relying on multicasting data transfers. As AI models scale, the requirement for high-bandwidth and low-latency networking compounds. The common use of electrical packet switching faces limitations due to its optical-electrical-optical conversion bottleneck. Optical switches, while bandwidth-agnostic and low-latency, suffer from having only unicast or non-scalable multicasting capability. This paper introduces an optical switching technique addressing the scalable multicasting challenge. Our approach enables arbitrarily programmable simultaneous unicast and multicast connectivity, eliminating the need for optical splitters that hinder scalability due to optical power loss. We use phase modulation in multiple planes, tailored to implement any multicast connectivity map. Using phase modulation enables wavelength selectivity on top of spatial selectivity, resulting in an optical switch that implements space-wavelength routing. We conducted simulations and experiments to validate our approach. Our results affirm the concept&rsquo;s feasibility and effectiveness, as a multicasting switch.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=8090-exploring-the-distinctive-tweeting-patterns-of-toxic-twitter-users-hina-qayyum-et-al-2024>(80/90) Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users (Hina Qayyum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hina Qayyum, Muhammad Ikram, Benjamin Zi Hao Zhao, Ian D. Wood, Nicolas Kourtellis, Mohamed Ali Kaafar. (2024)<br><strong>Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users</strong></p><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keywords: Twitter<br><a href=http://arxiv.org/abs/2401.14141v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>In the pursuit of bolstering user safety, social media platforms deploy active moderation strategies, including content removal and user suspension. These measures target users engaged in discussions marked by hate speech or toxicity, often linked to specific keywords or hashtags. Nonetheless, the increasing prevalence of toxicity indicates that certain users adeptly circumvent these measures. This study examines consistently toxic users on Twitter (rebranded as X) Rather than relying on traditional methods based on specific topics or hashtags, we employ a novel approach based on patterns of toxic tweets, yielding deeper insights into their behavior. We analyzed 38 million tweets from the timelines of 12,148 Twitter users and identified the top 1,457 users who consistently exhibit toxic behavior, relying on metrics like the Gini index and Toxicity score. By comparing their posting patterns to those of non-consistently toxic users, we have uncovered distinctive temporal patterns, including contiguous activity spans, inter-tweet intervals (referred to as &lsquo;Burstiness&rsquo;), and churn analysis. These findings provide strong evidence for the existence of a unique tweeting pattern associated with toxic behavior on Twitter. Crucially, our methodology transcends Twitter and can be adapted to various social media platforms, facilitating the identification of consistently toxic users based on their posting behavior. This research contributes to ongoing efforts to combat online toxicity and offers insights for refining moderation strategies in the digital realm. We are committed to open research and will provide our code and data to the research community.</p></p class="citation"></blockquote><h3 id=8190-on-the-feasibility-of-simple-transformer-for-dynamic-graph-modeling-yuxia-wu-et-al-2024>(81/90) On the Feasibility of Simple Transformer for Dynamic Graph Modeling (Yuxia Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxia Wu, Yuan Fang, Lizi Liao. (2024)<br><strong>On the Feasibility of Simple Transformer for Dynamic Graph Modeling</strong></p><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keywords: Transformer<br><a href=http://arxiv.org/abs/2401.14009v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Dynamic graph modeling is crucial for understanding complex structures in web graphs, spanning applications in social networks, recommender systems, and more. Most existing methods primarily emphasize structural dependencies and their temporal changes. However, these approaches often overlook detailed temporal aspects or struggle with long-term dependencies. Furthermore, many solutions overly complicate the process by emphasizing intricate module designs to capture dynamic evolutions. In this work, we harness the strength of the Transformer&rsquo;s self-attention mechanism, known for adeptly handling long-range dependencies in sequence modeling. Our approach offers a simple Transformer model tailored for dynamic graph modeling without complex modifications. We re-conceptualize dynamic graphs as a sequence modeling challenge and introduce an innovative temporal alignment technique. This technique not only captures the inherent temporal evolution patterns within dynamic graphs but also streamlines the modeling process of their evolution. As a result, our method becomes versatile, catering to an array of applications. Our model&rsquo;s effectiveness is underscored through rigorous experiments on four real-world datasets from various sectors, solidifying its potential in dynamic graph modeling.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=8290-gqhan-a-grover-inspired-quantum-hard-attention-network-ren-xin-zhao-et-al-2024>(82/90) GQHAN: A Grover-inspired Quantum Hard Attention Network (Ren-Xin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ren-Xin Zhao, Jinjing Shi, Xuelong Li. (2024)<br><strong>GQHAN: A Grover-inspired Quantum Hard Attention Network</strong></p><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, quant-ph, quant-ph<br>Keywords: Attention, QA<br><a href=http://arxiv.org/abs/2401.14089v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy in discerning the significance of quantum data, resulting in diminished efficacy when handling extensive quantum datasets. Hard Attention Mechanism (HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters the substantial challenge of non-differentiability, consequently constraining its extensive applicability. In response to the dilemma of HAM and QML, a Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed. Notably, the FO is designed to surmount the non-differentiable issue by executing the activation or masking of Discrete Primitives (DPs) with Flexible Control (FC) to weave various discrete destinies. Based on this, such discrete choice can be visualized with a specially defined Quantum Hard Attention Score (QHAS). Furthermore, a trainable ADO is devised to boost the generality and flexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network (GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST binary classification. Experimental findings demonstrate that GQHAN adeptly surmounts the non-differentiability hurdle, surpassing the efficacy of extant quantum soft self-attention mechanisms in accuracies and learning ability. In noise experiments, GQHAN is robuster to bit-flip noise in accuracy and amplitude damping noise in learning performance. Predictably, the proposal of GQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for future quantum computers to process large-scale data, and promotes the development of quantum computer vision.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=8390-generating-likely-counterfactuals-using-sum-product-networks-jiri-nemecek-et-al-2024>(83/90) Generating Likely Counterfactuals Using Sum-Product Networks (Jiri Nemecek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiri Nemecek, Tomas Pevny, Jakub Marecek. (2024)<br><strong>Generating Likely Counterfactuals Using Sum-Product Networks</strong></p><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, math-OC<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.14086v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where &ldquo;distance from the sample&rdquo; is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is provided.</p></p class="citation"></blockquote><h3 id=8490-a-new-paradigm-for-counterfactual-reasoning-in-fairness-and-recourse-lucius-e-j-bynum-et-al-2024>(84/90) A New Paradigm for Counterfactual Reasoning in Fairness and Recourse (Lucius E. J. Bynum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucius E. J. Bynum, Joshua R. Loftus, Julia Stoyanovich. (2024)<br><strong>A New Paradigm for Counterfactual Reasoning in Fairness and Recourse</strong></p><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs.AI, stat-ML<br>Keywords: AI, Reasoning<br><a href=http://arxiv.org/abs/2401.13935v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Counterfactuals and counterfactual reasoning underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions &ndash; like interventions on race &ndash; may not translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.</p></p class="citation"></blockquote><h3 id=8590-domain-independent-dynamic-programming-ryo-kuroiwa-et-al-2024>(85/90) Domain-Independent Dynamic Programming (Ryo Kuroiwa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryo Kuroiwa, J. Christopher Beck. (2024)<br><strong>Domain-Independent Dynamic Programming</strong></p><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.13883v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>For combinatorial optimization problems, model-based paradigms such as mixed-integer programming (MIP) and constraint programming (CP) aim to decouple modeling and solving a problem: the `holy grail&rsquo; of declarative problem solving. We propose domain-independent dynamic programming (DIDP), a new model-based paradigm based on dynamic programming (DP). While DP is not new, it has typically been implemented as a problem-specific method. We introduce Dynamic Programming Description Language (DyPDL), a formalism to define DP models based on a state transition system, inspired by AI planning. We show that heuristic search algorithms can be used to solve DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP solvers with commercial MIP and CP solvers (solving MIP and CP models, respectively) on common benchmark instances of eleven combinatorial optimization problem classes. We show that DIDP outperforms MIP in nine problem classes, CP also in nine problem classes, and both MIP and CP in seven.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=8690-on-sparse-covers-of-minor-free-graphs-low-dimensional-metric-embeddings-and-other-applications-arnold-filtser-2024>(86/90) On Sparse Covers of Minor Free Graphs, Low Dimensional Metric Embeddings, and other applications (Arnold Filtser, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnold Filtser. (2024)<br><strong>On Sparse Covers of Minor Free Graphs, Low Dimensional Metric Embeddings, and other applications</strong></p><hr><p>Primary Category: cs.DS<br>Categories: cs-CG, cs-DS, cs.DS, math-CO<br>Keywords: Embedding<br><a href=http://arxiv.org/abs/2401.14060v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Given a metric space $(X,d_X)$, a $(\beta,s,\Delta)$-sparse cover is a collection of clusters $\mathcal{C}\subseteq P(X)$ with diameter at most $\Delta$, such that for every point $x\in X$, the ball $B_X(x,\frac\Delta\beta)$ is fully contained in some cluster $C\in \mathcal{C}$, and $x$ belongs to at most $s$ clusters in $\mathcal{C}$. Our main contribution is to show that the shortest path metric of every $K_r$-minor free graphs admits $(O(r),O(r^2),\Delta)$-sparse cover, and for every $\epsilon>0$, $(4+\epsilon,O(\frac1\epsilon)^r,\Delta)$-sparse cover (for arbitrary $\Delta>0$). We then use this sparse cover to show that every $K_r$-minor free graph embeds into $\ell_\infty^{\tilde{O}(\frac1\epsilon)^{r+1}\cdot\log n}$ with distortion $3+\eps$ (resp. into $\ell_\infty^{\tilde{O}(r^2)\cdot\log n}$ with distortion $O(r)$). Further, we provide applications of these sparse covers into padded decompositions, sparse partitions, universal TSP / Steiner tree, oblivious buy at bulk, name independent routing, and path reporting distance oracles.</p></p class="citation"></blockquote><h2 id=eesssy-2>eess.SY (2)</h2><h3 id=8790-networked-multiagent-reinforcement-learning-for-peer-to-peer-energy-trading-chen-feng-et-al-2024>(87/90) Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading (Chen Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Feng, Andrew L. Liu. (2024)<br><strong>Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading</strong></p><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-MA, cs-SY, eess-SY, eess.SY<br>Keywords: Reinforcement Learning<br><a href=http://arxiv.org/abs/2401.13947v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems&rsquo; resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers&rsquo; bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.</p></p class="citation"></blockquote><h3 id=8890-learning-based-sensing-and-computing-decision-for-data-freshness-in-edge-computing-enabled-networks-sinwoong-yun-et-al-2024>(88/90) Learning-based sensing and computing decision for data freshness in edge computing-enabled networks (Sinwoong Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sinwoong Yun, Dongsun Kim, Chanwon Park, Jemin Lee. (2024)<br><strong>Learning-based sensing and computing decision for data freshness in edge computing-enabled networks</strong></p><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keywords: AI<br><a href=http://arxiv.org/abs/2401.13936v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>As the demand on artificial intelligence (AI)-based applications increases, the freshness of sensed data becomes crucial in the wireless sensor networks. Since those applications require a large amount of computation for processing the sensed data, it is essential to offload the computation load to the edge computing (EC) server. In this paper, we propose the sensing and computing decision (SCD) algorithms for data freshness in the EC-enabled wireless sensor networks. We define the {\eta}-coverage probability to show the probability of maintaining fresh data for more than {\eta} ratio of the network, where the spatial-temporal correlation of information is considered. We then propose the probability-based SCD for the single pre-charged sensor case with providing the optimal point after deriving the {\eta}-coverage probability. We also propose the reinforcement learning (RL)- based SCD by training the SCD policy of sensors for both the single pre-charged and multiple energy harvesting (EH) sensor cases, to make a real-time decision based on its observation. Our simulation results verify the performance of the proposed algorithms under various environment settings, and show that the RL-based SCD algorithm achieves higher performance compared to baseline algorithms for both the single pre-charged sensor and multiple EH sensor cases.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=8990-constant-stepsize-q-learning-distributional-convergence-bias-and-extrapolation-yixuan-zhang-et-al-2024>(89/90) Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation (Yixuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixuan Zhang, Qiaomin Xie. (2024)<br><strong>Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation</strong></p><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keywords: Bias<br><a href=http://arxiv.org/abs/2401.13884v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows the application of Richardson-Romberg (RR) extrapolation technique to construct a new estimate that is provably closer to the optimal Q function. Numerical results corroborate our theoretical finding on the improvement of the RR extrapolation method.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=9090-integrating-large-language-models-into-recommendation-via-mutual-augmentation-and-adaptive-aggregation-sichun-luo-et-al-2024>(90/90) Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation (Sichun Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sichun Luo, Yuxuan Yao, Bowei He, Yinya Huang, Aojun Zhou, Xinyi Zhang, Yuanzhang Xiao, Mingjie Zhan, Linqi Song. (2024)<br><strong>Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation</strong></p><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keywords: Augmentation, Language Model<br><a href=http://arxiv.org/abs/2401.13870v1>Paper Link</a></p><hr><p><strong>ABSTRACT</strong><br>Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as \textbf{L}arge \textbf{la}nguage model with \textbf{m}utual augmentation and \textbf{a}daptive aggregation for \textbf{Rec}ommendation (\textbf{Llama4Rec}). Llama4Rec synergistically combines conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent outperformance of baseline methods and significant improvements in recommendation performance.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.01.26</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#cscv-14>cs.CV (14)</a><ul><li><a href=#190-multimodal-pathway-improve-transformers-with-irrelevant-data-from-other-modalities-yiyuan-zhang-et-al-2024>(1/90) Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities (Yiyuan Zhang et al., 2024)</a></li><li><a href=#290-deconstructing-denoising-diffusion-models-for-self-supervised-learning-xinlei-chen-et-al-2024>(2/90) Deconstructing Denoising Diffusion Models for Self-Supervised Learning (Xinlei Chen et al., 2024)</a></li><li><a href=#390-rethinking-patch-dependence-for-masked-autoencoders-letian-fu-et-al-2024>(3/90) Rethinking Patch Dependence for Masked Autoencoders (Letian Fu et al., 2024)</a></li><li><a href=#490-urbangenai-reconstructing-urban-landscapes-using-panoptic-segmentation-and-diffusion-models-timo-kapsalis-2024>(4/90) UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models (Timo Kapsalis, 2024)</a></li><li><a href=#590-unlocking-past-information-temporal-embeddings-in-cooperative-birds-eye-view-prediction-dominik-rößle-et-al-2024>(5/90) Unlocking Past Information: Temporal Embeddings in Cooperative Bird&rsquo;s Eye View Prediction (Dominik Rößle et al., 2024)</a></li><li><a href=#690-sketch2nerf-multi-view-sketch-guided-text-to-3d-generation-minglin-chen-et-al-2024>(6/90) Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation (Minglin Chen et al., 2024)</a></li><li><a href=#790-exploring-the-unexplored-understanding-the-impact-of-layer-adjustments-on-image-classification-haixia-liu-et-al-2024>(7/90) Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification (Haixia Liu et al., 2024)</a></li><li><a href=#890-vivim-a-video-vision-mamba-for-medical-video-object-segmentation-yijun-yang-et-al-2024>(8/90) Vivim: a Video Vision Mamba for Medical Video Object Segmentation (Yijun Yang et al., 2024)</a></li><li><a href=#990-diffusion-based-data-augmentation-for-object-counting-problems-zhen-wang-et-al-2024>(9/90) Diffusion-based Data Augmentation for Object Counting Problems (Zhen Wang et al., 2024)</a></li><li><a href=#1090-improving-pseudo-labelling-and-enhancing-robustness-for-semi-supervised-domain-generalization-adnan-khan-et-al-2024>(10/90) Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization (Adnan Khan et al., 2024)</a></li><li><a href=#1190-an-extensible-framework-for-open-heterogeneous-collaborative-perception-yifan-lu-et-al-2024>(11/90) An Extensible Framework for Open Heterogeneous Collaborative Perception (Yifan Lu et al., 2024)</a></li><li><a href=#1290-am-sort-adaptable-motion-predictor-with-historical-trajectory-embedding-for-multi-object-tracking-vitaliy-kim-et-al-2024>(12/90) AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking (Vitaliy Kim et al., 2024)</a></li><li><a href=#1390-self-supervised-video-object-segmentation-with-distillation-learning-of-deformable-attention-quang-trung-truong-et-al-2024>(13/90) Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention (Quang-Trung Truong et al., 2024)</a></li><li><a href=#1490-knowledge-graph-supported-benchmark-and-video-captioning-for-basketball-zeyu-xi-et-al-2024>(14/90) Knowledge Graph Supported Benchmark and Video Captioning for Basketball (Zeyu Xi et al., 2024)</a></li></ul></li><li><a href=#cscl-25>cs.CL (25)</a><ul><li><a href=#1590-modular-adaptation-of-multilingual-encoders-to-written-swiss-german-dialect-jannis-vamvas-et-al-2024>(15/90) Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect (Jannis Vamvas et al., 2024)</a></li><li><a href=#1690-turna-a-turkish-encoder-decoder-language-model-for-enhanced-understanding-and-generation-gökçe-uludoğan-et-al-2024>(16/90) TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation (Gökçe Uludoğan et al., 2024)</a></li><li><a href=#1790-genie-achieving-human-parity-in-content-grounded-datasets-generation-asaf-yehudai-et-al-2024>(17/90) Genie: Achieving Human Parity in Content-Grounded Datasets Generation (Asaf Yehudai et al., 2024)</a></li><li><a href=#1890-a-comparative-analysis-of-noise-reduction-methods-in-sentiment-analysis-on-noisy-bengali-texts-kazi-toufique-elahi-et-al-2024>(18/90) A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts (Kazi Toufique Elahi et al., 2024)</a></li><li><a href=#1990-topologies-of-reasoning-demystifying-chains-trees-and-graphs-of-thoughts-maciej-besta-et-al-2024>(19/90) Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts (Maciej Besta et al., 2024)</a></li><li><a href=#2090-romansetu-efficiently-unlocking-multilingual-capabilities-of-large-language-models-models-via-romanization-jaavid-aktar-husain-et-al-2024>(20/90) RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization (Jaavid Aktar Husain et al., 2024)</a></li><li><a href=#2190-transformers-and-cortical-waves-encoders-for-pulling-in-context-across-time-lyle-muller-et-al-2024>(21/90) Transformers and Cortical Waves: Encoders for Pulling In Context Across Time (Lyle Muller et al., 2024)</a></li><li><a href=#2290-improving-natural-language-capability-of-code-large-language-model-wei-li-et-al-2024>(22/90) Improving Natural Language Capability of Code Large Language Model (Wei Li et al., 2024)</a></li><li><a href=#2390-bayesprompt-prompting-large-scale-pre-trained-language-models-on-few-shot-inference-via-debiased-domain-abstraction-jiangmeng-li-et-al-2024>(23/90) BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction (Jiangmeng Li et al., 2024)</a></li><li><a href=#2490-on-the-affinity-rationality-and-diversity-of-hierarchical-topic-modeling-xiaobao-wu-et-al-2024>(24/90) On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling (Xiaobao Wu et al., 2024)</a></li><li><a href=#2590-compactifai-extreme-compression-of-large-language-models-using-quantum-inspired-tensor-networks-andrei-tomut-et-al-2024>(25/90) CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks (Andrei Tomut et al., 2024)</a></li><li><a href=#2690-takeed-the-first-generative-fact-checking-system-for-arabic-claims-saud-althabiti-et-al-2024>(26/90) Ta&rsquo;keed: The First Generative Fact-Checking System for Arabic Claims (Saud Althabiti et al., 2024)</a></li><li><a href=#2790-towards-goal-oriented-large-language-model-prompting-a-survey-haochen-li-et-al-2024>(27/90) Towards Goal-oriented Large Language Model Prompting: A Survey (Haochen Li et al., 2024)</a></li><li><a href=#2890-chatgpt-v-bert-dawn-of-justice-for-semantic-change-detection-francesco-periti-et-al-2024>(28/90) (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection (Francesco Periti et al., 2024)</a></li><li><a href=#2990-unitxt-flexible-shareable-and-reusable-data-preparation-and-evaluation-for-generative-ai-elron-bandel-et-al-2024>(29/90) Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI (Elron Bandel et al., 2024)</a></li><li><a href=#3090-towards-uncertainty-aware-language-agent-jiuzhou-han-et-al-2024>(30/90) Towards Uncertainty-Aware Language Agent (Jiuzhou Han et al., 2024)</a></li><li><a href=#3190-cmmu-a-benchmark-for-chinese-multi-modal-multi-type-question-understanding-and-reasoning-zheqi-he-et-al-2024>(31/90) CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning (Zheqi He et al., 2024)</a></li><li><a href=#3290-constraintchecker-a-plugin-for-large-language-models-to-reason-on-commonsense-knowledge-bases-quyet-v-do-et-al-2024>(32/90) ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases (Quyet V. Do et al., 2024)</a></li><li><a href=#3390-investigate-consolidate-exploit-a-general-strategy-for-inter-task-agent-self-evolution-cheng-qian-et-al-2024>(33/90) Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution (Cheng Qian et al., 2024)</a></li><li><a href=#3490-leeroo-orchestrator-elevating-llms-performance-through-model-integration-alireza-mohammadshahi-et-al-2024>(34/90) Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration (Alireza Mohammadshahi et al., 2024)</a></li><li><a href=#3590-adaptive-text-watermark-for-large-language-models-yepeng-liu-et-al-2024>(35/90) Adaptive Text Watermark for Large Language Models (Yepeng Liu et al., 2024)</a></li><li><a href=#3690-webvoyager-building-an-end-to-end-web-agent-with-large-multimodal-models-hongliang-he-et-al-2024>(36/90) WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models (Hongliang He et al., 2024)</a></li><li><a href=#3790-no-more-distractions-an-adaptive-up-sampling-algorithm-to-reduce-data-artifacts-han-chen-2024>(37/90) No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts (Han Chen, 2024)</a></li><li><a href=#3890-a-comparative-study-of-zero-shot-inference-with-large-language-models-and-supervised-modeling-in-breast-cancer-pathology-classification-madhumita-sushil-et-al-2024>(38/90) A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification (Madhumita Sushil et al., 2024)</a></li><li><a href=#3990-unmasking-and-quantifying-racial-bias-of-large-language-models-in-medical-report-generation-yifan-yang-et-al-2024>(39/90) Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation (Yifan Yang et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#4090-the-typing-cure-experiences-with-large-language-model-chatbots-for-mental-health-support-inhwa-song-et-al-2024>(40/90) The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support (Inhwa Song et al., 2024)</a></li><li><a href=#4190-gptvoicetasker-llm-powered-virtual-assistant-for-smartphone-minh-duc-vu-et-al-2024>(41/90) GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone (Minh Duc Vu et al., 2024)</a></li><li><a href=#4290-mapping-the-design-space-of-teachable-social-media-feed-experiences-k-j-kevin-feng-et-al-2024>(42/90) Mapping the Design Space of Teachable Social Media Feed Experiences (K. J. Kevin Feng et al., 2024)</a></li><li><a href=#4390-cuichi-2024-building-trust-in-cuis-from-design-to-deployment-smit-desai-et-al-2024>(43/90) CUI@CHI 2024: Building Trust in CUIs-From Design to Deployment (Smit Desai et al., 2024)</a></li></ul></li><li><a href=#cslg-17>cs.LG (17)</a><ul><li><a href=#4490-serverlessllm-locality-enhanced-serverless-inference-for-large-language-models-yao-fu-et-al-2024>(44/90) ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models (Yao Fu et al., 2024)</a></li><li><a href=#4590-interpretable-solutions-for-breast-cancer-diagnosis-with-grammatical-evolution-and-data-augmentation-yumnah-hasan-et-al-2024>(45/90) Interpretable Solutions for Breast Cancer Diagnosis with Grammatical Evolution and Data Augmentation (Yumnah Hasan et al., 2024)</a></li><li><a href=#4690-sample-efficient-reinforcement-learning-by-automatically-learning-to-compose-subtasks-shuai-han-et-al-2024>(46/90) Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks (Shuai Han et al., 2024)</a></li><li><a href=#4790-how-can-large-language-models-understand-spatial-temporal-data-lei-liu-et-al-2024>(47/90) How Can Large Language Models Understand Spatial-Temporal Data? (Lei Liu et al., 2024)</a></li><li><a href=#4890-alleviating-structural-distribution-shift-in-graph-anomaly-detection-yuan-gao-et-al-2024>(48/90) Alleviating Structural Distribution Shift in Graph Anomaly Detection (Yuan Gao et al., 2024)</a></li><li><a href=#4990-true-knowledge-comes-from-practice-aligning-llms-with-embodied-environments-via-reinforcement-learning-weihao-tan-et-al-2024>(49/90) True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning (Weihao Tan et al., 2024)</a></li><li><a href=#5090-fp6-llm-efficiently-serving-large-language-models-through-fp6-centric-algorithm-system-co-design-haojun-xia-et-al-2024>(50/90) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design (Haojun Xia et al., 2024)</a></li><li><a href=#5190-learning-under-label-noise-through-few-shot-human-in-the-loop-refinement-aaqib-saeed-et-al-2024>(51/90) Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement (Aaqib Saeed et al., 2024)</a></li><li><a href=#5290-sparse-and-transferable-universal-singular-vectors-attack-kseniia-kuvshinova-et-al-2024>(52/90) Sparse and Transferable Universal Singular Vectors Attack (Kseniia Kuvshinova et al., 2024)</a></li><li><a href=#5390-accelerating-retrieval-augmented-language-model-serving-with-speculation-zhihao-zhang-et-al-2024>(53/90) Accelerating Retrieval-Augmented Language Model Serving with Speculation (Zhihao Zhang et al., 2024)</a></li><li><a href=#5490-cross-domain-few-shot-learning-via-adaptive-transformer-networks-naeem-paeedeh-et-al-2024>(54/90) Cross-Domain Few-Shot Learning via Adaptive Transformer Networks (Naeem Paeedeh et al., 2024)</a></li><li><a href=#5590-dynamic-long-term-time-series-forecasting-via-meta-transformer-networks-muhammad-anwar-masum-et-al-2024>(55/90) Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks (Muhammad Anwar Ma&rsquo;sum et al., 2024)</a></li><li><a href=#5690-reinforcement-learning-with-hidden-markov-models-for-discovering-decision-making-dynamics-xingche-guo-et-al-2024>(56/90) Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics (Xingche Guo et al., 2024)</a></li><li><a href=#5790-towards-3d-molecule-text-interpretation-in-language-models-sihang-li-et-al-2024>(57/90) Towards 3D Molecule-Text Interpretation in Language Models (Sihang Li et al., 2024)</a></li><li><a href=#5890-locmoe-a-low-overhead-moe-for-large-language-model-training-jing-li-et-al-2024>(58/90) LocMoE: A Low-overhead MoE for Large Language Model Training (Jing Li et al., 2024)</a></li><li><a href=#5990-a-survey-of-deep-learning-and-foundation-models-for-time-series-forecasting-john-a-miller-et-al-2024>(59/90) A Survey of Deep Learning and Foundation Models for Time Series Forecasting (John A. Miller et al., 2024)</a></li><li><a href=#6090-edge-conditional-node-update-graph-neural-network-for-multi-variate-time-series-anomaly-detection-hayoung-jo-et-al-2024>(60/90) Edge Conditional Node Update Graph Neural Network for Multi-variate Time Series Anomaly Detection (Hayoung Jo et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#6190-5g-network-security-practices-an-overview-and-survey-fatema-bannat-wala-et-al-2024>(61/90) 5G Network Security Practices: An Overview and Survey (Fatema Bannat Wala et al., 2024)</a></li></ul></li><li><a href=#csro-2>cs.RO (2)</a><ul><li><a href=#6290-learning-to-navigate-efficiently-and-precisely-in-real-environments-guillaume-bono-et-al-2024>(62/90) Learning to navigate efficiently and precisely in real environments (Guillaume Bono et al., 2024)</a></li><li><a href=#6390-concept-dynamic-risk-assessment-for-ai-controlled-robotic-systems-philipp-grimmeisen-et-al-2024>(63/90) Concept: Dynamic Risk Assessment for AI-Controlled Robotic Systems (Philipp Grimmeisen et al., 2024)</a></li></ul></li><li><a href=#cscr-2>cs.CR (2)</a><ul><li><a href=#6490-sunblock-cloudless-protection-for-iot-systems-vadim-safronov-et-al-2024>(64/90) SunBlock: Cloudless Protection for IoT Systems (Vadim Safronov et al., 2024)</a></li><li><a href=#6590-cyber-twin-digital-twin-boosted-autonomous-attack-detection-for-vehicular-ad-hoc-networks-yagmur-yigit-et-al-2024>(65/90) Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for Vehicular Ad-Hoc Networks (Yagmur Yigit et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#6690-vall-t-decoder-only-generative-transducer-for-robust-and-decoding-controllable-text-to-speech-chenpeng-du-et-al-2024>(66/90) VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech (Chenpeng Du et al., 2024)</a></li><li><a href=#6790-intelli-z-toward-intelligible-zero-shot-tts-sunghee-jung-et-al-2024>(67/90) Intelli-Z: Toward Intelligible Zero-Shot TTS (Sunghee Jung et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#6890-multitest-physical-aware-object-insertion-for-testing-multi-sensor-fusion-perception-systems-xinyu-gao-et-al-2024>(68/90) MultiTest: Physical-Aware Object Insertion for Testing Multi-sensor Fusion Perception Systems (Xinyu Gao et al., 2024)</a></li><li><a href=#6990-zs4c-zero-shot-synthesis-of-compilable-code-for-incomplete-code-snippets-using-chatgpt-azmain-kabir-et-al-2024>(69/90) ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT (Azmain Kabir et al., 2024)</a></li><li><a href=#7090-deepseek-coder-when-the-large-language-model-meets-programming----the-rise-of-code-intelligence-daya-guo-et-al-2024>(70/90) DeepSeek-Coder: When the Large Language Model Meets Programming &ndash; The Rise of Code Intelligence (Daya Guo et al., 2024)</a></li><li><a href=#7190-copilot-refinement-addressing-code-smells-in-copilot-generated-python-code-beiqi-zhang-et-al-2024>(71/90) Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code (Beiqi Zhang et al., 2024)</a></li><li><a href=#7290-mcudi-model-centric-unsupervised-degradation-indicator-for-failure-prediction-aiops-solutions-lorena-poenaru-olaru-et-al-2024>(72/90) McUDI: Model-Centric Unsupervised Degradation Indicator for Failure Prediction AIOps Solutions (Lorena Poenaru-Olaru et al., 2024)</a></li><li><a href=#7390-from-requirements-to-architecture-an-ai-based-journey-to-semi-automatically-generate-software-architectures-tobias-eisenreich-et-al-2024>(73/90) From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures (Tobias Eisenreich et al., 2024)</a></li><li><a href=#7490-chatgpt-and-human-synergy-in-black-box-testing-a-comparative-analysis-hiroyuki-kirinuki-et-al-2024>(74/90) ChatGPT and Human Synergy in Black-Box Testing: A Comparative Analysis (Hiroyuki Kirinuki et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#7590-chiron-accelerating-node-synchronization-without-security-trade-offs-in-distributed-ledgers-ray-neiheiser-et-al-2024>(75/90) CHIRON: Accelerating Node Synchronization without Security Trade-offs in Distributed Ledgers (Ray Neiheiser et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#7690-on-mission-twitter-profiles-a-study-of-selective-toxic-behavior-hina-qayyum-et-al-2024>(76/90) On mission Twitter Profiles: A Study of Selective Toxic Behavior (Hina Qayyum et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#7790-clinical-melanoma-diagnosis-with-artificial-intelligence-insights-from-a-prospective-multicenter-study-lukas-heinlein-et-al-2024>(77/90) Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study (Lukas Heinlein et al., 2024)</a></li><li><a href=#7890-attention-based-efficient-classification-for-3d-mri-image-of-alzheimers-disease-yihao-lin-et-al-2024>(78/90) Attention-based Efficient Classification for 3D MRI Image of Alzheimer&rsquo;s Disease (Yihao Lin et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#7990-multicasting-optical-reconfigurable-switch-niyazi-ulas-dinc-et-al-2024>(79/90) Multicasting Optical Reconfigurable Switch (Niyazi Ulas Dinc et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#8090-exploring-the-distinctive-tweeting-patterns-of-toxic-twitter-users-hina-qayyum-et-al-2024>(80/90) Exploring the Distinctive Tweeting Patterns of Toxic Twitter Users (Hina Qayyum et al., 2024)</a></li><li><a href=#8190-on-the-feasibility-of-simple-transformer-for-dynamic-graph-modeling-yuxia-wu-et-al-2024>(81/90) On the Feasibility of Simple Transformer for Dynamic Graph Modeling (Yuxia Wu et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#8290-gqhan-a-grover-inspired-quantum-hard-attention-network-ren-xin-zhao-et-al-2024>(82/90) GQHAN: A Grover-inspired Quantum Hard Attention Network (Ren-Xin Zhao et al., 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#8390-generating-likely-counterfactuals-using-sum-product-networks-jiri-nemecek-et-al-2024>(83/90) Generating Likely Counterfactuals Using Sum-Product Networks (Jiri Nemecek et al., 2024)</a></li><li><a href=#8490-a-new-paradigm-for-counterfactual-reasoning-in-fairness-and-recourse-lucius-e-j-bynum-et-al-2024>(84/90) A New Paradigm for Counterfactual Reasoning in Fairness and Recourse (Lucius E. J. Bynum et al., 2024)</a></li><li><a href=#8590-domain-independent-dynamic-programming-ryo-kuroiwa-et-al-2024>(85/90) Domain-Independent Dynamic Programming (Ryo Kuroiwa et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#8690-on-sparse-covers-of-minor-free-graphs-low-dimensional-metric-embeddings-and-other-applications-arnold-filtser-2024>(86/90) On Sparse Covers of Minor Free Graphs, Low Dimensional Metric Embeddings, and other applications (Arnold Filtser, 2024)</a></li></ul></li><li><a href=#eesssy-2>eess.SY (2)</a><ul><li><a href=#8790-networked-multiagent-reinforcement-learning-for-peer-to-peer-energy-trading-chen-feng-et-al-2024>(87/90) Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading (Chen Feng et al., 2024)</a></li><li><a href=#8890-learning-based-sensing-and-computing-decision-for-data-freshness-in-edge-computing-enabled-networks-sinwoong-yun-et-al-2024>(88/90) Learning-based sensing and computing decision for data freshness in edge computing-enabled networks (Sinwoong Yun et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#8990-constant-stepsize-q-learning-distributional-convergence-bias-and-extrapolation-yixuan-zhang-et-al-2024>(89/90) Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation (Yixuan Zhang et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#9090-integrating-large-language-models-into-recommendation-via-mutual-augmentation-and-adaptive-aggregation-sichun-luo-et-al-2024>(90/90) Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation (Sichun Luo et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>