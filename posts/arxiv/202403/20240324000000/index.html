<!doctype html><html><head><title>arXiv @ 2024.03.24</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.24"><meta property="og:description" content="Primary Categories cond-mat.dis-nn (1) cond-mat.quant-gas (1) cs.AI (11) cs.AR (1) cs.CE (2) cs.CL (27) cs.CR (6) cs.CV (72) cs.CY (8) cs.DB (1) cs.DC (2) cs.DS (3) cs.ET (1) cs.GT (5) cs.HC (1) cs.IR (2) cs.IT (8) cs.LG (33) cs.LO (1) cs.MM (2) cs.NI (2) cs.PL (1) cs.RO (18) cs.SD (1) cs.SE (7) cs.SI (6) eess.IV (8) eess.SP (2) eess.SY (6) math.CO (1) math.NA (4) math.OC (3) math.ST (1) physics.geo-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240324000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-24T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.24"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240324000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Mar 24, 2024</p></div><div class=title><h1>arXiv @ 2024.03.24</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cond-matdis-nn-1>cond-mat.dis-nn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cond-matquant-gas-1>cond-mat.quant-gas (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csai-11>cs.AI (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cscl-27>cs.CL (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cscr-6>cs.CR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cscv-72>cs.CV (72)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cscy-8>cs.CY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csds-3>cs.DS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csgt-5>cs.GT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csir-2>cs.IR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csit-8>cs.IT (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cslg-33>cs.LG (33)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csro-18>cs.RO (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#csse-7>cs.SE (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#cssi-6>cs.SI (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#eessiv-8>eess.IV (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#eesssy-6>eess.SY (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#physicsgeo-ph-1>physics.geo-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#q-bioot-1>q-bio.OT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#q-fincp-2>q-fin.CP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/#statml-2>stat.ML (2)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>2</td><td>3</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Automatic Evaluation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td>1</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Benchmarking</td><td></td><td>2</td><td>12</td><td>5</td><td>2</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>2</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>8</td><td>5</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>4</td><td>5</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td>2</td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>10</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Event-Relation Extraction</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fake News Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot</td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td></td><td>9</td><td>4</td><td>2</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>GPT</td><td></td><td>2</td><td>3</td><td>2</td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-4</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>1</td><td>1</td><td>5</td><td></td></tr><tr><td>Graph Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>6</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Hallucination Detection</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>In-context Learning</td><td></td><td>5</td><td>2</td><td>1</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>1</td><td>7</td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>1</td><td>4</td><td></td></tr><tr><td>Large Language Model</td><td>8</td><td>28</td><td>4</td><td>4</td><td>3</td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>2</td><td>2</td><td>22</td><td></td><td>1</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>6</td><td>1</td><td>1</td></tr><tr><td>Out-of-distribution</td><td>1</td><td>1</td><td>4</td><td>1</td><td></td></tr><tr><td>Outlier Detection</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Planning Domain Descrition Language</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>1</td><td>7</td><td>8</td><td>1</td><td>1</td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td>2</td><td>1</td></tr><tr><td>Question Answering</td><td>2</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>5</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td></td><td>6</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>Rerank</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>8</td><td>3</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td></td><td>2</td><td>1</td><td>7</td></tr><tr><td>Simulator</td><td>2</td><td></td><td>2</td><td>1</td><td>7</td></tr><tr><td>Stance Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>1</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>9</td><td>3</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Clustering</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>9</td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Transformer</td><td>1</td><td>3</td><td>9</td><td>2</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>3</td><td>2</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>3</td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>5</td><td>6</td><td></td><td>2</td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-33>cs.LG (33)</h2><h3 id=133--1259-can-large-language-models-explore-in-context-akshay-krishnamurthy-et-al-2024>(1/33 | 1/259) Can large language models explore in-context? (Akshay Krishnamurthy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang, Aleksandrs Slivkins. (2024)<br><strong>Can large language models explore in-context?</strong><br><button class=copy-to-clipboard title="Can large language models explore in-context?" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 140<br>Keywords: Bandit Algorithm, Fine-tuning, Reinforcement Learning, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, In-context Learning, Large Language Model, Large Language Model, Prompt, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15371v1.pdf filename=2403.15371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the extent to which contemporary <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can engage in exploration, a core capability in <b>reinforcement</b> <b>learning</b> and decision making. We focus on native performance of existing <b>LLMs,</b> without training interventions. We deploy <b>LLMs</b> as agents in simple multi-armed <b>bandit</b> environments, specifying the environment description and interaction history entirely <b>in-context,</b> i.e., within the <b>LLM</b> <b>prompt.</b> We experiment with <b>GPT-3.5,</b> <b>GPT-4,</b> and Llama2, using a variety of <b>prompt</b> designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: <b>GPT-4</b> with chain-of-thought <b>reasoning</b> and an externally <b>summarized</b> interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought <b>reasoning</b> but unsummarized history. Although these findings can be interpreted positively, they suggest that external <b>summarization</b> &ndash; which may not be possible in more complex settings &ndash; is important for obtaining desirable behavior from <b>LLM</b> agents. We conclude that non-trivial algorithmic interventions, such as <b>fine-tuning</b> or dataset curation, may be required to empower <b>LLM-based</b> decision making agents in complex settings.</p></p class="citation"></blockquote><h3 id=233--2259-gtc-gnn-transformer-co-contrastive-learning-for-self-supervised-heterogeneous-graph-representation-yundong-sun-et-al-2024>(2/33 | 2/259) GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation (Yundong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian. (2024)<br><strong>GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation</strong><br><button class=copy-to-clipboard title="GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 78<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Contrastive Learning, Representation Learning, Self-supervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15520v1.pdf filename=2403.15520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as the most powerful weapon for various <b>graph</b> <b>tasks</b> <b>due</b> to the <b>message-passing</b> mechanism&rsquo;s great local information aggregation ability. However, over-smoothing has always hindered <b>GNNs</b> from going deeper and capturing multi-hop neighbors. Unlike <b>GNNs,</b> <b>Transformers</b> can model global information and multi-hop interactions via multi-head <b>self-attention</b> and a proper <b>Transformer</b> structure can show more immunity to the over-smoothing problem. So, can we propose a novel framework to combine <b>GNN</b> and <b>Transformer,</b> integrating both <b>GNN&rsquo;s</b> local information aggregation and <b>Transformer&rsquo;s</b> global information modeling ability to eliminate the over-smoothing problem? To realize this, this paper proposes a collaborative learning scheme for <b>GNN-Transformer</b> and constructs GTC architecture. GTC leverages the <b>GNN</b> and <b>Transformer</b> branch to encode node information from different views respectively, and establishes <b>contrastive</b> <b>learning</b> tasks based on the encoded cross-view information to realize <b>self-supervised</b> heterogeneous <b>graph</b> <b>representation.</b> <b>For</b> the <b>Transformer</b> branch, we propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with <b>GNN</b> to attentively encode neighborhood information from different levels. As far as we know, this is the first attempt in the field of <b>graph</b> <b>representation</b> <b>learning</b> to utilize both <b>GNN</b> and <b>Transformer</b> to collaboratively capture different view information and conduct cross-view <b>contrastive</b> <b>learning.</b> The experiments on real datasets show that GTC exhibits superior performance compared with state-of-the-art methods. Codes can be available at <a href=https://github.com/PHD-lanyu/GTC>https://github.com/PHD-lanyu/GTC</a>.</p></p class="citation"></blockquote><h3 id=333--3259-gtagcn-generalized-topology-adaptive-graph-convolutional-networks-sukhdeep-singh-et-al-2024>(3/33 | 3/259) GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks (Sukhdeep Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sukhdeep Singh, Anuj Sharma, Vinod Kumar Chauhan. (2024)<br><strong>GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph Classification, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15077v1.pdf filename=2403.15077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> have emerged as a popular and standard approach for learning from <b>graph-structured</b> <b>data.</b> <b>The</b> literature on <b>GNN</b> highlights the potential of this evolving research area and its widespread adoption in real-life applications. However, most of the approaches are either new in concept or derived from specific techniques. Therefore, the potential of more than one approach in hybrid form has not been studied extensively, which can be well utilized for sequenced data or static data together. We derive a hybrid approach based on two established techniques as generalized aggregation networks and topology adaptive <b>graph</b> <b>convolution</b> <b>networks</b> that solve our purpose to apply on both types of sequenced and static nature of data, effectively. The proposed method applies to both node and <b>graph</b> <b>classification.</b> <b>Our</b> empirical analysis reveals that the results are at par with literature results and better for handwritten strokes as sequenced data, where <b>graph</b> <b>structures</b> <b>have</b> not been explored.</p></p class="citation"></blockquote><h3 id=433--4259-improved-long-short-term-memory-based-wastewater-treatment-simulators-for-deep-reinforcement-learning-esmaeel-mohammadi-et-al-2024>(4/33 | 4/259) Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning (Esmaeel Mohammadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esmaeel Mohammadi, Daniel Ortiz-Arroyo, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Petar Durdevic. (2024)<br><strong>Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Simulation, Simulator, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15091v1.pdf filename=2403.15091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Even though Deep <b>Reinforcement</b> <b>Learning</b> (DRL) showed outstanding results in the fields of Robotics and Games, it is still challenging to implement it in the optimization of industrial processes like wastewater treatment. One of the challenges is the lack of a <b>simulation</b> environment that will represent the actual plant as accurately as possible to train DRL policies. Stochasticity and non-linearity of wastewater treatment data lead to unstable and incorrect predictions of models over <b>long</b> <b>time</b> <b>horizons.</b> <b>One</b> possible reason for the models&rsquo; incorrect <b>simulation</b> behavior can be related to the issue of compounding error, which is the accumulation of errors throughout the <b>simulation.</b> The compounding error occurs because the model utilizes its predictions as inputs at each time step. The error between the actual data and the prediction accumulates as the <b>simulation</b> continues. We implemented two methods to improve the trained models for wastewater treatment data, which resulted in more accurate simulators: 1- Using the model&rsquo;s prediction data as input in the training step as a tool of correction, and 2- Change in the loss function to consider the <b>long-term</b> <b>predicted</b> <b>shape</b> <b>(dynamics).</b> The experimental results showed that implementing these methods can improve the behavior of simulators in terms of Dynamic Time Warping throughout a year up to 98% compared to the base model. These improvements demonstrate significant promise in creating simulators for biological processes that do not need pre-existing knowledge of the process but instead depend exclusively on time series data obtained from the system.</p></p class="citation"></blockquote><h3 id=533--5259-magic-for-the-age-of-quantized-dnns-yoshihide-sawada-et-al-2024>(5/33 | 5/259) Magic for the Age of Quantized DNNs (Yoshihide Sawada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake. (2024)<br><strong>Magic for the Age of Quantized DNNs</strong><br><button class=copy-to-clipboard title="Magic for the Age of Quantized DNNs" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 50<br>Keywords: Model Compression, Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14999v1.pdf filename=2403.14999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the number of parameters in DNNs has explosively increased, as exemplified by <b>LLMs</b> <b>(Large</b> <b>Language</b> <b>Models),</b> <b>making</b> inference on small-scale computers more difficult. <b>Model</b> <b>compression</b> technology is, therefore, essential for integration into products. In this paper, we propose a method of <b>quantization-aware</b> training. We introduce a novel normalization (Layer-Batch Normalization) that is independent of the mini-batch size and does not require any additional computation cost during inference. Then, we <b>quantize</b> the weights by the scaled round-clip function with the weight standardization. We also <b>quantize</b> activation functions using the same function and apply surrogate gradients to train the <b>model</b> <b>with</b> both <b>quantized</b> weights and the <b>quantized</b> activation functions. We call this method Magic for the age of Quantised DNNs (MaQD). Experimental results show that our <b>quantization</b> method can be achieved with minimal accuracy degradation.</p></p class="citation"></blockquote><h3 id=633--6259-robust-optimization-for-adversarial-learning-with-finite-sample-complexity-guarantees-andré-bertolace-et-al-2024>(6/33 | 6/259) Robust optimization for adversarial learning with finite sample complexity guarantees (André Bertolace et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Bertolace, Konstatinos Gatsis, Kostas Margellos. (2024)<br><strong>Robust optimization for adversarial learning with finite sample complexity guarantees</strong><br><button class=copy-to-clipboard title="Robust optimization for adversarial learning with finite sample complexity guarantees" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 43<br>Keywords: MNIST, Adversarial Learning, Adversarial Learning, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15207v1.pdf filename=2403.15207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision making and learning in the presence of uncertainty has attracted significant attention in view of the increasing need to achieve robust and reliable operations. In the case where uncertainty stems from the presence of <b>adversarial</b> <b>attacks</b> this need is becoming more prominent. In this paper we focus on linear and nonlinear classification problems and propose a novel <b>adversarial</b> <b>training</b> method for robust classifiers, inspired by Support Vector Machine (SVM) margins. We view robustness under a data driven lens, and derive finite sample complexity bounds for both linear and non-linear classifiers in binary and multi-class scenarios. Notably, our bounds match natural classifiers&rsquo; complexity. Our algorithm minimizes a worst-case surrogate loss using Linear Programming (LP) and Second Order Cone Programming (SOCP) for linear and non-linear models. Numerical experiments on the <b>benchmark</b> <b>MNIST</b> and CIFAR10 datasets show our approach&rsquo;s comparable performance to state-of-the-art methods, without needing <b>adversarial</b> <b>examples</b> during training. Our work offers a comprehensive framework for enhancing binary linear and non-linear classifier robustness, embedding robustness in learning under the presence of adversaries.</p></p class="citation"></blockquote><h3 id=733--7259-self-improvement-for-neural-combinatorial-optimization-sample-without-replacement-but-improvement-jonathan-pirnay-et-al-2024>(7/33 | 7/259) Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement (Jonathan Pirnay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Pirnay, Dominik G. Grimm. (2024)<br><strong>Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement</strong><br><button class=copy-to-clipboard title="Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15180v1.pdf filename=2403.15180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current methods for end-to-end constructive neural combinatorial optimization usually train a policy using behavior cloning from expert solutions or policy gradient methods from <b>reinforcement</b> <b>learning.</b> While behavior cloning is straightforward, it requires expensive expert solutions, and policy gradient methods are often computationally demanding and complex to <b>fine-tune.</b> In this work, we bridge the two and simplify the training process by sampling multiple solutions for random instances using the current model in each epoch and then selecting the best solution as an expert trajectory for <b>supervised</b> imitation learning. To achieve progressively improving solutions with minimal sampling, we introduce a method that combines round-wise Stochastic Beam Search with an update strategy derived from a provable policy improvement. This strategy refines the policy between rounds by utilizing the advantage of the sampled sequences with almost no computational overhead. We evaluate our approach on the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The models trained with our method achieve comparable performance and generalization to those trained with expert data. Additionally, we apply our method to the Job Shop Scheduling Problem using a <b>transformer-based</b> architecture and outperform existing state-of-the-art methods by a wide margin.</p></p class="citation"></blockquote><h3 id=833--8259-dp-dueling-learning-from-preference-feedback-without-compromising-user-privacy-aadirupa-saha-et-al-2024>(8/33 | 8/259) DP-Dueling: Learning from Preference Feedback without Compromising User Privacy (Aadirupa Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aadirupa Saha, Hilal Asi. (2024)<br><strong>DP-Dueling: Learning from Preference Feedback without Compromising User Privacy</strong><br><button class=copy-to-clipboard title="DP-Dueling: Learning from Preference Feedback without Compromising User Privacy" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Active Learning, Bandit Algorithm, Bandit Algorithm, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15045v1.pdf filename=2403.15045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the well-studied dueling <b>bandit</b> <b>problem,</b> where a learner aims to identify near-optimal actions using pairwise comparisons, under the constraint of <b>differential</b> <b>privacy.</b> We consider a general class of utility-based preference matrices for large (potentially unbounded) decision spaces and give the first differentially private dueling <b>bandit</b> <b>algorithm</b> for <b>active</b> <b>learning</b> with user preferences. Our proposed algorithms are computationally efficient with near-optimal performance, both in terms of the private and non-private regret bound. More precisely, we show that when the decision space is of finite size $K$, our proposed algorithm yields order optimal $O\Big(\sum_{i = 2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$ regret bound for pure $\epsilon$-DP, where $\Delta_i$ denotes the suboptimality gap of the $i$-th arm. We also present a matching lower bound analysis which proves the optimality of our algorithms. Finally, we extend our results to any general decision space in $d$-dimensions with potentially infinite arms and design an $\epsilon$-DP algorithm with regret $\tilde{O} \left( \frac{d^6}{\kappa \epsilon } + \frac{ d\sqrt{T }}{\kappa} \right)$, providing privacy for free when $T \gg d$.</p></p class="citation"></blockquote><h3 id=933--9259-simple-graph-condensation-zhenbang-xiao-et-al-2024>(9/33 | 9/259) Simple Graph Condensation (Zhenbang Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenbang Xiao, Yu Wang, Shunyu Liu, Huiqiong Wang, Mingli Song, Tongya Zheng. (2024)<br><strong>Simple Graph Condensation</strong><br><button class=copy-to-clipboard title="Simple Graph Condensation" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 36<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14951v1.pdf filename=2403.14951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burdensome training costs on large-scale <b>graphs</b> <b>have</b> <b>aroused</b> significant interest in <b>graph</b> <b>condensation,</b> <b>which</b> involves tuning <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> on a small condensed <b>graph</b> <b>for</b> <b>use</b> on the large-scale original <b>graph.</b> <b>Existing</b> <b>methods</b> primarily focus on aligning key metrics between the condensed and original <b>graphs,</b> <b>such</b> <b>as</b> gradients, distribution and trajectory of <b>GNNs,</b> yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation <b>graph,</b> <b>making</b> <b>the</b> condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in <b>graph</b> <b>condensation,</b> <b>aiming</b> to reduce unnecessary complexity inherited from <b>GNNs.</b> In our approach, we eliminate external parameters and exclusively retain the target condensed <b>graph</b> <b>during</b> <b>the</b> condensation process. Following the hierarchical aggregation principles of <b>GNNs,</b> we introduce the Simple <b>Graph</b> <b>Condensation</b> <b>(SimGC)</b> framework, which aligns the condensed <b>graph</b> <b>with</b> <b>the</b> original <b>graph</b> <b>from</b> <b>the</b> input layer to the prediction layer, guided by a pre-trained Simple <b>Graph</b> <b>Convolution</b> <b>(SGC)</b> model on the original <b>graph.</b> <b>As</b> <b>a</b> result, both <b>graphs</b> <b>possess</b> <b>the</b> similar capability to train <b>GNNs.</b> This straightforward yet effective strategy achieves a significant speedup of up to 10 times compared to existing <b>graph</b> <b>condensation</b> <b>methods</b> while performing on par with state-of-the-art baselines. Comprehensive experiments conducted on seven <b>benchmark</b> datasets demonstrate the effectiveness of SimGC in prediction accuracy, condensation time, and generalization capability. Our code will be made publicly available.</p></p class="citation"></blockquote><h3 id=1033--10259-deep-learning-based-method-for-weather-forecasting-a-case-study-in-itoshima-yuzhong-cheng-et-al-2024>(10/33 | 10/259) Deep learning-based method for weather forecasting: A case study in Itoshima (Yuzhong Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhong Cheng, Linh Thi Hoai Nguyen, Akinori Ozaki, Ton Viet Ta. (2024)<br><strong>Deep learning-based method for weather forecasting: A case study in Itoshima</strong><br><button class=copy-to-clipboard title="Deep learning-based method for weather forecasting: A case study in Itoshima" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14918v1.pdf filename=2403.14918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate weather forecasting is of paramount importance for a wide range of practical applications, drawing substantial scientific and societal interest. However, the intricacies of weather systems pose substantial challenges to accurate predictions. This research introduces a multilayer perceptron model tailored for weather forecasting in Itoshima, Kyushu, Japan. Our meticulously designed architecture demonstrates superior performance compared to existing models, surpassing <b>benchmarks</b> such as <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>and</b> <b>Recurrent</b> <b>Neural</b> <b>Networks.</b></p></p class="citation"></blockquote><h3 id=1133--11259-pde-cnns-axiomatic-derivations-and-applications-gijs-bellaard-et-al-2024>(11/33 | 11/259) PDE-CNNs: Axiomatic Derivations and Applications (Gijs Bellaard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits. (2024)<br><strong>PDE-CNNs: Axiomatic Derivations and Applications</strong><br><button class=copy-to-clipboard title="PDE-CNNs: Axiomatic Derivations and Applications" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15182v1.pdf filename=2403.15182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>PDE-based Group <b>Convolutional</b> <b>Neural</b> <b>Networks</b> (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in comparison to <b>CNNs.</b> We also investigate what effect the use of different semifields has on the performance of the models.</p></p class="citation"></blockquote><h3 id=1233--12259-addressing-concept-shift-in-online-time-series-forecasting-detect-then-adapt-yifan-zhang-et-al-2024>(12/33 | 12/259) Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt (YiFan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>YiFan Zhang, Weiqi Chen, Zhaoyang Zhu, Dalin Qin, Liang Sun, Xue Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin. (2024)<br><strong>Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt</strong><br><button class=copy-to-clipboard title="Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14949v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14949v1.pdf filename=2403.14949v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online updating of time series forecasting models aims to tackle the challenge of concept drifting by adjusting forecasting models based on streaming <b>data.</b> <b>While</b> numerous algorithms have been developed, most of them focus on model design and updating. In practice, many of these methods struggle with continuous performance regression in the face of accumulated concept drifts over time. To address this limitation, we present a novel approach, Concept \textbf{D}rift \textbf{D}etection an\textbf{D} \textbf{A}daptation (D3A), that first detects drifting conception and then aggressively adapts the current model to the drifted concepts after the detection for rapid adaption. To best harness the utility of historical <b>data</b> <b>for</b> model adaptation, we propose a <b>data</b> <b>augmentation</b> strategy introducing Gaussian noise into existing training instances. It helps mitigate the <b>data</b> <b>distribution</b> gap, a critical factor contributing to train-test performance inconsistency. The significance of our <b>data</b> <b>augmentation</b> process is verified by our theoretical analysis. Our empirical studies across six datasets demonstrate the effectiveness of D3A in improving model adaptation capability. Notably, compared to a simple Temporal <b>Convolutional</b> <b>Network</b> (TCN) baseline, D3A reduces the average Mean Squared Error (MSE) by $43.9%$. For the state-of-the-art (SOTA) model, the MSE is reduced by $33.3%$.</p></p class="citation"></blockquote><h3 id=1333--13259-multiple-input-auto-encoder-guided-feature-selection-for-iot-intrusion-detection-systems-phai-vu-dinh-et-al-2024>(13/33 | 13/259) Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems (Phai Vu Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk Dutkiewicz, Son Pham Bao. (2024)<br><strong>Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems</strong><br><button class=copy-to-clipboard title="Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Representation Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15511v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15511v1.pdf filename=2403.15511v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an <b>unsupervised</b> <b>learning</b> mode to transform the heterogeneous inputs into lower-dimensional <b>representation,</b> <b>which</b> helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process, we further design and embed a feature selection layer right after the <b>representation</b> <b>layer</b> of MIAE resulting in a new model called MIAEFS. This layer learns the importance of features in the <b>representation</b> <b>vector,</b> facilitating the selection of informative features from the <b>representation</b> <b>vector.</b> The results on three IDS datasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance of MIAE and MIAEFS compared to other methods, e.g., conventional classifiers, dimensionality reduction models, <b>unsupervised</b> <b>representation</b> <b>learning</b> methods with different input dimensions, and <b>unsupervised</b> <b>feature</b> selection models. Moreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier achieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris. The average running time for detecting an attack sample using RF with the <b>representation</b> <b>of</b> MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the model size is lower than 1 MB.</p></p class="citation"></blockquote><h3 id=1433--14259-coda-a-cost-efficient-test-time-domain-adaptation-mechanism-for-har-minghui-qiu-et-al-2024>(14/33 | 14/259) CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR (Minghui Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghui Qiu, Yandao Huang, Lin Chen, Lu Wang, Kaishun Wu. (2024)<br><strong>CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR</strong><br><button class=copy-to-clipboard title="CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 23<br>Keywords: Active Learning, Clustering, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14922v1.pdf filename=2403.14922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, emerging research on mobile sensing has led to novel scenarios that enhance daily life for humans, but dynamic usage conditions often result in performance degradation when systems are deployed in real-world settings. Existing solutions typically employ one-off adaptation schemes based on neural networks, which struggle to ensure robustness against uncertain drifting conditions in human-centric sensing scenarios. In this paper, we propose CODA, a COst-efficient <b>Domain</b> <b>Adaptation</b> mechanism for mobile sensing that addresses real-time drifts from the data distribution perspective with <b>active</b> <b>learning</b> theory, ensuring cost-efficient adaptation directly on the device. By incorporating a <b>clustering</b> loss and importance-weighted <b>active</b> <b>learning</b> algorithm, CODA retains the relationship between different clusters during cost-effective instance-level updates, preserving meaningful structure within the data distribution. We also showcase its generalization by seamlessly integrating it with Neural Network-based solutions for Human Activity Recognition tasks. Through meticulous evaluations across diverse datasets, including phone-based, watch-based, and integrated sensor-based sensing tasks, we demonstrate the feasibility and potential of online adaptation with CODA. The promising results achieved by CODA, even without learnable parameters, also suggest the possibility of realizing unobtrusive adaptation through specific application designs with sufficient feedback.</p></p class="citation"></blockquote><h3 id=1533--15259-exploring-the-task-agnostic-trait-of-self-supervised-learning-in-the-context-of-detecting-mental-disorders-rohan-kumar-gupta-et-al-2024>(15/33 | 15/259) Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders (Rohan Kumar Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Kumar Gupta, Rohit Sinha. (2024)<br><strong>Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders</strong><br><button class=copy-to-clipboard title="Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15170v1.pdf filename=2403.15170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) has been investigated to generate task-agnostic representations across various domains. However, such investigation has not been conducted for detecting multiple mental disorders. The rationale behind the existence of a task-agnostic representation lies in the overlapping symptoms among multiple mental disorders. Consequently, the behavioural data collected for mental health assessment may carry a mixed bag of attributes related to multiple disorders. Motivated by that, in this study, we explore a task-agnostic representation derived through SSL in the context of detecting major depressive disorder (MDD) and post-traumatic stress disorder (PTSD) using audio and video data collected during interactive sessions. This study employs SSL models trained by predicting multiple fixed targets or masked frames. We propose a list of fixed targets to make the generated representation more efficient for detecting MDD and PTSD. Furthermore, we modify the hyper-parameters of the SSL encoder predicting fixed targets to generate global representations that capture varying temporal contexts. Both these innovations are noted to yield improved detection performances for considered mental disorders and exhibit task-agnostic traits. In the context of the SSL model predicting masked frames, the generated global representations are also noted to exhibit task-agnostic traits.</p></p class="citation"></blockquote><h3 id=1633--16259-quantification-using-permutation-invariant-networks-based-on-histograms-olaya-pérez-mon-et-al-2024>(16/33 | 16/259) Quantification using Permutation-Invariant Networks based on Histograms (Olaya Pérez-Mon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olaya Pérez-Mon, Alejandro Moreo, Juan José del Coz, Pablo González. (2024)<br><strong>Quantification using Permutation-Invariant Networks based on Histograms</strong><br><button class=copy-to-clipboard title="Quantification using Permutation-Invariant Networks based on Histograms" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15123v1.pdf filename=2403.15123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantification, also known as class prevalence estimation, is the <b>supervised</b> <b>learning</b> task in which a model is trained to predict the prevalence of each class in a given bag of examples. This paper investigates the application of deep neural networks to tasks of quantification in scenarios where it is possible to apply a symmetric <b>supervised</b> <b>approach</b> that eliminates the need for classification as an intermediary step, directly addressing the quantification problem. Additionally, it discusses existing permutation-invariant layers designed for set processing and assesses their suitability for quantification. In light of our analysis, we propose HistNetQ, a novel neural architecture that relies on a permutation-invariant representation based on histograms that is specially suited for quantification problems. Our experiments carried out in the only quantification competition held to date, show that HistNetQ outperforms other deep neural architectures devised for set processing, as well as the state-of-the-art quantification methods. Furthermore, HistNetQ offers two significant advantages over traditional quantification methods: i) it does not require the labels of the training examples but only the prevalence values of a collection of training bags, making it applicable to new scenarios; and ii) it is able to optimize any custom quantification-oriented loss function.</p></p class="citation"></blockquote><h3 id=1733--17259-adapprox-adaptive-approximation-in-adam-optimization-via-randomized-low-rank-matrices-pengxiang-zhao-et-al-2024>(17/33 | 17/259) Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices (Pengxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger Kölker, Zhefeng Wang, Xiaoming Yuan. (2024)<br><strong>Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices</strong><br><button class=copy-to-clipboard title="Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14958v1.pdf filename=2403.14958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam&rsquo;s second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In <b>GPT-2</b> training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings without the first moment. Besides, it enhances convergence speed and improves downstream task performance relative to its counterparts.</p></p class="citation"></blockquote><h3 id=1833--18259-do-not-trust-what-you-trust-miscalibration-in-semi-supervised-learning-shambhavi-mishra-et-al-2024>(18/33 | 18/259) Do not trust what you trust: Miscalibration in Semi-supervised Learning (Shambhavi Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shambhavi Mishra, Balamurali Murugesan, Ismail Ben Ayed, Marco Pedersoli, Jose Dolz. (2024)<br><strong>Do not trust what you trust: Miscalibration in Semi-supervised Learning</strong><br><button class=copy-to-clipboard title="Do not trust what you trust: Miscalibration in Semi-supervised Learning" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15567v1.pdf filename=2403.15567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art <b>semi-supervised</b> <b>learning</b> (SSL) approaches rely on highly confident predictions to serve as pseudo-labels that guide the training on unlabeled samples. An inherent drawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctness of their predictions. Thus, assessing and enhancing the uncertainty of network predictions is of paramount importance in the pseudo-labeling process. In this work, we empirically demonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, and formally demonstrate the minimization of the min-entropy, a lower bound of the Shannon entropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simple penalty term, which enforces the logit distances of the predictions on unlabeled samples to remain low, preventing the network predictions to become overconfident. Comprehensive experiments on a variety of SSL image classification <b>benchmarks</b> demonstrate that the proposed solution systematically improves the calibration performance of relevant SSL models, while also enhancing their discriminative power, being an appealing addition to tackle SSL tasks.</p></p class="citation"></blockquote><h3 id=1933--19259-parametric-encoding-with-attention-and-convolution-mitigate-spectral-bias-of-neural-partial-differential-equation-solvers-mehdi-shishehbor-et-al-2024>(19/33 | 19/259) Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers (Mehdi Shishehbor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdi Shishehbor, Shirin Hosseinmardi, Ramin Bostanabad. (2024)<br><strong>Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers</strong><br><button class=copy-to-clipboard title="Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15652v1.pdf filename=2403.15652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) are increasingly used to solve partial differential equations (PDEs) that naturally arise while modeling a wide range of systems and physical phenomena. However, the accuracy of such DNNs decreases as the PDE complexity increases and they also suffer from spectral bias as they tend to learn the low-frequency solution characteristics. To address these issues, we introduce Parametric Grid <b>Convolutional</b> Attention Networks (PGCANs) that can solve PDE systems without leveraging any labeled data in the domain. The main idea of PGCAN is to parameterize the input space with a grid-based encoder whose parameters are connected to the output via a DNN decoder that leverages attention to prioritize feature training. Our encoder provides a localized learning ability and uses <b>convolution</b> layers to avoid overfitting and improve information propagation rate from the boundaries to the interior of the domain. We test the performance of PGCAN on a wide range of PDE systems and show that it effectively addresses spectral bias and provides more accurate solutions compared to competing methods.</p></p class="citation"></blockquote><h3 id=2033--20259-planning-with-a-learned-policy-basis-to-optimally-solve-complex-tasks-guillermo-infante-et-al-2024>(20/33 | 20/259) Planning with a Learned Policy Basis to Optimally Solve Complex Tasks (Guillermo Infante et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillermo Infante, David Kuric, Anders Jonsson, Vicenç Gómez, Herke van Hoof. (2024)<br><strong>Planning with a Learned Policy Basis to Optimally Solve Complex Tasks</strong><br><button class=copy-to-clipboard title="Planning with a Learned Policy Basis to Optimally Solve Complex Tasks" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15301v1.pdf filename=2403.15301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional <b>reinforcement</b> <b>learning</b> (RL) methods can successfully solve a wide range of sequential decision problems. However, learning policies that can generalize predictably across multiple tasks in a setting with non-Markovian reward specifications is a challenging problem. We propose to use successor features to learn a policy basis so that each (sub)policy in it solves a well-defined subproblem. In a task described by a finite state automaton (FSA) that involves the same set of subproblems, the combination of these (sub)policies can then be used to generate an optimal solution without additional learning. In contrast to other methods that combine (sub)policies via planning, our method asymptotically attains global optimality, even in stochastic environments.</p></p class="citation"></blockquote><h3 id=2133--21259-parametric-pde-control-with-deep-reinforcement-learning-and-differentiable-l0-sparse-polynomial-policies-nicolò-botteghi-et-al-2024>(21/33 | 21/259) Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies (Nicolò Botteghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolò Botteghi, Urban Fasel. (2024)<br><strong>Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies</strong><br><button class=copy-to-clipboard title="Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15267v1.pdf filename=2403.15267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep <b>reinforcement</b> <b>learning</b> (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradient and actor-critic DRL algorithms without changing their policy-optimization procedure. We test our approach on the challenging tasks of controlling parametric Kuramoto-Sivashinsky and convection-diffusion-reaction PDEs. We show that our method (1) outperforms baseline DNN-based DRL policies, (2) allows for the derivation of interpretable equations of the learned optimal control laws, and (3) generalizes to unseen parameters of the PDE without retraining the policies.</p></p class="citation"></blockquote><h3 id=2233--22259-federated-bayesian-deep-learning-the-application-of-statistical-aggregation-methods-to-bayesian-models-john-fischer-et-al-2024>(22/33 | 22/259) Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models (John Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure. (2024)<br><strong>Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models</strong><br><button class=copy-to-clipboard title="Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15263v1.pdf filename=2403.15263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is an approach to training machine learning models that takes advantage of multiple distributed datasets while maintaining data privacy and reducing communication costs associated with sharing local datasets. Aggregation strategies have been developed to pool or fuse the weights and biases of distributed deterministic models; however, modern deterministic deep learning (DL) models are often poorly calibrated and lack the ability to communicate a measure of epistemic uncertainty in prediction, which is desirable for remote sensing platforms and safety-critical applications. Conversely, Bayesian DL models are often well calibrated and capable of quantifying and communicating a measure of epistemic uncertainty along with a competitive prediction accuracy. Unfortunately, because the weights and biases in Bayesian DL models are defined by a probability distribution, simple application of the aggregation methods associated with FL schemes for deterministic models is either impossible or results in sub-optimal performance. In this work, we use independent and identically distributed (IID) and non-IID partitions of the CIFAR-10 dataset and a fully variational ResNet-20 architecture to analyze six different aggregation strategies for Bayesian DL models. Additionally, we analyze the traditional <b>federated</b> <b>averaging</b> approach applied to an approximate Bayesian Monte Carlo dropout model as a lightweight alternative to more complex variational inference methods in FL. We show that aggregation strategy is a key hyperparameter in the design of a Bayesian FL system with downstream effects on accuracy, calibration, uncertainty quantification, training stability, and client compute requirements.</p></p class="citation"></blockquote><h3 id=2333--23259-early-period-of-training-impacts-out-of-distribution-generalization-chen-cecilia-liu-et-al-2024>(23/33 | 23/259) Early Period of Training Impacts Out-of-Distribution Generalization (Chen Cecilia Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Cecilia Liu, Iryna Gurevych. (2024)<br><strong>Early Period of Training Impacts Out-of-Distribution Generalization</strong><br><button class=copy-to-clipboard title="Early Period of Training Impacts Out-of-Distribution Generalization" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15210v1.pdf filename=2403.15210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to <b>out-of-distribution</b> (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual unfreezing &ndash; has a minuscule impact on ID results, but greatly affects the generalization to OOD data; 2) the absolute values of sharpness and trace of Fisher Information at the initial period of training are not indicative for OOD generalization, but the relative values could be; 3) the trace of Fisher Information and sharpness may be used as indicators for the removal of interventions during early period of training for better OOD generalization.</p></p class="citation"></blockquote><h3 id=2433--24259-an-in-depth-analysis-of-data-reduction-methods-for-sustainable-deep-learning-víctor-toscano-durán-et-al-2024>(24/33 | 24/259) An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning (Víctor Toscano-Durán et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Víctor Toscano-Durán, Javier Perera-Lago, Eduardo Paluzo-Hidalgo, Rocío Gonzalez-Diaz, Miguel Ángel Gutierrez-Naranjo, Matteo Rucco. (2024)<br><strong>An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning</strong><br><button class=copy-to-clipboard title="An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15150v1.pdf filename=2403.15150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Deep Learning has gained popularity for its ability to solve complex classification tasks, increasingly delivering better results thanks to the development of more accurate models, the availability of huge volumes of data and the improved computational capabilities of modern computers. However, these improvements in performance also bring efficiency problems, related to the storage of datasets and models, and to the waste of energy and time involved in both the training and inference processes. In this context, data reduction can help reduce energy consumption when training a deep learning model. In this paper, we present up to eight different methods to reduce the size of a tabular training dataset, and we develop a Python package to apply them. We also introduce a representativeness metric based on topology to measure how similar are the reduced datasets and the full training dataset. Additionally, we develop a methodology to apply these data reduction methods to image datasets for <b>object</b> <b>detection</b> tasks. Finally, we experimentally compare how these data reduction methods affect the representativeness of the reduced dataset, the energy consumption and the predictive performance of the model.</p></p class="citation"></blockquote><h3 id=2533--25259-on-the-convergence-of-adam-under-non-uniform-smoothness-separability-from-sgdm-and-beyond-bohan-wang-et-al-2024>(25/33 | 25/259) On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond (Bohan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohan Wang, Huishuai Zhang, Qi Meng, Ruoyu Sun, Zhi-Ming Ma, Wei Chen. (2024)<br><strong>On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond</strong><br><button class=copy-to-clipboard title="On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15146v1.pdf filename=2403.15146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims to clearly distinguish between <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> with Momentum (SGDM) and Adam in terms of their convergence rates. We demonstrate that Adam achieves a faster convergence compared to SGDM under the condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in deterministic environments, Adam can attain the known lower bound for the convergence rate of deterministic first-order optimizers, whereas the convergence rate of Gradient Descent with Momentum (GDM) has higher order dependence on the initial function value; (2) in <b>stochastic</b> <b>setting,</b> <b>Adam&rsquo;s</b> convergence rate upper bound matches the lower bounds of <b>stochastic</b> <b>first-order</b> <b>optimizers,</b> considering both the initial function value and the final error, whereas there are instances where SGDM fails to converge with any learning rate. These insights distinctly differentiate Adam and SGDM regarding their convergence rates. Additionally, by introducing a novel stopping-time based technique, we further prove that if we consider the minimum gradient norm during iterations, the corresponding convergence rate can match the lower bounds across all problem hyperparameters. The technique can also help proving that Adam with a specific hyperparameter scheduler is parameter-agnostic, which hence can be of independent interest.</p></p class="citation"></blockquote><h3 id=2633--26259-improving-forward-compatibility-in-class-incremental-learning-by-increasing-representation-rank-and-feature-richness-jaeill-kim-et-al-2024>(26/33 | 26/259) Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness (Jaeill Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaeill Kim, Wonseok Lee, Moonjung Eo, Wonjong Rhee. (2024)<br><strong>Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness</strong><br><button class=copy-to-clipboard title="Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15517v1.pdf filename=2403.15517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class Incremental Learning (CIL) constitutes a pivotal subfield within <b>continual</b> <b>learning,</b> aimed at enabling models to progressively learn new classification tasks while retaining knowledge obtained from prior tasks. Although previous studies have predominantly focused on backward compatible approaches to mitigate catastrophic forgetting, recent investigations have introduced forward compatible methods to enhance performance on novel tasks and complement existing backward compatible methods. In this study, we introduce an effective-Rank based Feature Richness enhancement (RFR) method, designed for improving forward compatibility. Specifically, this method increases the effective rank of representations during the base session, thereby facilitating the incorporation of more informative features pertinent to unseen novel tasks. Consequently, RFR achieves dual objectives in backward and forward compatibility: minimizing feature extractor modifications and enhancing novel task performance, respectively. To validate the efficacy of our approach, we establish a theoretical connection between effective rank and the Shannon entropy of representations. Subsequently, we conduct comprehensive experiments by integrating RFR into eleven well-known CIL methods. Our results demonstrate the effectiveness of our approach in enhancing novel-task performance while mitigating catastrophic forgetting. Furthermore, our method notably improves the average incremental accuracy across all eleven cases examined.</p></p class="citation"></blockquote><h3 id=2733--27259-active-learning-for-regression-based-on-wasserstein-distance-and-groupsort-neural-networks-benjamin-bobbia-et-al-2024>(27/33 | 27/259) Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks (Benjamin Bobbia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Bobbia, Matthias Picard. (2024)<br><strong>Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks</strong><br><button class=copy-to-clipboard title="Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15108v1.pdf filename=2403.15108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses a new <b>active</b> <b>learning</b> strategy for regression problems. The presented Wasserstein <b>active</b> <b>regression</b> model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein <b>active</b> <b>regression</b> often achieves more precise estimations and tends to improve accuracy faster than other models.</p></p class="citation"></blockquote><h3 id=2833--28259-automated-feature-selection-for-inverse-reinforcement-learning-daulet-baimukashev-et-al-2024>(28/33 | 28/259) Automated Feature Selection for Inverse Reinforcement Learning (Daulet Baimukashev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daulet Baimukashev, Gokhan Alcan, Ville Kyrki. (2024)<br><strong>Automated Feature Selection for Inverse Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Automated Feature Selection for Inverse Reinforcement Learning" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T40, 68T05, cs-LG, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15079v1.pdf filename=2403.15079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse <b>reinforcement</b> <b>learning</b> (IRL) is an imitation learning approach to learning reward functions from expert demonstrations. Its use avoids the difficult and tedious procedure of manual reward specification while retaining the generalization power of <b>reinforcement</b> <b>learning.</b> In IRL, the reward is usually represented as a linear combination of features. In continuous state spaces, the state variables alone are not sufficiently rich to be used as features, but which features are good is not known in general. To address this issue, we propose a method that employs polynomial basis functions to form a candidate set of features, which are shown to allow the matching of statistical moments of state distributions. Feature selection is then performed for the candidates by leveraging the correlation between trajectory probabilities and feature expectations. We demonstrate the approach&rsquo;s effectiveness by recovering reward functions that capture expert policies across non-linear control tasks of increasing complexity. Code, data, and videos are available at <a href=https://sites.google.com/view/feature4irl>https://sites.google.com/view/feature4irl</a>.</p></p class="citation"></blockquote><h3 id=2933--29259-robust-conformal-prediction-under-distribution-shift-via-physics-informed-structural-causal-model-rui-xu-et-al-2024>(29/33 | 29/259) Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model (Rui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Xu, Yue Sun, Chao Chen, Parv Venkitasubramaniam, Sihong Xie. (2024)<br><strong>Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model</strong><br><button class=copy-to-clipboard title="Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15025v1.pdf filename=2403.15025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty is critical to reliable decision-making with machine learning. Conformal prediction (CP) handles uncertainty by predicting a set on a test input, hoping the set to cover the true label with at least $(1-\alpha)$ confidence. This coverage can be guaranteed on test data even if the marginal <b>distributions</b> <b>$P_X$</b> differ between calibration and test datasets. However, as it is common in practice, when the conditional <b>distribution</b> <b>$P_{Y|X}$</b> is different on calibration and test data, the coverage is not guaranteed and it is essential to measure and minimize the coverage loss under <b>distributional</b> <b>shift</b> at \textit{all} possible confidence levels. To address these issues, we upper bound the coverage difference at all levels using the cumulative density functions of calibration and test conformal scores and Wasserstein distance. Inspired by the invariance of physics across data <b>distributions,</b> <b>we</b> propose a physics-informed structural causal model (PI-SCM) to reduce the upper bound. We validated that PI-SCM can improve coverage robustness along confidence level and test domain on a traffic speed prediction task and an epidemic spread task with multiple real-world datasets.</p></p class="citation"></blockquote><h3 id=3033--30259-insights-into-the-lottery-ticket-hypothesis-and-iterative-magnitude-pruning-tausifa-jan-saleem-et-al-2024>(30/33 | 30/259) Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning (Tausifa Jan Saleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall. (2024)<br><strong>Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning</strong><br><button class=copy-to-clipboard title="Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15022v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15022v2.pdf filename=2403.15022v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude <b>pruning</b> process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude <b>pruning,</b> like the <b>pruning</b> of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude <b>pruning</b> process.</p></p class="citation"></blockquote><h3 id=3133--31259-unifying-lane-level-traffic-prediction-from-a-graph-structural-perspective-benchmark-and-baseline-shuhao-li-et-al-2024>(31/33 | 31/259) Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline (Shuhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhao Li, Yue Cui, Jingyi Xu, Libin Li, Lingkai Meng, Weidong Yang, Fan Zhang, Xiaofang Zhou. (2024)<br><strong>Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline</strong><br><button class=copy-to-clipboard title="Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14941v1.pdf filename=2403.14941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic prediction has long been a focal and pivotal area in research, witnessing both significant strides from city-level to road-level predictions in recent years. With the advancement of Vehicle-to-Everything (V2X) technologies, autonomous driving, and large-scale models in the traffic domain, lane-level traffic prediction has emerged as an indispensable direction. However, further progress in this field is hindered by the absence of comprehensive and unified evaluation standards, coupled with limited public availability of data and code. This paper extensively analyzes and categorizes existing research in lane-level traffic prediction, establishes a unified spatial topology structure and prediction tasks, and introduces a simple baseline model, GraphMLP, based on <b>graph</b> structure and MLP networks. We have replicated codes not publicly available in existing studies and, based on this, thoroughly and fairly assessed various models in terms of effectiveness, efficiency, and applicability, providing insights for practical applications. Additionally, we have released three new datasets and corresponding codes to accelerate progress in this field, all of which can be found on <a href=https://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark>https://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark</a>.</p></p class="citation"></blockquote><h3 id=3233--32259-grey-informed-neural-network-for-time-series-forecasting-wanli-xie-et-al-2024>(32/33 | 32/259) Grey-informed neural network for time-series forecasting (Wanli Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanli Xie, Ruibin Zhao, Zhenguo Xu, Tingting Liang. (2024)<br><strong>Grey-informed neural network for time-series forecasting</strong><br><button class=copy-to-clipboard title="Grey-informed neural network for time-series forecasting" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15027v1.pdf filename=2403.15027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network models have shown outstanding performance and successful resolutions to complex problems in various fields. However, the majority of these models are viewed as <b>black-box,</b> <b>requiring</b> a significant amount of data for development. Consequently, in situations with limited data, constructing appropriate models becomes challenging due to the lack of transparency and scarcity of data. To tackle these challenges, this study suggests the implementation of a grey-informed neural network (GINN). The GINN ensures that the output of the neural network follows the differential equation model of the grey system, improving interpretability. Moreover, incorporating prior knowledge from grey system theory enables traditional neural networks to effectively handle small data samples. Our proposed model has been observed to uncover underlying patterns in the real world and produce reliable forecasts based on empirical data.</p></p class="citation"></blockquote><h3 id=3333--33259-transition-graph-properties-of-target-class-classification-levon-aslanyan-et-al-2024>(33/33 | 33/259) Transition Graph Properties of Target Class Classification (Levon Aslanyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Levon Aslanyan, Hasmik Sahakyan. (2024)<br><strong>Transition Graph Properties of Target Class Classification</strong><br><button class=copy-to-clipboard title="Transition Graph Properties of Target Class Classification" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DM, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15167v1.pdf filename=2403.15167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Target class classification is a mixed classification and transition model whose integrated goal is to assign objects to a certain, so called target or normal class. The classification process is iterative, and in each step an object in a certain class undergoes an action attached to that class, initiating the transition of the object to one of the classes. The sequence of transitions, which we call class transitions, must be designed to provide the final assignment of objects to the target class. The transition process can be described in the form of a directed <b>graph,</b> and the success of the final classification is mainly due to the properties of this <b>graph.</b> In our previous research we showed that the desirable structure of the transition <b>graph</b> is an oriented rooted tree with orientation towards the root vertex, which corresponds to the normal class. It is clear that the transition <b>graph</b> of an arbitrary algorithm (policy) may not have this property. In this paper we study the structure of realistic transition <b>graphs,</b> which makes it possible to find classification inconsistencies, helping to transfer it into the desired form. The medical interpretation of dynamic treatment regime considered in the article further clarifies the investigated framework.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=17--34259-comprehensive-evaluation-and-insights-into-the-use-of-large-language-models-in-the-automation-of-behavior-driven-development-acceptance-test-formulation-shanthi-karpurapu-et-al-2024>(1/7 | 34/259) Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation (Shanthi Karpurapu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanthi Karpurapu, Sravanthy Myneni, Unnati Nettur, Likhit Sagar Gajja, Dave Burke, Tom Stiehm, Jeffery Payne. (2024)<br><strong>Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation</strong><br><button class=copy-to-clipboard title="Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: I-2-7; I-2-1, cs-AI, cs-SE, cs.SE<br>Keyword Score: 130<br>Keywords: Few-shot, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, PaLM, Question Answering, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14965v1.pdf filename=2403.14965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Behavior-driven development (BDD) is an Agile testing methodology fostering collaboration among developers, <b>QA</b> analysts, and stakeholders. In this manuscript, we propose a novel approach to enhance BDD practices using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to automate acceptance test generation. Our study uses zero and <b>few-shot</b> <b>prompts</b> to evaluate <b>LLMs</b> such as <b>GPT-3.5,</b> <b>GPT-4,</b> <b>Llama-2-13B,</b> and <b>PaLM-2.</b> The paper presents a detailed methodology that includes the dataset, <b>prompt</b> techniques, <b>LLMs,</b> and the evaluation process. The results demonstrate that <b>GPT-3.5</b> and <b>GPT-4</b> generate error-free BDD acceptance tests with better performance. The <b>few-shot</b> <b>prompt</b> technique highlights its ability to provide higher accuracy by incorporating examples for <b>in-context</b> <b>learning.</b> Furthermore, the study examines syntax errors, validation accuracy, and comparative analysis of <b>LLMs,</b> revealing their effectiveness in enhancing BDD practices. However, our study acknowledges that there are limitations to the proposed approach. We emphasize that this approach can support collaborative BDD processes and create opportunities for future research into automated BDD acceptance test generation using <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=27--35259-allhands-ask-me-anything-on-large-scale-verbatim-feedback-via-large-language-models-chaoyun-zhang-et-al-2024>(2/7 | 35/259) AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models (Chaoyun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoyun Zhang, Zicheng Ma, Yuhao Wu, Shilin He, Si Qin, Minghua Ma, Xiaoting Qin, Yu Kang, Yuyi Liang, Xiaoyu Gou, Yajie Xue, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang. (2024)<br><strong>AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models</strong><br><button class=copy-to-clipboard title="AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Multi-modal, Topic Model, Large Language Model, Large Language Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15157v1.pdf filename=2403.15157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Verbatim feedback constitutes a valuable repository of user experiences, opinions, and requirements essential for software development. Effectively and efficiently extracting valuable insights from such data poses a challenging task. This paper introduces Allhands , an innovative analytic framework designed for <b>large-scale</b> <b>feedback</b> <b>analysis</b> through a natural language interface, leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Allhands adheres to a conventional feedback analytic workflow, initially conducting classification and <b>topic</b> <b>modeling</b> on the feedback to convert them into a structurally augmented format, incorporating <b>LLMs</b> to enhance accuracy, robustness, generalization, and user-friendliness. Subsequently, an <b>LLM</b> agent is employed to interpret users&rsquo; diverse questions in natural language on feedback, translating them into Python code for execution, and delivering comprehensive <b>multi-modal</b> responses, including text, code, tables, and images. We evaluate Allhands across three diverse feedback datasets. The experiments demonstrate that Allhands achieves superior efficacy at all stages of analysis, including classification and <b>topic</b> <b>modeling,</b> eventually providing users with an ``ask me anything&rsquo;&rsquo; experience with comprehensive, correct and human-readable response. To the best of our knowledge, Allhands stands as the first comprehensive feedback analysis framework that supports diverse and customized requirements for insight extraction through a natural language interface.</p></p class="citation"></blockquote><h3 id=37--36259-just-another-copy-and-paste-comparing-the-security-vulnerabilities-of-chatgpt-generated-code-and-stackoverflow-answers-sivana-hamer-et-al-2024>(3/7 | 36/259) Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers (Sivana Hamer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sivana Hamer, Marcelo d&rsquo;Amorim, Laurie Williams. (2024)<br><strong>Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers</strong><br><button class=copy-to-clipboard title="Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CR, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15600v1.pdf filename=2403.15600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sonatype&rsquo;s 2023 report found that 97% of developers and security leads integrate generative Artificial Intelligence (AI), particularly <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> into their development process. Concerns about the security implications of this trend have been raised. Developers are now weighing the benefits and risks of <b>LLMs</b> against other relied-upon information sources, such as StackOverflow (SO), requiring empirical data to inform their choice. In this work, our goal is to raise software developers awareness of the security implications when selecting code snippets by empirically comparing the vulnerabilities of <b>ChatGPT</b> and StackOverflow. To achieve this, we used an existing Java dataset from SO with security-related questions and answers. Then, we asked <b>ChatGPT</b> the same SO questions, gathering the generated code for comparison. After curating the dataset, we analyzed the number and types of Common Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each platform using CodeQL. <b>ChatGPT-generated</b> code contained 248 vulnerabilities compared to the 302 vulnerabilities found in SO snippets, producing 20% fewer vulnerabilities with a statistically significant difference. Additionally, <b>ChatGPT</b> generated 19 types of CWE, fewer than the 22 found in SO. Our findings suggest developers are under-educated on insecure code propagation from both platforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code copied and pasted, created by AI or humans, cannot be trusted blindly, requiring good software engineering practices to reduce risk. Future work can help minimize insecure code propagation from any platform.</p></p class="citation"></blockquote><h3 id=47--37259-enhancing-testing-at-meta-with-rich-state-simulated-populations-nadia-alshahwan-et-al-2024>(4/7 | 37/259) Enhancing Testing at Meta with Rich-State Simulated Populations (Nadia Alshahwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadia Alshahwan, Arianna Blasi, Kinga Bojarczuk, Andrea Ciancone, Natalija Gucevska, Mark Harman, Simon Schellaert, Inna Harper, Yue Jia, Michał Królikowski, Will Lewis, Dragos Martac, Rubmary Rojas, Kate Ustiuzhanina. (2024)<br><strong>Enhancing Testing at Meta with Rich-State Simulated Populations</strong><br><button class=copy-to-clipboard title="Enhancing Testing at Meta with Rich-State Simulated Populations" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15374v1.pdf filename=2403.15374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper reports the results of the deployment of Rich-State Simulated Populations at Meta for both automated and manual testing. We use simulated users (aka test users) to mimic user interactions and acquire state in much the same way that real user accounts acquire state. For automated testing, we present empirical results from deployment on the Facebook, Messenger, and Instagram apps for iOS and Android Platforms. These apps consist of tens of millions of lines of code, communicating with hundreds of millions of lines of backend code, and are used by over 2 billion people every day. Our results reveal that rich state increases average code coverage by 38%, and endpoint coverage by 61%. More importantly, it also yields an average increase of 115% in the faults found by automated testing. The rich-state test user populations are also deployed in a (continually evolving) Test Universe; a web-enabled <b>simulation</b> platform for privacy-safe manual testing, which has been used by over 21,000 Meta engineers since its deployment in November 2022.</p></p class="citation"></blockquote><h3 id=57--38259-an-exploratory-investigation-into-code-license-infringements-in-large-language-model-training-datasets-jonathan-katzy-et-al-2024>(5/7 | 38/259) An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets (Jonathan Katzy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Katzy, Răzvan-Mihai Popescu, Arie van Deursen, Maliheh Izadi. (2024)<br><strong>An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets</strong><br><button class=copy-to-clipboard title="An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Recommendation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15230v1.pdf filename=2403.15230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Does the training of <b>large</b> <b>language</b> <b>models</b> potentially infringe upon code licenses? Furthermore, are there any datasets available that can be safely used for training these models without violating such licenses? In our study, we assess the current trends in the field and the importance of incorporating code into the training of <b>large</b> <b>language</b> <b>models.</b> Additionally, we examine publicly available datasets to see whether these models can be trained on them without the risk of legal issues in the future. To accomplish this, we compiled a list of 53 <b>large</b> <b>language</b> <b>models</b> trained on file-level code. We then extracted their datasets and analyzed how much they overlap with a dataset we created, consisting exclusively of strong copyleft code. Our analysis revealed that every dataset we examined contained license inconsistencies, despite being selected based on their associated repository licenses. We analyzed a total of 514 million code files, discovering 38 million exact duplicates present in our strong copyleft dataset. Additionally, we examined 171 million file-leading comments, identifying 16 million with strong copyleft licenses and another 11 million comments that discouraged copying without explicitly mentioning a license. Based on the findings of our study, which highlights the pervasive issue of license inconsistencies in <b>large</b> <b>language</b> <b>models</b> trained on code, our <b>recommendation</b> for both researchers and the community is to prioritize the development and adoption of best practices for dataset creation and management.</p></p class="citation"></blockquote><h3 id=67--39259-on-the-generalizability-of-deep-learning-based-code-completion-across-programming-language-versions-matteo-ciniselli-et-al-2024>(6/7 | 39/259) On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions (Matteo Ciniselli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Ciniselli, Alberto Martin-Lopez, Gabriele Bavota. (2024)<br><strong>On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions</strong><br><button class=copy-to-clipboard title="On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15149v1.pdf filename=2403.15149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code completion is a key feature of Integrated Development Environments (IDEs), aimed at predicting the next tokens a developer is likely to write, helping them write code faster and with less effort. Modern code completion approaches are often powered by deep learning (DL) models. However, the swift evolution of programming languages poses a critical challenge to the performance of DL-based code completion models: Can these models generalize across different language versions? This paper delves into such a question. In particular, we assess the capabilities of a state-of-the-art model, CodeT5, to generalize across nine different Java versions, ranging from Java 2 to Java 17, while being exclusively trained on Java 8 code. Our evaluation spans three completion scenarios, namely, predicting tokens, constructs (e.g., the condition of an if statement) and entire code blocks. The results of our study reveal a noticeable disparity among language versions, with the worst performance being obtained in Java 2 and 17 - the most far apart versions compared to Java 8. We investigate possible causes for the performance degradation and show that the adoption of a limited version-specific <b>fine-tuning</b> can partially alleviate the problem. Our work raises awareness on the importance of continuous model refinement, and it can inform the design of alternatives to make code completion models more robust to language evolution.</p></p class="citation"></blockquote><h3 id=77--40259-testing-for-fault-diversity-in-reinforcement-learning-quentin-mazouni-et-al-2024>(7/7 | 40/259) Testing for Fault Diversity in Reinforcement Learning (Quentin Mazouni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quentin Mazouni, Helge Spieker, Arnaud Gotlieb, Mathieu Acher. (2024)<br><strong>Testing for Fault Diversity in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Testing for Fault Diversity in Reinforcement Learning" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15065v1.pdf filename=2403.15065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> is the premier technique to approach sequential decision problems, including complex tasks such as driving cars and landing spacecraft. Among the software validation and verification practices, testing for functional fault detection is a convenient way to build trustworthiness in the learned decision model. While recent works seek to maximise the number of detected faults, none consider fault characterisation during the search for more diversity. We argue that policy testing should not find as many failures as possible (e.g., inputs that trigger similar car crashes) but rather aim at revealing as informative and diverse faults as possible in the model. In this paper, we explore the use of quality diversity optimisation to solve the problem of fault diversity in policy testing. Quality diversity (QD) optimisation is a type of evolutionary algorithm to solve hard combinatorial optimisation problems where high-quality diverse solutions are sought. We define and address the underlying challenges of adapting QD optimisation to the test of action policies. Furthermore, we compare classical QD optimisers to state-of-the-art frameworks dedicated to policy testing, both in terms of search efficiency and fault diversity. We show that QD optimisation, while being conceptually simple and generally applicable, finds effectively more diverse faults in the decision model, and conclude that QD-based policy testing is a promising approach.</p></p class="citation"></blockquote><h2 id=csir-2>cs.IR (2)</h2><h3 id=12--41259-bilateral-unsymmetrical-graph-contrastive-learning-for-recommendation-jiaheng-yu-et-al-2024>(1/2 | 41/259) Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation (Jiaheng Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaheng Yu, Jing Li, Yue He, Kai Zhu, Shuyi Zhang, Wen Hu. (2024)<br><strong>Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation</strong><br><button class=copy-to-clipboard title="Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 83<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Contrastive Learning, Contrastive Learning, Convolution, Convolutional Neural Network, Recommendation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15075v1.pdf filename=2403.15075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent methods utilize <b>graph</b> <b>contrastive</b> <b>Learning</b> within <b>graph-structured</b> <b>user-item</b> <b>interaction</b> data for collaborative filtering and have demonstrated their efficacy in <b>recommendation</b> tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of <b>graphs</b> <b>on</b> <b>bilateral</b> nodes to be different after multi-hop <b>graph</b> <b>interaction</b> <b>calculation,</b> which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for <b>recommendation</b> tasks called Bilateral Unsymmetrical <b>Graph</b> <b>Contrastive</b> <b>Learning</b> (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item <b>graph</b> <b>reasoning</b> <b>better</b> with bilateral slicing <b>contrastive</b> <b>training.</b> Especially, taking into account the aggregation ability of hypergraph-based <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> in digging implicit similarities is more suitable for user nodes, embeddings generated from three different modules: hypergraph-based <b>GCN,</b> <b>GCN</b> and perturbed <b>GCN,</b> are sliced into two subviews by the user- and item-side respectively, and selectively combined into subview pairs bilaterally based on the characteristics of inter-node relation structure. Furthermore, to align the distribution of user and item embeddings after aggregation, a dispersing loss is leveraged to adjust the mutual distance between all embeddings for maintaining learning ability. Comprehensive experiments on two public datasets have proved the superiority of BusGCL in comparison to various <b>recommendation</b> methods. Other models can simply utilize our bilateral slicing <b>contrastive</b> <b>learning</b> to enhance recommending performance without incurring extra expenses.</p></p class="citation"></blockquote><h3 id=22--42259-followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions-orion-weller-et-al-2024>(2/2 | 42/259) FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions (Orion Weller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini. (2024)<br><strong>FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions</strong><br><button class=copy-to-clipboard title="FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15246v1.pdf filename=2403.15246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite <b>Information</b> <b>Retrieval</b> (IR) models using <b>LLMs</b> as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it&rsquo;s unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation <b>benchmark</b> as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation <b>benchmark</b> starts with three deeply judged TREC collections and alters the annotator instructions, re-annotating relevant documents. Through this process, we can measure how well IR models follow instructions, through a new pairwise evaluation framework. Our results indicate that existing retrieval models fail to correctly use instructions, using them for basic keywords and struggling to understand long-form <b>information.</b> <b>However,</b> we show that it is possible for IR models to learn to follow complex instructions: our new FollowIR-7B model has significant improvements (over 13%) after <b>fine-tuning</b> on our training set.</p></p class="citation"></blockquote><h2 id=cscl-27>cs.CL (27)</h2><h3 id=127--43259-masontigers-at-semeval-2024-task-9-solving-puzzles-with-an-ensemble-of-chain-of-thoughts-md-nishat-raihan-et-al-2024>(1/27 | 43/259) MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts (Md Nishat Raihan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Sadiya Sayara Chowdhury Puspo, Amrita Ganguly, Marcos Zampieri. (2024)<br><strong>MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts</strong><br><button class=copy-to-clipboard title="MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Zero-shot, Natural Language Understanding, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14982v1.pdf filename=2403.14982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing <b>natural</b> <b>language</b> <b>understanding.</b> We employ <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve this task through several <b>prompting</b> techniques. <b>Zero-shot</b> and <b>few-shot</b> <b>prompting</b> generate reasonably good results when tested with proprietary <b>LLMs,</b> compared to the open-source models. We obtain further improved results with <b>chain-of-thought</b> <b>prompting,</b> an iterative <b>prompting</b> method that breaks down the <b>reasoning</b> process step-by-step. We obtain our best results by utilizing an ensemble of <b>chain-of-thought</b> <b>prompts,</b> placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of <b>prompted</b> <b>LLMs</b> demonstrates their capability for complex <b>reasoning</b> when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory <b>prompts</b> can unlock more of the knowledge encoded in the parameters of large models.</p></p class="citation"></blockquote><h3 id=227--44259-on-zero-shot-counterspeech-generation-by-llms-punyajoy-saha-et-al-2024>(2/27 | 44/259) On Zero-Shot Counterspeech Generation by LLMs (Punyajoy Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Punyajoy Saha, Aalok Agrawal, Abhik Jana, Chris Biemann, Animesh Mukherjee. (2024)<br><strong>On Zero-Shot Counterspeech Generation by LLMs</strong><br><button class=copy-to-clipboard title="On Zero-Shot Counterspeech Generation by LLMs" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Zero-shot, ChatGPT, GPT, GPT-2, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14938v1.pdf filename=2403.14938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the emergence of numerous <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by <b>fine-tuning</b> <b>LLMs</b> with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of <b>large</b> <b>language</b> <b>models</b> in <b>zero-shot</b> settings. In this work, we present a comprehensive analysis of the performances of four <b>LLMs</b> namely <b>GPT-2,</b> DialoGPT, <b>ChatGPT</b> and FlanT5 in <b>zero-shot</b> settings for counterspeech generation, which is the first of its kind. For <b>GPT-2</b> and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, <b>large)</b> <b>of</b> <b>the</b> models. On the other hand, we propose three different <b>prompting</b> strategies for generating different types of counterspeech and analyse the impact of such strategies on the performance of the models. Our analysis shows that there is an improvement in generation quality for two datasets (17%), however the toxicity increase (25%) with increase in model size. Considering type of model, <b>GPT-2</b> and FlanT5 models are significantly better in terms of counterspeech quality but also have high toxicity as compared to DialoGPT. <b>ChatGPT</b> are much better at generating counter speech than other models across all metrics. In terms of <b>prompting,</b> we find that our proposed strategies help in improving counter speech generation across all the models.</p></p class="citation"></blockquote><h3 id=327--45259-towards-knowledge-grounded-natural-language-understanding-and-generation-chenxi-whitehouse-2024>(3/27 | 45/259) Towards Knowledge-Grounded Natural Language Understanding and Generation (Chenxi Whitehouse, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Whitehouse. (2024)<br><strong>Towards Knowledge-Grounded Natural Language Understanding and Generation</strong><br><button class=copy-to-clipboard title="Towards Knowledge-Grounded Natural Language Understanding and Generation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 76<br>Keywords: Knowledge Distillation, Multi-modal, Multi-modal, Zero-shot, Transformer, Fake News Detection, Grounding, Natural Language Understanding, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15364v1.pdf filename=2403.15364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This thesis investigates how <b>natural</b> <b>language</b> <b>understanding</b> and generation with <b>transformer</b> models can benefit from <b>grounding</b> the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks? Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits <b>fake</b> <b>news</b> <b>detection,</b> and entity-focused code-switching significantly enhances <b>zero-shot</b> cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning significantly improves performance. Additionally, it is established that other general forms of knowledge, such as parametric and <b>distilled</b> knowledge, enhance <b>multimodal</b> and multilingual knowledge-intensive tasks. This research shows the tangible benefits of diverse knowledge integration and motivates further exploration in this direction.</p></p class="citation"></blockquote><h3 id=427--46259-college-concept-embedding-generation-for-large-language-models-ryan-teehan-et-al-2024>(4/27 | 46/259) CoLLEGe: Concept Embedding Generation for Large Language Models (Ryan Teehan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Teehan, Brenden Lake, Mengye Ren. (2024)<br><strong>CoLLEGe: Concept Embedding Generation for Large Language Models</strong><br><button class=copy-to-clipboard title="CoLLEGe: Concept Embedding Generation for Large Language Models" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Meta Learning, Reasoning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15362v1.pdf filename=2403.15362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved <b>finetuning</b> process to learn robustly. <b>Prompting</b> <b>in-context</b> is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for <b>few-shot</b> word learning in NLP, relying on global word vectors, are less applicable to <b>large</b> <b>language</b> <b>models.</b> In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize <b>few-shot</b> concept learning. CoLLEGe is a <b>meta-learning</b> <b>framework</b> capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary <b>meta-learning</b> <b>objective</b> is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept learning in challenging real-world scenarios, including new word acquisition, definition inference, and verbal <b>reasoning,</b> and demonstrate that our method succeeds in each setting without task-specific training.</p></p class="citation"></blockquote><h3 id=527--47259-evidence-driven-retrieval-augmented-response-generation-for-online-misinformation-zhenrui-yue-et-al-2024>(5/27 | 47/259) Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation (Zhenrui Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong Wang. (2024)<br><strong>Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation</strong><br><button class=copy-to-clipboard title="Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Rerank, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14952v1.pdf filename=2403.14952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, <b>text</b> <b>generation</b> approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar <b>text</b> <b>quality</b> and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and <b>rerank</b> evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate evidence-based responses via <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF).</b> We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated <b>text,</b> <b>which</b> yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses.</p></p class="citation"></blockquote><h3 id=627--48259-stance-reasoner-zero-shot-stance-detection-on-social-media-with-explicit-reasoning-maksym-taranukhin-et-al-2024>(6/27 | 48/259) Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning (Maksym Taranukhin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksym Taranukhin, Vered Shwartz, Evangelos Milios. (2024)<br><strong>Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning</strong><br><button class=copy-to-clipboard title="Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Supervised Learning, Zero-shot, Reasoning, Stance Detection, In-context Learning, In-context Learning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14895v1.pdf filename=2403.14895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms are rich sources of opinionated content. <b>Stance</b> <b>detection</b> allows the automatic extraction of users&rsquo; opinions on various topics from such content. We focus on <b>zero-shot</b> <b>stance</b> <b>detection,</b> where the model&rsquo;s success relies on (a) having knowledge about the target topic; and (b) learning general <b>reasoning</b> strategies that can be employed for new topics. We present <b>Stance</b> <b>Reasoner,</b> an approach to <b>zero-shot</b> <b>stance</b> <b>detection</b> on social media that leverages explicit <b>reasoning</b> over background knowledge to guide the model&rsquo;s inference about the document&rsquo;s <b>stance</b> <b>on</b> a target. Specifically, our method uses a <b>pre-trained</b> <b>language</b> <b>model</b> as a source of world knowledge, with the chain-of-thought <b>in-context</b> <b>learning</b> approach to generate intermediate <b>reasoning</b> steps. <b>Stance</b> <b>Reasoner</b> outperforms the current state-of-the-art models on 3 Twitter datasets, including fully <b>supervised</b> models. It can better generalize across targets, while at the same time providing explicit and interpretable explanations for its predictions.</p></p class="citation"></blockquote><h3 id=727--49259-knowla-enhancing-parameter-efficient-finetuning-with-knowledgeable-adaptation-xindi-luo-et-al-2024>(7/27 | 49/259) KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation (Xindi Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, Wei Hu. (2024)<br><strong>KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation</strong><br><button class=copy-to-clipboard title="KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 61<br>Keywords: Graph, Graph Embedding, Benchmarking, Fine-tuning, Knowledge Graph, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14950v1.pdf filename=2403.14950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>finetuning</b> (PEFT) is a key technique for adapting <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to downstream tasks. In this paper, we study leveraging <b>knowledge</b> <b>graph</b> <b>embeddings</b> to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an <b>LLM</b> to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six <b>benchmarks</b> with two popular <b>LLMs</b> and three <b>knowledge</b> <b>graphs</b> <b>demonstrate</b> the effectiveness and robustness of KnowLA. We show that \modelname can help activate the relevant parameterized <b>knowledge</b> <b>in</b> an <b>LLM</b> to answer a question without changing its parameters or input <b>prompts.</b></p></p class="citation"></blockquote><h3 id=827--50259-imagination-augmented-generation-learning-to-imagine-richer-context-for-question-answering-over-large-language-models-huanxuan-liao-et-al-2024>(8/27 | 50/259) Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models (Huanxuan Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao. (2024)<br><strong>Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models</strong><br><button class=copy-to-clipboard title="Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Out-of-distribution, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15268v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15268v2.pdf filename=2403.15268v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-Augmented-Generation</b> <b>and</b> <b>Gener-ation-Augmented-Generation</b> have been proposed to enhance the knowledge required for <b>question</b> <b>answering</b> over <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that <b>LLMs</b> have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering <b>questions</b> <b>solely</b> through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for <b>question</b> <b>answering</b> (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a short dummy document with long context compress and implicit imagination with HyperNetwork for generating adapter weights. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in both in-distribution performance and <b>out-of-distribution</b> generalizations. Our code will be available at <a href=https://github.com/Xnhyacinth/IAG>https://github.com/Xnhyacinth/IAG</a>.</p></p class="citation"></blockquote><h3 id=927--51259-chisiec-an-information-extraction-corpus-for-ancient-chinese-history-xuemei-tang-et-al-2024>(9/27 | 51/259) CHisIEC: An Information Extraction Corpus for Ancient Chinese History (Xuemei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemei Tang, Zekun Deng, Qi Su, Hao Yang, Jun Wang. (2024)<br><strong>CHisIEC: An Information Extraction Corpus for Ancient Chinese History</strong><br><button class=copy-to-clipboard title="CHisIEC: An Information Extraction Corpus for Ancient Chinese History" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Information Retrieval, Named Entity Recognition, Named Entity Recognition, Relation Extraction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15088v1.pdf filename=2403.15088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> and <b>relation</b> <b>extraction</b> (RE). In our commitment to expediting ancient history and culture, we present the ``Chinese Historical <b>Information</b> <b>Extraction</b> Corpus&rsquo;&rsquo;(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate <b>NER</b> and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve <b>relation</b> <b>types,</b> resulting in a meticulously labeled dataset comprising 14,194 entities and 8,609 <b>relations.</b> <b>To</b> establish the robustness and versatility of our dataset, we have undertaken comprehensive experimentation involving models of various sizes and paradigms. Additionally, we have evaluated the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the context of tasks related to ancient Chinese history. The dataset and code are available at \url{https://github.com/tangxuemei1995/CHisIEC}.</p></p class="citation"></blockquote><h3 id=1027--52259-esg-classification-by-implicit-rule-learning-via-gpt-4-hyo-jeong-yun-et-al-2024>(10/27 | 52/259) ESG Classification by Implicit Rule Learning via GPT-4 (Hyo Jeong Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyo Jeong Yun, Chanyoung Kim, Moonjeong Hahm, Kyuri Kim, Guijin Son. (2024)<br><strong>ESG Classification by Implicit Rule Learning via GPT-4</strong><br><button class=copy-to-clipboard title="ESG Classification by Implicit Rule Learning via GPT-4" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Reasoning, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15040v1.pdf filename=2403.15040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like <b>GPT-4</b> can be guided to align with unknown ESG evaluation criteria through strategies such as <b>prompting,</b> chain-of-thought <b>reasoning,</b> and dynamic <b>in-context</b> <b>learning.</b> We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting <b>prompts</b> impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights. We observe longer general pre-training to correlate with enhanced performance in financial downstream tasks. Our findings showcase the potential of language models to navigate complex, subjective evaluation guidelines despite lacking explicit training examples, revealing opportunities for training-free solutions for financial downstream tasks.</p></p class="citation"></blockquote><h3 id=1127--53259-masontigers-at-semeval-2024-task-8-performance-analysis-of-transformer-based-models-on-machine-generated-text-detection-sadiya-sayara-chowdhury-puspo-et-al-2024>(11/27 | 53/259) MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection (Sadiya Sayara Chowdhury Puspo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Amrita Ganguly, Ozlem Uzuner. (2024)<br><strong>MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection</strong><br><button class=copy-to-clipboard title="MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Zero-shot, Transformer, Text Classification, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14989v1.pdf filename=2403.14989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual <b>Black-Box</b> <b>Machine-Generated</b> <b>Text</b> <b>Detection.</b> The task encompasses Binary Human-Written vs. Machine-Generated <b>Text</b> <b>Classification</b> (Track A), Multi-Way Machine-Generated <b>Text</b> <b>Classification</b> (Track B), and Human-Machine Mixed <b>Text</b> <b>Detection</b> (Track C). Our best performing approaches utilize mainly the ensemble of discriminator <b>transformer</b> models along with sentence <b>transformer</b> and statistical machine learning approaches in specific cases. Moreover, <b>zero-shot</b> <b>prompting</b> and <b>fine-tuning</b> of FLAN-T5 are used for Track A and B.</p></p class="citation"></blockquote><h3 id=1227--54259-event-temporal-relation-extraction-based-on-retrieval-augmented-on-llms-xiaobin-zhang-et-al-2024>(12/27 | 54/259) Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs (Xiaobin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu. (2024)<br><strong>Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs</strong><br><button class=copy-to-clipboard title="Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Event-Relation Extraction, Relation Extraction, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15273v1.pdf filename=2403.15273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event temporal <b>relation</b> <b>(TempRel)</b> is a primary subject of the event <b>relation</b> <b>extraction</b> task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of <b>prompt</b> engineering, it is important to design effective <b>prompt</b> templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to enhance <b>prompt</b> templates and verbalizers. Our method capitalizes on the diverse capabilities of various <b>LLMs</b> to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of <b>LLMs</b> for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our method in improving the performance of event temporal <b>relation</b> <b>extraction</b> tasks.</p></p class="citation"></blockquote><h3 id=1327--55259-llm2llm-boosting-llms-with-novel-iterative-data-enhancement-nicholas-lee-et-al-2024>(13/27 | 55/259) LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement (Nicholas Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipali, Michael W. Mahoney, Kurt Keutzer, Amir Gholami. (2024)<br><strong>LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement</strong><br><button class=copy-to-clipboard title="LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15042v1.pdf filename=2403.15042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require <b>fine-tuning</b> to reach satisfactory levels of performance, many of them are in the low-data regime, making <b>fine-tuning</b> challenging. To address this, we propose LLM2LLM, a targeted and iterative <b>data</b> <b>augmentation</b> strategy that uses a teacher <b>LLM</b> to enhance a small seed dataset by augmenting additional <b>data</b> <b>that</b> can be used for <b>fine-tuning</b> on a specific task. LLM2LLM (1) <b>fine-tunes</b> a baseline student <b>LLM</b> on the initial seed <b>data,</b> <b>(2)</b> evaluates and extracts <b>data</b> <b>points</b> that the model gets wrong, and (3) uses a teacher <b>LLM</b> to generate synthetic <b>data</b> <b>based</b> on these incorrect <b>data</b> <b>points,</b> which are then added back into the training <b>data.</b> <b>This</b> approach amplifies the signal from incorrectly predicted <b>data</b> <b>points</b> by the <b>LLM</b> during training and reintegrates them into the dataset to focus on more challenging examples for the <b>LLM.</b> Our results show that LLM2LLM significantly enhances the performance of <b>LLMs</b> in the low-data regime, outperforming both traditional <b>fine-tuning</b> and other <b>data</b> <b>augmentation</b> baselines. LLM2LLM reduces the dependence on labor-intensive <b>data</b> <b>curation</b> and paves the way for more scalable and performant <b>LLM</b> solutions, allowing us to tackle <b>data-constrained</b> <b>domains</b> and tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular <b>fine-tuning</b> in the low-data regime using a LLaMA2-7B student model.</p></p class="citation"></blockquote><h3 id=1427--56259-text-clustering-with-llm-embeddings-alina-petukhova-et-al-2024>(14/27 | 56/259) Text clustering with LLM embeddings (Alina Petukhova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada. (2024)<br><strong>Text clustering with LLM embeddings</strong><br><button class=copy-to-clipboard title="Text clustering with LLM embeddings" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, BERT, Text Clustering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15112v1.pdf filename=2403.15112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>clustering</b> is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> - and <b>clustering</b> algorithms affect how <b>text</b> <b>datasets</b> are clustered. A series of experiments were conducted to assess how embeddings influence <b>clustering</b> results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that <b>LLM</b> embeddings excel at capturing the nuances of structured language, while <b>BERT</b> leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve <b>clustering</b> efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a complex balance between the need for nuanced <b>text</b> <b>representation</b> and computational feasibility in <b>text</b> <b>clustering</b> applications. This study extends traditional <b>text</b> <b>clustering</b> frameworks by incorporating embeddings from <b>LLMs,</b> thereby paving the way for improved methodologies and opening new avenues for future research in various types of textual analysis.</p></p class="citation"></blockquote><h3 id=1527--57259-masontigers-at-semeval-2024-task-1-an-ensemble-approach-for-semantic-textual-relatedness-dhiman-goswami-et-al-2024>(15/27 | 57/259) MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness (Dhiman Goswami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri. (2024)<br><strong>MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness</strong><br><button class=copy-to-clipboard title="MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Unsupervised Learning, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14990v1.pdf filename=2403.14990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses <b>supervised</b> (Track A), <b>unsupervised</b> (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific <b>BERT</b> based models and sentence <b>transformers.</b></p></p class="citation"></blockquote><h3 id=1627--58259-multi-review-fusion-in-context-aviv-slobodkin-et-al-2024>(16/27 | 58/259) Multi-Review Fusion-in-Context (Aviv Slobodkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan. (2024)<br><strong>Multi-Review Fusion-in-Context</strong><br><button class=copy-to-clipboard title="Multi-Review Fusion-in-Context" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Question Answering, Text Generation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15351v1.pdf filename=2403.15351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grounded <b>text</b> <b>generation,</b> encompassing tasks such as long-form <b>question-answering</b> <b>and</b> <b>summarization,</b> necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent <b>text</b> <b>given</b> pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source <b>texts</b> <b>with</b> highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses. This study lays the groundwork for further exploration of modular <b>text</b> <b>generation</b> in the multi-document setting, offering potential improvements in the quality and reliability of generated content. \footnote{Our <b>benchmark,</b> FuseReviews, including the dataset, evaluation framework and designated leaderboard, can be found at \url{https://fusereviews.github.io/}.}</p></p class="citation"></blockquote><h3 id=1727--59259-investigating-the-performance-of-language-models-for-completing-code-in-functional-programming-languages-a-haskell-case-study-tim-van-dam-et-al-2024>(17/27 | 59/259) Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study (Tim van Dam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim van Dam, Frank van der Heijden, Philippe de Bekker, Berend Nieuwschepen, Marc Otten, Maliheh Izadi. (2024)<br><strong>Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study</strong><br><button class=copy-to-clipboard title="Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15185v1.pdf filename=2403.15185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We <b>fine-tune</b> and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our <b>automatic</b> <b>evaluation</b> shows that knowledge of imperative programming languages in the pre-training of <b>LLMs</b> may not transfer well to functional languages, but that code completion on functional languages is feasible. Consequently, this shows the need for more high-quality Haskell datasets. A manual evaluation on HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and extra comments, while UniXcoder more often produces incomplete or incorrect predictions. Finally, we release HumanEval-Haskell, along with the <b>fine-tuned</b> models and all code required to reproduce our experiments on GitHub (<a href=https://github.com/AISE-TUDelft/HaskellCCEval)>https://github.com/AISE-TUDelft/HaskellCCEval)</a>.</p></p class="citation"></blockquote><h3 id=1827--60259-risk-and-response-in-large-language-models-evaluating-key-threat-categories-bahareh-harandizadeh-et-al-2024>(18/27 | 60/259) Risk and Response in Large Language Models: Evaluating Key Threat Categories (Bahareh Harandizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bahareh Harandizadeh, Abel Salinas, Fred Morstatter. (2024)<br><strong>Risk and Response in Large Language Models: Evaluating Key Threat Categories</strong><br><button class=copy-to-clipboard title="Risk and Response in Large Language Models: Evaluating Key Threat Categories" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14988v1.pdf filename=2403.14988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the pressing issue of risk assessment in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to <b>fine-tune</b> pretrained <b>LLMs</b> to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that <b>LLMs</b> tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that <b>LLMs</b> respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of <b>LLMs</b> to jailbreaking attacks in Information Hazard scenarios, highlighting a critical security concern in <b>LLM</b> risk assessment and emphasizing the need for improved AI safety measures.</p></p class="citation"></blockquote><h3 id=1927--61259-enhancing-effectiveness-and-robustness-in-a-low-resource-regime-via-decision-boundary-aware-data-augmentation-kyohoon-jin-et-al-2024>(19/27 | 61/259) Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation (Kyohoon Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyohoon Jin, Junho Lee, Juhwan Choi, Sangmin Song, Youngbin Kim. (2024)<br><strong>Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation</strong><br><button class=copy-to-clipboard title="Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Data Augmentation, Low-Resource, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15512v1.pdf filename=2403.15512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efforts to leverage deep learning models in <b>low-resource</b> regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text <b>data,</b> <b>is</b> limited due to their discrete characteristics. While methods using <b>pretrained</b> <b>language</b> <b>models</b> have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware <b>data</b> <b>augmentation</b> strategy to enhance robustness using <b>pretrained</b> <b>language</b> <b>models.</b> The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive experiments. Furthermore, the ablation study reveals the effect of soft labels and mid-K sampling and the extensibility of the method with curriculum <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=2027--62259-attention-driven-reasoning-unlocking-the-potential-of-large-language-models-bingli-liao-et-al-2024>(20/27 | 62/259) Attention-Driven Reasoning: Unlocking the Potential of Large Language Models (Bingli Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingli Liao, Danilo Vasconcellos Vargas. (2024)<br><strong>Attention-Driven Reasoning: Unlocking the Potential of Large Language Models</strong><br><button class=copy-to-clipboard title="Attention-Driven Reasoning: Unlocking the Potential of Large Language Models" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14932v1.pdf filename=2403.14932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown remarkable capabilities, but their <b>reasoning</b> abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance <b>LLMs&rsquo;</b> <b>reasoning</b> through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved <b>reasoning</b> capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in <b>LLMs&rsquo;</b> <b>reasoning</b> and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.</p></p class="citation"></blockquote><h3 id=2127--63259-limgen-probing-the-llms-for-generating-suggestive-limitations-of-research-papers-abdur-rahman-bin-md-faizullah-et-al-2024>(21/27 | 63/259) LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers (Abdur Rahman Bin Md Faizullah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra. (2024)<br><strong>LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers</strong><br><button class=copy-to-clipboard title="LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15529v1.pdf filename=2403.15529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at <a href=https://github.com/armbf/LimGen>https://github.com/armbf/LimGen</a>.</p></p class="citation"></blockquote><h3 id=2227--64259-co-fun-a-german-dataset-on-company-outsourcing-in-fund-prospectuses-for-named-entity-recognition-and-relation-extraction-neda-foroutan-et-al-2024>(22/27 | 64/259) CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction (Neda Foroutan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neda Foroutan, Markus Schröder, Andreas Dengel. (2024)<br><strong>CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction</strong><br><button class=copy-to-clipboard title="CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Named Entity Recognition, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15322v1.pdf filename=2403.15322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for <b>named</b> <b>entity</b> <b>recognition</b> and <b>relation</b> <b>extraction</b> tasks. The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 <b>relation</b> <b>annotations</b> (Outsourcing-Company, Company-Location). State-of-the-art deep learning models were trained to recognize entities and extract <b>relations</b> <b>showing</b> first promising results. An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at <a href=https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip>https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip</a>.</p></p class="citation"></blockquote><h3 id=2327--65259-hierarchical-skip-decoding-for-efficient-autoregressive-text-generation-yunqi-zhu-et-al-2024>(23/27 | 65/259) Hierarchical Skip Decoding for Efficient Autoregressive Text Generation (Yunqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang. (2024)<br><strong>Hierarchical Skip Decoding for Efficient Autoregressive Text Generation</strong><br><button class=copy-to-clipboard title="Hierarchical Skip Decoding for Efficient Autoregressive Text Generation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Generation, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14919v1.pdf filename=2403.14919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive decoding strategy is a commonly used method for <b>text</b> <b>generation</b> tasks with <b>pre-trained</b> <b>language</b> <b>models,</b> while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive <b>text</b> <b>generation.</b> Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive <b>text</b> <b>generation</b> models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five <b>text</b> <b>generation</b> datasets with <b>pre-trained</b> <b>language</b> <b>models</b> demonstrate HSD&rsquo;s advantages in balancing efficiency and <b>text</b> <b>quality.</b> With almost half of the layers skipped, HSD can sustain 90% of the <b>text</b> <b>quality</b> compared to vanilla autoregressive decoding, outperforming the competitive approaches.</p></p class="citation"></blockquote><h3 id=2427--66259-comprehensive-reassessment-of-large-scale-evaluation-outcomes-in-llms-a-multifaceted-statistical-approach-kun-sun-et-al-2024>(24/27 | 66/259) Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach (Kun Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Sun, Rong Wang, Haitao Liu, Anders Søgaard. (2024)<br><strong>Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach</strong><br><button class=copy-to-clipboard title="Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Clustering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15250v1.pdf filename=2403.15250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Amidst the rapid evolution of <b>LLMs,</b> the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of <b>LLMs.</b> However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these <b>LLMs,</b> targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and <b>clustering</b> technique, offering a robust and transparent approach to deciphering <b>LLM</b> performance data. Contrary to prevailing findings, our results challenge assumptions about emergent abilities and the influence of given training types and architectures in <b>LLMs.</b> These findings furnish new perspectives on the characteristics, intrinsic nature, and developmental trajectories of <b>LLMs.</b> By providing straightforward and reliable methods to scrutinize and reassess <b>LLM</b> performance data, this study contributes a nuanced perspective on <b>LLM</b> efficiency and potentials.</p></p class="citation"></blockquote><h3 id=2527--67259-language-models-in-dialogue-conversational-maxims-for-human-ai-interactions-erik-miehling-et-al-2024>(25/27 | 67/259) Language Models in Dialogue: Conversational Maxims for Human-AI Interactions (Erik Miehling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, John T. Richards. (2024)<br><strong>Language Models in Dialogue: Conversational Maxims for Human-AI Interactions</strong><br><button class=copy-to-clipboard title="Language Models in Dialogue: Conversational Maxims for Human-AI Interactions" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15115v1.pdf filename=2403.15115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims &ndash; quantity, quality, relevance, manner, benevolence, and transparency &ndash; for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one&rsquo;s knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how to assess conversational quality between humans and <b>LLM-driven</b> conversational agents, informing both their evaluation and improved design.</p></p class="citation"></blockquote><h3 id=2627--68259-ctsm-combining-trait-and-state-emotions-for-empathetic-response-model-wang-yufeng-et-al-2024>(26/27 | 68/259) CTSM: Combining Trait and State Emotions for Empathetic Response Model (Wang Yufeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wang Yufeng, Chen Chao, Yang Zhou, Wang Shuhui, Liao Xiangwen. (2024)<br><strong>CTSM: Combining Trait and State Emotions for Empathetic Response Model</strong><br><button class=copy-to-clipboard title="CTSM: Combining Trait and State Emotions for Empathetic Response Model" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15516v1.pdf filename=2403.15516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empathetic response generation endeavors to empower <b>dialogue</b> <b>systems</b> to perceive speakers&rsquo; emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in <b>dialogue,</b> <b>we</b> first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learning decoder to enhance the model&rsquo;s empathetic expression capability by aligning trait and state emotions between generated responses and contexts. Both automatic and manual evaluation results demonstrate that CTSM outperforms state-of-the-art baselines and can generate more empathetic responses. Our code is available at <a href=https://github.com/wangyufeng-empty/CTSM>https://github.com/wangyufeng-empty/CTSM</a></p></p class="citation"></blockquote><h3 id=2727--69259-a-single-linear-layer-yields-task-adapted-low-rank-matrices-hwichan-kim-et-al-2024>(27/27 | 69/259) A Single Linear Layer Yields Task-Adapted Low-Rank Matrices (Hwichan Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hwichan Kim, Shota Sasaki, Sho Hoshino, Ukyo Honda. (2024)<br><strong>A Single Linear Layer Yields Task-Adapted Low-Rank Matrices</strong><br><button class=copy-to-clipboard title="A Single Linear Layer Yields Task-Adapted Low-Rank Matrices" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14946v1.pdf filename=2403.14946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient <b>Fine-Tuning</b> (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer&rsquo;s $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weight matrices with low-rank matrices derived from a single linear layer. Our empirical results show that CondLoRA maintains a performance on par with LoRA, despite the fact that the trainable parameters of CondLoRA are fewer than those of LoRA. Therefore, we conclude that &ldquo;a single linear layer yields task-adapted low-rank matrices.&rdquo;</p></p class="citation"></blockquote><h2 id=cscv-72>cs.CV (72)</h2><h3 id=172--70259-medpromptx-grounded-multimodal-prompting-for-chest-x-ray-diagnosis-mai-a-shaaban-et-al-2024>(1/72 | 70/259) MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis (Mai A. Shaaban et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mai A. Shaaban, Adnan Khan, Mohammad Yaqub. (2024)<br><strong>MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis</strong><br><button class=copy-to-clipboard title="MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Few-shot, Multi-modal, Multi-modal, Grounding, Question Answering, Visual Question Answering, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15585v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15585v2.pdf filename=2403.15585v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), <b>few-shot</b> <b>prompting</b> (FP) and <b>visual</b> <b>grounding</b> <b>(VG)</b> to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients&rsquo; medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of <b>few-shot</b> examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynamically refines <b>few-shot</b> data for real-time adjustment to new patient scenarios. Moreover, VG aids in focusing the model&rsquo;s attention on relevant regions of interest in X-ray images, enhancing the identification of abnormalities. We release MedPromptX-VQA, a new <b>in-context</b> <b>visual</b> <b>question</b> <b>answering</b> dataset encompassing interleaved image and EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the SOTA performance of MedPromptX, achieving an 11% improvement in F1-score compared to the baselines. Code and data are available at <a href=https://github.com/BioMedIA-MBZUAI/MedPromptX>https://github.com/BioMedIA-MBZUAI/MedPromptX</a></p></p class="citation"></blockquote><h3 id=272--71259-controlled-training-data-generation-with-diffusion-models-teresa-yeo-et-al-2024>(2/72 | 71/259) Controlled Training Data Generation with Diffusion Models (Teresa Yeo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira Ray, Pooya Esmaeil Akhoondi, Amir Zamir. (2024)<br><strong>Controlled Training Data Generation with Diffusion Models</strong><br><button class=copy-to-clipboard title="Controlled Training Data Generation with Diffusion Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Distribution Shift, Distribution Shift, Supervised Learning, Supervised Learning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15309v1.pdf filename=2403.15309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a method to control a <b>text-to-image</b> generative model to produce training data specifically &ldquo;useful&rdquo; for <b>supervised</b> <b>learning.</b> Unlike previous works that employ an open-loop approach and pre-define <b>prompts</b> to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given <b>supervised</b> <b>model</b> and finds adversarial <b>prompts</b> that result in image generations that maximize the model loss. While these adversarial <b>prompts</b> result in diverse data informed by the model, they are not informed of the target <b>distribution,</b> <b>which</b> can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target <b>distribution.</b> <b>We</b> call the method combining these two mechanisms Guided Adversarial <b>Prompts.</b> We perform our evaluations on different tasks, datasets and architectures, with different types of <b>distribution</b> <b>shifts</b> (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.</p></p class="citation"></blockquote><h3 id=372--72259-internvideo2-scaling-video-foundation-models-for-multimodal-video-understanding-yi-wang-et-al-2024>(3/72 | 72/259) InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding (Yi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang. (2024)<br><strong>InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding</strong><br><button class=copy-to-clipboard title="InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 59<br>Keywords: Benchmarking, Contrastive Learning, Foundation Model, Multi-modal, Multi-modal, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15377v1.pdf filename=2403.15377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce InternVideo2, a new video <b>foundation</b> <b>model</b> (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue. Our approach employs a progressive training paradigm that unifies the different self- or <b>weakly-supervised</b> <b>learning</b> <b>frameworks</b> of masked video token reconstruction, cross-modal <b>contrastive</b> <b>learning,</b> and next token prediction. Different training stages would guide our model to capture different levels of structure and semantic information through different pretext tasks. At the data level, we prioritize the spatiotemporal consistency by semantically segmenting videos and generating video-audio-speech captions. This improves the alignment between video and text. We scale both data and model size for our InternVideo2. Through extensive experiments, we validate our designs and demonstrate the state-of-the-art performance on over 60 video and audio tasks. Notably, our model outperforms others on various video-related captioning, dialogue, and long video understanding <b>benchmarks,</b> highlighting its ability to reason and comprehend long temporal contexts. Code and models are available at <a href=https://github.com/OpenGVLab/InternVideo2/>https://github.com/OpenGVLab/InternVideo2/</a>.</p></p class="citation"></blockquote><h3 id=472--73259-llava-prumerge-adaptive-token-reduction-for-efficient-large-multimodal-models-yuzhang-shang-et-al-2024>(4/72 | 73/259) LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models (Yuzhang Shang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan. (2024)<br><strong>LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models</strong><br><button class=copy-to-clipboard title="LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Transformer, Question Answering, Reasoning, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15388v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15388v2.pdf filename=2403.15388v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Multimodal</b> <b>Models</b> (LMMs) have shown significant <b>reasoning</b> capabilities by connecting a <b>visual</b> <b>encoder</b> <b>and</b> a <b>large</b> <b>language</b> <b>model.</b> LMMs typically use a fixed amount of <b>visual</b> <b>tokens,</b> <b>such</b> as the penultimate layer features in the CLIP <b>visual</b> <b>encoder,</b> <b>as</b> the prefix content. Recent LMMs incorporate more complex <b>visual</b> <b>inputs,</b> <b>such</b> as high-resolution images and videos, which increase the number of <b>visual</b> <b>tokens</b> <b>significantly.</b> However, due to the design of the <b>Transformer</b> architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many <b>visual</b> <b>tokens</b> <b>are</b> spatially redundant. Based on this, we propose PruMerge, a novel adaptive <b>visual</b> <b>token</b> <b>reduction</b> approach, which largely reduces the number of <b>visual</b> <b>tokens</b> <b>while</b> maintaining comparable model performance. We first select the unpruned <b>visual</b> <b>tokens</b> <b>based</b> on their similarity to class tokens and spatial tokens. We then cluster the pruned tokens based on key similarity and merge the clustered tokens with the unpruned tokens to supplement their information. Empirically, when applied to LLaVA-1.5, our approach can compress the <b>visual</b> <b>tokens</b> <b>by</b> 18 times on average, and achieve comparable performance across diverse <b>visual</b> <b>question-answering</b> <b>and</b> <b>reasoning</b> tasks. Code and checkpoints are at <a href=https://llava-prumerge.github.io/>https://llava-prumerge.github.io/</a>.</p></p class="citation"></blockquote><h3 id=572--74259-clip-vqdiffusion--langauge-free-training-of-text-to-image-generation-using-clip-and-vector-quantized-diffusion-model-seungdae-han-et-al-2024>(5/72 | 74/259) CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model (Seungdae Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungdae Han, Joohee Kim. (2024)<br><strong>CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model</strong><br><button class=copy-to-clipboard title="CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Out-of-distribution, Quantization, Text2image, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14944v1.pdf filename=2403.14944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been a significant progress in text conditional image generation models. Recent advancements in this field depend not only on improvements in model structures, but also vast quantities of <b>text-image</b> paired datasets. However, creating these kinds of datasets is very costly and requires a substantial amount of labor. Famous face datasets don&rsquo;t have corresponding text captions, making it difficult to develop text conditional image generation models on these datasets. Some research has focused on developing text to image generation models using only images without text captions. Here, we propose CLIP-VQDiffusion, which leverage the pretrained CLIP model to provide <b>multimodal</b> <b>text-image</b> representations and strong image generation capabilities. On the FFHQ dataset, our model outperformed previous state-of-the-art methods by 4.4% in clipscore and generated very realistic images even when the text was both in and out of distribution. The pretrained models and codes will soon be available at <a href=https://github.com/INFINIQ-AI1/CLIPVQDiffusion>https://github.com/INFINIQ-AI1/CLIPVQDiffusion</a></p></p class="citation"></blockquote><h3 id=672--75259-cell-variational-information-bottleneck-network-zhonghua-zhai-et-al-2024>(6/72 | 75/259) Cell Variational Information Bottleneck Network (Zhonghua Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhonghua Zhai, Chen Ju, Jinsong Lan, Shuai Xiao. (2024)<br><strong>Cell Variational Information Bottleneck Network</strong><br><button class=copy-to-clipboard title="Cell Variational Information Bottleneck Network" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Face Recognition, MNIST, Convolution, Convolutional Neural Network, Mutual Information, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15082v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15082v2.pdf filename=2403.15082v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we propose Cell Variational Information Bottleneck Network (cellVIB), a <b>convolutional</b> <b>neural</b> <b>network</b> using information bottleneck mechanism, which can be combined with the latest feedforward network architecture in an end-to-end training method. Our Cell Variational Information Bottleneck Network is constructed by stacking VIB cells, which generate feature maps with uncertainty. As layers going deeper, the regularization effect will gradually increase, instead of directly adding excessive regular constraints to the output layer of the model as in Deep VIB. Under each VIB cell, the feedforward process learns an independent mean term and an standard deviation term, and predicts the Gaussian distribution based on them. The feedback process is based on reparameterization trick for effective training. This work performs an extensive analysis on <b>MNIST</b> dataset to verify the effectiveness of each VIB cells, and provides an insightful analysis on how the VIB cells affect <b>mutual</b> <b>information.</b> Experiments conducted on CIFAR-10 also prove that our cellVIB is robust against noisy labels during training and against corrupted images during testing. Then, we validate our method on PACS dataset, whose results show that the VIB cells can significantly improve the generalization performance of the basic model. Finally, in a more complex <b>representation</b> <b>learning</b> task, <b>face</b> <b>recognition,</b> our network structure has also achieved very competitive results.</p></p class="citation"></blockquote><h3 id=772--76259-cartoon-hallucinations-detection-pose-aware-in-context-visual-learning-bumsoo-kim-et-al-2024>(7/72 | 76/259) Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning (Bumsoo Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo. (2024)<br><strong>Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning</strong><br><button class=copy-to-clipboard title="Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Hallucination Detection, Text2image, In-context Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15048v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15048v2.pdf filename=2403.15048v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>Text-to-Image</b> (TTI) models have become a common approach for generating training data in various generative fields. However, visual <b>hallucinations,</b> <b>which</b> contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual <b>hallucination</b> <b>detection</b> system for cartoon character images generated by TTI models. Our approach leverages pose-aware <b>in-context</b> visual learning (PA-ICVL) with <b>Vision-Language</b> Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a <b>fine-tuned</b> pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual <b>hallucinations</b> <b>compared</b> to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual <b>hallucinations,</b> <b>expanding</b> their potential in non-photorealistic domains.</p></p class="citation"></blockquote><h3 id=872--77259-bsnet-box-supervised-simulation-assisted-mean-teacher-for-3d-instance-segmentation-jiahao-lu-et-al-2024>(8/72 | 77/259) BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (Jiahao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Lu, Jiacheng Deng, Tianzhu Zhang. (2024)<br><strong>BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation</strong><br><button class=copy-to-clipboard title="BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Supervised Learning, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15019v1.pdf filename=2403.15019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D instance segmentation (3DIS) is a crucial task, but point-level annotations are tedious in fully <b>supervised</b> settings. Thus, using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process, involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However, due to the presence of intersections among bboxes, not every point has a determined instance label, especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly <b>supervised</b> 3DIS results, we propose the Box-Supervised <b>Simulation-assisted</b> Mean Teacher for 3D Instance Segmentation (BSNet), which devises a novel pseudo-labeler called <b>Simulation-assisted</b> <b>Transformer.</b> The labeler consists of two main components. The first is <b>Simulation-assisted</b> Mean Teacher, which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure, we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs. Code is available at \href{https://github.com/peoplelu/BSNet}{https://github.com/peoplelu/BSNet}.</p></p class="citation"></blockquote><h3 id=972--78259-parformer-vision-transformer-baseline-with-parallel-local-global-token-mixer-and-convolution-attention-patch-embedding-novendra-setyawan-et-al-2024>(9/72 | 78/259) ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding (Novendra Setyawan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Jun-Wei Hsieh, Hui-Kai Su, Wen-Kai Kuo. (2024)<br><strong>ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding</strong><br><button class=copy-to-clipboard title="ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15004v1.pdf filename=2403.15004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents ParFormer as an enhanced <b>transformer</b> architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the <b>Convolutional</b> Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a <b>convolutional</b> attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms <b>CNN-based</b> and state-of-the-art <b>transformer-based</b> architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Identity Mapping Token Mixer, resulting in a 0.5% increase in accuracy. The ParFormer models outperformed ConvNeXt and Swin <b>Transformer</b> for the pure <b>convolution</b> and <b>transformer</b> model in accuracy. Furthermore, our model surpasses the current leading hybrid <b>transformer</b> by reaching competitive Top-1 scores in the ImageNet-1K classification test. Specifically, our model variants with 11M, 23M, and 34M parameters achieve scores of 80.4%, 82.1%, and 83.1%, respectively. Code: <a href=https://github.com/novendrastywn/ParFormer-CAPE-2024>https://github.com/novendrastywn/ParFormer-CAPE-2024</a></p></p class="citation"></blockquote><h3 id=1072--79259-geometric-generative-models-based-on-morphological-equivariant-pdes-and-gans-el-hadji-s-diop-et-al-2024>(10/72 | 79/259) Geometric Generative Models based on Morphological Equivariant PDEs and GANs (El Hadji S. Diop et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>El Hadji S. Diop, Thierno Fall, Alioune Mbengue, Mohamed Daoudi. (2024)<br><strong>Geometric Generative Models based on Morphological Equivariant PDEs and GANs</strong><br><button class=copy-to-clipboard title="Geometric Generative Models based on Morphological Equivariant PDEs and GANs" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV, math-DG<br>Keyword Score: 50<br>Keywords: MNIST, Convolution, Convolutional Neural Network, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14897v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14897v2.pdf filename=2403.14897v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Content and image generation consist in creating or generating data from noisy information by extracting specific features such as texture, edges, and other thin image structures. We are interested here in <b>generative</b> <b>models,</b> <b>and</b> two main problems are addressed. Firstly, the improvements of specific feature extraction while accounting at multiscale levels intrinsic geometric features; and secondly, the equivariance of the network to reduce its complexity and provide a geometric interpretability. To proceed, we propose a geometric <b>generative</b> <b>model</b> <b>based</b> on an equivariant partial differential equation (PDE) for group <b>convolution</b> neural networks (G-CNNs), so called PDE-G-CNNs, built on morphology operators and <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs).</b> Equivariant morphological PDE layers are composed of multiscale dilations and erosions formulated in Riemannian manifolds, while group symmetries are defined on a Lie group. We take advantage of the Lie group structure to properly integrate the equivariance in layers, and are able to use the Riemannian metric to solve the multiscale morphological operations. Each point of the Lie group is associated with a unique point in the manifold, which helps us derive a metric on the Riemannian manifold from a tensor field invariant under the Lie group so that the induced metric has the same symmetries. The proposed geometric morphological <b>GAN</b> (GM-GAN) is obtained by using the proposed morphological equivariant <b>convolutions</b> in PDE-G-CNNs to bring nonlinearity in classical <b>CNNs.</b> GM-GAN is evaluated on <b>MNIST</b> data and compared with <b>GANs.</b> Preliminary results show that GM-GAN model outperforms classical <b>GAN.</b></p></p class="citation"></blockquote><h3 id=1172--80259-trajectory-regularization-enhances-self-supervised-geometric-representation-jiayun-wang-et-al-2024>(11/72 | 80/259) Trajectory Regularization Enhances Self-Supervised Geometric Representation (Jiayun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayun Wang, Stella X. Yu, Yubei Chen. (2024)<br><strong>Trajectory Regularization Enhances Self-Supervised Geometric Representation</strong><br><button class=copy-to-clipboard title="Trajectory Regularization Enhances Self-Supervised Geometric Representation" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 48<br>Keywords: Benchmarking, Out-of-distribution, Representation Learning, Self-supervised Learning, Self-supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14973v1.pdf filename=2403.14973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) has proven effective in learning high-quality <b>representations</b> <b>for</b> various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric <b>representations.</b> <b>To</b> address this gap, we introduce a new pose-estimation <b>benchmark</b> for assessing SSL geometric <b>representations,</b> <b>which</b> demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this <b>benchmark,</b> we study enhancing SSL geometric <b>representations</b> <b>without</b> sacrificing semantic classification accuracy. We find that leveraging mid-layer <b>representations</b> <b>improves</b> pose-estimation performance by 10-20%. Further, we introduce an <b>unsupervised</b> trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on <b>out-of-distribution</b> data. We hope the proposed <b>benchmark</b> and methods offer new insights and improvements in <b>self-supervised</b> <b>geometric</b> <b>representation</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1272--81259-selectively-informative-description-can-reduce-undesired-embedding-entanglements-in-text-to-image-personalization-jimyeong-kim-et-al-2024>(12/72 | 81/259) Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization (Jimyeong Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jimyeong Kim, Jungwon Park, Wonjong Rhee. (2024)<br><strong>Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization</strong><br><button class=copy-to-clipboard title="Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15330v1.pdf filename=2403.15330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>text-to-image</b> personalization, a timely and crucial challenge is the tendency of generated images overfitting to the biases present in the reference images. We initiate our study with a comprehensive categorization of the biases into background, nearby-object, tied-object, substance (in style re-contextualization), and pose biases. These biases manifest in the generated images due to their entanglement into the subject embedding. This undesired embedding entanglement not only results in the reflection of biases from the reference images into the generated images but also notably diminishes the alignment of the generated images with the given generation <b>prompt.</b> To address this challenge, we propose SID~(Selectively Informative Description), a text description strategy that deviates from the prevalent approach of only characterizing the subject&rsquo;s class identification. SID is generated utilizing <b>multimodal</b> <b>GPT-4</b> and can be seamlessly integrated into optimization-based models. We present comprehensive experimental results along with analyses of cross-attention maps, subject-alignment, non-subject-disentanglement, and text-alignment.</p></p class="citation"></blockquote><h3 id=1372--82259-gcn-devlstm-path-development-for-skeleton-based-action-recognition-lei-jiang-et-al-2024>(13/72 | 82/259) GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition (Lei Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Jiang, Weixin Yang, Xin Zhang, Hao Ni. (2024)<br><strong>GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition</strong><br><button class=copy-to-clipboard title="GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15212v1.pdf filename=2403.15212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skeleton-based action recognition (SAR) in videos is an important but challenging task in computer vision. The recent state-of-the-art models for SAR are primarily based on <b>graph</b> <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(GCNs),</b> which are powerful in extracting the spatial information of skeleton data. However, it is yet clear that such <b>GCN-based</b> models can effectively capture the temporal dynamics of human action sequences. To this end, we propose the DevLSTM module, which exploits the path development &ndash; a principled and parsimonious representation for sequential data by leveraging the Lie group structure. The path development, originated from Rough path theory, can effectively capture the order of events in high-dimensional stream data with massive dimension reduction and consequently enhance the <b>LSTM</b> module substantially. Our proposed G-DevLSTM module can be conveniently plugged into the temporal <b>graph,</b> complementing existing advanced <b>GCN-based</b> models. Our empirical studies on the NTU60, NTU120 and Chalearn2013 datasets demonstrate that our proposed hybrid model significantly outperforms the current best-performing methods in SAR tasks. The code is available at <a href=https://github.com/DeepIntoStreams/GCN-DevLSTM>https://github.com/DeepIntoStreams/GCN-DevLSTM</a>.</p></p class="citation"></blockquote><h3 id=1472--83259-mscotdet-language-driven-multi-modal-fusion-for-improved-multispectral-pedestrian-detection-taeheon-kim-et-al-2024>(14/72 | 83/259) MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection (Taeheon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taeheon Kim, Sangyun Chung, Damin Yeom, Youngjoon Yu, Hak Gu Kim, Yong Man Ro. (2024)<br><strong>MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection</strong><br><button class=copy-to-clipboard title="MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15209v1.pdf filename=2403.15209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multispectral pedestrian detection is attractive for around-the-clock applications due to the complementary information between RGB and thermal modalities. However, current models often fail to detect pedestrians in obvious cases, especially due to the modality bias learned from statistically biased datasets. From these problems, we anticipate that maybe understanding the complementary information itself is difficult to achieve from vision-only models. Accordingly, we propose a novel Multispectral Chain-of-Thought Detection (MSCoTDet) framework, which incorporates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to understand the complementary information at the semantic level and further enhance the fusion process. Specifically, we generate text descriptions of the pedestrian in each RGB and thermal modality and design a Multispectral Chain-of-Thought (MSCoT) <b>prompting,</b> which models a step-by-step process to facilitate cross-modal <b>reasoning</b> at the semantic level and perform accurate detection. Moreover, we design a Language-driven <b>Multi-modal</b> Fusion (LMF) strategy that enables fusing vision-driven and language-driven detections. Extensive experiments validate that MSCoTDet improves multispectral pedestrian detection.</p></p class="citation"></blockquote><h3 id=1572--84259-long-clip-unlocking-the-long-text-capability-of-clip-beichen-zhang-et-al-2024>(15/72 | 84/259) Long-CLIP: Unlocking the Long-Text Capability of CLIP (Beichen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Jiaqi Wang. (2024)<br><strong>Long-CLIP: Unlocking the Long-Text Capability of CLIP</strong><br><button class=copy-to-clipboard title="Long-CLIP: Unlocking the Long-Text Capability of CLIP" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Zero-shot, Text2image, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15378v1.pdf filename=2403.15378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for <b>zero-shot</b> classification, <b>text-image</b> retrieval, and <b>text-image</b> generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its applications for image retrieval and <b>text-to-image</b> generation with extensive prerequisites. To this end, we propose Long-CLIP as a plug-and-play alternative to CLIP that supports long-text input, retains or even surpasses its <b>zero-shot</b> generalizability, and aligns the CLIP latent space, making it readily replace CLIP without any further adaptation in downstream frameworks. Nevertheless, achieving this goal is far from straightforward, as simplistic <b>fine-tuning</b> can result in a significant degradation of CLIP&rsquo;s performance. Moreover, substituting the text encoder with a language model supporting longer contexts necessitates pretraining with vast amounts of data, incurring significant expenses. Accordingly, Long-CLIP introduces an efficient <b>fine-tuning</b> solution on CLIP with two novel strategies designed to maintain the original capabilities, including (1) a knowledge-preserved stretching of positional embedding and (2) a primary component matching of CLIP features. With leveraging just one million extra long <b>text-image</b> pairs, Long-CLIP has shown the superiority to CLIP for about 20% in long caption <b>text-image</b> retrieval and 6% in traditional <b>text-image</b> retrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers enhanced capabilities for generating images from detailed text descriptions by replacing CLIP in a plug-and-play manner.</p></p class="citation"></blockquote><h3 id=1672--85259-point-detr3d-leveraging-imagery-data-with-spatial-point-prior-for-weakly-semi-supervised-3d-object-detection-hongzhi-gao-et-al-2024>(16/72 | 85/259) Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection (Hongzhi Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang Zhang, Feng Zhao. (2024)<br><strong>Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection</strong><br><button class=copy-to-clipboard title="Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15317v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15317v2.pdf filename=2403.15317v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for <b>object</b> <b>localization.</b> In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from Point-DETR which encodes 3D positional information solely through a point encoder, we propose an explicit positional query initialization strategy to enhance the positional prior. Considering the low quality of pseudo labels at distant regions produced by the teacher model, we enhance the detector&rsquo;s perception by incorporating dense imagery data through a novel Cross-Modal Deformable RoI Fusion (D-RoI).Moreover, an innovative point-guided <b>self-supervised</b> <b>learning</b> technique is proposed to allow for fully exploiting point priors, even in student models.Extensive experiments on representative nuScenes dataset demonstrate our Point-DETR3D obtains significant improvements compared to previous works. Notably, with only 5% of labeled data, Point-DETR3D achieves over 90% performance of its fully <b>supervised</b> counterpart.</p></p class="citation"></blockquote><h3 id=1772--86259-dreamflow-high-quality-text-to-3d-generation-by-approximating-probability-flow-kyungmin-lee-et-al-2024>(17/72 | 86/259) DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow (Kyungmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungmin Lee, Kihyuk Sohn, Jinwoo Shin. (2024)<br><strong>DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow</strong><br><button class=copy-to-clipboard title="DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14966v1.pdf filename=2403.14966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in text-to-3D generation has been achieved through the utilization of score <b>distillation</b> methods: they make use of the pre-trained <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models</b> by <b>distilling</b> via the <b>diffusion</b> <b>model</b> training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by leveraging the T2I <b>diffusion</b> <b>prior</b> in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to3D optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design DreamFlow, a practical three-stage coarseto-fine text-to-3D optimization framework that enables fast generation of highquality and high-resolution (i.e., 1024x1024) 3D contents. For example, we demonstrate that DreamFlow is 5 times faster than the existing state-of-the-art text-to-3D method, while producing more photorealistic 3D contents. Visit our project page (<a href=https://kyungmnlee.github.io/dreamflow.github.io/>https://kyungmnlee.github.io/dreamflow.github.io/</a>) for visualizations.</p></p class="citation"></blockquote><h3 id=1872--87259-stag4d-spatial-temporal-anchored-generative-4d-gaussians-yifei-zeng-et-al-2024>(18/72 | 87/259) STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians (Yifei Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Zeng, Yanqin Jiang, Siyu Zhu, Yuanxun Lu, Youtian Lin, Hao Zhu, Weiming Hu, Xun Cao, Yao Yao. (2024)<br><strong>STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians</strong><br><button class=copy-to-clipboard title="STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Knowledge Distillation, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14939v1.pdf filename=2403.14939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in pre-trained <b>diffusion</b> <b>models</b> and 3D generation have spurred interest in 4D content creation. However, achieving high-fidelity 4D generation with spatial-temporal consistency remains a challenge. In this work, we propose STAG4D, a novel framework that combines pre-trained <b>diffusion</b> <b>models</b> with dynamic 3D Gaussian splatting for high-fidelity 4D generation. Drawing inspiration from 3D generation techniques, we utilize a multi-view <b>diffusion</b> <b>model</b> to initialize multi-view images anchoring on the input video frames, where the video can be either real-world captured or generated by a video <b>diffusion</b> <b>model.</b> To ensure the temporal consistency of the multi-view sequence initialization, we introduce a simple yet effective fusion strategy to leverage the first frame as a temporal anchor in the <b>self-attention</b> computation. With the almost consistent multi-view sequences, we then apply the score <b>distillation</b> sampling to optimize the 4D Gaussian point cloud. The 4D Gaussian spatting is specially crafted for the generation task, where an adaptive densification strategy is proposed to mitigate the unstable Gaussian gradient for robust optimization. Notably, the proposed pipeline does not require any pre-training or <b>fine-tuning</b> of <b>diffusion</b> <b>networks,</b> offering a more accessible and practical solution for the 4D generation task. Extensive experiments demonstrate that our method outperforms prior 4D generation works in rendering quality, spatial-temporal consistency, and generation robustness, setting a new state-of-the-art for 4D generation from diverse inputs, including text, image, and video.</p></p class="citation"></blockquote><h3 id=1972--88259-self-supervised-backbone-framework-for-diverse-agricultural-vision-tasks-sudhir-sornapudi-et-al-2024>(19/72 | 88/259) Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks (Sudhir Sornapudi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sudhir Sornapudi, Rajhans Singh. (2024)<br><strong>Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks</strong><br><button class=copy-to-clipboard title="Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 35<br>Keywords: Contrastive Learning, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15248v1.pdf filename=2403.15248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computer vision in agriculture is game-changing with its ability to transform farming into a data-driven, precise, and sustainable industry. Deep learning has empowered agriculture vision to analyze vast, complex visual data, but heavily rely on the availability of large annotated datasets. This remains a bottleneck as manual labeling is error-prone, time-consuming, and expensive. The lack of efficient labeling approaches inspired us to consider <b>self-supervised</b> <b>learning</b> as a paradigm shift, learning meaningful feature <b>representations</b> <b>from</b> raw agricultural image data. In this work, we explore how <b>self-supervised</b> <b>representation</b> <b>learning</b> unlocks the potential applicability to diverse agriculture vision tasks by eliminating the need for large-scale annotated datasets. We propose a lightweight framework utilizing SimCLR, a <b>contrastive</b> <b>learning</b> approach, to pre-train a ResNet-50 backbone on a large, unannotated dataset of real-world agriculture field images. Our experimental analysis and results indicate that the model learns robust features applicable to a broad range of downstream agriculture tasks discussed in the paper. Additionally, the reduced reliance on annotated data makes our approach more cost-effective and accessible, paving the way for broader adoption of computer vision in agriculture.</p></p class="citation"></blockquote><h3 id=2072--89259-fairerclip-debiasing-clips-zero-shot-predictions-using-functions-in-rkhss-sepehr-dehdashtian-et-al-2024>(20/72 | 89/259) FairerCLIP: Debiasing CLIP&rsquo;s Zero-Shot Predictions using Functions in RKHSs (Sepehr Dehdashtian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sepehr Dehdashtian, Lan Wang, Vishnu Naresh Boddeti. (2024)<br><strong>FairerCLIP: Debiasing CLIP&rsquo;s Zero-Shot Predictions using Functions in RKHSs</strong><br><button class=copy-to-clipboard title="FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Fairness, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15593v1.pdf filename=2403.15593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large pre-trained <b>vision-language</b> models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream <b>zero-shot</b> prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making <b>zero-shot</b> predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP&rsquo;s image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to $4\times$-$10\times$ faster training than the existing methods. 3) Sample Efficiency: Under sample-limited conditions, FairerCLIP significantly outperforms baselines when they fail entirely. And, 4) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on <b>benchmark</b> <b>fairness</b> and spurious correlation datasets over their respective baselines.</p></p class="citation"></blockquote><h3 id=2172--90259-simba-simplified-mamba-based-architecture-for-vision-and-multivariate-time-series-badri-n-patro-et-al-2024>(21/72 | 90/259) SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series (Badri N. Patro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Badri N. Patro, Vijay S. Agneeswaran. (2024)<br><strong>SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series</strong><br><button class=copy-to-clipboard title="SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-SY, cs.CV, eess-IV, eess-SY<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15360v1.pdf filename=2403.15360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global <b>Convolutions,</b> liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series <b>benchmarks</b> demonstrate that SiMBA outperforms existing SSMs, bridging the performance gap with state-of-the-art <b>transformers.</b> Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet and <b>transfer</b> <b>learning</b> <b>benchmarks</b> such as Stanford Car and Flower as well as task learning <b>benchmarks</b> as well as seven time series <b>benchmark</b> datasets. The project page is available on this website ~\url{https://github.com/badripatro/Simba}.</p></p class="citation"></blockquote><h3 id=2272--91259-piecewise-linear-manifolds-for-deep-metric-learning-shubhang-bhatnagar-et-al-2024>(22/72 | 91/259) Piecewise-Linear Manifolds for Deep Metric Learning (Shubhang Bhatnagar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhang Bhatnagar, Narendra Ahuja. (2024)<br><strong>Piecewise-Linear Manifolds for Deep Metric Learning</strong><br><button class=copy-to-clipboard title="Piecewise-Linear Manifolds for Deep Metric Learning" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Supervised Learning, Unsupervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14977v1.pdf filename=2403.14977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> deep metric learning (UDML) focuses on learning a semantic representation space using only unlabeled data. This challenging problem requires accurately estimating the similarity between data points, which is used to supervise a deep network. For this purpose, we propose to model the high-dimensional data manifold using a piecewise-linear approximation, with each low-dimensional linear piece approximating the data manifold in a small neighborhood of a point. These neighborhoods are used to estimate similarity between data points. We empirically show that this similarity estimate correlates better with the ground truth than the similarity estimates of current state-of-the-art techniques. We also show that proxies, commonly used in <b>supervised</b> metric learning, can be used to model the piecewise-linear manifold in an <b>unsupervised</b> setting, helping improve performance. Our method outperforms existing <b>unsupervised</b> metric learning approaches on standard <b>zero-shot</b> image retrieval <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2372--92259-augmented-reality-based-simulated-data-arsim-with-multi-view-consistency-for-av-perception-networks-aqeel-anwar-et-al-2024>(23/72 | 92/259) Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks (Aqeel Anwar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park. (2024)<br><strong>Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks</strong><br><button class=copy-to-clipboard title="Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15370v1.pdf filename=2403.15370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting a diverse range of objects under various driving scenarios is essential for the effectiveness of autonomous driving systems. However, the real-world data collected often lacks the necessary diversity presenting a long-tail distribution. Although synthetic data has been utilized to overcome this issue by generating virtual scenes, it faces hurdles such as a significant <b>domain</b> <b>gap</b> and the substantial efforts required from 3D artists to create realistic environments. To overcome these challenges, we present ARSim, a fully automated, comprehensive, modular framework designed to enhance real multi-view image data with 3D synthetic objects of interest. The proposed method integrates <b>domain</b> <b>adaptation</b> and randomization strategies to address covariate shift between real and simulated data by inferring essential <b>domain</b> <b>attributes</b> from real data and employing <b>simulation-based</b> randomization for other attributes. We construct a simplified virtual scene using real data and strategically place 3D synthetic assets within it. Illumination is achieved by estimating light distribution from multiple images capturing the surroundings of the vehicle. Camera parameters from real data are employed to render synthetic assets in each frame. The resulting augmented multi-view consistent dataset is used to train a multi-camera perception network for autonomous vehicles. Experimental results on various AV perception tasks demonstrate the superior performance of networks trained on the augmented dataset.</p></p class="citation"></blockquote><h3 id=2472--93259-shadow-generation-for-composite-image-using-diffusion-model-qingyang-liu-et-al-2024>(24/72 | 93/259) Shadow Generation for Composite Image Using Diffusion model (Qingyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyang Liu, Junqi You, Jianting Wang, Xinhao Tao, Bo Zhang, Li Niu. (2024)<br><strong>Shadow Generation for Composite Image Using Diffusion model</strong><br><button class=copy-to-clipboard title="Shadow Generation for Composite Image Using Diffusion model" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Diffusion Model, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15234v1.pdf filename=2403.15234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of image composition, generating realistic shadow for the inserted foreground remains a formidable challenge. Previous works have developed image-to-image translation models which are trained on paired training data. However, they are struggling to generate shadows with accurate shapes and intensities, hindered by data scarcity and inherent task complexity. In this paper, we resort to <b>foundation</b> <b>model</b> with rich prior knowledge of natural shadow images. Specifically, we first adapt <b>ControlNet</b> to our task and then propose intensity modulation modules to improve the shadow intensity. Moreover, we extend the small-scale DESOBA dataset to DESOBAv2 using a novel data acquisition pipeline. Experimental results on both DESOBA and DESOBAv2 datasets as well as real composite images demonstrate the superior capability of our model for shadow generation task. The dataset, code, and model are released at <a href=https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2>https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBAv2</a>.</p></p class="citation"></blockquote><h3 id=2572--94259-transfer-clip-for-generalizable-image-denoising-jun-cheng-et-al-2024>(25/72 | 94/259) Transfer CLIP for Generalizable Image Denoising (Jun Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Cheng, Dong Liang, Shan Tan. (2024)<br><strong>Transfer CLIP for Generalizable Image Denoising</strong><br><button class=copy-to-clipboard title="Transfer CLIP for Generalizable Image Denoising" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15132v1.pdf filename=2403.15132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image denoising is a fundamental task in computer vision. While prevailing deep learning-based <b>supervised</b> and <b>self-supervised</b> methods have excelled in eliminating in-distribution noise, their susceptibility to <b>out-of-distribution</b> (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.</p></p class="citation"></blockquote><h3 id=2672--95259-continual-vision-and-language-navigation-seongjun-jeong-et-al-2024>(26/72 | 95/259) Continual Vision-and-Language Navigation (Seongjun Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang. (2024)<br><strong>Continual Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="Continual Vision-and-Language Navigation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Continual Learning, Perplexity, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15049v1.pdf filename=2403.15049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the <b>Continual</b> <b>Vision-and-Language</b> Navigation (CVLN) paradigm, designed to evaluate agents trained through a <b>continual</b> <b>learning</b> process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based methods for CVLN, <b>Perplexity</b> Replay (PerpR) and Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging episodes based on action <b>perplexity,</b> while ESR replays previously predicted action logits to preserve learned behaviors. We demonstrate the effectiveness of the proposed methods on CVLN through extensive experiments.</p></p class="citation"></blockquote><h3 id=2772--96259-vehicle-detection-performance-in-nordic-region-hamam-mokayed-et-al-2024>(27/72 | 96/259) Vehicle Detection Performance in Nordic Region (Hamam Mokayed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamam Mokayed, Rajkumar Saini, Oluwatosin Adewumi, Lama Alkhaled, Bjorn Backe, Palaiahnakote Shivakumara, Olle Hagner, Yan Chai Hum. (2024)<br><strong>Vehicle Detection Performance in Nordic Region</strong><br><button class=copy-to-clipboard title="Vehicle Detection Performance in Nordic Region" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15017v1.pdf filename=2403.15017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the critical challenge of vehicle detection in the harsh winter conditions in the Nordic regions, characterized by heavy snowfall, reduced visibility, and low lighting. Due to their susceptibility to environmental distortions and occlusions, traditional vehicle detection methods have struggled in these adverse conditions. The advanced proposed deep learning architectures brought promise, yet the unique difficulties of detecting vehicles in Nordic winters remain inadequately addressed. This study uses the Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to evaluate the performance of state-of-the-art vehicle detection algorithms under challenging weather conditions. Our methodology includes a comprehensive evaluation of single-stage, two-stage, and <b>transformer-based</b> detectors against the NVD. We propose a series of enhancements tailored to each detection framework, including <b>data</b> <b>augmentation,</b> hyperparameter tuning, <b>transfer</b> <b>learning,</b> and novel strategies designed explicitly for the DETR model. Our findings not only highlight the limitations of current detection systems in the Nordic environment but also offer promising directions for enhancing these algorithms for improved robustness and accuracy in vehicle detection amidst the complexities of winter landscapes. The code and the dataset are available at <a href=https://nvd.ltu-ai.dev>https://nvd.ltu-ai.dev</a></p></p class="citation"></blockquote><h3 id=2872--97259-improve-cross-domain-mixed-sampling-with-guidance-training-for-adaptive-segmentation-wenlve-zhou-et-al-2024>(28/72 | 97/259) Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation (Wenlve Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenlve Zhou, Zhiheng Zhou, Tianlei Wang, Delu Zeng. (2024)<br><strong>Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation</strong><br><button class=copy-to-clipboard title="Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14995v1.pdf filename=2403.14995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (UDA) endeavors to adjust models trained on a source <b>domain</b> <b>to</b> perform well on a target <b>domain</b> <b>without</b> requiring additional annotations. In the context of <b>domain</b> <b>adaptive</b> semantic segmentation, which tackles UDA for dense prediction, the goal is to circumvent the need for costly pixel-level annotations. Typically, various prevailing methods baseline rely on constructing intermediate <b>domains</b> <b>via</b> cross-domain mixed sampling techniques to mitigate the performance decline caused by <b>domain</b> <b>gaps.</b> However, such approaches generate synthetic data that diverge from real-world <b>distributions,</b> <b>potentially</b> leading the model astray from the true target <b>distribution.</b> <b>To</b> address this challenge, we propose a novel auxiliary task called Guidance Training. This task facilitates the effective utilization of cross-domain mixed sampling techniques while mitigating <b>distribution</b> <b>shifts</b> from the real world. Specifically, Guidance Training guides the model to extract and reconstruct the target-domain feature <b>distribution</b> <b>from</b> mixed data, followed by decoding the reconstructed target-domain features to make pseudo-label predictions. Importantly, integrating Guidance Training incurs minimal training overhead and imposes no additional inference burden. We demonstrate the efficacy of our approach by integrating it with existing methods, consistently improving performance. The implementation will be available at <a href=https://github.com/Wenlve-Zhou/Guidance-Training>https://github.com/Wenlve-Zhou/Guidance-Training</a>.</p></p class="citation"></blockquote><h3 id=2972--98259-gpt-connect-interaction-between-text-driven-human-motion-generator-and-3d-scenes-in-a-training-free-manner-haoxuan-qu-et-al-2024>(29/72 | 98/259) GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner (Haoxuan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxuan Qu, Ziyan Guo, Jun Liu. (2024)<br><strong>GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner</strong><br><button class=copy-to-clipboard title="GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, ChatGPT, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14947v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14947v1.pdf filename=2403.14947v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, while text-driven human motion generation has received massive research attention, most existing text-driven motion generators are generally only designed to generate motion sequences in a blank background. While this is the case, in practice, human beings naturally perform their motions in 3D scenes, rather than in a blank background. Considering this, we here aim to perform scene-aware text-drive motion generation instead. Yet, intuitively training a separate scene-aware motion generator in a <b>supervised</b> way can require a large amount of motion samples to be troublesomely collected and annotated in a large scale of different 3D scenes. To handle this task rather in a relatively convenient manner, in this paper, we propose a novel <b>GPT-connect</b> framework. In <b>GPT-connect,</b> we enable scene-aware motion sequences to be generated directly utilizing the existing blank-background human motion generator, via leveraging <b>ChatGPT</b> to connect the existing motion generator with the 3D scene in a totally training-free manner. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework.</p></p class="citation"></blockquote><h3 id=3072--99259-is-fusion-instance-scene-collaborative-fusion-for-multimodal-3d-object-detection-junbo-yin-et-al-2024>(30/72 | 99/259) IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection (Junbo Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbo Yin, Jianbing Shen, Runnan Chen, Wei Li, Ruigang Yang, Pascal Frossard, Wenguan Wang. (2024)<br><strong>IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection</strong><br><button class=copy-to-clipboard title="IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Object Detection, Benchmarking, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15241v1.pdf filename=2403.15241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bird&rsquo;s eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, <b>objects</b> <b>in</b> the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-Fusion, an innovative <b>multimodal</b> fusion framework that jointly captures the Instance- and Scene-level contextual information. IS-Fusion essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level <b>multimodal</b> information, thus facilitating the instance-centric tasks like 3D <b>object</b> <b>detection.</b> It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region <b>transformers</b> to capture the <b>multimodal</b> scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local <b>multimodal</b> context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes <b>benchmark,</b> IS-Fusion outperforms all the published <b>multimodal</b> works to date. Code is available at: <a href=https://github.com/yinjunbo/IS-Fusion>https://github.com/yinjunbo/IS-Fusion</a>.</p></p class="citation"></blockquote><h3 id=3172--100259-neural-plasticity-inspired-foundation-model-for-observing-the-earth-crossing-modalities-zhitong-xiong-et-al-2024>(31/72 | 100/259) Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities (Zhitong Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhitong Xiong, Yi Wang, Fahong Zhang, Adam J. Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, Xiao Xiang Zhu. (2024)<br><strong>Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities</strong><br><button class=copy-to-clipboard title="Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15356v1.pdf filename=2403.15356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of <b>foundation</b> <b>models</b> has revolutionized our ability to interpret the Earth&rsquo;s surface using satellite observational data. Traditional models have been siloed, tailored to specific sensors or data types like optical, radar, and hyperspectral, each with its own unique characteristics. This specialization hinders the potential for a holistic analysis that could benefit from the combined strengths of these diverse data sources. Our novel approach introduces the Dynamic One-For-All (DOFA) model, leveraging the concept of neural plasticity in brain science to integrate various data modalities into a single framework adaptively. This dynamic hypernetwork, adjusting to different wavelengths, enables a single versatile <b>Transformer</b> jointly trained on data from five sensors to excel across 12 distinct Earth observation tasks, including sensors never seen during pretraining. DOFA&rsquo;s innovative design offers a promising leap towards more accurate, efficient, and unified Earth observation analysis, showcasing remarkable adaptability and performance in harnessing the potential of <b>multimodal</b> Earth observation data.</p></p class="citation"></blockquote><h3 id=3272--101259-mm-diff-high-fidelity-image-personalization-via-multi-modal-condition-integration-zhichao-wei-et-al-2024>(32/72 | 101/259) MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration (Zhichao Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Wei, Qingkun Su, Long Qin, Weizhi Wang. (2024)<br><strong>MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration</strong><br><button class=copy-to-clipboard title="MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15059v1.pdf filename=2403.15059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in tuning-free personalized image generation based on <b>diffusion</b> <b>models</b> are impressive. However, to improve subject fidelity, existing methods either retrain the <b>diffusion</b> <b>model</b> or infuse it with dense visual embeddings, both of which suffer from poor generalization and efficiency. Also, these methods falter in multi-subject image generation due to the unconstrained cross-attention mechanism. In this paper, we propose MM-Diff, a unified and tuning-free image personalization framework capable of generating high-fidelity images of both single and multiple subjects in seconds. Specifically, to simultaneously enhance <b>text</b> <b>consistency</b> and subject fidelity, MM-Diff employs a vision encoder to transform the input image into CLS and patch embeddings. CLS embeddings are used on the one hand to augment the <b>text</b> <b>embeddings,</b> and on the other hand together with patch embeddings to derive a small number of detail-rich subject embeddings, both of which are efficiently integrated into the <b>diffusion</b> <b>model</b> through the well-designed <b>multimodal</b> cross-attention mechanism. Additionally, MM-Diff introduces cross-attention map constraints during the training phase, ensuring flexible multi-subject image sampling during inference without any predefined inputs (e.g., layout). Extensive experiments demonstrate the superior performance of MM-Diff over other leading methods.</p></p class="citation"></blockquote><h3 id=3372--102259-forward-learning-for-gradient-based-black-box-saliency-map-generation-zeliang-zhang-et-al-2024>(33/72 | 102/259) Forward Learning for Gradient-based Black-box Saliency Map Generation (Zeliang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeliang Zhang, Mingqian Feng, Jinyang Jiang, Rongyi Zhu, Yijie Peng, Chenliang Xu. (2024)<br><strong>Forward Learning for Gradient-based Black-box Saliency Map Generation</strong><br><button class=copy-to-clipboard title="Forward Learning for Gradient-based Black-box Saliency Map Generation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, ChatGPT, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15603v1.pdf filename=2403.15603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient-based saliency maps are widely used to explain deep neural network decisions. However, as models become deeper and more <b>black-box,</b> <b>such</b> as in closed-source APIs like <b>ChatGPT,</b> computing gradients become challenging, hindering conventional explanation methods. In this work, we introduce a novel unified framework for estimating gradients in <b>black-box</b> <b>settings</b> and generating saliency maps to interpret model decisions. We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation. Additionally, we propose blockwise computation techniques to enhance estimation accuracy. Extensive experiments in <b>black-box</b> <b>settings</b> validate the effectiveness of our method, demonstrating accurate gradient estimation and explainability of generated saliency maps. Furthermore, we showcase the scalability of our approach by applying it to explain <b>GPT-Vision,</b> revealing the continued relevance of gradient-based explanation methods in the era of large, closed-source, and <b>black-box</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3472--103259-hyperbolic-metric-learning-for-visual-outlier-detection-alvaro-gonzalez-jimenez-et-al-2024>(34/72 | 103/259) Hyperbolic Metric Learning for Visual Outlier Detection (Alvaro Gonzalez-Jimenez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvaro Gonzalez-Jimenez, Simone Lionetti, Dena Bazazian, Philippe Gottfrois, Fabian Gröger, Marc Pouly, Alexander Navarini. (2024)<br><strong>Hyperbolic Metric Learning for Visual Outlier Detection</strong><br><button class=copy-to-clipboard title="Hyperbolic Metric Learning for Visual Outlier Detection" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Geometry, Out-of-distribution, Outlier Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15260v1.pdf filename=2403.15260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-Of-Distribution</b> (OOD) detection is critical to deploy deep learning models in safety-critical applications. However, the inherent hierarchical concept structure of visual data, which is instrumental to OOD detection, is often poorly captured by conventional methods based on Euclidean <b>geometry.</b> This work proposes a metric framework that leverages the strengths of Hyperbolic <b>geometry</b> for OOD detection. Inspired by previous works that refine the decision boundary for OOD data with synthetic <b>outliers,</b> <b>we</b> extend this method to Hyperbolic space. Interestingly, we find that synthetic <b>outliers</b> <b>do</b> not benefit OOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we explore the relationship between OOD detection performance and Hyperbolic embedding dimension, addressing practical concerns in resource-constrained environments. Extensive experiments show that our framework improves the FPR95 for OOD detection from 22% to 15% and from 49% to 28% on CIFAR-10 and CIFAR-100 respectively compared to Euclidean methods.</p></p class="citation"></blockquote><h3 id=3572--104259-lsk3dnet-towards-effective-and-efficient-3d-perception-with-large-sparse-kernels-tuo-feng-et-al-2024>(35/72 | 104/259) LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels (Tuo Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuo Feng, Wenguan Wang, Fan Ma, Yi Yang. (2024)<br><strong>LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels</strong><br><button class=copy-to-clipboard title="LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15173v1.pdf filename=2403.15173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous systems need to process large-scale, sparse, and irregular point clouds with limited compute resources. Consequently, it is essential to develop LiDAR perception methods that are both efficient and effective. Although naively enlarging 3D kernel size can enhance performance, it will also lead to a cubically-increasing overhead. Therefore, it is crucial to develop streamlined 3D large kernel designs that eliminate redundant weights and work effectively with larger kernels. In this paper, we propose an efficient and effective Large Sparse Kernel 3D Neural Network (LSK3DNet) that leverages dynamic <b>pruning</b> to amplify the 3D kernel size. Our method comprises two core components: Spatial-wise Dynamic Sparsity (SDS) and Channel-wise Weight Selection (CWS). SDS dynamically prunes and regrows volumetric weights from the beginning to learn a large sparse 3D kernel. It not only boosts performance but also significantly reduces model size and computational cost. Moreover, CWS selects the most important channels for 3D <b>convolution</b> during training and subsequently prunes the redundant channels to accelerate inference for 3D vision tasks. We demonstrate the effectiveness of LSK3DNet on three <b>benchmark</b> datasets and five tracks compared with classical models and large kernel designs. Notably, LSK3DNet achieves the state-of-the-art performance on SemanticKITTI (i.e., 75.6% on single-scan and 63.4% on multi-scan), with roughly 40% model size reduction and 60% computing operations reduction compared to the naive large 3D kernel model.</p></p class="citation"></blockquote><h3 id=3672--105259-fastcad-real-time-cad-retrieval-and-alignment-from-scans-and-videos-florian-langer-et-al-2024>(36/72 | 105/259) FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos (Florian Langer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Langer, Jihong Ju, Georgi Dikov, Gerhard Reitmayr, Mohsen Ghafoorian. (2024)<br><strong>FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos</strong><br><button class=copy-to-clipboard title="FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15161v1.pdf filename=2403.15161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digitising the 3D world into a clean, CAD model-based representation has important applications for augmented reality and robotics. Current state-of-the-art methods are computationally intensive as they individually encode each detected object and optimise CAD alignments in a second stage. In this work, we propose FastCAD, a real-time method that simultaneously retrieves and aligns CAD models for all objects in a given scene. In contrast to previous works, we directly predict alignment parameters and shape embeddings. We achieve high-quality shape retrievals by learning CAD embeddings in a <b>contrastive</b> <b>learning</b> framework and <b>distilling</b> those into FastCAD. Our single-stage method accelerates the inference time by a factor of 50 compared to other methods operating on RGB-D scans while outperforming them on the challenging Scan2CAD alignment <b>benchmark.</b> Further, our approach collaborates seamlessly with online 3D reconstruction techniques. This enables the real-time generation of precise CAD model-based reconstructions from videos at 10 FPS. Doing so, we significantly improve the Scan2CAD alignment accuracy in the video setting from 43.0% to 48.2% and the reconstruction accuracy from 22.9% to 29.6%.</p></p class="citation"></blockquote><h3 id=3772--106259-avt2-dwf-improving-deepfake-detection-with-audio-visual-fusion-and-dynamic-weighting-strategies-rui-wang-et-al-2024>(37/72 | 106/259) AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies (Rui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng. (2024)<br><strong>AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies</strong><br><button class=copy-to-clipboard title="AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14974v1.pdf filename=2403.14974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the continuous improvements of deepfake methods, forgery messages have transitioned from single-modality to <b>multi-modal</b> fusion, posing new challenges for existing forgery detection algorithms. In this paper, we propose AVT2-DWF, the Audio-Visual dual <b>Transformers</b> grounded in Dynamic Weight Fusion, which aims to amplify both intra- and cross-modal forgery cues, thereby enhancing detection capabilities. AVT2-DWF adopts a dual-stage approach to capture both spatial characteristics and temporal dynamics of facial expressions. This is achieved through a face <b>transformer</b> with an n-frame-wise <b>tokenization</b> strategy encoder and an audio <b>transformer</b> encoder. Subsequently, it uses <b>multi-modal</b> conversion with dynamic weight fusion to address the challenge of heterogeneous information fusion between audio and visual modalities. Experiments on DeepfakeTIMIT, FakeAVCeleb, and DFDC datasets indicate that AVT2-DWF achieves state-of-the-art performance intra- and cross-dataset Deepfake detection. Code is available at <a href=https://github.com/raining-dev/AVT2-DWF>https://github.com/raining-dev/AVT2-DWF</a>.</p></p class="citation"></blockquote><h3 id=3872--107259-an-optimization-framework-to-enforce-multi-view-consistency-for-texturing-3d-meshes-using-pre-trained-text-to-image-models-zhengyi-zhao-et-al-2024>(38/72 | 107/259) An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models (Zhengyi Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengyi Zhao, Chen Song, Xiaodong Gu, Yuan Dong, Qi Zuo, Weihao Yuan, Zilong Dong, Liefeng Bo, Qixing Huang. (2024)<br><strong>An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models</strong><br><button class=copy-to-clipboard title="An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15559v1.pdf filename=2403.15559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A fundamental problem in the texturing of 3D meshes using pre-trained <b>text-to-image</b> models is to ensure multi-view consistency. State-of-the-art approaches typically use <b>diffusion</b> <b>models</b> to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent <b>diffusion</b> <b>process.</b> The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem to associate each mesh face with a selected view. In particular, the third and fourth stages are iterated, with the cuts obtained in the fourth stage encouraging non-rigid alignment in the third stage to focus on regions close to the cuts. Experimental results show that our approach significantly outperforms baseline approaches both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=3972--108259-wscloc-weakly-supervised-sparse-view-camera-relocalization-jialu-wang-et-al-2024>(39/72 | 108/259) WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization (Jialu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Wang, Kaichen Zhou, Andrew Markham, Niki Trigoni. (2024)<br><strong>WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization</strong><br><button class=copy-to-clipboard title="WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15272v1.pdf filename=2403.15272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the advancements in deep learning for camera relocalization tasks, obtaining ground truth pose labels required for the training process remains a costly endeavor. While current weakly <b>supervised</b> methods excel in lightweight label generation, their performance notably declines in scenarios with sparse views. In response to this challenge, we introduce WSCLoc, a system capable of being customized to various deep learning-based relocalization models to enhance their performance under <b>weakly-supervised</b> and sparse view conditions. This is realized with two stages. In the initial stage, WSCLoc employs a multilayer perceptron-based structure called WFT-NeRF to co-optimize image reconstruction quality and initial pose information. To ensure a stable learning process, we incorporate temporal information as input. Furthermore, instead of optimizing SE(3), we opt for $\mathfrak{sim}(3)$ optimization to explicitly enforce a scale constraint. In the second stage, we co-optimize the pre-trained WFT-NeRF and WFT-Pose. This optimization is enhanced by Time-Encoding based Random View Synthesis and <b>supervised</b> by inter-frame geometric constraints that consider pose, depth, and RGB information. We validate our approaches on two publicly available datasets, one outdoor and one indoor. Our experimental results demonstrate that our <b>weakly-supervised</b> relocalization solutions achieve superior pose estimation accuracy in sparse-view scenarios, comparable to state-of-the-art camera relocalization methods. We will make our code publicly available.</p></p class="citation"></blockquote><h3 id=4072--109259-spectral-motion-alignment-for-video-motion-transfer-using-diffusion-models-geon-yeong-park-et-al-2024>(40/72 | 109/259) Spectral Motion Alignment for Video Motion Transfer using Diffusion Models (Geon Yeong Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye. (2024)<br><strong>Spectral Motion Alignment for Video Motion Transfer using Diffusion Models</strong><br><button class=copy-to-clipboard title="Spectral Motion Alignment for Video Motion Transfer using Diffusion Models" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15249v1.pdf filename=2403.15249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of <b>diffusion</b> <b>models</b> has greatly impacted video generation and understanding. Particularly, text-to-video <b>diffusion</b> <b>models</b> (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately <b>distilling</b> motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA&rsquo;s efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.</p></p class="citation"></blockquote><h3 id=4172--110259-reasoning-enhanced-object-centric-learning-for-videos-jian-li-et-al-2024>(41/72 | 110/259) Reasoning-Enhanced Object-Centric Learning for Videos (Jian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Li, Pu Ren, Yang Liu, Hao Sun. (2024)<br><strong>Reasoning-Enhanced Object-Centric Learning for Videos</strong><br><button class=copy-to-clipboard title="Reasoning-Enhanced Object-Centric Learning for Videos" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15245v1.pdf filename=2403.15245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and <b>reasoning</b> abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective <b>reasoning</b> module. In the real world, <b>reasoning</b> and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel <b>reasoning</b> module called the Slot-based Time-Space <b>Transformer</b> with Memory buffer (STATM) to enhance the model&rsquo;s perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space <b>Transformer</b> makes predictions through slot-based spatiotemporal attention computations and fusion. Our experiment results on various datasets show that STATM can significantly enhance object-centric learning capabilities of slot-based video models.</p></p class="citation"></blockquote><h3 id=4272--111259-anytime-anywhere-anyone-investigating-the-feasibility-of-segment-anything-model-for-crowd-sourcing-medical-image-annotations-pranav-kulkarni-et-al-2024>(42/72 | 111/259) Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations (Pranav Kulkarni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina Chatterjee, Paul H. Yi, Vishwa S. Parekh. (2024)<br><strong>Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations</strong><br><button class=copy-to-clipboard title="Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15218v1.pdf filename=2403.15218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Curating annotations for medical image segmentation is a labor-intensive and time-consuming task that requires domain expertise, resulting in &ldquo;narrowly&rdquo; focused deep learning (DL) models with limited translational utility. Recently, <b>foundation</b> <b>models</b> like the Segment Anything Model (SAM) have revolutionized semantic segmentation with exceptional <b>zero-shot</b> generalizability across various domains, including medical imaging, and hold a lot of promise for streamlining the annotation process. However, SAM has yet to be evaluated in a crowd-sourced setting to curate annotations for training 3D DL segmentation models. In this work, we explore the potential of SAM for crowd-sourcing &ldquo;sparse&rdquo; annotations from non-experts to generate &ldquo;dense&rdquo; segmentation masks for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our results indicate that while SAM-generated annotations exhibit high mean Dice scores compared to ground-truth annotations, nnU-Net models trained on SAM-generated annotations perform significantly worse than nnU-Net models trained on ground-truth annotations ($p&lt;0.001$, all).</p></p class="citation"></blockquote><h3 id=4372--112259-syncs-synthetic-data-and-contrastive-self-supervised-training-for-central-sulcus-segmentation-vladyslav-zalevskyi-et-al-2024>(43/72 | 112/259) SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation (Vladyslav Zalevskyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladyslav Zalevskyi, Kristoffer Hougaard Madsen. (2024)<br><strong>SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation</strong><br><button class=copy-to-clipboard title="SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15121v1.pdf filename=2403.15121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bipolar disorder (BD) and schizophrenia (SZ) are severe mental disorders with profound societal impact. Identifying risk markers early is crucial for understanding disease progression and enabling preventive measures. The Danish High Risk and Resilience Study (VIA) focuses on understanding early disease processes, particularly in children with familial high risk (FHR). Understanding structural brain changes associated with these diseases during early stages is essential for effective interventions. The central sulcus (CS) is a prominent brain landmark related to brain regions involved in motor and sensory processing. Analyzing CS morphology can provide valuable insights into neurodevelopmental abnormalities in the FHR group. However, segmenting the central sulcus (CS) presents challenges due to its variability, especially in adolescents. This study introduces two novel approaches to improve CS segmentation: synthetic data generation to model CS variability and <b>self-supervised</b> <b>pre-training</b> with multi-task learning to adapt models to new cohorts. These methods aim to enhance segmentation performance across diverse populations, eliminating the need for extensive preprocessing.</p></p class="citation"></blockquote><h3 id=4472--113259-towards-a-comprehensive-efficient-and-promptable-anatomic-structure-segmentation-model-using-3d-whole-body-ct-scans-heng-guo-et-al-2024>(44/72 | 113/259) Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans (Heng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heng Guo, Jianfeng Zhang, Jiaxing Huang, Tony C. W. Mok, Dazhou Guo, Ke Yan, Le Lu, Dakai Jin, Minfeng Xu. (2024)<br><strong>Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans</strong><br><button class=copy-to-clipboard title="Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15063v1.pdf filename=2403.15063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segment anything model (SAM) demonstrates strong generalization ability on natural image segmentation. However, its direct adaption in medical image segmentation tasks shows significant performance drops with inferior accuracy and unstable results. It may also requires an excessive number of <b>prompt</b> <b>points</b> to obtain a reasonable accuracy. For segmenting 3D radiological CT or MRI scans, a 2D SAM model has to separately handle hundreds of 2D slices. Although quite a few studies explore adapting SAM into medical image volumes, the efficiency of 2D adaption methods is unsatisfactory and 3D adaptation methods only capable of segmenting specific organs/tumors. In this work, we propose a comprehensive and scalable 3D SAM model for whole-body CT segmentation, named CT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation model using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively, ensuring the model&rsquo;s accurate responses to higher-dimensional spatial <b>prompts</b> <b>is</b> crucial, and 3D patch-wise training is required due to GPU memory constraints. For this purpose, we propose two key technical developments: 1) a progressively and spatially aligned <b>prompt</b> <b>encoding</b> method to effectively encode click <b>prompts</b> <b>in</b> local 3D space; and 2) a cross-patch <b>prompt</b> <b>learning</b> scheme to capture more 3D spatial context, which is beneficial for reducing the editing workloads when interactively <b>prompting</b> <b>on</b> large organs. CT-SAM3D is trained and validated using a curated dataset of 1204 CT scans containing 107 whole-body anatomies, reporting significantly better quantitative performance against all previous SAM-derived models by a large margin with much fewer click <b>prompts.</b> <b>Our</b> model can handle segmenting unseen organ as well. Code, data, and our 3D interactive segmentation tool with quasi-real-time responses will be made publicly available.</p></p class="citation"></blockquote><h3 id=4572--114259-toward-tiny-and-high-quality-facial-makeup-with-data-amplify-learning-qiaoqiao-jin-et-al-2024>(45/72 | 114/259) Toward Tiny and High-quality Facial Makeup with Data Amplify Learning (Qiaoqiao Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiaoqiao Jin, Xuanhong Chen, Meiguang Jin, Ying Cheng, Rui Shi, Yucheng Zheng, Yupeng Zhu, Bingbing Ni. (2024)<br><strong>Toward Tiny and High-quality Facial Makeup with Data Amplify Learning</strong><br><button class=copy-to-clipboard title="Toward Tiny and High-quality Facial Makeup with Data Amplify Learning" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15033v1.pdf filename=2403.15033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial <b>prompts</b> (including face parsing, and landmark detection). These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices. To solve above problems, we propose a brand-new learning paradigm, termed &ldquo;Data Amplify Learning (DAL),&rdquo; alongside a compact makeup model named &ldquo;TinyBeauty.&rdquo; The core idea of DAL lies in employing a <b>Diffusion-based</b> <b>Data</b> Amplifier (DDA) to &ldquo;amplify&rdquo; limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations. Two pivotal innovations in DDA facilitate the above training approach: (1) A Residual <b>Diffusion</b> <b>Model</b> (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla <b>diffusion</b> <b>models;</b> (2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face <b>prompts.</b> Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13. Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs.</p></p class="citation"></blockquote><h3 id=4672--115259-multimodal-fusion-with-pre-trained-model-features-in-affective-behaviour-analysis-in-the-wild-zhuofan-wen-et-al-2024>(46/72 | 115/259) Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild (Zhuofan Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuofan Wen, Fengyu Zhang, Siyuan Zhang, Haiyang Sun, Mingyu Xu, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao. (2024)<br><strong>Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild</strong><br><button class=copy-to-clipboard title="Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Convolution, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15044v1.pdf filename=2403.15044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> fusion is a significant method for most <b>multimodal</b> tasks. With the recent surge in the number of large pre-trained models, combining both <b>multimodal</b> fusion methods and pre-trained model features can achieve outstanding performance in many <b>multimodal</b> tasks. In this paper, we present our approach, which leverages both advantages for addressing the task of Expression (Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the Aff-Wild2 database using pre-trained models, then extract the final hidden layers of the models as features. Following preprocessing and interpolation or <b>convolution</b> to align the extracted features, different models are employed for modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.</p></p class="citation"></blockquote><h3 id=4772--116259-latte3d-large-scale-amortized-text-to-enhanced3d-synthesis-kevin-xie-et-al-2024>(47/72 | 116/259) LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis (Kevin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng. (2024)<br><strong>LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis</strong><br><button class=copy-to-clipboard title="LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, I-2-6; I-2-7; I-3-6; I-3-7, cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15385v1.pdf filename=2403.15385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per <b>prompt.</b> Amortized methods like ATT3D optimize multiple <b>prompts</b> simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, they cannot capture high-frequency <b>geometry</b> and texture details and struggle to scale to large <b>prompt</b> sets, so they generalize poorly. We introduce LATTE3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger <b>prompt</b> set. Key to our method is 1) building a scalable architecture and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training <b>prompts.</b> LATTE3D amortizes both neural field and textured surface generation to produce highly detailed textured meshes in a single forward pass. LATTE3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization.</p></p class="citation"></blockquote><h3 id=4872--117259-tri-perspective-view-decomposition-for-geometry-aware-depth-completion-zhiqiang-yan-et-al-2024>(48/72 | 117/259) Tri-Perspective View Decomposition for Geometry-Aware Depth Completion (Zhiqiang Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Yan, Yuankai Lin, Kun Wang, Yupeng Zheng, Yufei Wang, Zhenyu Zhang, Jun Li, Jian Yang. (2024)<br><strong>Tri-Perspective View Decomposition for Geometry-Aware Depth Completion</strong><br><button class=copy-to-clipboard title="Tri-Perspective View Decomposition for Geometry-Aware Depth Completion" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15008v1.pdf filename=2403.15008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth completion is a vital task for autonomous driving, as it involves reconstructing the precise 3D <b>geometry</b> of a scene from sparse and noisy depth measurements. However, most existing methods either rely only on 2D depth representations or directly incorporate raw 3D point clouds for compensation, which are still insufficient to capture the fine-grained 3D <b>geometry</b> of the scene. To address this challenge, we introduce Tri-Perspective view Decomposition (TPVD), a novel framework that can explicitly model 3D <b>geometry.</b> In particular, (1) TPVD ingeniously decomposes the original point cloud into three 2D views, one of which corresponds to the sparse depth input. (2) We design TPV Fusion to update the 2D TPV features through recurrent 2D-3D-2D aggregation, where a Distance-Aware Spherical <b>Convolution</b> (DASC) is applied. (3) By adaptively choosing TPV affinitive neighbors, the newly proposed Geometric Spatial Propagation Network (GSPN) further improves the geometric consistency. As a result, our TPVD outperforms existing methods on KITTI, NYUv2, and SUN RGBD. Furthermore, we build a novel depth completion dataset named TOFDC, which is acquired by the time-of-flight (TOF) sensor and the color camera on smartphones. Project page: <a href=https://yanzq95.github.io/projectpage/TOFDC/index.html>https://yanzq95.github.io/projectpage/TOFDC/index.html</a></p></p class="citation"></blockquote><h3 id=4972--118259-efficiently-assemble-normalization-layers-and-regularization-for-federated-domain-generalization-khiem-le-et-al-2024>(49/72 | 118/259) Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization (Khiem Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khiem Le, Long Ho, Cuong Do, Danh Le-Phuoc, Kok-Seng Wong. (2024)<br><strong>Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization</strong><br><button class=copy-to-clipboard title="Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15605v1.pdf filename=2403.15605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. <b>Federated</b> <b>Domain</b> Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the <b>Federated</b> <b>Learning</b> paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while retaining discrimination of those features. Then, we incorporate a simple yet effective regularizer to guide these models in directly capturing domain-invariant representations that the global model&rsquo;s classifier can leverage. Extensive experimental results on two <b>benchmark</b> datasets, i.e., PACS and Office-Home, and a real-world medical dataset, Camelyon17, indicate that our proposed method outperforms other existing methods in addressing this particular problem.</p></p class="citation"></blockquote><h3 id=5072--119259-diffusionmtl-learning-multi-task-denoising-diffusion-model-from-partially-annotated-data-hanrong-ye-et-al-2024>(50/72 | 119/259) DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data (Hanrong Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanrong Ye, Dan Xu. (2024)<br><strong>DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data</strong><br><button class=copy-to-clipboard title="DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15389v1.pdf filename=2403.15389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been an increased interest in the practical problem of learning multiple dense scene understanding tasks from partially annotated data, where each training sample is only labeled for a subset of the tasks. The missing of task labels in training leads to low-quality and noisy predictions, as can be observed from state-of-the-art methods. To tackle this issue, we reformulate the partially-labeled multi-task dense prediction as a pixel-level denoising problem, and propose a novel multi-task denoising <b>diffusion</b> <b>framework</b> coined as DiffusionMTL. It designs a joint <b>diffusion</b> <b>and</b> denoising paradigm to model a potential noisy distribution in the task prediction or feature maps and generate rectified outputs for different tasks. To exploit multi-task consistency in denoising, we further introduce a Multi-Task Conditioning strategy, which can implicitly utilize the complementary nature of the tasks to help learn the unlabeled tasks, leading to an improvement in the denoising performance of the different tasks. Extensive quantitative and qualitative experiments demonstrate that the proposed multi-task denoising <b>diffusion</b> <b>model</b> can significantly improve multi-task prediction maps, and outperform the state-of-the-art methods on three challenging multi-task <b>benchmarks,</b> under two different partial-labeling evaluation settings. The code is available at <a href=https://prismformore.github.io/diffusionmtl/>https://prismformore.github.io/diffusionmtl/</a>.</p></p class="citation"></blockquote><h3 id=5172--120259-cr3dt-camera-radar-fusion-for-3d-detection-and-tracking-nicolas-baumann-et-al-2024>(51/72 | 120/259) CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking (Nicolas Baumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas Kühne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno. (2024)<br><strong>CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking</strong><br><button class=copy-to-clipboard title="CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15313v1.pdf filename=2403.15313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate detection and tracking of surrounding <b>objects</b> <b>is</b> essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the <b>benchmark</b> for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D <b>object</b> <b>detection,</b> and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications.</p></p class="citation"></blockquote><h3 id=5272--121259-semantic-gaussians-open-vocabulary-scene-understanding-with-3d-gaussian-splatting-jun-guo-et-al-2024>(52/72 | 121/259) Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting (Jun Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Guo, Xiaojian Ma, Yue Fan, Huaping Liu, Qing Li. (2024)<br><strong>Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15624v1.pdf filename=2403.15624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-vocabulary 3D scene understanding presents a significant challenge in computer vision, withwide-ranging applications in embodied agents and augmented reality systems. Previous approaches haveadopted Neural Radiance Fields (NeRFs) to analyze 3D scenes. In this paper, we introduce SemanticGaussians, a novel open-vocabulary scene understanding approach based on 3D Gaussian Splatting. Our keyidea is <b>distilling</b> pre-trained 2D semantics into 3D Gaussians. We design a versatile projection approachthat maps various 2Dsemantic features from pre-trained image encoders into a novel semantic component of 3D Gaussians, withoutthe additional training required by NeRFs. We further build a 3D semantic network that directly predictsthe semantic component from raw 3D Gaussians for fast inference. We explore several applications ofSemantic Gaussians: semantic segmentation on ScanNet-20, where our approach attains a 4.2% mIoU and 4.0%mAcc improvement over prior open-vocabulary scene understanding counterparts; object part segmentation,sceneediting, and spatial-temporal segmentation with better qualitative results over 2D and 3D baselines,highlighting its versatility and effectiveness on supporting diverse downstream tasks.</p></p class="citation"></blockquote><h3 id=5372--122259-interfusion-text-driven-generation-of-3d-human-object-interaction-sisi-dai-et-al-2024>(53/72 | 122/259) InterFusion: Text-Driven Generation of 3D Human-Object Interaction (Sisi Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sisi Dai, Wenhao Li, Haowen Sun, Haibin Huang, Chongyang Ma, Hui Huang, Kai Xu, Ruizhen Hu. (2024)<br><strong>InterFusion: Text-Driven Generation of 3D Human-Object Interaction</strong><br><button class=copy-to-clipboard title="InterFusion: Text-Driven Generation of 3D Human-Object Interaction" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15612v1.pdf filename=2403.15612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we tackle the complex task of generating 3D human-object interactions (HOI) from textual descriptions in a <b>zero-shot</b> text-to-3D manner. We identify and address two key challenges: the unsatisfactory outcomes of direct text-to-3D methods in HOI, largely due to the lack of paired text-interaction data, and the inherent difficulties in simultaneously generating multiple concepts with complex spatial relationships. To effectively address these issues, we present InterFusion, a two-stage framework specifically designed for HOI generation. InterFusion involves human pose estimations derived from text as geometric priors, which simplifies the text-to-3D conversion process and introduces additional constraints for accurate object generation. At the first stage, InterFusion extracts 3D human poses from a synthesized image dataset depicting a wide range of interactions, subsequently mapping these poses to interaction descriptions. The second stage of InterFusion capitalizes on the latest developments in text-to-3D generation, enabling the production of realistic and high-quality 3D HOI scenes. This is achieved through a local-global optimization process, where the generation of human body and object is optimized separately, and jointly refined with a global optimization of the entire scene, ensuring a seamless and contextually coherent integration. Our experimental results affirm that InterFusion significantly outperforms existing state-of-the-art methods in 3D HOI generation.</p></p class="citation"></blockquote><h3 id=5472--123259-pixel-gs-density-control-with-pixel-aware-gradient-for-3d-gaussian-splatting-zheng-zhang-et-al-2024>(54/72 | 123/259) Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting (Zheng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Zhang, Wenbo Hu, Yixing Lao, Tong He, Hengshuang Zhao. (2024)<br><strong>Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting</strong><br><button class=copy-to-clipboard title="Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15530v1.pdf filename=2403.15530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis results while advancing real-time rendering performance. However, it relies heavily on the quality of the initial point cloud, resulting in blurring and needle-like artifacts in areas with insufficient initializing points. This is mainly attributed to the point cloud growth condition in 3DGS that only considers the average gradient magnitude of points from observable views, thereby failing to grow for large Gaussians that are observable for many viewpoints while many of them are only covered in the boundaries. To this end, we propose a novel method, named Pixel-GS, to take into account the number of pixels covered by the Gaussian in each view during the computation of the growth condition. We regard the covered pixel numbers as the weights to dynamically average the gradients from different views, such that the growth of large Gaussians can be <b>prompted.</b> As a result, points within the areas with insufficient initializing points can be grown more effectively, leading to a more accurate and detailed reconstruction. In addition, we propose a simple yet effective strategy to scale the gradient field according to the distance to the camera, to suppress the growth of floaters near the camera. Extensive experiments both qualitatively and quantitatively demonstrate that our method achieves state-of-the-art rendering quality while maintaining real-time rendering speed, on the challenging Mip-NeRF 360 and Tanks & Temples datasets.</p></p class="citation"></blockquote><h3 id=5572--124259-themestation-generating-theme-aware-3d-assets-from-few-exemplars-zhenwei-wang-et-al-2024>(55/72 | 124/259) ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars (Zhenwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W. H. Lau. (2024)<br><strong>ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars</strong><br><button class=copy-to-clipboard title="ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15383v1.pdf filename=2403.15383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world applications often require a large gallery of 3D assets that share a consistent theme. While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem. In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations. To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage. We propose a novel dual score <b>distillation</b> (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image. Extensive experiments and user studies confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality. ThemeStation also enables various applications such as controllable 3D-to-3D generation.</p></p class="citation"></blockquote><h3 id=5672--125259-dragapart-learning-a-part-level-motion-prior-for-articulated-objects-ruining-li-et-al-2024>(56/72 | 125/259) DragAPart: Learning a Part-Level Motion Prior for Articulated Objects (Ruining Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi. (2024)<br><strong>DragAPart: Learning a Part-Level Motion Prior for Articulated Objects</strong><br><button class=copy-to-clipboard title="DragAPart: Learning a Part-Level Motion Prior for Articulated Objects" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15382v1.pdf filename=2403.15382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce DragAPart, a method that, given an image and a set of drags as input, can generate a new image of the same object in a new state, compatible with the action of the drags. Differently from prior works that focused on repositioning objects, DragAPart predicts part-level interactions, such as opening and closing a drawer. We study this problem as a proxy for learning a generalist motion model, not restricted to a specific kinematic structure or object category. To this end, we start from a pre-trained image generator and <b>fine-tune</b> it on a new synthetic dataset, Drag-a-Move, which we introduce. Combined with a new encoding for the drags and dataset randomization, the new model generalizes well to real images and different categories. Compared to prior motion-controlled generators, we demonstrate much better part-level motion understanding.</p></p class="citation"></blockquote><h3 id=5772--126259-lego-leveraging-a-surface-deformation-network-for-animatable-stylized-face-generation-with-one-example-soyeon-yoon-et-al-2024>(57/72 | 126/259) LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example (Soyeon Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soyeon Yoon, Kwan Yun, Kwanggyoon Seo, Sihun Cha, Jung Eun Yoo, Junyong Noh. (2024)<br><strong>LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</strong><br><button class=copy-to-clipboard title="LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, I-4-9, cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15227v1.pdf filename=2403.15227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in 3D face stylization have made significant strides in few to <b>zero-shot</b> settings. However, the degree of stylization achieved by existing methods is often not sufficient for practical applications because they are mostly based on statistical 3D Morphable Models (3DMM) with limited variations. To this end, we propose a method that can produce a highly stylized 3D face model with desired topology. Our methods train a surface deformation network with 3DMM and translate its domain to the target style using a paired exemplar. The network achieves stylization of the 3D face mesh by mimicking the style of the target using a differentiable renderer and directional CLIP losses. Additionally, during the inference process, we utilize a Mesh Agnostic Encoder (MAGE) that takes deformation target, a mesh of diverse topologies as input to the stylization process and encodes its shape into our latent space. The resulting stylized face model can be animated by commonly used 3DMM blend shapes. A set of quantitative and qualitative evaluations demonstrate that our method can produce highly stylized face meshes according to a given style and output them in a desired topology. We also demonstrate example applications of our method including image-based stylized avatar generation, linear interpolation of geometric styles, and facial animation of stylized avatars.</p></p class="citation"></blockquote><h3 id=5872--127259-your-image-is-my-video-reshaping-the-receptive-field-via-image-to-video-differentiable-autoaugmentation-and-fusion-sofia-casarin-et-al-2024>(58/72 | 127/259) Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion (Sofia Casarin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sofia Casarin, Cynthia I. Ugwu, Sergio Escalera, Oswald Lanz. (2024)<br><strong>Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion</strong><br><button class=copy-to-clipboard title="Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15194v1.pdf filename=2403.15194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The landscape of deep learning research is moving towards innovative strategies to harness the true potential of <b>data.</b> <b>Traditionally,</b> emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, <b>data</b> <b>quality</b> (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available <b>data</b> <b>through</b> the use of automatic <b>data</b> <b>augmentation</b> for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging-in our DAS over different light-weight video backbones.</p></p class="citation"></blockquote><h3 id=5972--128259-sfod-spiking-fusion-object-detector-yimeng-fan-et-al-2024>(59/72 | 128/259) SFOD: Spiking Fusion Object Detector (Yimeng Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu. (2024)<br><strong>SFOD: Spiking Fusion Object Detector</strong><br><button class=copy-to-clipboard title="SFOD: Spiking Fusion Object Detector" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15192v1.pdf filename=2403.15192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras, characterized by high temporal resolution, high dynamic range, low power consumption, and high pixel bandwidth, offer unique capabilities for <b>object</b> <b>detection</b> in specialized contexts. Despite these advantages, the inherent sparsity and asynchrony of event data pose challenges to existing <b>object</b> <b>detection</b> algorithms. Spiking Neural Networks (SNNs), inspired by the way the human brain codes and processes information, offer a potential solution to these difficulties. However, their performance in <b>object</b> <b>detection</b> using event cameras is limited in current implementations. In this paper, we propose the Spiking Fusion <b>Object</b> <b>Detector</b> (SFOD), a simple and efficient approach to SNN-based <b>object</b> <b>detection.</b> Specifically, we design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally, through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset, we delve deeply into the impact of spiking decoding strategies and loss functions on model performance. Thereby, we establish state-of-the-art classification results based on SNNs, achieving 93.7% accuracy on the NCAR dataset. Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1%, outperforming existing SNN-based approaches. Our research not only underscores the potential of SNNs in <b>object</b> <b>detection</b> with event cameras but also propels the advancement of SNNs. Code is available at <a href=https://github.com/yimeng-fan/SFOD>https://github.com/yimeng-fan/SFOD</a>.</p></p class="citation"></blockquote><h3 id=6072--129259-modular-deep-active-learning-framework-for-image-annotation-a-technical-report-for-the-ophthalmo-ai-project-md-abdul-kadir-et-al-2024>(60/72 | 129/259) Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project (Md Abdul Kadir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abdul Kadir, Hasan Md Tusfiqur Alam, Pascale Maul, Hans-Jürgen Profitlich, Moritz Wolf, Daniel Sonntag. (2024)<br><strong>Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project</strong><br><button class=copy-to-clipboard title="Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15143v1.pdf filename=2403.15143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating <b>Active</b> <b>Learning</b> (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annotation tool that supports the classification and segmentation of medical images. The user-friendly interface allows for easy alteration of the AL and DL model settings through a configuration file, requiring no prior programming experience. While MedDeepCyleAL can be applied to any kind of image data, we have specifically applied it to ophthalmology data in this project.</p></p class="citation"></blockquote><h3 id=6172--130259-gradient-based-sampling-for-class-imbalanced-semi-supervised-object-detection-jiaming-li-et-al-2024>(61/72 | 130/259) Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection (Jiaming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Yingying Li, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li. (2024)<br><strong>Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection</strong><br><button class=copy-to-clipboard title="Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15127v1.pdf filename=2403.15127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current semi-supervised <b>object</b> <b>detection</b> (SSOD) algorithms typically assume class balanced datasets (PASCAL VOC etc.) or slightly class imbalanced datasets (MS-COCO, etc). This assumption can be easily violated since real world datasets can be extremely class imbalanced in nature, thus making the performance of semi-supervised <b>object</b> <b>detectors</b> far from satisfactory. Besides, the research for this problem in SSOD is severely under-explored. To bridge this research gap, we comprehensively study the class imbalance problem for SSOD under more challenging scenarios, thus forming the first experimental setting for class imbalanced SSOD (CI-SSOD). Moreover, we propose a simple yet effective gradient-based sampling framework that tackles the class imbalance problem from the perspective of two types of confirmation biases. To tackle confirmation bias towards majority classes, the gradient-based reweighting and gradient-based thresholding modules leverage the gradients from each class to fully balance the influence of the majority and minority classes. To tackle the confirmation bias from incorrect pseudo labels of minority classes, the class-rebalancing sampling module resamples unlabeled data following the guidance of the gradient-based reweighting module. Experiments on three proposed sub-tasks, namely MS-COCO, MS-COCO to Object365 and LVIS, suggest that our method outperforms current class imbalanced <b>object</b> <b>detectors</b> by clear margins, serving as a baseline for future research in CI-SSOD. Code will be available at <a href=https://github.com/nightkeepers/CI-SSOD>https://github.com/nightkeepers/CI-SSOD</a>.</p></p class="citation"></blockquote><h3 id=6272--131259-ifsenet--harnessing-sparse-iterations-for-interactive-few-shot-segmentation-excellence-shreyas-chandgothia-et-al-2024>(62/72 | 131/259) IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence (Shreyas Chandgothia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shreyas Chandgothia, Ardhendu Sekhar, Amit Sethi. (2024)<br><strong>IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence</strong><br><button class=copy-to-clipboard title="IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15089v1.pdf filename=2403.15089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a computer vision system to segment a novel class typically requires collecting and painstakingly annotating lots of images with objects from that class. <b>Few-shot</b> segmentation techniques reduce the required number of images to learn to segment a new class, but careful annotations of object boundaries are still required. On the other hand, interactive segmentation techniques only focus on incrementally improving the segmentation of one object at a time (typically, using clicks given by an expert) in a class-agnostic manner. We combine the two concepts to drastically reduce the effort required to train segmentation models for novel classes. Instead of trivially feeding interactive segmentation masks as ground truth to a <b>few-shot</b> segmentation model, we propose IFSENet, which can accept sparse supervision on a single or few support images in the form of clicks to generate masks on support (training, at least clicked upon once) as well as query (test, never clicked upon) images. To trade-off effort for accuracy flexibly, the number of images and clicks can be incrementally added to the support set to further improve the segmentation of support as well as query images. The proposed model approaches the accuracy of previous state-of-the-art <b>few-shot</b> segmentation models with considerably lower annotation effort (clicks instead of maps), when tested on Pascal and SBD datasets on query images. It also works well as an interactive segmentation method on support images.</p></p class="citation"></blockquote><h3 id=6372--132259-an-integrated-neighborhood-and-scale-information-network-for-open-pit-mine-change-detection-in-high-resolution-remote-sensing-images-zilin-xie-et-al-2024>(63/72 | 132/259) An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images (Zilin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilin Xie, Kangning Li, Jinbao Jiang, Jinzhong Yang, Xiaojun Qiao, Deshuai Yuan, Cheng Nie. (2024)<br><strong>An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images</strong><br><button class=copy-to-clipboard title="An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15032v1.pdf filename=2403.15032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection. Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques. However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance. Therefore, by exploring the influence patterns of neighborhood and scale information, this paper proposes an Integrated Neighborhood and Scale Information Network (INSINet) for open-pit mine CD in HR remote sensing images. Specifically, INSINet introduces 8-neighborhood-image information to acquire a larger receptive field, improving the recognition of center image boundary regions. Drawing on techniques of skip connection, deep supervision, and attention mechanism, the multi-path deep <b>supervised</b> attention (MDSA) module is designed to enhance multi-scale information fusion and change feature extraction. Experimental analysis reveals that incorporating neighborhood and scale information enhances the F1 score of INSINet by 6.40%, with improvements of 3.08% and 3.32% respectively. INSINet outperforms existing methods with an Overall Accuracy of 97.69%, Intersection over Union of 71.26%, and F1 score of 83.22%. INSINet shows significance for open-pit mine CD in HR remote sensing images.</p></p class="citation"></blockquote><h3 id=6472--133259-vrso-visual-centric-reconstruction-for-static-object-annotation-chenyao-yu-et-al-2024>(64/72 | 133/259) VRSO: Visual-Centric Reconstruction for Static Object Annotation (Chenyao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyao Yu, Yingfeng Cai, Jiaxin Zhang, Hui Kong, Wei Sui, Cong Yang. (2024)<br><strong>VRSO: Visual-Centric Reconstruction for Static Object Annotation</strong><br><button class=copy-to-clipboard title="VRSO: Visual-Centric Reconstruction for Static Object Annotation" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15026v1.pdf filename=2403.15026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a part of the perception results of intelligent driving systems, static <b>object</b> <b>detection</b> (SOD) in 3D space provides crucial cues for driving environment understanding. With the rapid deployment of deep neural networks for SOD tasks, the demand for high-quality training samples soars. The traditional, also reliable, way is manual labeling over the dense LiDAR point clouds and reference images. Though most public driving datasets adopt this strategy to provide SOD ground truth (GT), it is still expensive (requires LiDAR scanners) and low-efficient (time-consuming and unscalable) in practice. This paper introduces VRSO, a visual-centric approach for static <b>object</b> <b>annotation.</b> VRSO is distinguished in low cost, high efficiency, and high quality: (1) It recovers static <b>objects</b> <b>in</b> 3D space with only camera images as input, and (2) manual labeling is barely involved since GT for SOD tasks is generated based on an automatic reconstruction and annotation pipeline. (3) Experiments on the Waymo Open Dataset show that the mean reprojection error from VRSO annotation is only 2.6 pixels, around four times lower than the Waymo labeling (10.6 pixels). Source code is available at: <a href=https://github.com/CaiYingFeng/VRSO>https://github.com/CaiYingFeng/VRSO</a>.</p></p class="citation"></blockquote><h3 id=6572--134259-cell-tracking-according-to-biological-needs----strong-mitosis-aware-random-finite-sets-tracker-with-aleatoric-uncertainty-timo-kaiser-et-al-2024>(65/72 | 134/259) Cell Tracking according to Biological Needs &ndash; Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty (Timo Kaiser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timo Kaiser, Maximilian Schier, Bodo Rosenhahn. (2024)<br><strong>Cell Tracking according to Biological Needs &ndash; Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty</strong><br><button class=copy-to-clipboard title="Cell Tracking according to Biological Needs -- Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15011v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15011v2.pdf filename=2403.15011v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cell tracking and segmentation assist biologists in extracting insights from large-scale microscopy time-lapse data. Driven by local accuracy metrics, current tracking approaches often suffer from a lack of long-term consistency. To address this issue, we introduce an uncertainty estimation technique for neural tracking-by-regression frameworks and incorporate it into our novel extended Poisson multi-Bernoulli mixture tracker. Our uncertainty estimation identifies uncertain associations within high-performing tracking-by-regression methods using problem-specific test-time augmentations. Leveraging this uncertainty, along with a novel mitosis-aware assignment problem formulation, our tracker resolves false associations and mitosis detections <b>stemming</b> from long-term conflicts. We evaluate our approach on nine competitive datasets and demonstrate that it outperforms the current state-of-the-art on biologically relevant metrics substantially, achieving improvements by a factor of approximately $5.75$. Furthermore, we uncover new insights into the behavior of tracking-by-regression uncertainty.</p></p class="citation"></blockquote><h3 id=6672--135259-clean-image-backdoor-attacks-dazhong-rong-et-al-2024>(66/72 | 135/259) Clean-image Backdoor Attacks (Dazhong Rong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang. (2024)<br><strong>Clean-image Backdoor Attacks</strong><br><button class=copy-to-clipboard title="Clean-image Backdoor Attacks" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15010v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15010v2.pdf filename=2403.15010v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To gather a significant quantity of annotated training data for high-performance image classification models, numerous companies opt to enlist third-party providers to label their unlabeled data. This practice is widely regarded as secure, even in cases where some annotated errors occur, as the impact of these minor inaccuracies on the final performance of the models is negligible and existing backdoor attacks require attacker&rsquo;s ability to poison the training images. Nevertheless, in this paper, we propose clean-image backdoor attacks which uncover that backdoors can still be injected via a fraction of incorrect labels without modifying the training images. Specifically, in our attacks, the attacker first seeks a trigger feature to divide the training images into two parts: those with the feature and those without it. Subsequently, the attacker falsifies the labels of the former part to a backdoor class. The backdoor will be finally implanted into the target model after it is trained on the poisoned data. During the inference phase, the attacker can activate the backdoor in two ways: slightly modifying the input image to obtain the trigger feature, or taking an image that naturally has the trigger feature as input. We conduct extensive experiments to demonstrate the effectiveness and practicality of our attacks. According to the experimental results, we conclude that our attacks seriously jeopardize the <b>fairness</b> and robustness of image classification models, and it is necessary to be vigilant about the incorrect labels in outsourced labeling.</p></p class="citation"></blockquote><h3 id=6772--136259-generative-active-learning-for-image-synthesis-personalization-xulu-zhang-et-al-2024>(67/72 | 136/259) Generative Active Learning for Image Synthesis Personalization (Xulu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xulu Zhang, Wengyu Zhang, Xiao-Yong Wei, Jinlin Wu, Zhaoxiang Zhang, Zhen Lei, Qing Li. (2024)<br><strong>Generative Active Learning for Image Synthesis Personalization</strong><br><button class=copy-to-clipboard title="Generative Active Learning for Image Synthesis Personalization" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14987v1.pdf filename=2403.14987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a pilot study that explores the application of <b>active</b> <b>learning,</b> traditionally studied in the context of discriminative models, to generative models. We specifically focus on image synthesis personalization tasks. The primary challenge in conducting <b>active</b> <b>learning</b> on generative models lies in the open-ended nature of querying, which differs from the closed form of querying in discriminative models that typically target a single concept. We introduce the concept of anchor directions to transform the querying process into a semi-open problem. We propose a direction-based uncertainty sampling strategy to enable generative <b>active</b> <b>learning</b> and tackle the exploitation-exploration dilemma. Extensive experiments are conducted to validate the effectiveness of our approach, demonstrating that an open-source model can achieve superior performance compared to closed-source models developed by large companies, such as Google&rsquo;s StyleDrop. The source code is available at <a href=https://github.com/zhangxulu1996/GAL4Personalization>https://github.com/zhangxulu1996/GAL4Personalization</a>.</p></p class="citation"></blockquote><h3 id=6872--137259-a-multimodal-approach-for-cross-domain-image-retrieval-lucas-iijima-et-al-2024>(68/72 | 137/259) A Multimodal Approach for Cross-Domain Image Retrieval (Lucas Iijima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Iijima, Tania Stathaki. (2024)<br><strong>A Multimodal Approach for Cross-Domain Image Retrieval</strong><br><button class=copy-to-clipboard title="A Multimodal Approach for Cross-Domain Image Retrieval" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15152v1.pdf filename=2403.15152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image generators are gaining vast amount of popularity and have rapidly changed how digital content is created. With the latest AI technology, millions of high quality images are being generated by the public, which are constantly motivating the research community to push the limits of generative models to create more complex and realistic images. This paper focuses on Cross-Domain Image Retrieval (CDIR) which can be used as an additional tool to inspect collections of generated images by determining the level of similarity between images in a dataset. An ideal retrieval system would be able to generalize to unseen complex images from multiple domains (e.g., photos, drawings and paintings). To address this goal, we propose a novel caption-matching approach that leverages <b>multimodal</b> language-vision architectures pre-trained on large datasets. The method is tested on DomainNet and Office-Home datasets and consistently achieves state-of-the-art performance over the latest approaches in the literature for cross-domain image retrieval. In order to verify the effectiveness with AI-generated images, the method was also put to test with a database composed by samples collected from Midjourney, which is a widely used generative platform for content creation.</p></p class="citation"></blockquote><h3 id=6972--138259-gani-global-and-near-field-illumination-aware-neural-inverse-rendering-jiaye-wu-et-al-2024>(69/72 | 138/259) GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering (Jiaye Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaye Wu, Saeed Hadadan, Geng Lin, Matthias Zwicker, David Jacobs, Roni Sengupta. (2024)<br><strong>GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering</strong><br><button class=copy-to-clipboard title="GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15651v1.pdf filename=2403.15651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present GaNI, a Global and Near-field Illumination-aware neural inverse rendering technique that can reconstruct <b>geometry,</b> albedo, and roughness parameters from images of a scene captured with co-located light and camera. Existing inverse rendering techniques with co-located light-camera focus on single objects only, without modeling global illumination and near-field lighting more prominent in scenes with multiple objects. We introduce a system that solves this problem in two stages; we first reconstruct the <b>geometry</b> powered by neural volumetric rendering NeuS, followed by inverse neural radiosity that uses the previously predicted <b>geometry</b> to estimate albedo and roughness. However, such a naive combination fails and we propose multiple technical contributions that enable this two-stage approach. We observe that NeuS fails to handle near-field illumination and strong specular reflections from the flashlight in a scene. We propose to implicitly model the effects of near-field illumination and introduce a surface angle loss function to handle specular reflections. Similarly, we observe that invNeRad assumes constant illumination throughout the capture and cannot handle moving flashlights during capture. We propose a light position-aware radiance cache network and additional smoothness priors on roughness to reconstruct reflectance. Experimental evaluation on synthetic and real data shows that our method outperforms the existing co-located light-camera-based inverse rendering techniques. Our approach produces significantly better reflectance and slightly better <b>geometry</b> than capture strategies that do not require a dark room.</p></p class="citation"></blockquote><h3 id=7072--139259-recent-trends-in-3d-reconstruction-of-general-non-rigid-scenes-raza-yunus-et-al-2024>(70/72 | 139/259) Recent Trends in 3D Reconstruction of General Non-Rigid Scenes (Raza Yunus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raza Yunus, Jan Eric Lenssen, Michael Niemeyer, Yiyi Liao, Christian Rupprecht, Christian Theobalt, Gerard Pons-Moll, Jia-Bin Huang, Vladislav Golyanik, Eddy Ilg. (2024)<br><strong>Recent Trends in 3D Reconstruction of General Non-Rigid Scenes</strong><br><button class=copy-to-clipboard title="Recent Trends in 3D Reconstruction of General Non-Rigid Scenes" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15064v1.pdf filename=2403.15064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstructing models of the real world, including 3D <b>geometry,</b> appearance, and motion of real scenes, is essential for computer graphics and computer vision. It enables the synthesizing of photorealistic novel views, useful for the movie industry and AR/VR applications. It also facilitates the content creation necessary in computer games and AR/VR by avoiding laborious manual design processes. Further, such models are fundamental for intelligent computing systems that need to interpret real-world scenes and actions to act and interact safely with the human world. Notably, the world surrounding us is dynamic, and reconstructing models of dynamic, non-rigidly moving scenes is a severely underconstrained and challenging problem. This state-of-the-art report (STAR) offers the reader a comprehensive summary of state-of-the-art techniques with monocular and multi-view inputs such as data from RGB and RGB-D sensors, among others, conveying an understanding of different approaches, their potential applications, and promising further research directions. The report covers 3D reconstruction of general non-rigid scenes and further addresses the techniques for scene decomposition, editing and controlling, and generalizable and generative modeling. More specifically, we first review the common and fundamental concepts necessary to understand and navigate the field and then discuss the state-of-the-art techniques by reviewing recent approaches that use traditional and machine-learning-based neural representations, including a discussion on the newly enabled applications. The STAR is concluded with a discussion of the remaining limitations and open challenges.</p></p class="citation"></blockquote><h3 id=7172--140259-survey-on-modeling-of-articulated-objects-jiayi-liu-et-al-2024>(71/72 | 140/259) Survey on Modeling of Articulated Objects (Jiayi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Liu, Manolis Savva, Ali Mahdavi-Amiri. (2024)<br><strong>Survey on Modeling of Articulated Objects</strong><br><button class=copy-to-clipboard title="Survey on Modeling of Articulated Objects" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14937v1.pdf filename=2403.14937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D modeling of articulated objects is a research problem within computer vision, graphics, and robotics. Its objective is to understand the shape and motion of the articulated components, represent the <b>geometry</b> and mobility of object parts, and create realistic models that reflect articulated objects in the real world. This survey provides a comprehensive overview of the current state-of-the-art in 3D modeling of articulated objects, with a specific focus on the task of articulated part perception and articulated object creation (reconstruction and generation). We systematically review and discuss the relevant literature from two perspectives: <b>geometry</b> processing and articulation modeling. Through this survey, we highlight the substantial progress made in these areas, outline the ongoing challenges, and identify gaps for future research. Our survey aims to serve as a foundational reference for researchers and practitioners in computer vision and graphics, offering insights into the complexities of articulated object modeling.</p></p class="citation"></blockquote><h3 id=7272--141259-an-open-world-diverse-cross-spatial-temporal-benchmark-for-dynamic-wild-person-re-identification-lei-zhang-et-al-2024>(72/72 | 141/259) An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification (Lei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Zhang, Xiaowei Fu, Fuxiang Huang, Yi Yang, Xinbo Gao. (2024)<br><strong>An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification</strong><br><button class=copy-to-clipboard title="An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15119v1.pdf filename=2403.15119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Person re-identification (ReID) has made great strides thanks to the data-driven deep learning techniques. However, the existing <b>benchmark</b> datasets lack diversity, and models trained on these data cannot generalize well to dynamic wild scenarios. To meet the goal of improving the explicit generalization of ReID models, we develop a new Open-World, Diverse, Cross-Spatial-Temporal dataset named OWD with several distinct features. 1) Diverse collection scenes: multiple independent open-world and highly dynamic collecting scenes, including streets, intersections, shopping malls, etc. 2) Diverse lighting variations: long time spans from daytime to nighttime with abundant illumination changes. 3) Diverse person status: multiple camera networks in all seasons with normal/adverse weather conditions and diverse pedestrian appearances (e.g., clothes, personal belongings, poses, etc.). 4) Protected privacy: invisible faces for privacy critical applications. To improve the implicit generalization of ReID, we further propose a Latent Domain Expansion (LDE) method to develop the potential of source data, which decouples discriminative identity-relevant and trustworthy domain-relevant features and implicitly enforces domain-randomized identity feature space expansion with richer domain diversity to facilitate domain invariant representations. Our comprehensive evaluations with most <b>benchmark</b> datasets in the community are crucial for progress, although this work is far from the grand goal toward open-world and dynamic wild applications.</p></p class="citation"></blockquote><h2 id=eessiv-8>eess.IV (8)</h2><h3 id=18--142259-integrating-multiscale-topology-in-digital-pathology-with-pyramidal-graph-convolutional-networks-victor-ibañez-et-al-2024>(1/8 | 142/259) Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks (Victor Ibañez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Ibañez, Przemyslaw Szostak, Quincy Wong, Konstanty Korski, Samaneh Abbasi-Sureshjani, Alvaro Gomariz. (2024)<br><strong>Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks</strong><br><button class=copy-to-clipboard title="Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Message-Passing, Graph, Convolution, Convolutional Neural Network, Convolutional Neural Network, Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15068v1.pdf filename=2403.15068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>convolutional</b> <b>networks</b> <b>(GCNs)</b> have emerged as a powerful alternative to <b>multiple</b> <b>instance</b> <b>learning</b> with <b>convolutional</b> <b>neural</b> <b>networks</b> in digital pathology, offering superior handling of structural information across various spatial ranges - a crucial aspect of learning from gigapixel H&amp;E-stained whole slide images (WSI). However, <b>graph</b> <b>message-passing</b> <b>algorithms</b> often suffer from oversmoothing when aggregating a large neighborhood. Hence, effective modeling of multi-range interactions relies on the careful construction of the <b>graph.</b> <b>Our</b> <b>proposed</b> multi-scale <b>GCN</b> (MS-GCN) tackles this issue by leveraging information across <b>multiple</b> <b>magnification</b> <b>levels</b> in WSIs. MS-GCN enables the simultaneous modeling of long-range structural dependencies at lower magnifications and high-resolution cellular details at higher magnifications, akin to analysis pipelines usually conducted by pathologists. The architecture&rsquo;s unique configuration allows for the concurrent modeling of structural patterns at lower magnifications and detailed cellular features at higher ones, while also quantifying the contribution of each magnification level to the prediction. Through testing on different datasets, MS-GCN demonstrates superior performance over existing single-magnification <b>GCN</b> methods. The enhancement in performance and interpretability afforded by our method holds promise for advancing computational pathology models, especially in tasks requiring extensive spatial context.</p></p class="citation"></blockquote><h3 id=28--143259-weep-a-method-for-spatial-interpretation-of-weakly-supervised-cnn-models-in-computational-pathology-abhinav-sharma-et-al-2024>(2/8 | 143/259) WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology (Abhinav Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhinav Sharma, Bojing Liu, Mattias Rantalainen. (2024)<br><strong>WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology</strong><br><button class=copy-to-clipboard title="WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, stat-ME<br>Keyword Score: 60<br>Keywords: Convolutional Neural Network, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15238v1.pdf filename=2403.15238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning enables the modelling of high-resolution histopathology whole-slide images (WSI). <b>Weakly</b> <b>supervised</b> <b>learning</b> of tile-level data is typically applied for tasks where labels only exist on the patient or WSI level (e.g. patient outcomes or histological grading). In this context, there is a need for improved spatial interpretability of predictions from such models. We propose a novel method, Wsi rEgion sElection aPproach (WEEP), for model interpretation. It provides a principled yet straightforward way to establish the spatial area of WSI required for assigning a particular prediction label. We demonstrate WEEP on a binary classification task in the area of breast cancer computational pathology. WEEP is easy to implement, is directly connected to the model-based decision process, and offers information relevant to both research and diagnostic applications.</p></p class="citation"></blockquote><h3 id=38--144259-evaluating-gpt-4-with-vision-on-detection-of-radiological-findings-on-chest-radiographs-yiliang-zhou-et-al-2024>(3/8 | 144/259) Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs (Yiliang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiliang Zhou, Hanley Ong, Patrick Kennedy, Carol Wu, Jacob Kazam, Keith Hentel, Adam Flanders, George Shih, Yifan Peng. (2024)<br><strong>Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs</strong><br><button class=copy-to-clipboard title="Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 33<br>Keywords: Multi-modal, GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15528v1.pdf filename=2403.15528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study examines the application of <b>GPT-4V,</b> a <b>multi-modal</b> <b>large</b> <b>language</b> <b>model</b> equipped with visual recognition, in detecting radiological findings from a set of 100 chest radiographs and suggests that <b>GPT-4V</b> is currently not ready for real-world diagnostic usage in interpreting chest radiographs.</p></p class="citation"></blockquote><h3 id=48--145259-ultrasound-imaging-based-on-the-variance-of-a-diffusion-restoration-model-yuxin-zhang-et-al-2024>(4/8 | 145/259) Ultrasound Imaging based on the Variance of a Diffusion Restoration Model (Yuxin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus. (2024)<br><strong>Ultrasound Imaging based on the Variance of a Diffusion Restoration Model</strong><br><button class=copy-to-clipboard title="Ultrasound Imaging based on the Variance of a Diffusion Restoration Model" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15316v1.pdf filename=2403.15316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite today&rsquo;s prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts. Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising <b>Diffusion</b> <b>model.</b> More specifically, we rely on the <b>unsupervised</b> <b>fine-tuning</b> of a pre-trained Denoising <b>Diffusion</b> <b>Restoration</b> Model (DDRM). Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of <b>diffusion</b> <b>reconstruction</b> of ultrasound images, and shows the interest of its variance as an echogenicity map estimator. We conduct experiments on synthetic, in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging approach in achieving high-quality image reconstructions from single plane-wave acquisitions and in comparison to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=58--146259-a2dmn-anatomy-aware-dilated-multiscale-network-for-breast-ultrasound-semantic-segmentation-kyle-lucke-et-al-2024>(5/8 | 146/259) A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation (Kyle Lucke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle Lucke, Aleksandar Vakanski, Min Xian. (2024)<br><strong>A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation</strong><br><button class=copy-to-clipboard title="A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15560v1.pdf filename=2403.15560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>convolutional</b> <b>neural</b> <b>networks</b> for semantic segmentation of breast ultrasound (BUS) images have shown great success; however, two major challenges still exist. 1) Most current approaches inherently lack the ability to utilize tissue anatomy, resulting in misclassified image regions. 2) They struggle to produce accurate boundaries due to the repeated down-sampling operations. To address these issues, we propose a novel breast anatomy-aware network for capturing fine image details and a new smoothness term that encodes breast anatomy. It incorporates context information across multiple spatial scales to generate more accurate semantic boundaries. Extensive experiments are conducted to compare the proposed method and eight state-of-the-art approaches using a BUS dataset with 325 images. The results demonstrate the proposed method significantly improves the segmentation of the muscle, mammary, and tumor classes and produces more accurate fine details of tissue boundaries.</p></p class="citation"></blockquote><h3 id=68--147259-latent-neural-cellular-automata-for-resource-efficient-image-restoration-andrea-menta-et-al-2024>(6/8 | 147/259) Latent Neural Cellular Automata for Resource-Efficient Image Restoration (Andrea Menta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Menta, Alberto Archetti, Matteo Matteucci. (2024)<br><strong>Latent Neural Cellular Automata for Resource-Efficient Image Restoration</strong><br><button class=copy-to-clipboard title="Latent Neural Cellular Automata for Resource-Efficient Image Restoration" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, cs-NE, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15525v1.pdf filename=2403.15525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural cellular automata represent an evolution of the traditional cellular automata model, enhanced by the integration of a deep learning-based transition function. This shift from a manual to a data-driven approach significantly increases the adaptability of these models, enabling their application in diverse domains, including content generation and artificial life. However, their widespread application has been hampered by significant computational requirements. In this work, we introduce the Latent Neural Cellular Automata (LNCA) model, a novel architecture designed to address the resource limitations of neural cellular automata. Our approach shifts the computation from the conventional input space to a specially designed latent space, relying on a pre-trained <b>autoencoder.</b> We apply our model in the context of image restoration, which aims to reconstruct high-quality images from their degraded versions. This modification not only reduces the model&rsquo;s resource consumption but also maintains a flexible framework suitable for various applications. Our model achieves a significant reduction in computational requirements while maintaining high reconstruction fidelity. This increase in efficiency allows for inputs up to 16 times larger than current state-of-the-art neural cellular automata models, using the same resources.</p></p class="citation"></blockquote><h3 id=78--148259-improving-cross-domain-brain-tissue-segmentation-in-fetal-mri-with-synthetic-data-vladyslav-zalevskyi-et-al-2024>(7/8 | 148/259) Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data (Vladyslav Zalevskyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, Jordina Aviles Verddera, Jana Hutter, Hamza Kebiri, Meritxell Bach Cuadra. (2024)<br><strong>Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data</strong><br><button class=copy-to-clipboard title="Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15103v1.pdf filename=2403.15103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Segmentation of fetal brain tissue from magnetic resonance imaging (MRI) plays a crucial role in the study of in utero neurodevelopment. However, automated tools face substantial domain shift challenges as they must be robust to highly heterogeneous clinical data, often limited in numbers and lacking annotations. Indeed, high variability of the fetal brain morphology, MRI acquisition parameters, and superresolution reconstruction (SR) algorithms adversely affect the model&rsquo;s performance when evaluated <b>out-of-domain.</b> In this work, we introduce FetalSynthSeg, a domain randomization method to segment fetal brain MRI, inspired by SynthSeg. Our results show that models trained solely on synthetic data outperform models trained on real data in <b>out-ofdomain</b> settings, validated on a 120-subject cross-domain dataset. Furthermore, we extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and reconstructed with novel SR models, showcasing robustness across different magnetic field strengths and SR algorithms. Leveraging a generative synthetic approach, we tackle the domain shift problem in fetal brain MRI and offer compelling prospects for applications in fields with limited and highly heterogeneous data.</p></p class="citation"></blockquote><h3 id=88--149259-subjective-quality-assessment-of-compressed-tone-mapped-high-dynamic-range-videos-abhinau-k-venkataramanan-et-al-2024>(8/8 | 149/259) Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos (Abhinau K. Venkataramanan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhinau K. Venkataramanan, Alan C. Bovik. (2024)<br><strong>Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos</strong><br><button class=copy-to-clipboard title="Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15061v1.pdf filename=2403.15061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High Dynamic Range (HDR) videos are able to represent wider ranges of contrasts and colors than Standard Dynamic Range (SDR) videos, giving more vivid experiences. Due to this, HDR videos are expected to grow into the dominant video modality of the future. However, HDR videos are incompatible with existing SDR displays, which form the majority of affordable consumer displays on the market. Because of this, HDR videos must be processed by tone-mapping them to reduced bit-depths to service a broad swath of SDR-limited video consumers. Here, we analyze the impact of tone-mapping operators on the visual quality of streaming HDR videos. To this end, we built the first large-scale subjectively annotated open-source database of compressed tone-mapped HDR videos, containing 15,000 tone-mapped sequences derived from 40 unique HDR source contents. The videos in the database were labeled with more than 750,000 subjective quality annotations, collected from more than 1,600 unique human observers. We demonstrate the usefulness of the new subjective database by <b>benchmarking</b> objective models of visual quality on it. We envision that the new LIVE Tone-Mapped HDR (LIVE-TMHDR) database will enable significant progress on HDR video tone mapping and quality assessment in the future. To this end, we make the database freely available to the community at <a href=https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html>https://live.ece.utexas.edu/research/LIVE_TMHDR/index.html</a></p></p class="citation"></blockquote><h2 id=statml-2>stat.ML (2)</h2><h3 id=12--150259-contrastive-learning-on-multimodal-analysis-of-electronic-health-records-tianxi-cai-et-al-2024>(1/2 | 150/259) Contrastive Learning on Multimodal Analysis of Electronic Health Records (Tianxi Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxi Cai, Feiqing Huang, Ryumei Nakada, Linjun Zhang, Doudou Zhou. (2024)<br><strong>Contrastive Learning on Multimodal Analysis of Electronic Health Records</strong><br><button class=copy-to-clipboard title="Contrastive Learning on Multimodal Analysis of Electronic Health Records" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 61<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Mutual Information, Representation Learning, Simulation, Simulator, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14926v1.pdf filename=2403.14926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electronic health record (EHR) systems contain a wealth of <b>multimodal</b> clinical data including structured data like clinical codes and unstructured data such as clinical notes. However, many existing EHR-focused studies has traditionally either concentrated on an individual modality or merged different modalities in a rather rudimentary fashion. This approach often results in the perception of structured and unstructured data as separate entities, neglecting the inherent synergy between them. Specifically, the two important modalities contain clinically relevant, inextricably linked and complementary health information. A more complete picture of a patient&rsquo;s medical history is captured by the joint analysis of the two modalities of data. Despite the great success of <b>multimodal</b> <b>contrastive</b> <b>learning</b> on <b>vision-language,</b> its potential remains under-explored in the realm of <b>multimodal</b> EHR, particularly in terms of its theoretical understanding. To accommodate the statistical analysis of <b>multimodal</b> EHR data, in this paper, we propose a novel <b>multimodal</b> feature embedding generative model and design a <b>multimodal</b> <b>contrastive</b> <b>loss</b> to obtain the <b>multimodal</b> EHR feature <b>representation.</b> <b>Our</b> theoretical analysis demonstrates the effectiveness of <b>multimodal</b> learning compared to single-modality learning and connects the solution of the loss function to the singular value decomposition of a pointwise <b>mutual</b> <b>information</b> matrix. This connection paves the way for a privacy-preserving algorithm tailored for <b>multimodal</b> EHR feature <b>representation</b> <b>learning.</b> <b>Simulation</b> studies show that the proposed algorithm performs well under a variety of configurations. We further validate the clinical utility of the proposed algorithm in real-world EHR data.</p></p class="citation"></blockquote><h3 id=22--151259-conformal-online-model-aggregation-matteo-gasparin-et-al-2024>(2/2 | 151/259) Conformal online model aggregation (Matteo Gasparin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Gasparin, Aaditya Ramdas. (2024)<br><strong>Conformal online model aggregation</strong><br><button class=copy-to-clipboard title="Conformal online model aggregation" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15527v1.pdf filename=2403.15527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conformal prediction equips machine learning models with a reasonable notion of uncertainty quantification without making strong distributional assumptions. It wraps around any <b>black-box</b> <b>prediction</b> model and converts point predictions into set predictions that have a predefined marginal coverage guarantee. However, conformal prediction only works if we fix the underlying machine learning model in advance. A relatively unaddressed issue in conformal prediction is that of model selection and/or aggregation: for a given problem, which of the plethora of prediction methods (random forests, neural nets, regularized linear models, etc.) should we conformalize? This paper proposes a new approach towards conformal model aggregation in online settings that is based on combining the prediction sets from several algorithms by voting, where weights on the models are adapted over time based on past performance.</p></p class="citation"></blockquote><h2 id=cscr-6>cs.CR (6)</h2><h3 id=16--152259-differentially-private-next-token-prediction-of-large-language-models-james-flemings-et-al-2024>(1/6 | 152/259) Differentially Private Next-Token Prediction of Large Language Models (James Flemings et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Flemings, Meisam Razaviyayn, Murali Annavaram. (2024)<br><strong>Differentially Private Next-Token Prediction of Large Language Models</strong><br><button class=copy-to-clipboard title="Differentially Private Next-Token Prediction of Large Language Models" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Stochastic Gradient Descent, Large Language Model, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15638v1.pdf filename=2403.15638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring the privacy of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model in such a way that guarantees <b>Differential</b> <b>Privacy</b> (DP). However, DP-SGD requires longer training times and larger memory requirements than <b>SGD,</b> while overestimating an adversary&rsquo;s capabilities in having white box access to the model. A more realistic scenario assumes only <b>black-box</b> <b>access</b> to a privacy-sensitive <b>LLM.</b> Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol that achieves practical next-token prediction by projecting each of the model&rsquo;s output distribution from an ensemble of <b>fine-tuned</b> <b>LLMs</b> onto a set around a public <b>LLM&rsquo;s</b> output distribution, then averaging the projected distributions and sampling from it. Our approach is more lightweight than DP-SGD in that it is model agnostic, instead providing <b>differential</b> <b>privacy</b> at prediction rather than during training. Our results show that PMixED achieves a stronger privacy guarantee than sample-level privacy and outperforms DP-SGD for privacy $\epsilon = 8$ on large-scale datasets.</p></p class="citation"></blockquote><h3 id=26--153259-privacy-preserving-end-to-end-spoken-language-understanding-yinggui-wang-et-al-2024>(2/6 | 153/259) Privacy-Preserving End-to-End Spoken Language Understanding (Yinggui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinggui Wang, Wei Huang, Le Yang. (2024)<br><strong>Privacy-Preserving End-to-End Spoken Language Understanding</strong><br><button class=copy-to-clipboard title="Privacy-Preserving End-to-End Spoken Language Understanding" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR, eess-AS<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15510v1.pdf filename=2403.15510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spoken language understanding (SLU), one of the key enabling technologies for human-computer interaction in IoT devices, provides an easy-to-use user interface. Human <b>speech</b> <b>can</b> contain a lot of user-sensitive information, such as gender, identity, and sensitive content. New types of security and privacy breaches have thus emerged. Users do not want to expose their personal sensitive information to malicious attacks by untrusted third parties. Thus, the SLU system needs to ensure that a potential malicious attacker cannot deduce the sensitive attributes of the users, while it should avoid greatly compromising the SLU accuracy. To address the above challenge, this paper proposes a novel SLU multi-task privacy-preserving model to prevent both the <b>speech</b> <b>recognition</b> <b>(ASR)</b> and identity recognition (IR) attacks. The model uses the hidden layer separation technique so that SLU information is distributed only in a specific portion of the hidden layer, and the other two types of information are removed to obtain a privacy-secure hidden layer. In order to achieve good balance between efficiency and privacy, we introduce a new mechanism of model pre-training, namely joint <b>adversarial</b> <b>training,</b> to further enhance the user privacy. Experiments over two SLU datasets show that the proposed method can reduce the accuracy of both the <b>ASR</b> and IR attacks close to that of a random guess, while leaving the SLU performance largely unaffected.</p></p class="citation"></blockquote><h3 id=36--154259-attacking-with-something-that-does-not-exist-low-rate-flood-with-proof-of-non-existence-can-exhaust-dns-resolver-cpu-olivia-gruza-et-al-2024>(3/6 | 154/259) Attacking with Something That Does Not Exist: Low-Rate Flood with &lsquo;Proof of Non-Existence&rsquo; Can Exhaust DNS Resolver CPU (Olivia Gruza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivia Gruza, Elias Heftrig, Oliver Jacobsen, Haya Schulmann, Niklas Vogel, Michael Waidner. (2024)<br><strong>Attacking with Something That Does Not Exist: Low-Rate Flood with &lsquo;Proof of Non-Existence&rsquo; Can Exhaust DNS Resolver CPU</strong><br><button class=copy-to-clipboard title="Attacking with Something That Does Not Exist: Low-Rate Flood with 'Proof of Non-Existence' Can Exhaust DNS Resolver CPU" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15233v1.pdf filename=2403.15233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NSEC3 is a proof of non-existence in DNSSEC, which provides an authenticated assertion that a queried resource does not exist in the target domain. NSEC3 consists of alphabetically sorted hashed names before and after the queried hostname. To make dictionary attacks harder, the hash function can be applied in multiple iterations, which however also increases the load on the DNS resolver during the computation of the SHA-1 hashes in NSEC3 records. Concerns about the load created by the computation of NSEC3 records on the DNS resolvers were already considered in the NSEC3 specifications RFC5155 and RFC9276. In February 2024, the potential of NSEC3 to exhaust DNS resolvers&rsquo; resources was assigned a CVE-2023-50868, confirming that extra iterations of NSEC3 created substantial load. However, there is no published evaluation of the attack and the impact of the attack on the resolvers was not clarified. In this work we perform the first evaluation of the NSEC3-encloser attack against DNS resolver implementations and find that the NSEC3-encloser attack can still create a 72x increase in CPU instruction count, despite the victim resolver following RFC5155 <b>recommendations</b> in limiting hash iteration counts. The impact of the attack varies across the different DNS resolvers, but we show that with a sufficient volume of DNS packets the attack can increase CPU load and cause packet loss. We find that at a rate of 150 malicious NSEC3 records per second, depending on the DNS implementation, the loss rate of benign DNS requests varies between 2.7% and 30%. We provide a detailed description and implementation the NSEC3-encloser attack along with evaluation against five popular DNS resolver implementations. We also develop the first analysis how each NSEC3 parameter impacts the load inflicted on the victim resolver during NSEC3-encloser attack.</p></p class="citation"></blockquote><h3 id=46--155259-differentially-private-ad-conversion-measurement-john-delaney-et-al-2024>(4/6 | 155/259) Differentially Private Ad Conversion Measurement (John Delaney et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Delaney, Badih Ghazi, Charlie Harrison, Christina Ilvento, Ravi Kumar, Pasin Manurangsi, Martin Pal, Karthik Prabhakar, Mariana Raykova. (2024)<br><strong>Differentially Private Ad Conversion Measurement</strong><br><button class=copy-to-clipboard title="Differentially Private Ad Conversion Measurement" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DS, cs.CR<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15224v1.pdf filename=2403.15224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study ad conversion measurement, a central functionality in digital advertising, where an advertiser seeks to estimate advertiser website (or mobile app) conversions attributed to ad impressions that users have interacted with on various publisher websites (or mobile apps). Using <b>differential</b> <b>privacy</b> (DP), a notion that has gained in popularity due to its strong mathematical guarantees, we develop a formal framework for private ad conversion measurement. In particular, we define the notion of an operationally valid configuration of the attribution rule, DP adjacency relation, contribution bounding scope and enforcement point. We then provide, for the set of configurations that most commonly arises in practice, a complete characterization, which uncovers a delicate interplay between attribution and privacy.</p></p class="citation"></blockquote><h3 id=56--156259-a-transfer-attack-to-image-watermarks-yuepeng-hu-et-al-2024>(5/6 | 156/259) A Transfer Attack to Image Watermarks (Yuepeng Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong. (2024)<br><strong>A Transfer Attack to Image Watermarks</strong><br><button class=copy-to-clipboard title="A Transfer Attack to Image Watermarks" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15365v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15365v2.pdf filename=2403.15365v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and <b>black-box</b> <b>settings</b> is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.</p></p class="citation"></blockquote><h3 id=66--157259-twin-auto-encoder-model-for-learning-separable-representation-in-cyberattack-detection-phai-vu-dinh-et-al-2024>(6/6 | 157/259) Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection (Phai Vu Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phai Vu Dinh, Quang Uy Nguyen, Thai Hoang Dinh, Diep N. Nguyen, Bao Son Pham, Eryk Dutkiewicz. (2024)<br><strong>Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection</strong><br><button class=copy-to-clipboard title="Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15509v1.pdf filename=2403.15509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Representation</b> <b>Learning</b> (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent <b>representation</b> <b>that</b> better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the <b>representation</b> <b>of</b> AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent <b>representation</b> <b>into</b> a more distinguishable <b>representation</b> <b>namely</b> the \textit{separable representation} and the reconstructsuct the separable <b>representation</b> <b>at</b> the output. The output of TAE called the \textit{reconstruction representation} is input to downstream models to detect cyberattacks. We extensively evaluate the effectiveness of TAE using a wide range of bench-marking datasets. Experiment results show the superior accuracy of TAE over state-of-the-art RL models and well-known machine learning algorithms. Moreover, TAE also outperforms state-of-the-art models on some sophisticated and challenging attacks. We then investigate various characteristics of TAE to further demonstrate its superiority.</p></p class="citation"></blockquote><h2 id=csai-11>cs.AI (11)</h2><h3 id=111--158259-large-language-models-for-crowd-decision-making-based-on-prompt-design-strategies-using-chatgpt-models-analysis-and-challenges-cristina-zuheros-et-al-2024>(1/11 | 158/259) Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges (Cristina Zuheros et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristina Zuheros, David Herrera-Poyatos, Rosana Montes, Francisco Herrera. (2024)<br><strong>Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges</strong><br><button class=copy-to-clipboard title="Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: ChatGPT, Sentiment Analysis, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15587v1.pdf filename=2403.15587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social Media and Internet have the potential to be exploited as a source of opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a methodology able to infer opinions and decisions from plain texts, such as reviews published in social media platforms, by means of <b>Sentiment</b> <b>Analysis.</b> Currently, the emergence and potential of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> lead us to explore new scenarios of automatically understand written texts, also known as natural language processing. This paper analyzes the use of <b>ChatGPT</b> based on <b>prompt</b> design strategies to assist in CDM processes to extract opinions and make decisions. We integrate <b>ChatGPT</b> in CDM processes as a flexible tool that infer the opinions expressed in texts, providing numerical or linguistic evaluations where the decision making models are based on the <b>prompt</b> design strategies. We include a multi-criteria decision making scenario with a category ontology for criteria. We also consider <b>ChatGPT</b> as an end-to-end CDM model able to provide a general opinion and score on the alternatives. We conduct empirical experiments on real data extracted from TripAdvisor, the TripR-2020Large dataset. The analysis of results show a promising branch for developing quality decision making models using <b>ChatGPT.</b> Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of <b>LLMs</b> in CDM processes, raising open questions for future studies.</p></p class="citation"></blockquote><h3 id=211--159259-sphere-neural-networks-for-rational-reasoning-tiansi-dong-et-al-2024>(2/11 | 159/259) Sphere Neural-Networks for Rational Reasoning (Tiansi Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiansi Dong, Mateja Jamnik, Pietro Liò. (2024)<br><strong>Sphere Neural-Networks for Rational Reasoning</strong><br><button class=copy-to-clipboard title="Sphere Neural-Networks for Rational Reasoning" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: ChatGPT, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15297v1.pdf filename=2403.15297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> e.g., <b>ChatGPT,</b> is witnessed by their planetary popularity, their capability of human-like <b>question-answering,</b> <b>and</b> also by their steadily improved <b>reasoning</b> performance. However, it remains unclear whether <b>LLMs</b> reason. It is an open problem how traditional neural networks can be qualitatively extended to go beyond the statistic paradigm and achieve high-level cognition. Here, we present a minimalist qualitative extension by generalising computational building blocks from vectors to spheres. We propose Sphere Neural Networks (SphNNs) for human-like <b>reasoning</b> through model construction and inspection, and develop SphNN for syllogistic <b>reasoning,</b> a microcosm of human rationality. Instead of training data, SphNN uses a neuro-symbolic transition map of neighbourhood spatial relations to guide transformations from the current sphere configuration towards the target. SphNN is the first neural model that can determine the validity of long-chained syllogistic <b>reasoning</b> in one epoch by constructing sphere configurations as Euler diagrams, with the worst computational complexity of O(N^2). SphNN can evolve into various types of <b>reasoning,</b> such as spatio-temporal <b>reasoning,</b> logical <b>reasoning</b> with negation and disjunction, event <b>reasoning,</b> neuro-symbolic <b>reasoning,</b> and humour understanding (the highest level of cognition). All these suggest a new kind of Herbert A. Simon&rsquo;s scissors with two neural blades. SphNNs will tremendously enhance interdisciplinary collaborations to develop the two neural blades and realise deterministic neural <b>reasoning</b> and human-bounded rationality and elevate <b>LLMs</b> to reliable psychological AI. This work suggests that the non-zero radii of spheres are the missing components that prevent traditional deep-learning systems from reaching the realm of rational <b>reasoning</b> and cause <b>LLMs</b> to be trapped in the swamp of hallucination.</p></p class="citation"></blockquote><h3 id=311--160259-a-picture-is-worth-a-graph-blueprint-debate-on-graph-for-multimodal-reasoning-changmeng-zheng-et-al-2024>(3/11 | 160/259) A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning (Changmeng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, Qing Li. (2024)<br><strong>A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning</strong><br><button class=copy-to-clipboard title="A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-MA, cs-MM, cs.AI<br>Keyword Score: 39<br>Keywords: Graph, Multi-modal, Multi-modal, Question Answering, Reasoning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14972v1.pdf filename=2403.14972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a pilot study aimed at introducing multi-agent debate into <b>multimodal</b> <b>reasoning.</b> The study addresses two key challenges: the trivialization of opinions resulting from excessive <b>summarization</b> and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on <b>Graphs</b> (BDoG). In BDoG, debates are confined to a blueprint <b>graph</b> to prevent opinion trivialization through world-level <b>summarization.</b> Moreover, by storing evidence in branches within the <b>graph,</b> BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science <b>QA</b> and MMBench with significant improvements over previous methods.</p></p class="citation"></blockquote><h3 id=411--161259-contextual-restless-multi-armed-bandits-with-application-to-demand-response-decision-making-xin-chen-et-al-2024>(4/11 | 161/259) Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making (Xin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Chen, I-Hong Hou. (2024)<br><strong>Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making</strong><br><button class=copy-to-clipboard title="Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15640v1.pdf filename=2403.15640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel multi-armed <b>bandits</b> framework, termed Contextual Restless <b>Bandits</b> (CRB), for complex online decision-making. This CRB framework incorporates the core features of contextual <b>bandits</b> and restless <b>bandits,</b> so that it can model both the internal state transitions of each arm and the influence of external global environmental contexts. Using the dual decomposition method, we develop a scalable index policy algorithm for solving the CRB problem, and theoretically analyze the asymptotical optimality of this algorithm. In the case when the arm models are unknown, we further propose a model-based online learning algorithm based on the index policy to learn the arm models and make decisions simultaneously. Furthermore, we apply the proposed CRB framework and the index policy algorithm specifically to the demand response decision-making problem in smart grids. The numerical <b>simulations</b> demonstrate the performance and efficiency of our proposed CRB approaches.</p></p class="citation"></blockquote><h3 id=511--162259-generative-ai-in-education-a-study-of-educators-awareness-sentiments-and-influencing-factors-aashish-ghimire-et-al-2024>(5/11 | 162/259) Generative AI in Education: A Study of Educators&rsquo; Awareness, Sentiments, and Influencing Factors (Aashish Ghimire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aashish Ghimire, James Prather, John Edwards. (2024)<br><strong>Generative AI in Education: A Study of Educators&rsquo; Awareness, Sentiments, and Influencing Factors</strong><br><button class=copy-to-clipboard title="Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15586v1.pdf filename=2403.15586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of artificial intelligence (AI) and the expanding integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have ignited a debate about their application in education. This study delves into university instructors&rsquo; experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators&rsquo; perspectives on AI&rsquo;s role in the classroom and its potential impacts on teaching and learning. The objective of this research is to investigate the level of awareness, overall sentiment towardsadoption, and the factors influencing these attitudes for <b>LLMs</b> and <b>generative</b> <b>AI-based</b> tools in higher education. Data was collected through a survey using a Likert scale, which was complemented by follow-up interviews to gain a more nuanced understanding of the instructors&rsquo; viewpoints. The collected data was processed using statistical and thematic analysis techniques. Our findings reveal that educators are increasingly aware of and generally positive towards these tools. We find no correlation between teaching style and attitude toward <b>generative</b> <b>AI.</b> Finally, while CS educators show far more confidence in their technical understanding of <b>generative</b> <b>AI</b> tools and more positivity towards them than educators in other fields, they show no more confidence in their ability to detect AI-generated work.</p></p class="citation"></blockquote><h3 id=611--163259-caca-agent-capability-collaboration-based-ai-agent-peng-xu-et-al-2024>(6/11 | 163/259) CACA Agent: Capability Collaboration based AI Agent (Peng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Xu, Haoran Wang, Chuang Wang, Xu Liu. (2024)<br><strong>CACA Agent: Capability Collaboration based AI Agent</strong><br><button class=copy-to-clipboard title="CACA Agent: Capability Collaboration based AI Agent" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15137v1.pdf filename=2403.15137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As AI Agents based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the <b>reasoning</b> capabilities of AI agents within a single <b>LLM,</b> which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single <b>LLM,</b> but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent.</p></p class="citation"></blockquote><h3 id=711--164259-symboslam-semantic-map-generation-in-a-multi-agent-system-brandon-curtis-colelough-2024>(7/11 | 164/259) SymboSLAM: Semantic Map Generation in a Multi-Agent System (Brandon Curtis Colelough, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brandon Curtis Colelough. (2024)<br><strong>SymboSLAM: Semantic Map Generation in a Multi-Agent System</strong><br><button class=copy-to-clipboard title="SymboSLAM: Semantic Map Generation in a Multi-Agent System" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15504v1.pdf filename=2403.15504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sub-symbolic artificial intelligence methods dominate the fields of environment-type classification and Simultaneous Localisation and Mapping. However, a significant area overlooked within these fields is solution transparency for the human-machine interaction space, as the sub-symbolic methods employed for map generation do not account for the explainability of the solutions generated. This paper proposes a novel approach to environment-type classification through Symbolic Simultaneous Localisation and Mapping, SymboSLAM, to bridge the explainability gap. Our method for environment-type classification observes ontological <b>reasoning</b> used to synthesise the context of an environment through the features found within. We achieve explainability within the model by presenting operators with environment-type classifications overlayed by a semantically labelled occupancy map of landmarks and features. We evaluate SymboSLAM with ground-truth maps of the Canberra region, demonstrating method effectiveness. We assessed the system through both <b>simulations</b> and real-world trials.</p></p class="citation"></blockquote><h3 id=811--165259-sensoryt5-infusing-sensorimotor-norms-into-t5-for-enhanced-fine-grained-emotion-classification-yuhan-xia-et-al-2024>(8/11 | 165/259) SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification (Yuhan Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhan Xia, Qingqing Zhao, Yunfei Long, Ge Xu, Jia Wang. (2024)<br><strong>SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification</strong><br><button class=copy-to-clipboard title="SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: T5, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15574v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15574v1.pdf filename=2403.15574v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In traditional research approaches, sensory perception and emotion classification have traditionally been considered separate domains. Yet, the significant influence of sensory experiences on emotional responses is undeniable. The natural language processing (NLP) community has often missed the opportunity to merge sensory knowledge with emotion classification. To address this gap, we propose SensoryT5, a neuro-cognitive approach that integrates sensory information into the <b>T5</b> (Text-to-Text Transfer <b>Transformer)</b> model, designed specifically for fine-grained emotion classification. This methodology incorporates sensory cues into the <b>T5&rsquo;s</b> attention mechanism, enabling a harmonious balance between contextual understanding and sensory awareness. The resulting model amplifies the richness of emotional representations. In rigorous tests across various detailed emotion classification datasets, SensoryT5 showcases improved performance, surpassing both the foundational <b>T5</b> model and current state-of-the-art works. Notably, SensoryT5&rsquo;s success signifies a pivotal change in the NLP domain, highlighting the potential influence of neuro-cognitive data in refining machine learning models&rsquo; emotional sensitivity.</p></p class="citation"></blockquote><h3 id=911--166259-autonomous-driving-with-perception-uncertainties-deep-ensemble-based-adaptive-cruise-control-xiao-li-et-al-2024>(9/11 | 166/259) Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control (Xiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Li, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky. (2024)<br><strong>Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control</strong><br><button class=copy-to-clipboard title="Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs-SY, cs.AI, eess-SY<br>Keyword Score: 15<br>Keywords: Black Box, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15577v1.pdf filename=2403.15577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving depends on perception systems to understand the environment and to inform downstream decision-making. While advanced perception systems utilizing <b>black-box</b> <b>Deep</b> Neural Networks (DNNs) demonstrate human-like comprehension, their unpredictable behavior and lack of interpretability may hinder their deployment in safety critical scenarios. In this paper, we develop an Ensemble of DNN regressors (Deep Ensemble) that generates predictions with quantification of prediction uncertainties. In the scenario of Adaptive Cruise Control (ACC), we employ the Deep Ensemble to estimate distance headway to the lead vehicle from RGB images and enable the downstream controller to account for the estimation uncertainty. We develop an adaptive cruise controller that utilizes Stochastic Model Predictive Control (MPC) with chance constraints to provide a probabilistic safety guarantee. We evaluate our ACC algorithm using a high-fidelity traffic simulator and a real-world traffic dataset and demonstrate the ability of the proposed approach to effect speed tracking and car following while maintaining a safe distance headway. The <b>out-of-distribution</b> scenarios are also examined.</p></p class="citation"></blockquote><h3 id=1011--167259-collaborative-ai-teaming-in-unknown-environments-via-active-goal-deduction-zuyuan-zhang-et-al-2024>(10/11 | 167/259) Collaborative AI Teaming in Unknown Environments via Active Goal Deduction (Zuyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuyuan Zhang, Hanhan Zhou, Mahdi Imani, Taeyoung Lee, Tian Lan. (2024)<br><strong>Collaborative AI Teaming in Unknown Environments via Active Goal Deduction</strong><br><button class=copy-to-clipboard title="Collaborative AI Teaming in Unknown Environments via Active Goal Deduction" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15341v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15341v1.pdf filename=2403.15341v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancements of artificial intelligence (AI), we&rsquo;re seeing more scenarios that require AI to work closely with other agents, whose goals and strategies might not be known beforehand. However, existing approaches for training collaborative agents often require defined and known reward signals and cannot address the problem of teaming with unknown agents that often have latent objectives/rewards. In response to this challenge, we propose teaming with unknown agents framework, which leverages kernel density Bayesian inverse learning method for active goal deduction and utilizes pre-trained, goal-conditioned policies to enable <b>zero-shot</b> policy adaptation. We prove that unbiased reward estimates in our framework are sufficient for optimal teaming with unknown agents. We further evaluate the framework of redesigned multi-agent particle and StarCraft II micromanagement environments with diverse unknown agents of different behaviors/rewards. Empirical results demonstrate that our framework significantly advances the teaming performance of AI and unknown agents in a wide range of collaborative scenarios.</p></p class="citation"></blockquote><h3 id=1111--168259-safe-learning-of-pddl-domains-with-conditional-effects----extended-version-argaman-mordoch-et-al-2024>(11/11 | 168/259) Safe Learning of PDDL Domains with Conditional Effects &ndash; Extended Version (Argaman Mordoch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Argaman Mordoch, Enrico Scala, Roni Stern, Brendan Juba. (2024)<br><strong>Safe Learning of PDDL Domains with Conditional Effects &ndash; Extended Version</strong><br><button class=copy-to-clipboard title="Safe Learning of PDDL Domains with Conditional Effects -- Extended Version" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Planning Domain Descrition Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15251v1.pdf filename=2403.15251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Powerful domain-independent planners have been developed to solve various types of planning problems. These planners often require a model of the acting agent&rsquo;s actions, given in some planning domain description language. Manually designing such an action model is a notoriously challenging task. An alternative is to automatically learn action models from observation. Such an action model is called safe if every plan created with it is consistent with the real, unknown action model. Algorithms for learning such safe action models exist, yet they cannot handle domains with conditional or universal effects, which are common constructs in many planning problems. We prove that learning non-trivial safe action models with conditional effects may require an exponential number of samples. Then, we identify reasonable assumptions under which such learning is tractable and propose SAM Learning of Conditional Effects (Conditional-SAM), the first algorithm capable of doing so. We analyze Conditional-SAM theoretically and evaluate it experimentally. Our results show that the action models learned by Conditional-SAM can be used to solve perfectly most of the test set problems in most of the experimented domains.</p></p class="citation"></blockquote><h2 id=q-bioot-1>q-bio.OT (1)</h2><h3 id=11--169259-bioinformatics-and-biomedical-informatics-with-chatgpt-year-one-review-jinge-wang-et-al-2024>(1/1 | 169/259) Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review (Jinge Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu. (2024)<br><strong>Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review</strong><br><button class=copy-to-clipboard title="Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.OT<br>Categories: cs-AI, q-bio-OT, q-bio.OT<br>Keyword Score: 50<br>Keywords: ChatGPT, Chatbot, Text Mining, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15274v1.pdf filename=2403.15274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The year 2023 marked a significant surge in the exploration of applying <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> <b>chatbots,</b> notably <b>ChatGPT,</b> across various disciplines. We surveyed the applications of <b>ChatGPT</b> in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical <b>text</b> <b>mining,</b> drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this <b>chatbot</b> in bioinformatics and offers insights into potential avenues for future development.</p></p class="citation"></blockquote><h2 id=cscy-8>cs.CY (8)</h2><h3 id=18--170259-instasynth-opportunities-and-challenges-in-generating-synthetic-instagram-data-with-chatgpt-for-sponsored-content-detection-thales-bertaglia-et-al-2024>(1/8 | 170/259) InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection (Thales Bertaglia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi. (2024)<br><strong>InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection</strong><br><button class=copy-to-clipboard title="InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs-SI, cs.CY<br>Keyword Score: 50<br>Keywords: ChatGPT, Content Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15214v1.pdf filename=2403.15214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored <b>content</b> <b>online.</b> We investigate the use of <b>LLMs</b> for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement <b>content-level</b> <b>and</b> network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored <b>content</b> <b>detection.</b> For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that <b>prompt</b> engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity, and realistic user interaction patterns.</p></p class="citation"></blockquote><h3 id=28--171259-investigating-bias-in-llm-based-bias-detection-disparities-between-llms-and-human-perception-luyang-lin-et-al-2024>(2/8 | 171/259) Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception (Luyang Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong. (2024)<br><strong>Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception</strong><br><button class=copy-to-clipboard title="Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14896v1.pdf filename=2403.14896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within <b>LLMs</b> and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the <b>LLM</b> systems themselves. Through meticulous examination, we probe whether <b>LLMs</b> exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the <b>LLM</b> framework. Importantly, we propose debiasing strategies, including <b>prompt</b> engineering and model <b>fine-tuning.</b> Extensive analysis of bias tendencies across different <b>LLMs</b> sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of <b>LLM</b> bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems</p></p class="citation"></blockquote><h3 id=38--172259-from-guidelines-to-governance-a-study-of-ai-policies-in-education-aashish-ghimire-et-al-2024>(3/8 | 172/259) From Guidelines to Governance: A Study of AI Policies in Education (Aashish Ghimire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aashish Ghimire, John Edwards. (2024)<br><strong>From Guidelines to Governance: A Study of AI Policies in Education</strong><br><button class=copy-to-clipboard title="From Guidelines to Governance: A Study of AI Policies in Education" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15601v1.pdf filename=2403.15601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging technologies like <b>generative</b> <b>AI</b> tools, including <b>ChatGPT,</b> are increasingly utilized in educational settings, offering innovative approaches to learning while simultaneously posing new challenges. This study employs a survey methodology to examine the policy landscape concerning these technologies, drawing insights from 102 high school principals and higher education provosts. Our results reveal a prominent policy gap: the majority of institutions lack specialized guide-lines for the ethical deployment of AI tools such as <b>ChatGPT.</b> Moreover,we observed that high schools are less inclined to work on policies than higher educational institutions. Where such policies do exist, they often overlook crucial issues, including student privacy and algorithmic transparency. Administrators overwhelmingly recognize the necessity of these policies, primarily to safeguard student safety and mitigate plagiarism risks. Our findings underscore the urgent need for flexible and iterative policy frameworks in educational contexts.</p></p class="citation"></blockquote><h3 id=48--173259-ai-teaches-the-art-of-elegant-coding-timely-fair-and-helpful-style-feedback-in-a-global-course-juliette-woodrow-et-al-2024>(4/8 | 173/259) AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course (Juliette Woodrow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juliette Woodrow, Ali Malik, Chris Piech. (2024)<br><strong>AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course</strong><br><button class=copy-to-clipboard title="AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14986v1.pdf filename=2403.14986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this &ldquo;style feedback&rdquo; in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a <b>large-scale</b> <b>online</b> <b>CS1</b> course. Our tool is based on the latest breakthroughs in <b>large-language</b> <b>models</b> <b>(LLMs)</b> and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of <b>LLM-based</b> tools for feedback, investigating the quality of the feedback generated, <b>LLM</b> limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.</p></p class="citation"></blockquote><h3 id=58--174259-analyzing-male-domestic-violence-through-exploratory-data-analysis-and-explainable-machine-learning-insights-md-abrar-jahin-et-al-2024>(5/8 | 174/259) Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights (Md Abrar Jahin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abrar Jahin, Saleh Akram Naife, Fatema Tuj Johora Lima, M. F. Mridha, Jungpil Shin. (2024)<br><strong>Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights</strong><br><button class=copy-to-clipboard title="Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-LG, cs.CY<br>Keyword Score: 15<br>Keywords: Black Box, Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15594v1.pdf filename=2403.15594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domestic violence, which is often perceived as a gendered issue among female victims, has gained increasing attention in recent years. Despite this focus, male victims of domestic abuse remain primarily overlooked, particularly in Bangladesh. Our study represents a pioneering exploration of the underexplored realm of male domestic violence (MDV) within the Bangladeshi context, shedding light on its prevalence, patterns, and underlying factors. Existing literature predominantly emphasizes female victimization in domestic violence scenarios, leading to an absence of research on male victims. We collected data from the major cities of Bangladesh and conducted exploratory data analysis to understand the underlying dynamics. We implemented 11 traditional machine learning models with default and optimized hyperparameters, 2 deep learning, and 4 ensemble models. Despite various approaches, CatBoost has emerged as the top performer due to its native support for categorical features, efficient handling of missing values, and robust regularization techniques, achieving 76% accuracy. In contrast, other models achieved accuracy rates in the range of 58-75%. The <b>eXplainable</b> <b>AI</b> techniques, SHAP and LIME, were employed to gain insights into the decision-making of <b>black-box</b> <b>machine</b> learning models. By shedding light on this topic and identifying factors associated with domestic abuse, the study contributes to identifying groups of people vulnerable to MDV, raising awareness, and informing policies and interventions aimed at reducing MDV. Our findings challenge the prevailing notion that domestic abuse primarily affects women, thus emphasizing the need for tailored interventions and support systems for male victims. ML techniques enhance the analysis and understanding of the data, providing valuable insights for developing effective strategies to combat this pressing social issue.</p></p class="citation"></blockquote><h3 id=68--175259-learners-teaching-novices-an-uplifting-alternative-assessment-ali-malik-et-al-2024>(6/8 | 175/259) Learners Teaching Novices: An Uplifting Alternative Assessment (Ali Malik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Malik, Juliette Woodrow, Chris Piech. (2024)<br><strong>Learners Teaching Novices: An Uplifting Alternative Assessment</strong><br><button class=copy-to-clipboard title="Learners Teaching Novices: An Uplifting Alternative Assessment" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14971v1.pdf filename=2403.14971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose and carry-out a novel method of formative assessment called Assessment via Teaching (AVT), in which learners demonstrate their understanding of CS1 topics by tutoring more novice students. AVT has powerful benefits over traditional forms of assessment: it is centered around service to others and is highly rewarding for the learners who teach. Moreover, teaching greatly improves the learners&rsquo; own understanding of the material and has a huge positive impact on novices, who receive free 1:1 tutoring. Lastly, this form of assessment is naturally difficult to cheat &ndash; a critical property for assessments in the era of <b>large-language</b> <b>models.</b> <b>We</b> use AVT in a randomised control trial with learners in a CS1 course at an R1 university. The learners provide tutoring sessions to more novice students taking a lagged online version of the same course. We show that learners who do an AVT session before the course exam performed 20 to 30 percentage points better than the class average on several questions. Moreover, compared to students who did a practice exam, the AVT learners enjoyed their experience more and were twice as likely to study for their teaching session. We believe AVT is a scalable and uplifting method for formative assessment that could one day replace traditional exams.</p></p class="citation"></blockquote><h3 id=78--176259-analyzing-potential-solutions-involving-regulation-to-escape-some-of-ais-ethical-concerns-jay-nemec-2024>(7/8 | 176/259) Analyzing Potential Solutions Involving Regulation to Escape Some of AI&rsquo;s Ethical Concerns (Jay Nemec, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jay Nemec. (2024)<br><strong>Analyzing Potential Solutions Involving Regulation to Escape Some of AI&rsquo;s Ethical Concerns</strong><br><button class=copy-to-clipboard title="Analyzing Potential Solutions Involving Regulation to Escape Some of AI's Ethical Concerns" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15507v1.pdf filename=2403.15507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI), although not able to currently capture the many complexities of humans, are slowly adapting to have certain capabilities of humans, many of which can revolutionize our world. AI systems, such as <b>ChatGPT</b> and others utilized within various industries for specific processes, have been transforming rapidly. However, this transformation can occur in an extremely concerning way if certain measures are not taken. This article touches on some of the current issues within the artificial intelligence ethical crisis, such as the concerns of discrimination within AI and false information that is becoming readily available with AI. Within this article, plausible solutions involving regulation are discussed and how they would mitigate ethical concerns. These include the self-regulation of businesses along with government regulation, and the effects these possible solutions can both have on current AI concerns.</p></p class="citation"></blockquote><h3 id=88--177259-ktbench-a-novel-data-leakage-free-framework-for-knowledge-tracing-yahya-badran-et-al-2024>(8/8 | 177/259) KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing (Yahya Badran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yahya Badran, Christine Preisach. (2024)<br><strong>KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing</strong><br><button class=copy-to-clipboard title="KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-LG, cs.CY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15304v1.pdf filename=2403.15304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge Tracing (KT) is concerned with predicting students&rsquo; future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models. The first problem is the model&rsquo;s ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available <b>benchmark</b> implementations ignore accounting for changes in sequence length when expanding KCs, leading to different models being tested with varying sequence lengths but still compared against the same <b>benchmark.</b> To address these problems, we introduce a general masking framework that mitigates the first problem and enhances the performance of such KT models while preserving the original model architecture without significant alterations. Additionally, we introduce KTbench, an open-source <b>benchmark</b> library designed to ensure the reproducibility of this work while mitigating the second problem.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--178259-adaptive-coded-federated-learning-privacy-preservation-and-straggler-mitigation-chengxi-li-et-al-2024>(1/2 | 178/259) Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation (Chengxi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxi Li, Ming Xiao, Mikael Skoglund. (2024)<br><strong>Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation</strong><br><button class=copy-to-clipboard title="Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CR, cs-LG, eess-SP, eess.SP<br>Keyword Score: 50<br>Keywords: Federated Learning, Mutual Information, Simulation, Simulator, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14905v1.pdf filename=2403.14905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we address the problem of <b>federated</b> <b>learning</b> in the presence of stragglers. For this problem, a coded <b>federated</b> <b>learning</b> framework has been proposed, where the central server aggregates gradients received from the non-stragglers and gradient computed from a privacy-preservation global coded dataset to mitigate the negative impact of the stragglers. However, when aggregating these gradients, fixed weights are consistently applied across iterations, neglecting the generation process of the global coded dataset and the dynamic nature of the trained model over iterations. This oversight may result in diminished learning performance. To overcome this drawback, we propose a new method named adaptive coded <b>federated</b> <b>learning</b> (ACFL). In ACFL, before the training, each device uploads a coded local dataset with additive noise to the central server to generate a global coded dataset under privacy preservation requirements. During each iteration of the training, the central server aggregates the gradients received from the non-stragglers and the gradient computed from the global coded dataset, where an adaptive policy for varying the aggregation weights is designed. Under this policy, we optimize the performance in terms of privacy and learning, where the learning performance is analyzed through convergence analysis and the privacy performance is characterized via <b>mutual</b> <b>information</b> <b>differential</b> <b>privacy.</b> Finally, we perform <b>simulations</b> to demonstrate the superiority of ACFL compared with the non-adaptive methods.</p></p class="citation"></blockquote><h3 id=22--179259-learning-to-walk-on-new-ground-calibration-free-decoding-for-c-vep-bci-j-thielen-et-al-2024>(2/2 | 179/259) Learning to walk on new ground: Calibration-free decoding for c-VEP BCI (J. Thielen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Thielen, J. Sosulski, M. Tangermann. (2024)<br><strong>Learning to walk on new ground: Calibration-free decoding for c-VEP BCI</strong><br><button class=copy-to-clipboard title="Learning to walk on new ground: Calibration-free decoding for c-VEP BCI" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15521v1.pdf filename=2403.15521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores two zero-training methods aimed at enhancing the usability of brain-computer interfaces (BCIs) by eliminating the need for a calibration session. We introduce a novel method rooted in the event-related potential (ERP) domain, <b>unsupervised</b> mean maximization (UMM), to the fast code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare UMM to the state-of-the-art c-VEP zero-training method that uses canonical correlation analysis (CCA). The comparison includes instantaneous classification and classification with cumulative learning from previously classified trials for both CCA and UMM. Our study shows the effectiveness of both methods in navigating the complexities of a c-VEP dataset, highlighting their differences and distinct strengths. This research not only provides insights into the practical implementation of calibration-free BCI methods but also paves the way for further exploration and refinement. Ultimately, the fusion of CCA and UMM holds promise for enhancing the accessibility and usability of BCI systems across various application domains and a multitude of stimulus protocols.</p></p class="citation"></blockquote><h2 id=cssi-6>cs.SI (6)</h2><h3 id=16--180259-hierarchical-information-enhancement-network-for-cascade-prediction-in-social-networks-fanrui-zhang-et-al-2024>(1/6 | 180/259) Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks (Fanrui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanrui Zhang, Jiawei Liu, Qiang Zhang, Xiaoling Zhu, Zheng-Jun Zha. (2024)<br><strong>Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks</strong><br><button class=copy-to-clipboard title="Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 46<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15257v1.pdf filename=2403.15257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding information cascades in networks is a fundamental issue in numerous applications. Current researches often sample cascade information into several independent paths or subgraphs to learn a simple cascade representation. However, these approaches fail to exploit the hierarchical semantic associations between different modalities, limiting their predictive performance. In this work, we propose a novel Hierarchical Information Enhancement Network (HIENet) for cascade prediction. Our approach integrates fundamental cascade sequence, user social <b>graphs,</b> <b>and</b> <b>sub-cascade</b> <b>graph</b> <b>into</b> <b>a</b> unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades information into a series of sequences. It then gathers path information between users to extract the social relationships of propagators. Additionally, we employ a time-stamped <b>graph</b> <b>convolutional</b> <b>network</b> to aggregate sub-cascade <b>graph</b> <b>information</b> <b>effectively.</b> Ultimately, we introduce a <b>Multi-modal</b> Cascade <b>Transformer</b> to powerfully fuse these clues, providing a comprehensive understanding of cascading process. Extensive experiments have demonstrated the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=26--181259-llm-driven-agents-for-influencer-selection-in-digital-advertising-campaigns-xiaoqing-zhang-et-al-2024>(2/6 | 181/259) LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns (Xiaoqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoqing Zhang, Xiuying Chen, Yuhan Liu, Jianzhou Wang, Zhenxing Hu, Rui Yan. (2024)<br><strong>LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns</strong><br><button class=copy-to-clipboard title="LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15105v1.pdf filename=2403.15105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. With the development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> we now have the opportunity to capture the nuanced exchanges of information within social networks. Hence, in this work, we first introduce an Influencer Dynamics Simulator (IDS), helping promoters identify and select the right influencers to market their products, based on <b>LLM</b> <b>simulation.</b> Concretely, we first propose an influencer-influencee engagement-based pre-selection module to screen potential influencer candidates. Subsequently, a <b>simulation</b> is constructed for these candidates and their influencees. Each user is represented as an <b>LLM-based</b> agent, drawing from their interaction history to deduce their profile and interests. The influencee agents will predict their behavior in response to influencer advertising. Finally, we develop a ranking metric designed to pinpoint influencers who are most likely to drive product purchases based on feedback from their influencees. To evaluate our framework, we collect a real-world advertising network dataset, including social relations, post and comment content, and user behaviors. Our dataset covers six products in diverse categories with their promoter influencers. Experiments show that IDS accurately identifies influencers from hundreds of candidates and provides valuable insights through detailed comments and specific attitudes.</p></p class="citation"></blockquote><h3 id=36--182259-multi-perspective-memory-enhanced-network-for-identifying-key-nodes-in-social-networks-qiang-zhang-et-al-2024>(3/6 | 182/259) Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks (Qiang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Zhang, Jiawei Liu, Fanrui Zhang, Xiaoling Zhu, Zheng-Jun Zha. (2024)<br><strong>Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks</strong><br><button class=copy-to-clipboard title="Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph Attention Networks, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15235v1.pdf filename=2403.15235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying key nodes in social networks plays a crucial role in timely blocking false information. Existing key node identification methods usually consider node influence only from the propagation structure perspective and have insufficient generalization ability to unknown scenarios. In this paper, we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for identifying key nodes in social networks, which mines key nodes from multiple perspectives and utilizes memory networks to store historical information. Specifically, MMEN first constructs two propagation networks from the perspectives of user attributes and propagation structure and updates node feature representations using <b>graph</b> <b>attention</b> <b>networks.</b> Meanwhile, the memory network is employed to store information of similar subgraphs, enhancing the model&rsquo;s generalization performance in unknown scenarios. Finally, MMEN applies adaptive weights to combine the node influence of the two propagation networks to select the ultimate key nodes. Extensive experiments demonstrate that our method significantly outperforms previous methods.</p></p class="citation"></blockquote><h3 id=46--183259-ellipsoidal-embeddings-of-graphs-michaël-fanuel-et-al-2024>(4/6 | 183/259) Ellipsoidal embeddings of graphs (Michaël Fanuel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michaël Fanuel, Antoine Aspeel, Michael T. Schaub, Jean-Charles Delvenne. (2024)<br><strong>Ellipsoidal embeddings of graphs</strong><br><button class=copy-to-clipboard title="Ellipsoidal embeddings of graphs" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-DM, cs-SI, cs.SI<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15023v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15023v2.pdf filename=2403.15023v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to their flexibility to represent almost any kind of relational data, <b>graph-based</b> <b>models</b> have enjoyed a tremendous success over the past decades. While <b>graphs</b> <b>are</b> inherently only combinatorial objects, however, many prominent analysis tools are based on the algebraic representation of <b>graphs</b> <b>via</b> matrices such as the <b>graph</b> <b>Laplacian,</b> or on associated <b>graph</b> <b>embeddings.</b> Such embeddings associate to each node a set of coordinates in a vector space, a representation which can then be employed for learning tasks such as the classification or alignment of the nodes of the <b>graph.</b> <b>As</b> the geometric picture provided by embedding methods enables the use of a multitude of methods developed for vector space data, embeddings have thus gained interest both from a theoretical as well as a practical perspective. Inspired by trace-optimization problems, often encountered in the analysis of <b>graph-based</b> <b>data,</b> here we present a method to derive ellipsoidal embeddings of the nodes of a <b>graph,</b> <b>in</b> which each node is assigned a set of coordinates on the surface of a hyperellipsoid. Our method may be seen as an alternative to popular spectral embedding techniques, to which it shares certain similarities we discuss. To illustrate the utility of the embedding we conduct a case study in which we analyse synthetic and real world networks with modular structure, and compare the results obtained with known methods in the literature.</p></p class="citation"></blockquote><h3 id=56--184259-nonparametric-inference-of-higher-order-interaction-patterns-in-networks-anatol-e-wegner-et-al-2024>(5/6 | 184/259) Nonparametric inference of higher order interaction patterns in networks (Anatol E. Wegner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anatol E. Wegner, Sofia C. Olhede. (2024)<br><strong>Nonparametric inference of higher order interaction patterns in networks</strong><br><button class=copy-to-clipboard title="Nonparametric inference of higher order interaction patterns in networks" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cond-mat-stat-mech, cs-IT, cs-SI, cs.SI, math-IT, physics-soc-ph, stat-ME<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15635v1.pdf filename=2403.15635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method for obtaining parsimonious decompositions of networks into higher order interactions which can take the form of arbitrary motifs.The method is based on a class of analytically solvable generative models, where vertices are connected via explicit copies of motifs, which in combination with non-parametric priors allow us to infer higher order interactions from dyadic <b>graph</b> data without any prior knowledge on the types or frequencies of such interactions. Crucially, we also consider &lsquo;degree&ndash;corrected&rsquo; models that correctly reflect the degree distribution of the network and consequently prove to be a better fit for many real world&ndash;networks compared to non-degree corrected models. We test the presented approach on simulated data for which we recover the set of underlying higher order interactions to a high degree of accuracy. For empirical networks the method identifies concise sets of atomic subgraphs from within thousands of candidates that cover a large fraction of edges and include higher order interactions of known structural and functional significance. The method not only produces an explicit higher order representation of the network but also a fit of the network to analytically tractable models opening new avenues for the systematic study of higher order network structures.</p></p class="citation"></blockquote><h3 id=66--185259-unraveling-contagion-origins-optimal-estimation-through-maximum-likelihood-and-starlike-tree-approximation-in-markovian-spreading-models-pei-duo-yu-et-al-2024>(6/6 | 185/259) Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models (Pei-Duo Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pei-Duo Yu, Chee Wei Tan, Liang Zheng, Chao Zhao. (2024)<br><strong>Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models</strong><br><button class=copy-to-clipboard title="Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14890v1.pdf filename=2403.14890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying the source of epidemic-like spread in networks is crucial for tasks like removing internet viruses or finding the rumor source in online social networks. The challenge lies in tracing the source from a snapshot observation of infected nodes. How do we accurately pinpoint the source? Utilizing snapshot data, we apply a probabilistic approach, focusing on the <b>graph</b> boundary and the observed time, to detect sources via an effective maximum likelihood algorithm. A novel starlike tree approximation extends applicability to general <b>graphs,</b> demonstrating versatility. We highlight the utility of the Gamma function for analyzing the asymptotic behavior of the likelihood ratio between nodes. Comprehensive evaluations confirm algorithmic effectiveness in diverse network scenarios, advancing rumor source detection in large-scale network analysis and information dissemination strategies.</p></p class="citation"></blockquote><h2 id=csro-18>cs.RO (18)</h2><h3 id=118--186259-convoi-context-aware-navigation-using-vision-language-models-in-outdoor-and-indoor-environments-adarsh-jagan-sathyamoorthy-et-al-2024>(1/18 | 186/259) CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments (Adarsh Jagan Sathyamoorthy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adarsh Jagan Sathyamoorthy, Kasun Weerakoon, Mohamed Elnoor, Anuj Zore, Brian Ichter, Fei Xia, Jie Tan, Wenhao Yu, Dinesh Manocha. (2024)<br><strong>CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments</strong><br><button class=copy-to-clipboard title="CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 43<br>Keywords: Multi-modal, Zero-shot, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15637v1.pdf filename=2403.15637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present ConVOI, a novel method for autonomous robot navigation in real-world indoor and outdoor environments using Vision Language Models (VLMs). We employ VLMs in two ways: first, we leverage their <b>zero-shot</b> image classification capability to identify the context or scenario (e.g., indoor corridor, outdoor terrain, crosswalk, etc) of the robot&rsquo;s surroundings, and formulate context-based navigation behaviors as simple text <b>prompts</b> (e.g. ``stay on the pavement"). Second, we utilize their state-of-the-art semantic understanding and logical <b>reasoning</b> capabilities to compute a suitable trajectory given the identified context. To this end, we propose a novel <b>multi-modal</b> visual marking approach to annotate the obstacle-free regions in the RGB image used as input to the VLM with numbers, by correlating it with a local occupancy map of the environment. The marked numbers ground image locations in the real-world, direct the VLM&rsquo;s attention solely to navigable locations, and elucidate the spatial relationships between them and terrains depicted in the image to the VLM. Next, we query the VLM to select numbers on the marked image that satisfy the context-based behavior text <b>prompt,</b> and construct a reference path using the selected numbers. Finally, we propose a method to extrapolate the reference trajectory when the robot&rsquo;s environmental context has not changed to prevent unnecessary VLM queries. We use the reference trajectory to guide a motion planner, and demonstrate that it leads to human-like behaviors (e.g. not cutting through a group of people, using crosswalks, etc.) in various real-world indoor and outdoor scenarios.</p></p class="citation"></blockquote><h3 id=218--187259-srlm-human-in-loop-interactive-social-robot-navigation-with-large-language-model-and-deep-reinforcement-learning-weizheng-wang-et-al-2024>(2/18 | 187/259) SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning (Weizheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizheng Wang, Le Mao, Ruiqi Wang, Byung-Cheol Min. (2024)<br><strong>SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15648v1.pdf filename=2403.15648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An interactive social robotic assistant must provide services in complex and crowded spaces while adapting its behavior based on real-time human language commands or feedback. In this paper, we propose a novel hybrid approach called Social Robot Planner (SRLM), which integrates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> and Deep <b>Reinforcement</b> <b>Learning</b> (DRL) to navigate through human-filled public spaces and provide multiple social services. SRLM infers global planning from human-in-loop commands in real-time, and encodes social information into a <b>LLM-based</b> <b>large</b> <b>navigation</b> <b>model</b> (LNM) for low-level motion execution. Moreover, a DRL-based planner is designed to maintain <b>benchmarking</b> performance, which is blended with LNM by a <b>large</b> <b>feedback</b> <b>model</b> (LFM) to address the instability of current text and <b>LLM-driven</b> LNM. Finally, SRLM demonstrates outstanding performance in extensive experiments. More details about this work are available at: <a href=https://sites.google.com/view/navi-srlm>https://sites.google.com/view/navi-srlm</a></p></p class="citation"></blockquote><h3 id=318--188259-safe-and-stable-teleoperation-of-quadrotor-uavs-under-haptic-shared-autonomy-dawei-zhang-et-al-2024>(3/18 | 188/259) Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy (Dawei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawei Zhang, Roberto Tron. (2024)<br><strong>Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy</strong><br><button class=copy-to-clipboard title="Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15335v1.pdf filename=2403.15335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach that aims to address both safety and stability of a haptic teleoperation system within a framework of Haptic Shared Autonomy (HSA). We use Control Barrier Functions (CBFs) to generate the control input that follows the user&rsquo;s input as closely as possible while guaranteeing safety. In the context of stability of the <b>human-in-the-loop</b> system, we limit the force feedback perceived by the user via a small $L_2$-gain, which is achieved by limiting the control and the force feedback via a differential constraint. Specifically, with the property of HSA, we propose two pathways to design the control and the force feedback: Sequential Control Force (SCF) and Joint Control Force (JCF). Both designs can achieve safety and stability but with different responses to the user&rsquo;s commands. We conducted experimental <b>simulations</b> to evaluate and investigate the properties of the designed methods. We also tested the proposed method on a physical quadrotor UAV and a haptic interface.</p></p class="citation"></blockquote><h3 id=418--189259-guided-decoding-for-robot-motion-generation-and-adaption-nutan-chen-et-al-2024>(4/18 | 189/259) Guided Decoding for Robot Motion Generation and Adaption (Nutan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt. (2024)<br><strong>Guided Decoding for Robot Motion Generation and Adaption</strong><br><button class=copy-to-clipboard title="Guided Decoding for Robot Motion Generation and Adaption" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Autoencoder, Variational Autoencoder, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15239v1.pdf filename=2403.15239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories. We train a <b>transformer</b> architecture on a large dataset of simulated trajectories. This architecture, based on a conditional <b>variational</b> <b>autoencoder</b> <b>transformer,</b> learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it can adapt trajectories in navigating complex tasks, including obstacle avoidance, via points, and meeting velocity and acceleration constraints, across platforms.</p></p class="citation"></blockquote><h3 id=518--190259-boundary-aware-value-function-generation-for-safe-stochastic-motion-planning-junhong-xu-et-al-2024>(5/18 | 190/259) Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning (Junhong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhong Xu, Kai Yin, Jason M. Gregory, Kris Hauser, Lantao Liu. (2024)<br><strong>Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning</strong><br><button class=copy-to-clipboard title="Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14956v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14956v1.pdf filename=2403.14956v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigation safety is critical for many autonomous systems such as self-driving vehicles in an urban environment. It requires an explicit consideration of boundary constraints that describe the borders of any infeasible, non-navigable, or unsafe regions. We propose a principled boundary-aware safe stochastic planning framework with promising results. Our method generates a value function that can strictly distinguish the state values between free (safe) and non-navigable (boundary) spaces in the continuous state, naturally leading to a safe boundary-aware policy. At the core of our solution lies a seamless integration of finite elements and kernel-based functions, where the finite elements allow us to characterize safety-critical states&rsquo; borders accurately, and the kernel-based function speeds up computation for the non-safety-critical states. The proposed method was evaluated through extensive <b>simulations</b> and demonstrated safe navigation behaviors in mobile navigation tasks. Additionally, we demonstrate that our approach can maneuver safely and efficiently in cluttered real-world environments using a ground vehicle with strong external disturbances, such as navigating on a slippery floor and against external <b>human</b> <b>intervention.</b></p></p class="citation"></blockquote><h3 id=618--191259-global-games-with-negative-feedback-for-autonomous-colony-maintenance-using-robot-teams-logan-e-beaver-2024>(6/18 | 191/259) Global Games with Negative Feedback for Autonomous Colony Maintenance using Robot Teams (Logan E. Beaver, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Logan E. Beaver. (2024)<br><strong>Global Games with Negative Feedback for Autonomous Colony Maintenance using Robot Teams</strong><br><button class=copy-to-clipboard title="Global Games with Negative Feedback for Autonomous Colony Maintenance using Robot Teams" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15621v1.pdf filename=2403.15621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article we address the colony maintenance problem, where a team of robots are tasked with continuously maintaining the energy supply of an autonomous colony. We model this as a global game, where robots measure the energy level of a central nest to determine whether or not to forage for energy sources. We design a mechanism that avoids the trivial equilibrium where all robots always forage. Furthermore, we demonstrate that when the game is played iteratively a negative feedback term stabilizes the number of foraging robots at a non-trivial Nash equilibrium. We compare our approach qualitatively to existing global games, where a positive positive feedback term admits threshold-based decision making, and encourages many robots to forage simultaneously. We discuss how positive feedback can lead to a cascading failure in the presence of a human who recruits robots for external tasks, and we demonstrate the performance of our approach in <b>simulation.</b></p></p class="citation"></blockquote><h3 id=718--192259-gesture-controlled-aerial-robot-formation-for-human-swarm-interaction-in-safety-monitoring-applications-vít-krátký-et-al-2024>(7/18 | 192/259) Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications (Vít Krátký et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vít Krátký, Giuseppe Silano, Matouš Vrba, Christos Papaioannidis, Ioannis Mademlis, Robert Pěnička, Ioannis Pitas, Martin Saska. (2024)<br><strong>Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications</strong><br><button class=copy-to-clipboard title="Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15333v1.pdf filename=2403.15333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a formation control approach for contactless gesture-based Human-Swarm Interaction (HSI) between a team of multi-rotor Unmanned Aerial Vehicles (UAVs) and a human worker. The approach is intended for monitoring the safety of human workers, especially those working at heights. In the proposed dynamic formation scheme, one UAV acts as the leader of the formation and is equipped with sensors for human worker detection and gesture recognition. The follower UAVs maintain a predetermined formation relative to the worker&rsquo;s position, thereby providing additional perspectives of the monitored scene. Hand gestures allow the human worker to specify movements and action commands for the UAV team and initiate other mission-related commands without the need for an additional communication channel or specific markers. Together with a novel unified human detection and tracking algorithm, human pose estimation approach and gesture detection pipeline, the proposed approach forms a first instance of an HSI system incorporating all these modules onboard real-world UAVs. <b>Simulations</b> and field experiments with three UAVs and a human worker in a mock-up scenario showcase the effectiveness and responsiveness of the proposed approach.</p></p class="citation"></blockquote><h3 id=818--193259-infrastructure-assisted-collaborative-perception-in-automated-valet-parking-a-safety-perspective-yukuan-jia-et-al-2024>(8/18 | 193/259) Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective (Yukuan Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yukuan Jia, Jiawen Zhang, Shimeng Lu, Baokang Fan, Ruiqing Mao, Sheng Zhou, Zhisheng Niu. (2024)<br><strong>Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective</strong><br><button class=copy-to-clipboard title="Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Autoencoder, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15156v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15156v1.pdf filename=2403.15156v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Environmental perception in Automated Valet Parking (AVP) has been a challenging task due to severe occlusions in parking garages. Although Collaborative Perception (CP) can be applied to broaden the field of view of connected vehicles, the limited bandwidth of vehicular communications restricts its application. In this work, we propose a BEV feature-based CP network architecture for infrastructure-assisted AVP systems. The model takes the roadside camera and LiDAR as optional inputs and adaptively fuses them with onboard sensors in a unified BEV representation. <b>Autoencoder</b> and downsampling are applied for channel-wise and spatial-wise dimension reduction, while sparsification and <b>quantization</b> further compress the feature map with little loss in data precision. Combining these techniques, the size of a BEV feature map is effectively compressed to fit in the feasible data rate of the NR-V2X network. With the synthetic AVP dataset, we observe that CP can effectively increase perception performance, especially for pedestrians. Moreover, the advantage of infrastructure-assisted CP is demonstrated in two typical safety-critical scenarios in the AVP setting, increasing the maximum safe cruising speed by up to 3m/s in both scenarios.</p></p class="citation"></blockquote><h3 id=918--194259-rhino-vr-experience-teaching-mobile-robotics-concepts-in-an-interactive-museum-exhibit-erik-schlachhoff-et-al-2024>(9/18 | 194/259) RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit (Erik Schlachhoff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Schlachhoff, Nils Dengler, Leif Van Holland, Patrick Stotko, Jorge de Heuvel, Reinhard Klein, Maren Bennewitz. (2024)<br><strong>RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit</strong><br><button class=copy-to-clipboard title="RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15151v1.pdf filename=2403.15151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 1997, the very first tour guide robot RHINO was deployed in a museum in Germany. With the ability to navigate autonomously through the environment, the robot gave tours to over 2,000 visitors. Today, RHINO itself has become an exhibit and is no longer operational. In this paper, we present RHINO-VR, an interactive museum exhibit using virtual reality (VR) that allows museum visitors to experience the historical robot RHINO in operation in a virtual museum. RHINO-VR, unlike static exhibits, enables users to familiarize themselves with basic mobile robotics concepts without the fear of damaging the exhibit. In the virtual environment, the user is able to interact with RHINO in VR by pointing to a location to which the robot should navigate and observing the corresponding actions of the robot. To include other visitors who cannot use the VR, we provide an external observation view to make RHINO visible to them. We evaluated our system by measuring the frame rate of the VR <b>simulation,</b> comparing the generated virtual 3D models with the originals, and conducting a user study. The user-study showed that RHINO-VR improved the visitors&rsquo; understanding of the robot&rsquo;s functionality and that they would recommend experiencing the VR exhibit to others.</p></p class="citation"></blockquote><h3 id=1018--195259-a-twin-delayed-deep-deterministic-policy-gradient-algorithm-for-autonomous-ground-vehicle-navigation-via-digital-twin-perception-awareness-kabirat-olayemi-et-al-2024>(10/18 | 195/259) A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness (Kabirat Olayemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kabirat Olayemi, Mien Van, Sean McLoone, Yuzhu Sun, Jack Close, Nguyen Minh Nhat, Stephen McIlvanna. (2024)<br><strong>A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness</strong><br><button class=copy-to-clipboard title="A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15067v1.pdf filename=2403.15067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous ground vehicle (UGV) navigation has the potential to revolutionize the transportation system by increasing accessibility to disabled people, ensure safety and convenience of use. However, UGV requires extensive and efficient testing and evaluation to ensure its acceptance for public use. This testing are mostly done in a simulator which result to sim2real transfer gap. In this paper, we propose a digital twin perception awareness approach for the control of robot navigation without prior creation of the virtual environment (VT) environment state. To achieve this, we develop a twin delayed deep deterministic policy gradient (TD3) algorithm that ensures collision avoidance and goal-based path planning. We demonstrate the performance of our approach on different environment dynamics. We show that our approach is capable of efficiently avoiding collision with obstacles and navigating to its desired destination, while at the same time safely avoids obstacles using the information received from the LIDAR sensor mounted on the robot. Our approach bridges the gap between sim-to-real transfer and contributes to the adoption of UGVs in real world. We validate our approach in <b>simulation</b> and a real-world application in an office space.</p></p class="citation"></blockquote><h3 id=1118--196259-linear-quadratic-guidance-law-for-joint-motion-planning-of-a-pursuer-turret-assembly-bhargav-jha-et-al-2024>(11/18 | 196/259) Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly (Bhargav Jha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhargav Jha, Shaunak Bopardikar, Alexander Von Moll, David Casbeer. (2024)<br><strong>Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly</strong><br><button class=copy-to-clipboard title="Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14997v1.pdf filename=2403.14997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents joint motion planning of a vehicle with an attached rotating turret. The turret has a limited range as well as the field of view. The objective is capture a maneuvering target such that at the terminal time it is withing the field-of-view and range limits. Catering to it, we present a minimum effort guidance law that commensurate for the turn rate abilities of the vehicle and the turret. The guidance law is obtained using linearization about the collision triangle and admits an analytical solution. <b>Simulation</b> results are presented to exemplify the cooperation between the turret and the vehicle.</p></p class="citation"></blockquote><h3 id=1218--197259-oceanplan-hierarchical-planning-and-replanning-for-natural-language-auv-piloting-in-large-scale-unexplored-ocean-environments-ruochu-yang-et-al-2024>(12/18 | 197/259) OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments (Ruochu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruochu Yang, Fumin Zhang, Mengxue Hou. (2024)<br><strong>OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments</strong><br><button class=copy-to-clipboard title="OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15369v1.pdf filename=2403.15369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a hierarchical <b>LLM-task-motion</b> planning and replanning framework to efficiently ground an abstracted human command into tangible Autonomous Underwater Vehicle (AUV) control through enhanced representations of the world. We also incorporate a holistic replanner to provide real-world feedback with all planners for robust AUV operation. While there has been extensive research in bridging the gap between <b>LLMs</b> and robotic missions, they are unable to guarantee success of AUV applications in the vast and unknown ocean environment. To tackle specific challenges in marine robotics, we design a hierarchical planner to compose executable motion plans, which achieves planning efficiency and solution quality by decomposing long-horizon missions into sub-tasks. At the same time, real-time data stream is obtained by a replanner to address environmental uncertainties during plan execution. Experiments validate that our proposed framework delivers successful AUV performance of long-duration missions through natural language piloting.</p></p class="citation"></blockquote><h3 id=1318--198259-hortibot-an-adaptive-multi-arm-system-for-robotic-horticulture-of-sweet-peppers-christian-lenz-et-al-2024>(13/18 | 198/259) HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers (Christian Lenz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Lenz, Rohit Menon, Michael Schreiber, Melvin Paul Jacob, Sven Behnke, Maren Bennewitz. (2024)<br><strong>HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers</strong><br><button class=copy-to-clipboard title="HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15306v1.pdf filename=2403.15306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Horticultural tasks such as <b>pruning</b> and selective harvesting are labor intensive and horticultural staff are hard to find. Automating these tasks is challenging due to the semi-structured greenhouse workspaces, changing environmental conditions such as lighting, dense plant growth with many occlusions, and the need for gentle manipulation of non-rigid plant organs. In this work, we present the three-armed system HortiBot, with two arms for manipulation and a third arm as an articulated head for active perception using stereo cameras. Its perception system detects not only peppers, but also peduncles and stems in real time, and performs online data association to build a world model of pepper plants. Collision-aware online trajectory generation allows all three arms to safely track their respective targets for observation, grasping, and cutting. We integrated perception and manipulation to perform selective harvesting of peppers and evaluated the system in lab experiments. Using active perception coupled with end-effector force torque sensing for compliant manipulation, HortiBot achieves high success rates.</p></p class="citation"></blockquote><h3 id=1418--199259-trihelper-zero-shot-object-navigation-with-dynamic-assistance-lingfeng-zhang-et-al-2024>(14/18 | 199/259) TriHelper: Zero-Shot Object Navigation with Dynamic Assistance (Lingfeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingfeng Zhang, Qiang Zhang, Hao Wang, Erjia Xiao, Zixuan Jiang, Honglei Chen, Renjing Xu. (2024)<br><strong>TriHelper: Zero-Shot Object Navigation with Dynamic Assistance</strong><br><button class=copy-to-clipboard title="TriHelper: Zero-Shot Object Navigation with Dynamic Assistance" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15223v1.pdf filename=2403.15223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigating toward specific objects in unknown environments without additional training, known as <b>Zero-Shot</b> object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in <b>Zero-Shot</b> object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent&rsquo;s navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.</p></p class="citation"></blockquote><h3 id=1518--200259-crplace-camera-radar-fusion-with-bev-representation-for-place-recognition-shaowei-fu-et-al-2024>(15/18 | 200/259) CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition (Shaowei Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaowei Fu, Yifan Duan, Yao Li, Chengzhen Meng, Yingjie Wang, Jianmin Ji, Yanyong Zhang. (2024)<br><strong>CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition</strong><br><button class=copy-to-clipboard title="CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15183v1.pdf filename=2403.15183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of complementary characteristics from camera and radar data has emerged as an effective approach in 3D <b>object</b> <b>detection.</b> However, such fusion-based methods remain unexplored for place recognition, an equally important task for autonomous systems. Given that place recognition relies on the similarity between a query scene and the corresponding candidate scene, the stationary background of a scene is expected to play a crucial role in the task. As such, current well-designed camera-radar fusion methods for 3D <b>object</b> <b>detection</b> can hardly take effect in place recognition because they mainly focus on dynamic foreground <b>objects.</b> <b>In</b> this paper, a background-attentive camera-radar fusion-based method, named CRPlace, is proposed to generate background-attentive global descriptors from multi-view images and radar point clouds for accurate place recognition. To extract stationary background features effectively, we design an adaptive module that generates the background-attentive mask by utilizing the camera BEV feature and radar dynamic points. With the guidance of a background mask, we devise a bidirectional cross-attention-based spatial fusion strategy to facilitate comprehensive spatial interaction between the background information of the camera BEV feature and the radar BEV feature. As the first camera-radar fusion-based place recognition network, CRPlace has been evaluated thoroughly on the nuScenes dataset. The results show that our algorithm outperforms a variety of baseline methods across a comprehensive set of metrics (recall@1 reaches 91.2%).</p></p class="citation"></blockquote><h3 id=1618--201259-alpine-a-climbing-robot-for-operations-in-mountain-environments-michele-focchi-et-al-2024>(16/18 | 201/259) ALPINE: a climbing robot for operations in mountain environments (Michele Focchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Marco Frego, Angelika Peer, Luigi Palopoli. (2024)<br><strong>ALPINE: a climbing robot for operations in mountain environments</strong><br><button class=copy-to-clipboard title="ALPINE: a climbing robot for operations in mountain environments" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15142v1.pdf filename=2403.15142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mountain slopes are perfect examples of harsh environments in which <b>humans</b> <b>are</b> required to perform difficult and dangerous operations such as removing unstable boulders, dangerous vegetation or deploying safety nets. A good replacement for <b>human</b> <b>intervention</b> can be offered by climbing robots. The different solutions existing in the literature are not up to the task for the difficulty of the requirements (navigation, heavy payloads, flexibility in the execution of the tasks). In this paper, we propose a robotic platform that can fill this gap. Our solution is based on a robot that hangs on ropes, and uses a retractable leg to jump away from the mountain walls. Our package of mechanical solutions, along with the algorithms developed for motion planning and control, delivers swift navigation on irregular and steep slopes, the possibility to overcome or travel around significant natural barriers, and the ability to carry heavy payloads and execute complex tasks. In the paper, we give a full account of our main design and algorithmic choices and show the feasibility of the solution through a large number of physically simulated scenarios.</p></p class="citation"></blockquote><h3 id=1718--202259-subequivariant-reinforcement-learning-framework-for-coordinated-motion-control-haoyu-wang-et-al-2024>(17/18 | 202/259) Subequivariant Reinforcement Learning Framework for Coordinated Motion Control (Haoyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Wang, Xiaoyu Tan, Xihe Qiu, Chao Qu. (2024)<br><strong>Subequivariant Reinforcement Learning Framework for Coordinated Motion Control</strong><br><button class=copy-to-clipboard title="Subequivariant Reinforcement Learning Framework for Coordinated Motion Control" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15100v1.pdf filename=2403.15100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective coordination is crucial for motion control with <b>reinforcement</b> <b>learning,</b> especially as the complexity of agents and their motions increases. However, many existing methods struggle to account for the intricate dependencies between joints. We introduce CoordiGraph, a novel architecture that leverages subequivariant principles from physics to enhance coordination of motion control with <b>reinforcement</b> <b>learning.</b> This method embeds the principles of equivariance as inherent patterns in the learning process under gravity influence, which aids in modeling the nuanced relationships between joints vital for motion control. Through extensive experimentation with sophisticated agents in diverse environments, we highlight the merits of our approach. Compared to current leading methods, CoordiGraph notably enhances generalization and sample efficiency.</p></p class="citation"></blockquote><h3 id=1818--203259-rethinking-6-dof-grasp-detection-a-flexible-framework-for-high-quality-grasping-wei-tang-et-al-2024>(18/18 | 203/259) Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping (Wei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Tang, Siang Chen, Pengwei Xie, Dingchang Hu, Wenming Yang, Guijin Wang. (2024)<br><strong>Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping</strong><br><button class=copy-to-clipboard title="Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15054v1.pdf filename=2403.15054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic grasping is a primitive skill for complex tasks and is fundamental to intelligence. For general 6-Dof grasping, most previous methods directly extract scene-level semantic or geometric information, while few of them consider the suitability for various downstream applications, such as target-oriented grasping. Addressing this issue, we rethink 6-Dof grasp detection from a grasp-centric view and propose a versatile grasp framework capable of handling both scene-level and target-oriented grasping. Our framework, FlexLoG, is composed of a Flexible Guidance Module and a Local Grasp Model. Specifically, the Flexible Guidance Module is compatible with both global (e.g., grasp heatmap) and local (e.g., visual <b>grounding)</b> guidance, enabling the generation of high-quality grasps across various tasks. The Local Grasp Model focuses on object-agnostic regional points and predicts grasps locally and intently. Experiment results reveal that our framework achieves over 18% and 23% improvement on unseen splits of the GraspNet-1Billion Dataset. Furthermore, real-world robotic tests in three distinct settings yield a 95% success rate.</p></p class="citation"></blockquote><h2 id=q-fincp-2>q-fin.CP (2)</h2><h3 id=12--204259-construction-of-a-japanese-financial-benchmark-for-large-language-models-masanori-hirano-2024>(1/2 | 204/259) Construction of a Japanese Financial Benchmark for Large Language Models (Masanori Hirano, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masanori Hirano. (2024)<br><strong>Construction of a Japanese Financial Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="Construction of a Japanese Financial Benchmark for Large Language Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-CL, q-fin-CP, q-fin.CP<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15062v1.pdf filename=2403.15062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the recent development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for <b>benchmarks</b> to evaluate the performance of current <b>LLMs</b> in each domain. Therefore, in this study, we constructed a <b>benchmark</b> comprising multiple tasks specific to the Japanese and financial domains and performed <b>benchmark</b> measurements on some models. Consequently, we confirmed that <b>GPT-4</b> is currently outstanding, and that the constructed <b>benchmarks</b> function effectively. According to our analysis, our <b>benchmark</b> can differentiate <b>benchmark</b> scores among models in all performance ranges by combining tasks with different difficulties.</p></p class="citation"></blockquote><h3 id=22--205259-robust-utility-optimization-via-a-gan-approach-florian-krach-et-al-2024>(2/2 | 205/259) Robust Utility Optimization via a GAN Approach (Florian Krach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Krach, Josef Teichmann, Hanna Wutte. (2024)<br><strong>Robust Utility Optimization via a GAN Approach</strong><br><button class=copy-to-clipboard title="Robust Utility Optimization via a GAN Approach" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: 91-08, 68T07, 91G10, 91G60, cs-LG, q-fin-CP, q-fin-MF, q-fin-PM, q-fin.CP<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15243v1.pdf filename=2403.15243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robust utility optimization enables an investor to deal with market uncertainty in a structured way, with the goal of maximizing the worst-case outcome. In this work, we propose a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> approach to (approximately) solve robust utility optimization problems in general and realistic settings. In particular, we model both the investor and the market by neural networks (NN) and train them in a mini-max zero-sum game. This approach is applicable for any continuous utility function and in realistic market settings with trading costs, where only observable information of the market can be used. A large empirical study shows the versatile usability of our method. Whenever an optimal reference strategy is available, our method performs on par with it and in the (many) settings without known optimal strategy, our method outperforms all other reference strategies. Moreover, we can conclude from our study that the trained path-dependent strategies do not outperform Markovian ones. Lastly, we uncover that our <b>generative</b> <b>approach</b> <b>for</b> learning optimal, (non-) robust investments under trading costs generates universally applicable alternatives to well known asymptotic strategies of idealized settings.</p></p class="citation"></blockquote><h2 id=csit-8>cs.IT (8)</h2><h3 id=18--206259-mutual-information-of-a-class-of-poisson-type-channels-using-markov-renewal-theory-maximilian-gehri-et-al-2024>(1/8 | 206/259) Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory (Maximilian Gehri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Gehri, Nicolai Engelmann, Heinz Koeppl. (2024)<br><strong>Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory</strong><br><button class=copy-to-clipboard title="Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Mutual Information, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15221v1.pdf filename=2403.15221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>mutual</b> <b>information</b> (MI) of Poisson-type channels has been linked to a filtering problem since the 70s, but its evaluation for specific <b>continuous-time,</b> <b>discrete-state</b> systems remains a demanding task. As an advantage, Markov renewal processes (MrP) retain their renewal property under state space filtering. This offers a way to solve the filtering problem analytically for small systems. We consider a class of communication systems $X \to Y$ that can be derived from a MrP by a custom filtering procedure. For the subclasses, where (i) $Y$ is a renewal process or (ii) $(X,Y)$ belongs to a class of MrPs, we provide an evolution equation for finite transmission duration $T>0$ and limit theorems for $T \to \infty$ that facilitate <b>simulation-free</b> evaluation of the MI $\mathbb{I}(X_{[0,T]}; Y_{[0,T]})$ and its associated <b>mutual</b> <b>information</b> rate (MIR). In other cases, <b>simulation</b> cost is significantly reduced. We show that systems with an additional $X$-modulating level $C$, which statically chooses between different processes $X_{[0,T]}(c)$, can naturally be included in our framework thereby giving an expression for $\mathbb{I}(C; Y_{[0,T]})$. The theoretical framework is showcased in an application to bacterial gene expression, where filtering is analytically tractable.</p></p class="citation"></blockquote><h3 id=28--207259-information-rates-of-successive-interference-cancellation-for-optical-fiber-alex-jäger-et-al-2024>(2/8 | 207/259) Information Rates of Successive Interference Cancellation for Optical Fiber (Alex Jäger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Jäger, Gerhard Kramer. (2024)<br><strong>Information Rates of Successive Interference Cancellation for Optical Fiber</strong><br><button class=copy-to-clipboard title="Information Rates of Successive Interference Cancellation for Optical Fiber" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15240v1.pdf filename=2403.15240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Successive interference cancellation (SIC) is used to approach the achievable information rates (AIRs) of joint detection and decoding for long-haul optical fiber links. The AIRs of memoryless ring constellations are compared to those of circularly symmetric complex Gaussian modulation for surrogate channel models with correlated phase noise. <b>Simulations</b> are performed for 1000 km of standard single-mode fiber with ideal Raman amplification. In this setup, 32 rings and 16 SIC-stages with Gaussian <b>message-passing</b> receivers achieve the AIR peaks of previous work. The computational complexity scales in proportion to the number of SIC-stages, where one stage has the complexity of separate detection and decoding.</p></p class="citation"></blockquote><h3 id=38--208259-uplink-soft-handover-for-leo-constellations-how-strong-the-inter-satellite-link-should-be-houcem-ben-salem-et-al-2024>(3/8 | 208/259) Uplink soft handover for LEO constellations: how strong the inter-satellite link should be (Houcem Ben Salem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Houcem Ben Salem, Alberto Tarable, Alessandro Nordio, Behrooz Makki. (2024)<br><strong>Uplink soft handover for LEO constellations: how strong the inter-satellite link should be</strong><br><button class=copy-to-clipboard title="Uplink soft handover for LEO constellations: how strong the inter-satellite link should be" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15131v1.pdf filename=2403.15131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a constellation of low-earth-orbit (LEO) satellites connected to a handheld device on the ground. Due to the very large orbital speed, an effective handover strategy becomes of paramount importance. In particular, we study the benefits of soft handover in the uplink from the physical-layer point of view. We give a realistic model for both the ground-to-satellite and the inter-satellite links, following the 3GPP channel model for the former. We suppose that, during handover from a serving satellite to a target satellite, one of the two satellites forwards the received signal from the ground user to the other, thus acting as a relay. We quantify through <b>simulations</b> the loss of hard handover, compared to soft handover. For the latter, we test both amplify-and-forward (AF) and decode-and-forward (DF) relaying techniques and verify that, at least in the simulated conditions, DF does not repay, in terms of block error rate (BLER), the increase of complexity with respect to AF. Also, we study the effect of the LEO constellation size on the network BLER. Finally, we show that, with soft handover, the impact of misalignment on the inter-satellite link is severe, especially at optical frequencies.</p></p class="citation"></blockquote><h3 id=48--209259-range-angle-estimation-for-fda-mimo-system-with-frequency-offset-mengjiang-sun-et-al-2024>(4/8 | 209/259) Range-Angle Estimation for FDA-MIMO System With Frequency Offset (Mengjiang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengjiang Sun, Peng Chen, Zhenxin Cao. (2024)<br><strong>Range-Angle Estimation for FDA-MIMO System With Frequency Offset</strong><br><button class=copy-to-clipboard title="Range-Angle Estimation for FDA-MIMO System With Frequency Offset" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14978v1.pdf filename=2403.14978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frequency diverse array multiple-input multiple-output (FDA-MIMO) radar differs from the traditional phased array (PA) radar, and can form range-angle-dependent beampattern and differentiate between closely spaced targets sharing the same angle but occupying distinct range cells. In the FDA-MIMO radar, target range estimation is achieved by employing a subtle frequency variation between adjacent array antennas, so the estimation performance is degraded severely in a practical scenario with frequency offset. In this paper, the range-angle estimation problem for FDA-MIMO radar is considered with frequency offsets in both transmitting and receiving arrays. First, we build a system model for the FDA-MIMO radar with transmitting and receiving frequency offsets. Then, the frequency offset is transferred into an equalized additional noise. The noise characteristics are analyzed in detail theoretically, together with the influence on the range-angle estimation. Moreover, since the effect of the transmitting frequency offset is similar to additional colored noise, denoising algorithms are introduced to mitigate the performance deterioration caused by the frequency offset. Finally, Cram'{e}r-Rao lower bounds (CRLB) for the range-angle estimation are derived in the scenario with the frequency offsets. <b>Simulation</b> results show the analysis of frequency offset and the corresponding estimation performance using different algorithms.</p></p class="citation"></blockquote><h3 id=58--210259-secure-outage-analysis-for-ris-aided-miso-systems-with-randomly-located-eavesdroppers-wei-shi-et-al-2024>(5/8 | 210/259) Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers (Wei Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Shi, Jindan Xu, Wei Xu, Chau Yuen, A. Lee Swindlehurst, Xiaohu You, Chunming Zhao. (2024)<br><strong>Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers</strong><br><button class=copy-to-clipboard title="Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14911v1.pdf filename=2403.14911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the physical layer security of an RIS-assisted multiple-antenna communication system with randomly located eavesdroppers. The exact distributions of the received signal-to-noise-ratios (SNRs) at the legitimate user and the eavesdroppers located according to a Poisson point process (PPP) are derived, and a closed-form expression for the secrecy outage probability (SOP) is obtained. It is revealed that the secrecy performance is mainly affected by the number of RIS reflecting elements, and the impact of the transmit antennas and transmit power at the base station is marginal. In addition, when the locations of the randomly located eavesdroppers are unknown, deploying the RIS closer to the legitimate user rather than to the base station is shown to be more efficient. We also perform an analytical study demonstrating that the secrecy diversity order depends on the path loss exponent of the RIS-to-ground links. Finally, numerical <b>simulations</b> are conducted to verify the accuracy of these theoretical observations.</p></p class="citation"></blockquote><h3 id=68--211259-ris-assisted-cell-free-massive-mimo-systems-with-two-timescale-design-and-hardware-impairments-jianxin-dai-et-al-2024>(6/8 | 211/259) RIS-assisted Cell-Free Massive MIMO Systems With Two-Timescale Design and Hardware Impairments (Jianxin Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianxin Dai, Jin Ge, Kangda Zhi, Cunhua Pan, Youguo Wang. (2024)<br><strong>RIS-assisted Cell-Free Massive MIMO Systems With Two-Timescale Design and Hardware Impairments</strong><br><button class=copy-to-clipboard title="RIS-assisted Cell-Free Massive MIMO Systems With Two-Timescale Design and Hardware Impairments" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15588v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15588v2.pdf filename=2403.15588v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating the reconfigurable intelligent surface (RIS) into a cell-free massive multiple-input multiple-output (CF-mMIMO) system is an effective solution to achieve high system capacity with low cost and power consumption. However, existing works of RIS-assisted systems mostly assumed perfect hardware, while the impact of hardware impairments (HWIs) is generally ignored. In this paper, we consider the general Rician fading channel and uplink transmission of the RIS-assisted CF-mMIMO system under transceiver impairments and RIS phase noise. To reduce the feedback overhead and power consumption, we propose a two-timescale transmission scheme to optimize the passive beamformers at RISs with statistical channel state information (CSI), while transmit beamformers at access points (APs) are designed based on instantaneous CSI. Also, the maximum ratio combining (MRC) detection is applied to the central processing unit (CPU). On this basis, we derive the closed-form approximate expression of the achievable rate, based on which the impact of HWIs and the power <b>scaling</b> <b>laws</b> are analyzed to draw useful theoretical insights. To maximize the users&rsquo; sum rate or minimum rate, we first transform our rate expression into a tractable form, and then optimize the phase shifts of RISs based on an accelerated gradient ascent method. Finally, numerical results are presented to demonstrate the correctness of our derived expressions and validate the previous analysis, which provide some guidelines for the practical application of the imperfect RISs in the CF-mMIMO with transceiver HWIs.</p></p class="citation"></blockquote><h3 id=78--212259-robust-resource-allocation-for-star-ris-assisted-swipt-systems-guangyu-zhu-et-al-2024>(7/8 | 212/259) Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems (Guangyu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyu Zhu, Xidong Mu, Li Guo, Ao Huang, Shibiao Xu. (2024)<br><strong>Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems</strong><br><button class=copy-to-clipboard title="Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15145v1.pdf filename=2403.15145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted simultaneous wireless information and power transfer (SWIPT) system is proposed. More particularly, an STAR-RIS is deployed to assist in the information/power transfer from a multi-antenna access point (AP) to multiple single-antenna information users (IUs) and energy users (EUs), where two practical STAR-RIS operating protocols, namely energy splitting (ES) and time switching (TS), are employed. Under the imperfect channel state information (CSI) condition, a multi-objective optimization problem (MOOP) framework, that simultaneously maximizes the minimum data rate and minimum harvested power, is employed to investigate the fundamental rate-energy trade-off between IUs and EUs. To obtain the optimal robust resource allocation strategy, the MOOP is first transformed into a single-objective optimization problem (SOOP) via the {\epsilon}-constraint method, which is then reformulated by approximating semi-infinite inequality constraints with the S-procedure. For ES, an alternating optimization (AO)-based algorithm is proposed to jointly design AP active beamforming and STAR-RIS passive beamforming, where a penalty method is leveraged in STAR-RIS beamforming design. Furthermore, the developed algorithm is extended to optimize the time allocation policy and beamforming vectors in a two-layer iterative manner for TS. Numerical results reveal that: 1) deploying STAR-RISs achieves a significant performance gain over conventional RISs, especially in terms of harvested power for EUs; 2) the ES protocol obtains a better user <b>fairness</b> performance when focusing only on IUs or EUs, while the TS protocol yields a better balance between IUs and EUs; 3) the imperfect CSI affects IUs more significantly than EUs, whereas TS can confer a more robust design to attenuate these effects.</p></p class="citation"></blockquote><h3 id=88--213259-coexisting-passive-ris-and-active-relay-assisted-noma-systems-ao-huang-et-al-2024>(8/8 | 213/259) Coexisting Passive RIS and Active Relay Assisted NOMA Systems (Ao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ao Huang, Li Guo, Xidong Mu, Chao Dong, Yuanwei Liu. (2024)<br><strong>Coexisting Passive RIS and Active Relay Assisted NOMA Systems</strong><br><button class=copy-to-clipboard title="Coexisting Passive RIS and Active Relay Assisted NOMA Systems" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15130v1.pdf filename=2403.15130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel coexisting passive reconfigurable intelligent surface (RIS) and active decode-and-forward (DF) relay assisted non-orthogonal multiple access (NOMA) transmission framework is proposed. In particular, two communication protocols are conceived, namely Hybrid NOMA (H-NOMA) and Full NOMA (F-NOMA). Based on the proposed two protocols, both the sum rate maximization and max-min rate <b>fairness</b> problems are formulated for jointly optimizing the power allocation at the access point and relay as well as the passive beamforming design at the RIS. To tackle the non-convex problems, an alternating optimization (AO) based algorithm is first developed, where the transmit power and the RIS phase-shift are alternatingly optimized by leveraging the two-dimensional search and rank-relaxed difference-of-convex (DC) programming, respectively. Then, a two-layer penalty based joint optimization (JO) algorithm is developed to jointly optimize the resource allocation coefficients within each iteration. Finally, numerical results demonstrate that: i) the proposed coexisting RIS and relay assisted transmission framework is capable of achieving a significant user performance improvement than conventional schemes without RIS or relay; ii) compared with the AO algorithm, the JO algorithm requires less execution time at the cost of a slight performance loss; and iii) the H-NOMA and F-NOMA protocols are generally preferable for ensuring user rate <b>fairness</b> and enhancing user sum rate, respectively.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--214259-allspark-workload-orchestration-for-visual-transformers-on-processing-in-memory-systems-mengke-ge-et-al-2024>(1/1 | 214/259) Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems (Mengke Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengke Ge, Junpeng Wang, Binhan Chen, Yingjian Zhong, Haitao Du, Song Chen, Yi Kang. (2024)<br><strong>Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems</strong><br><button class=copy-to-clipboard title="Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15069v1.pdf filename=2403.15069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>Transformers</b> has revolutionized computer vision, offering a powerful alternative to <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance. Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate <b>Transformer</b> with memory-intensive operations. However, the crucial challenge lies in efficiently deploying the entire model onto a resource-limited PIM system while parallelizing each <b>transformer</b> block with potentially many computational branches based on local attention mechanisms. We present Allspark, which focuses on workload orchestration for visual <b>Transformers</b> on PIM systems, aiming at minimizing inference latency. Firstly, to fully utilize the massive parallelism of PIM, Allspark empolys a finer-grained partitioning scheme for computational branches, and format a systematic layout and interleaved dataflow with maximized data locality and reduced data movement. Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem. Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a greedy-based mapping method to allocate computational branches onto the PIM system and minimize NoC communication costs. Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings 1.2x-24.0x inference speedup for various visual <b>Transformers</b> over baselines, and that Allspark-enriched PIM system yields average speedups of 2.3x and energy savings of 20x-55x over Nvidia V100 GPU.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=11--215259-digital-twin-model-of-colon-electromechanics-for-manometry-prediction-of-laser-tissue-soldering-rené-thierry-djoumessi-et-al-2024>(1/1 | 215/259) Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering (René Thierry Djoumessi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>René Thierry Djoumessi, Pietro Lenarda, Alessio Gizzi, Simone Giusti, Pietro Alduini, Marco Paggi. (2024)<br><strong>Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering</strong><br><button class=copy-to-clipboard title="Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CE, physics-med-ph, physics.med-ph<br>Keyword Score: 35<br>Keywords: Fine-tuning, Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15129v1.pdf filename=2403.15129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The present study introduces an advanced multi-physics and multi-scale modeling approach to investigate in silico colon motility. We introduce a generalized electromechanical framework, integrating cellular electrophysiology and smooth muscle contractility, thus advancing a first-of-its-kind computational model of laser tissue soldering after incision resection. The proposed theoretical framework comprises three main elements: a microstructural material model describing intestine wall <b>geometry</b> and composition of reinforcing fibers, with four fiber families, two active-conductive and two passive; an electrophysiological model describing the propagation of slow waves, based on a fully-coupled nonlinear phenomenological approach; and a thermodynamical consistent mechanical model describing the hyperelastic energetic contributions ruling tissue equilibrium under diverse loading conditions. The active strain approach was adopted to describe tissue electromechanics by exploiting the multiplicative decomposition of the deformation gradient for each active fiber family and solving the governing equations via a staggered finite element scheme. The computational framework was <b>fine-tuned</b> according to state-of-the-art experimental evidence, and extensive numerical analyses allowed us to compare manometric traces computed via numerical <b>simulations</b> with those obtained clinically in human patients. The model proved capable of reproducing both qualitatively and quantitatively high or low-amplitude propagation contractions. Colon motility after laser tissue soldering demonstrates that material properties and couplings of the deposited tissue are critical to reproducing a physiological muscular contraction, thus restoring a proper peristaltic activity.</p></p class="citation"></blockquote><h2 id=eesssy-6>eess.SY (6)</h2><h3 id=16--216259-cascading-blackout-severity-prediction-with-statistically-augmented-graph-neural-networks-joe-gorka-et-al-2024>(1/6 | 216/259) Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks (Joe Gorka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald. (2024)<br><strong>Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15363v1.pdf filename=2403.15363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Higher variability in grid conditions, resulting from growing renewable penetration and increased incidence of extreme weather events, has increased the difficulty of screening for scenarios that may lead to catastrophic cascading failures. Traditional power-flow-based tools for assessing cascading blackout risk are too slow to properly explore the space of possible failures and load/generation patterns. We add to the growing literature of faster <b>graph-neural-network</b> <b>(GNN)-based</b> <b>techniques,</b> developing two novel techniques for the estimation of blackout magnitude from initial grid conditions. First we propose several methods for employing an initial classification step to filter out safe &ldquo;non blackout&rdquo; scenarios prior to magnitude estimation. Second, using insights from the statistical properties of cascading blackouts, we propose a method for facilitating non-local message passing in our <b>GNN</b> models. We validate these two approaches on a large simulated dataset, and show the potential of both to increase blackout size estimation performance.</p></p class="citation"></blockquote><h3 id=26--217259-transactive-local-energy-markets-enable-community-level-resource-coordination-using-individual-rewards-daniel-c-may-et-al-2024>(2/6 | 217/259) Transactive Local Energy Markets Enable Community-Level Resource Coordination Using Individual Rewards (Daniel C. May et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel C. May, Petr Musilek. (2024)<br><strong>Transactive Local Energy Markets Enable Community-Level Resource Coordination Using Individual Rewards</strong><br><button class=copy-to-clipboard title="Transactive Local Energy Markets Enable Community-Level Resource Coordination Using Individual Rewards" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-MA, cs-SY, eess-SY, eess.SY<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15617v1.pdf filename=2403.15617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>ALEX (Autonomous Local Energy eXchange) is an economy-driven, transactive local energy market where each participating building is represented by a rational agent. Relying solely on building-level information, this agent minimizes its electricity bill by automating distributed energy resource utilization and trading. This study examines ALEX&rsquo;s capabilities to align participant and grid-stakeholder interests and assesses ALEX&rsquo;s impact on short- and long-term intermittence using a set of community net-load metrics, such as ramping rate, load factor, and peak load. The policies for ALEX&rsquo;s rational agents are generated using dynamic programming through value iteration in conjunction with iterative best response. This facilitates comparing ALEX and a <b>benchmark</b> energy management system, which optimizes building-level self-consumption, ramping rate, and peak net load. <b>Simulations</b> are performed using the open-source CityLearn2022 dataset to provide a pathway for <b>benchmarking</b> by future studies. The experiments demonstrate that ALEX enables the coordination of distributed energy resources across the community. Remarkably, this community-level coordination occurs even though the system is populated by agents who only access building-level information and selfishly maximize their own relative profit. Compared to the <b>benchmark</b> energy management system, ALEX improves across all metrics.</p></p class="citation"></blockquote><h3 id=36--218259-control-designs-for-critical-continegency-responsible-grid-following-inverters-and-seamless-transitions-to-and-from-grid-forming-modes-jaesang-park-et-al-2024>(3/6 | 218/259) Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes (Jaesang Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaesang Park, Alireza Askarian, Srinivasa Salapaka. (2024)<br><strong>Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes</strong><br><button class=copy-to-clipboard title="Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15380v1.pdf filename=2403.15380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces two control frameworks: one for Grid-Following (GFL) inverters aiding Grid-Forming (GFM) inverters in voltage regulation during large contingency events and optimizing power transactions under normal conditions; and another for seamless transitions between grid-tied and grid-isolated setups, managing voltage transient characteristics. In microgrids, GFM inverters regulate voltage, while GFL inverters handle power transactions. The proposed GFL control detects abrupt load/generation changes, adjusting power transactions using local storage to support GFM inverters during contingencies. Additionally, a transition control ensures smooth GFL-GFM shifts, reducing power and voltage fluctuations. <b>Simulation</b> results validate improved voltage regulation during contingencies and enhanced power tracking during slow changes, alongside minimized transient overshoot.</p></p class="citation"></blockquote><h3 id=46--219259-optimal-data-driven-prediction-and-predictive-control-using-signal-matrix-models-roy-s-smith-et-al-2024>(4/6 | 219/259) Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models (Roy S. Smith et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy S. Smith, Mohamed Abdalmoaty, Mingzhou Yin. (2024)<br><strong>Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models</strong><br><button class=copy-to-clipboard title="Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15329v1.pdf filename=2403.15329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven control uses a past signal trajectory to characterise the input-output behaviour of a system. Willems&rsquo; lemma provides a data-based prediction model allowing a control designer to bypass the step of identifying a state-space or transfer function model. This paper provides a more parsimonious formulation of Willems&rsquo; lemma that separates the model into initial condition matching and predictive control design parts. This avoids the need for regularisers in the predictive control problem that are found in other data-driven predictive control methods. It also gives a closed form expression for the optimal (minimum variance) unbiased predictor of the future output trajectory and applies it for predictive control. <b>Simulation</b> comparisons illustrate very good control performance.</p></p class="citation"></blockquote><h3 id=56--220259-uncertainty-propagation-in-stochastic-systems-via-mixture-models-with-error-quantification-eduardo-figueiredo-et-al-2024>(5/6 | 220/259) Uncertainty Propagation in Stochastic Systems via Mixture Models with Error Quantification (Eduardo Figueiredo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eduardo Figueiredo, Andrea Patane, Morteza Lahijanian, Luca Laurenti. (2024)<br><strong>Uncertainty Propagation in Stochastic Systems via Mixture Models with Error Quantification</strong><br><button class=copy-to-clipboard title="Uncertainty Propagation in Stochastic Systems via Mixture Models with Error Quantification" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15626v1.pdf filename=2403.15626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty propagation in non-linear dynamical systems has become a key problem in various fields including control theory and machine learning. In this work we focus on <b>discrete-time</b> <b>non-linear</b> stochastic dynamical systems. We present a novel approach to approximate the distribution of the system over a given finite time horizon with a mixture of distributions. The key novelty of our approach is that it not only provides tractable approximations for the distribution of a non-linear stochastic system, but also comes with formal guarantees of correctness. In particular, we consider the total variation (TV) distance to quantify the distance between two distributions and derive an upper bound on the TV between the distribution of the original system and the approximating mixture distribution derived with our framework. We show that in various cases of interest, including in the case of Gaussian noise, the resulting bound can be efficiently computed in closed form. This allows us to quantify the correctness of the approximation and to optimize the parameters of the resulting mixture distribution to minimize such distance. The effectiveness of our approach is illustrated on several <b>benchmarks</b> from the control community.</p></p class="citation"></blockquote><h3 id=66--221259-event-triggered-state-estimation-through-confidence-level-wei-liu-2024>(6/6 | 221/259) Event-Triggered State Estimation Through Confidence Level (Wei Liu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Liu. (2024)<br><strong>Event-Triggered State Estimation Through Confidence Level</strong><br><button class=copy-to-clipboard title="Event-Triggered State Estimation Through Confidence Level" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15289v1.pdf filename=2403.15289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the state estimation problem for <b>discrete-time</b> <b>linear</b> systems under event-triggered scheme. In order to improve performance, a novel event-triggered scheme based on confidence level is proposed using the chi-square distribution and mild regularity assumption. In terms of the novel event-triggered scheme, a minimum mean squared error (MMSE) state estimator is proposed using some results presented in this paper. Two algorithms for communication rate estimation of the proposed MMSE state estimator are developed where the first algorithm is based on information with one-step delay, and the second algorithm is based on information with two-step delay. The performance and effectiveness of the proposed MMSE state estimator and the two communication rate estimation algorithms are illustrated using a target tracking scenario.</p></p class="citation"></blockquote><h2 id=csgt-5>cs.GT (5)</h2><h3 id=15--222259-ppa-game-characterizing-and-learning-competitive-dynamics-among-online-content-creators-renzhe-xu-et-al-2024>(1/5 | 222/259) PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators (Renzhe Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renzhe Xu, Haotian Wang, Xingxuan Zhang, Bo Li, Peng Cui. (2024)<br><strong>PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators</strong><br><button class=copy-to-clipboard title="PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-LG, cs.GT<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15524v1.pdf filename=2403.15524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers&rsquo; attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our <b>simulations.</b> Beyond analyzing static payoffs, we further discuss the agents&rsquo; online learning about resource payoffs by integrating a multi-player multi-armed <b>bandit</b> framework. We propose an online algorithm facilitating each agent&rsquo;s maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta > 0$. Empirical results further validate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=25--223259-balancing-fairness-and-efficiency-in-energy-resource-allocations-jiayi-li-et-al-2024>(2/5 | 223/259) Balancing Fairness and Efficiency in Energy Resource Allocations (Jiayi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Li, Matthew Motoki, Baosen Zhang. (2024)<br><strong>Balancing Fairness and Efficiency in Energy Resource Allocations</strong><br><button class=copy-to-clipboard title="Balancing Fairness and Efficiency in Energy Resource Allocations" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs-SY, cs.GT, eess-SY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15616v1.pdf filename=2403.15616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bringing <b>fairness</b> to energy resource allocation remains a challenge, due to the complexity of system structures and economic interdependencies among users and system operators&rsquo; decision-making. The rise of distributed energy resources has introduced more diverse heterogeneous user groups, surpassing the capabilities of traditional efficiency-oriented allocation schemes. Without explicitly bringing <b>fairness</b> to user-system interaction, this disparity often leads to disproportionate payments for certain user groups due to their utility formats or group sizes. Our paper addresses this challenge by formalizing the problem of fair energy resource allocation and introducing the framework for aggregators. This framework enables optimal <b>fairness-efficiency</b> trade-offs by selecting appropriate objectives in a principled way. By jointly optimizing over the total resources to allocate and individual allocations, our approach reveals optimized allocation schemes that lie on the Pareto front, balancing <b>fairness</b> and efficiency in resource allocation strategies.</p></p class="citation"></blockquote><h3 id=35--224259-on-the-weighted-top-difference-distance-axioms-aggregation-and-approximation-andrea-aveni-et-al-2024>(3/5 | 224/259) On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation (Andrea Aveni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Aveni, Ludovico Crippa, Giulio Principi. (2024)<br><strong>On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation</strong><br><button class=copy-to-clipboard title="On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DM, cs-GT, cs.GT, econ-TH, stat-ME<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15198v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15198v2.pdf filename=2403.15198v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists. We provide a full axiomatic characterization of our distance. In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes. This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature. Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable. We apply it to preference aggregation, studying the features of the associated median voting rule. It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from <b>fairness</b> to majority and Pareto-related properties. We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms. Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010). These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem.</p></p class="citation"></blockquote><h3 id=45--225259-on-the-variational-interpretation-of-mirror-play-in-monotone-games-yunian-pan-et-al-2024>(4/5 | 225/259) On the Variational Interpretation of Mirror Play in Monotone Games (Yunian Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunian Pan, Tao Li, Quanyan Zhu. (2024)<br><strong>On the Variational Interpretation of Mirror Play in Monotone Games</strong><br><button class=copy-to-clipboard title="On the Variational Interpretation of Mirror Play in Monotone Games" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-SY, cs.GT, eess-SY<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15636v1.pdf filename=2403.15636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mirror play (MP) is a well-accepted primal-dual multi-agent learning algorithm where all agents simultaneously implement mirror descent in a distributed fashion. The advantage of MP over vanilla gradient play lies in its usage of mirror maps that better exploit the <b>geometry</b> of decision domains. Despite extensive literature dedicated to the asymptotic convergence of MP to equilibrium, the understanding of the finite-time behavior of MP before reaching equilibrium is still rudimentary. To facilitate the study of MP&rsquo;s non-equilibrium performance, this work establishes an equivalence between MP&rsquo;s finite-time primal-dual path (mirror path) in monotone games and the closed-loop Nash equilibrium path of a finite-horizon differential game, referred to as mirror differential game (MDG). Our construction of MDG rests on the Brezis-Ekeland variational principle, and the stage cost functional for MDG is Fenchel coupling between MP&rsquo;s iterates and associated gradient updates. The variational interpretation of mirror path in static games as the equilibrium path in MDG holds in deterministic and stochastic cases. Such a variational interpretation translates the non-equilibrium studies of learning dynamics into a more tractable equilibrium analysis of dynamic games, as demonstrated in a case study on the Cournot game, where MP dynamics corresponds to a linear quadratic game.</p></p class="citation"></blockquote><h3 id=55--226259-strategic-network-creation-for-enabling-greedy-routing-julian-berger-et-al-2024>(5/5 | 226/259) Strategic Network Creation for Enabling Greedy Routing (Julian Berger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Berger, Tobias Friedrich, Pascal Lenzner, Paraskevi Machaira, Janosch Ruff. (2024)<br><strong>Strategic Network Creation for Enabling Greedy Routing</strong><br><button class=copy-to-clipboard title="Strategic Network Creation for Enabling Greedy Routing" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15307v1.pdf filename=2403.15307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present the first game-theoretic network creation model that incorporates greedy routing, i.e., the agents in our model are embedded in some metric space and strive for creating a network where all-pairs greedy routing is enabled. In contrast to <b>graph-theoretic</b> shortest paths, our agents route their traffic along greedy paths, which are sequences of nodes where the distance in the metric space to the respective target node gets strictly smaller by each hop. Besides enabling greedy routing, the agents also optimize their connection quality within the created network by constructing greedy paths with low stretch. This ensures that greedy routing is always possible in equilibrium networks, while realistically modeling the agents&rsquo; incentives for local structural changes to the network. With this we augment the elegant network creation model by Moscibroda, Schmidt, and Wattenhofer (PODC'06) with the feature of greedy routing. For our model, we analyze the existence of (approximate)-equilibria and the computational hardness in different underlying metric spaces. E.g., we characterize the set of equilibria in 1-2-metrics and tree metrics, we show that in both metrics Nash equilibria always exist, and we prove that the well-known $\Theta$-graph construction yields constant-approximate Nash equilibria in Euclidean space. The latter justifies distributed network construction via $\Theta$-graphs from a new point-of-view, since it shows that this powerful technique not only guarantees networks having a low stretch but also networks that are almost stable.</p></p class="citation"></blockquote><h2 id=physicsgeo-ph-1>physics.geo-ph (1)</h2><h3 id=11--227259-end-to-end-mineral-exploration-with-artificial-intelligence-and-ambient-noise-tomography-jack-muir-et-al-2024>(1/1 | 227/259) End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography (Jack Muir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Muir, Gerrit Olivier, Anthony Reid. (2024)<br><strong>End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography</strong><br><button class=copy-to-clipboard title="End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.geo-ph<br>Categories: cs-LG, physics-geo-ph, physics.geo-ph<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15095v1.pdf filename=2403.15095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an innovative end-to-end workflow for mineral exploration, integrating ambient noise tomography (ANT) and artificial intelligence (AI) to enhance the discovery and delineation of mineral resources essential for the global transition to a low carbon economy. We focus on copper as a critical element, required in significant quantities for renewable energy solutions. We show the benefits of utilising ANT, characterised by its speed, scalability, depth penetration, resolution, and low environmental impact, alongside artificial intelligence (AI) techniques to refine a continent-scale prospectivity model at the deposit scale by <b>fine-tuning</b> our model on local high-resolution data. We show the promise of the method by first presenting a new data-driven AI prospectivity model for copper within Australia, which serves as our <b>foundation</b> <b>model</b> for further <b>fine-tuning.</b> We then focus on the Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with relatively few local training samples (orebody intercepts), we can fine tune the <b>foundation</b> <b>model</b> to provide a good estimate of the Hillside orebody outline. Our methodology demonstrates how AI can augment geophysical data interpretation, providing a novel approach to mineral exploration with improved decision-making capabilities for targeting mineralization, thereby addressing the urgent need for increased mineral resource discovery.</p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--228259-augmented-reality-warnings-in-roadway-work-zones-evaluating-the-effect-of-modality-on-worker-reaction-times-sepehr-sabeti-et-al-2024>(1/1 | 228/259) Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect of Modality on Worker Reaction Times (Sepehr Sabeti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sepehr Sabeti, Fatemeh Banani Ardacani, Omidreza Shoghli. (2024)<br><strong>Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect of Modality on Worker Reaction Times</strong><br><button class=copy-to-clipboard title="Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect of Modality on Worker Reaction Times" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CV, cs-HC, cs.HC, eess-IV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15571v1.pdf filename=2403.15571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the aging highway infrastructure requiring extensive rebuilding and enhancements, and the consequent rise in the number of work zones, there is an urgent need to develop advanced safety systems to protect workers. While Augmented Reality (AR) holds significant potential for delivering warnings to workers, its integration into roadway work zones remains relatively unexplored. The primary objective of this study is to improve safety measures within roadway work zones by conducting an extensive analysis of how different combinations of <b>multimodal</b> AR warnings influence the reaction times of workers. This paper addresses this gap through a series of experiments that aim to replicate the distinctive conditions of roadway work zones, both in real-world and virtual reality environments. Our approach comprises three key components: an advanced AR system prototype, a VR <b>simulation</b> of AR functionality within the work zone environment, and the Wizard of Oz technique to synchronize user experiences across experiments. To assess reaction times, we leverage both the simple reaction time (SRT) technique and an innovative vision-based metric that utilizes real-time pose estimation. By conducting five experiments in controlled outdoor work zones and indoor VR settings, our study provides valuable information on how various <b>multimodal</b> AR warnings impact workers reaction times. Furthermore, our findings reveal the disparities in reaction times between VR <b>simulations</b> and real-world scenarios, thereby gauging VR&rsquo;s capability to mirror the dynamics of roadway work zones. Furthermore, our results substantiate the potential and reliability of vision-based reaction time measurements. These insights resonate well with those derived using the SRT technique, underscoring the viability of this approach for tangible real-world uses.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--229259-not-all-attention-is-needed-parameter-and-computation-efficient-transfer-learning-for-multi-modal-large-language-models-qiong-wu-et-al-2024>(1/2 | 229/259) Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models (Qiong Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji. (2024)<br><strong>Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CL, cs-MM, cs.MM<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Transfer Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15226v1.pdf filename=2403.15226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel parameter and computation efficient tuning method for <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of <b>benchmarks.</b> The experiments show that EAS not only retains high performance and parameter efficiency, but also greatly speeds up inference speed. For instance, LaVIN-EAS can obtain 89.98% accuracy on ScineceQA while speeding up inference by 2.2 times to LaVIN</p></p class="citation"></blockquote><h3 id=22--230259-experimental-studies-of-metaverse-streaming-haopeng-wang-et-al-2024>(2/2 | 230/259) Experimental Studies of Metaverse Streaming (Haopeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haopeng Wang, Roberto Martinez-Velazquez, Haiwei Dong, Abdulmotaleb El Saddik. (2024)<br><strong>Experimental Studies of Metaverse Streaming</strong><br><button class=copy-to-clipboard title="Experimental Studies of Metaverse Streaming" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs-NI, cs.MM<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15256v1.pdf filename=2403.15256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Metaverse aims to construct a large, unified, immersive, and shared digital realm by combining various technologies, namely XR (extended reality), blockchain, and digital twin, among others. This article explores the Metaverse from the perspective of multimedia communication by conducting and analyzing real-world experiments on four different Metaverse platforms: VR (virtual reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual City. We first investigate the traffic patterns and network performance in the three VR platforms. After raising the challenges of the Metaverse streaming and investigating the potential methods to enhance Metaverse performance, we propose a remote rendering architecture and verify its advantages through a prototype involving the campus network and MR <b>multimodal</b> interaction by comparison with local rendering.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--231259-brain-grounding-of-semantic-vectors-improves-neural-decoding-of-visual-stimuli-shirin-vafaei-et-al-2024>(1/1 | 231/259) Brain-grounding of semantic vectors improves neural decoding of visual stimuli (Shirin Vafaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shirin Vafaei, Ryohei Fukuma, Huixiang Yang, Haruhiko Kishima, Takufumi Yanagisawa. (2024)<br><strong>Brain-grounding of semantic vectors improves neural decoding of visual stimuli</strong><br><button class=copy-to-clipboard title="Brain-grounding of semantic vectors improves neural decoding of visual stimuli" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, q-bio-NC, q-bio.NC<br>Keyword Score: 25<br>Keywords: Fine-tuning, Representation Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15176v1.pdf filename=2403.15176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing algorithms for accurate and comprehensive neural decoding of mental contents is one of the long-cherished goals in the field of neuroscience and brain-machine interfaces. Previous studies have demonstrated the feasibility of neural decoding by training machine learning models to map brain activity patterns into a semantic vector <b>representation</b> <b>of</b> stimuli. These vectors, hereafter referred as pretrained feature vectors, are usually derived from semantic spaces based solely on image and/or text features and therefore they might have a totally different characteristics than how visual stimuli is represented in the human brain, resulting in limiting the capability of brain decoders to learn this mapping. To address this issue, we propose a <b>representation</b> <b>learning</b> framework, termed brain-grounding of semantic vectors, which <b>fine-tunes</b> pretrained feature vectors to better align with the neural <b>representation</b> <b>of</b> visual stimuli in the human brain. We trained this model this model with functional magnetic resonance imaging (fMRI) of 150 different visual stimuli categories, and then performed <b>zero-shot</b> brain decoding and identification analyses on 1) fMRI and 2) magnetoencephalography (MEG). Interestingly, we observed that by using the brain-grounded vectors, the brain decoding and identification accuracy on brain data from different neuroimaging modalities increases. These findings underscore the potential of incorporating a richer array of brain-derived features to enhance performance of brain decoding algorithms.</p></p class="citation"></blockquote><h2 id=cond-matquant-gas-1>cond-mat.quant-gas (1)</h2><h3 id=11--232259-fast-real-time-arbitrary-waveform-generation-using-graphic-processing-units-juntian-tu-et-al-2024>(1/1 | 232/259) Fast real-time arbitrary waveform generation using graphic processing units (Juntian Tu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juntian Tu, Sarthak Subhankar. (2024)<br><strong>Fast real-time arbitrary waveform generation using graphic processing units</strong><br><button class=copy-to-clipboard title="Fast real-time arbitrary waveform generation using graphic processing units" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.quant-gas<br>Categories: cond-mat-quant-gas, cond-mat.quant-gas, cs-DC, eess-SP, physics-atom-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15582v1.pdf filename=2403.15582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time Arbitrary Waveform Generation (AWG) is essential in various engineering and research applications, and often requires complex bespoke hardware and software. This paper introduces an AWG framework using an NVIDIA Graphics Processing Unit (GPU) and a commercially available high-speed Digital-to-Analog Converter (DAC) card, both running on a desktop personal computer (PC). The GPU accelerates the &ldquo;embarrassingly&rdquo; data parallel additive waveform synthesis framework for AWG, and the DAC reconstructs the generated waveform in the analog domain at high speed. The AWG framework is programmed using the developer-friendly Compute Unified Device Architecture (CUDA) runtime application programming interface from NVIDIA and is readily customizable, and scalable with additional parallel hardware. We present and characterize two different pathways for computing modulated radio-frequency (rf) waveforms: one pathway offers high-complexity simultaneous chirping of 1000 individual Nyquist-limited single-frequency tones for 35 ms at a sampling rate of 560 MB/s, and the other pathway allows simultaneous continuous chirping of 194 individual Nyquist-limited single-frequency tones at 100 MB/s, or 20 individual tones at 560 MB/s. This AWG framework is designed for fast on-the-fly rearrangement of a large stochastically-loaded optical tweezer array of single atoms or molecules into a defect-free array needed for quantum <b>simulation</b> and quantum computation applications.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--233259-towards-a-statistical-probabilistic-lazy-lambda-calculus-radha-jagadeesan-2024>(1/1 | 233/259) (Towards a) Statistical Probabilistic Lazy Lambda Calculus (Radha Jagadeesan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Radha Jagadeesan. (2024)<br><strong>(Towards a) Statistical Probabilistic Lazy Lambda Calculus</strong><br><button class=copy-to-clipboard title="(Towards a) Statistical Probabilistic Lazy Lambda Calculus" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: D-3-1, cs-LO, cs.LO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15570v1.pdf filename=2403.15570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the desiderata on a model for statistical probabilistic programming languages. We argue that they can be met by a combination of traditional tools, namely open bisimulation and probabilistic <b>simulation.</b></p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--234259-efficiently-estimating-mutual-information-between-attributes-across-tables-aécio-santos-et-al-2024>(1/1 | 234/259) Efficiently Estimating Mutual Information Between Attributes Across Tables (Aécio Santos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aécio Santos, Flip Korn, Juliana Freire. (2024)<br><strong>Efficiently Estimating Mutual Information Between Attributes Across Tables</strong><br><button class=copy-to-clipboard title="Efficiently Estimating Mutual Information Between Attributes Across Tables" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: Data Augmentation, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15553v1.pdf filename=2403.15553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relational <b>data</b> <b>augmentation</b> is a powerful technique for enhancing <b>data</b> <b>analytics</b> and improving machine learning models by incorporating columns from external datasets. However, it is challenging to efficiently discover relevant external tables to join with a given input table. Existing approaches rely on <b>data</b> <b>discovery</b> systems to identify joinable tables from external sources, typically based on overlap or containment. However, the sheer number of tables obtained from these systems results in irrelevant joins that need to be performed; this can be computationally expensive or even infeasible in practice. We address this limitation by proposing the use of efficient <b>mutual</b> <b>information</b> (MI) estimation for finding relevant joinable tables. We introduce a new sketching method that enables efficient evaluation of relationship discovery queries by estimating MI without materializing the joins and returning a smaller set of tables that are more likely to be relevant. We also demonstrate the effectiveness of our approach at approximating MI in extensive experiments using synthetic and real-world datasets.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--235259-random-vortex-dynamics-and-monte-carlo-simulations-for-wall-bounded-viscous-flows-vladislav-cherepanov-et-al-2024>(1/4 | 235/259) Random vortex dynamics and Monte-Carlo simulations for wall-bounded viscous flows (Vladislav Cherepanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladislav Cherepanov, Sebastian W. Ertel, Zhongmin Qian, Jiang-Lun Wu. (2024)<br><strong>Random vortex dynamics and Monte-Carlo simulations for wall-bounded viscous flows</strong><br><button class=copy-to-clipboard title="Random vortex dynamics and Monte-Carlo simulations for wall-bounded viscous flows" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 76M35, 76M23, 60H30, 65C05, 68Q10, cs-NA, math-AP, math-NA, math-PR, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15549v1.pdf filename=2403.15549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional integral representations for solutions of the motion equations for wall-bounded incompressible viscous flows, expressed (implicitly) in terms of distributions of solutions to stochastic differential equations of McKean-Vlasov type, are established by using a perturbation technique. These representations are used to obtain exact random vortex dynamics for wall-bounded viscous flows. Numerical schemes therefore are proposed and the convergence of the numerical schemes for random vortex dynamics with an additional force term is established. Several numerical experiments are carried out for demonstrating the motion of a viscous flow within a thin layer next to the fluid boundary.</p></p class="citation"></blockquote><h3 id=24--236259-accelerating-aeroelastic-uvlm-simulations-by-inexact-newton-algorithms-jenny-schubert-et-al-2024>(2/4 | 236/259) Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms (Jenny Schubert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jenny Schubert, Marc C. Steinbach, Christian Hente, David Märtins, Daniel Schuster. (2024)<br><strong>Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms</strong><br><button class=copy-to-clipboard title="Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 49M15, 90C53, 74F10, 76B47, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15286v1.pdf filename=2403.15286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the aeroelastic <b>simulation</b> of flexible mechanical structures submerged in subsonic fluid flows at low Mach numbers. The nonlinear kinematics of flexible bodies are described in the total Lagrangian formulation and discretized by finite elements. The aerodynamic loads are computed using the unsteady vortex-lattice method wherein a free wake is tracked over time. Each implicit time step in the dynamic <b>simulation</b> then requires solving a nonlinear equation system in the structural variables with additional aerodynamic load terms. Our focus here is on the efficient numerical solution of this system by accelerating the Newton algorithm. The particular structure of the aeroelastic nonlinear system suggests the structural derivative as an approximation to the full derivative in the linear Newton system. We investigate and compare two promising algorithms based in this approximation, a quasi-Newton type algorithm and a novel inexact Newton algorithm. Numerical experiments are performed on a flexible plate and on a wind turbine. Our computational results show that the approximation can indeed accelerate the Newton algorithm substantially. Surprisingly, the theoretically preferable inexact Newton algorithm is much slower than the quasi-Newton algorithm, which motivates further research to speed up derivative evaluations.</p></p class="citation"></blockquote><h3 id=34--237259-two-scale-analysis-for-multiscale-landau-lifshitz-gilbert-equation-theory-and-numerical-methods-xiaofei-guan-et-al-2024>(3/4 | 237/259) Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods (Xiaofei Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaofei Guan, Hang Qi, Zhiwei Sun. (2024)<br><strong>Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods</strong><br><button class=copy-to-clipboard title="Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14957v1.pdf filename=2403.14957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper discusses the theory and numerical method of two-scale analysis for the multiscale Landau-Lifshitz-Gilbert equation in composite ferromagnetic materials. The novelty of this work can be <b>summarized</b> in three aspects: Firstly, the more realistic and complex model is considered, including the effects of the exchange field, anisotropy field, stray field, and external magnetic field. The explicit convergence orders in the $H^1$ norm between the classical solution and the two-scale solution are obtained. Secondly, we propose a robust numerical framework, which is employed in several comprehensive experiments to validate the convergence results for the Periodic and Neumann problems. Thirdly, we design an improved implicit numerical scheme to reduce the required number of iterations and relaxes the constraints on the time step size, which can significantly improve computational efficiency. Specifically, the projection and the expansion methods are given to overcome the inherent non-consistency in the initial data between the multiscale problem and homogenized problem.</p></p class="citation"></blockquote><h3 id=44--238259-sparse-additive-function-decompositions-facing-basis-transforms-fatima-antarou-ba-et-al-2024>(4/4 | 238/259) Sparse additive function decompositions facing basis transforms (Fatima Antarou Ba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatima Antarou Ba, Oleh Melnyk, Christian Wald, Gabriele Steidl. (2024)<br><strong>Sparse additive function decompositions facing basis transforms</strong><br><button class=copy-to-clipboard title="Sparse additive function decompositions facing basis transforms" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 26Bxx, 33F05, 58C05, 90C26, 65Kxx, 65F25, 15B99, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15563v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15563v2.pdf filename=2403.15563v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-dimensional real-world systems can often be well characterized by a small number of simultaneous low-complexity interactions. The analysis of variance (ANOVA) decomposition and the anchored decomposition are typical techniques to find sparse additive decompositions of functions. In this paper, we are interested in a setting, where these decompositions are not directly spare, but become so after an appropriate basis transform. Noting that the sparsity of those additive function decompositions is equivalent to the fact that most of its mixed partial derivatives vanish, we can exploit a connection to the underlying function <b>graphs</b> to determine an orthogonal transform that realizes the appropriate basis change. This is done in three steps: we apply singular value decomposition to minimize the number of vertices of the function <b>graph,</b> and joint block diagonalization techniques of families of matrices followed by sparse minimization based on relaxations of the zero &lsquo;&rsquo;norm&rsquo;&rsquo; for minimizing the number of edges. For the latter one, we propose and analyze minimization techniques over the manifold of special orthogonal matrices. Various numerical examples illustrate the reliability of our approach for functions having, after a basis transform, a sparse additive decomposition into summands with at most two variables.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--239259-a-modular-end-to-end-next-generation-network-testbed-towards-a-fully-automated-network-management-platform-ali-chouman-et-al-2024>(1/2 | 239/259) A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform (Ali Chouman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Chouman, Dimitrios Michael Manias, Abdallah Shami. (2024)<br><strong>A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform</strong><br><button class=copy-to-clipboard title="A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15376v1.pdf filename=2403.15376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experimentation in practical, end-to-end (E2E) next-generation networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research. The prevalence of fifth-generation technology (5G) testbeds and the emergence of developing networks systems, for the purposes of research and testing, focus on the capabilities and features of analytics, intelligence, and automated management using novel testbed designs and architectures, ranging from simple <b>simulations</b> and setups to complex networking systems; however, with the ever-demanding application requirements for modern and future networks, 5G-and-beyond (denoted as 5G+) testbed experimentation can be useful in assessing the creation of large-scale network infrastructures that are capable of supporting E2E virtualized mobile network services. To this end, this paper presents a functional, modular E2E 5G+ system, complete with the integration of a Radio Access Network (RAN) and handling the connection of User Equipment (UE) in real-world scenarios. As well, this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities, including a complete description of user-plane data, from UE registrations to communications sequences, and leads to the presentation of a future outlook in powering new experimentation for 6G and next-generation networks.</p></p class="citation"></blockquote><h3 id=22--240259-blockchain-based-pseudonym-management-for-vehicle-twin-migrations-in-vehicular-edge-metaverse-jiawen-kang-et-al-2024>(2/2 | 240/259) Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse (Jiawen Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie. (2024)<br><strong>Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse</strong><br><button class=copy-to-clipboard title="Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-CR, cs-HC, cs-LG, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15285v1.pdf filename=2403.15285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driven by the great advances in metaverse and edge computing technologies, vehicular edge metaverses are expected to disrupt the current paradigm of intelligent transportation systems. As highly computerized avatars of Vehicular Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can provide valuable metaverse services to improve driving safety and on-board satisfaction for their VMUs throughout journeys. To maintain uninterrupted metaverse experiences, VTs must be migrated among edge servers following the movements of vehicles. This can raise concerns about privacy breaches during the dynamic communications among vehicular edge metaverses. To address these concerns and safeguard location privacy, pseudonyms as temporary identifiers can be leveraged by both VMUs and VTs to realize anonymous communications in the physical space and virtual spaces. However, existing pseudonym management methods fall short in meeting the extensive pseudonym demands in vehicular edge metaverses, thus dramatically diminishing the performance of privacy preservation. To this end, we present a cross-metaverse empowered dual pseudonym management framework. We utilize cross-chain technology to enhance management efficiency and data security for pseudonyms. Furthermore, we propose a metric to assess the privacy level and employ a Multi-Agent Deep <b>Reinforcement</b> <b>Learning</b> (MADRL) approach to obtain an optimal pseudonym generating strategy. Numerical results demonstrate that our proposed schemes are high-efficiency and cost-effective, showcasing their promising applications in vehicular edge metaverses.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--241259-a-wasserstein-perspective-of-vanilla-gans-lea-kunkel-et-al-2024>(1/1 | 241/259) A Wasserstein perspective of Vanilla GANs (Lea Kunkel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lea Kunkel, Mathias Trabs. (2024)<br><strong>A Wasserstein perspective of Vanilla GANs</strong><br><button class=copy-to-clipboard title="A Wasserstein perspective of Vanilla GANs" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: 62E17, 62G05, 68T07, cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15312v1.pdf filename=2403.15312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The empirical success of <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein <b>GANs</b> and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla <b>GANs,</b> the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla <b>GANs</b> to the Wasserstein distance. By doing so, existing results for Wasserstein <b>GANs</b> can be extended to Vanilla <b>GANs.</b> In particular, we obtain an oracle inequality for Vanilla <b>GANs</b> in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H"older norm, we conclude a rate of convergence for Vanilla <b>GANs</b> as well as Wasserstein <b>GANs</b> as estimators of the unknown probability distribution.</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=13--242259-pursuit-evasion-on-a-sphere-and-when-it-can-be-considered-flat-dejan-milutinovic-et-al-2024>(1/3 | 242/259) Pursuit-Evasion on a Sphere and When It Can Be Considered Flat (Dejan Milutinovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dejan Milutinovic, Alexander Von Moll, Satyanarayana G. Manyam, David W. Casbeer, Isaac E. Weintraub, Meir Pachter. (2024)<br><strong>Pursuit-Evasion on a Sphere and When It Can Be Considered Flat</strong><br><button class=copy-to-clipboard title="Pursuit-Evasion on a Sphere and When It Can Be Considered Flat" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 58E10, 86A30, 37D40, 91A23, 49L12, 49Q10, cs-SY, eess-SY, math-AG, math-DG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15188v1.pdf filename=2403.15188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In classical works on a planar differential pursuit-evasion game with a faster pursuer, the intercept point resulting from the equilibrium strategies lies on the Apollonius circle. This property was exploited for the construction of the equilibrium strategies for two faster pursuers against one evader. Extensions for planar multiple-pursuer single-evader scenarios have been considered. We study a pursuit-evasion game on a sphere and the relation of the equilibrium intercept point to the Apollonius domain on the sphere. The domain is a generalization of the planar Apollonius circle set. We find a condition resulting in the intercept point belonging to the Apollonius domain, which is the characteristic of the planar game solution. Finally, we use this characteristic to discuss pursuit and evasion strategies in the context of two pursuers and a single slower evader on the sphere and illustrate it using numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=23--243259-optimal-contract-design-for-end-of-life-care-payments-muyan-jiang-et-al-2024>(2/3 | 243/259) Optimal Contract Design for End-of-Life Care Payments (Muyan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muyan Jiang, Ying Chen, Xin Chen, Javad Lavaei, Anil Aswani. (2024)<br><strong>Optimal Contract Design for End-of-Life Care Payments</strong><br><button class=copy-to-clipboard title="Optimal Contract Design for End-of-Life Care Payments" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-NA, math-NA, math-OC, math.OC, stat-AP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15099v1.pdf filename=2403.15099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A large fraction of total healthcare expenditure occurs due to end-of-life (EOL) care, which means it is important to study the problem of more carefully incentivizing necessary versus unnecessary EOL care because this has the potential to reduce overall healthcare spending. This paper introduces a principal-agent model that integrates a mixed payment system of fee-for-service and pay-for-performance in order to analyze whether it is possible to better align healthcare provider incentives with patient outcomes and cost-efficiency in EOL care. The primary contributions are to derive optimal contracts for EOL care payments using a principal-agent framework under three separate models for the healthcare provider, where each model considers a different level of risk tolerance for the provider. We derive these optimal contracts by converting the underlying principal-agent models from a bilevel optimization problem into a single-level optimization problem that can be analytically solved. Our results are demonstrated using a <b>simulation</b> where an optimal contract is used to price intracranial pressure monitoring for traumatic brain injuries.</p></p class="citation"></blockquote><h3 id=33--244259-network-learning-with-directional-sign-patterns-anqi-dong-et-al-2024>(3/3 | 244/259) Network Learning with Directional Sign Patterns (Anqi Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anqi Dong, Can Chen, Tryphon T. Georgiou. (2024)<br><strong>Network Learning with Directional Sign Patterns</strong><br><button class=copy-to-clipboard title="Network Learning with Directional Sign Patterns" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: 62F15, 49Q22, 05Cxx, 92C42, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14915v1.pdf filename=2403.14915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex systems can be effectively modeled via <b>graphs</b> that encode networked interactions, where relations between entities or nodes are often quantified by signed edge weights, e.g., promotion/inhibition in gene regulatory networks, or encoding political of friendship differences in social networks. However, it is often the case that only an aggregate consequence of such edge weights that characterize relations may be directly observable, as in protein expression of in gene regulatory networks. Thus, learning edge weights poses a significant challenge that is further exacerbated for intricate and large-scale networks. In this article, we address a model problem to determine the strength of sign-indefinite relations that explain marginal distributions that constitute our data. To this end, we develop a paradigm akin to that of the Schr"odinger bridge problem and an efficient Sinkhorn type algorithm (more properly, Schr"odinger-Fortet-Sinkhorn algorithm) that allows fast convergence to parameters that minimize a relative entropy/likelihood criterion between the sought signed adjacency matrix and a prior. The formalism that we present represents a novel generalization of the earlier Schr"odinger formalism in that marginal computations may incorporate weights that model directionality in underlying relations, and further, that it can be extended to high-order networks &ndash; the Schr"odinger-Fortet-Sinkhorn algorithm that we derive is applicable all the same and allows geometric convergence to a sought sign-indefinite adjacency matrix or tensor, for high-order networks. We demonstrate our framework with synthetic and real-world examples.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--245259-a-hybrid-approach-to-semi-automated-rust-verification-sacha-élie-ayoun-et-al-2024>(1/1 | 245/259) A hybrid approach to semi-automated Rust verification (Sacha-Élie Ayoun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sacha-Élie Ayoun, Xavier Denis, Petar Maksimović, Philippa Gardner. (2024)<br><strong>A hybrid approach to semi-automated Rust verification</strong><br><button class=copy-to-clipboard title="A hybrid approach to semi-automated Rust verification" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15122v1.pdf filename=2403.15122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent years have been witness to a large body of work on efficient and automated verification of safe Rust code, enabled by the rich guarantees of the Rust type system, much less progress has been made on <b>reasoning</b> about unsafe code due to its unique complexities. We propose a hybrid approach to end-to-end Rust verification in which powerful automated verification of safe Rust is combined with targeted semi-automated verification of unsafe~Rust. To this end, we present Gillian-Rust, a proof-of-concept semi-automated verification tool that is able to reason about type safety and functional correctness of unsafe~code. Built on top of the Gillian parametric compositional verification platform, Gillian-Rust automates a rich separation logic for real-world Rust, embedding the lifetime logic of RustBelt and the parametric propheciees of RustHornBelt. Using the unique extensibility of Gillian, our novel encoding of these features is <b>fine-tuned</b> to maximise automation and exposes a user-friendly API, allowing for low-effort verification of unsafe code. We link Gillian-Rust with Creusot, a state-of-the-art verifier for safe Rust, by providing a systematic encoding of unsafe code specifications that Creusot may use but not verify, demonstrating the feasibility of our hybrid~approach.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--246259-comprehensive-lipidomic-automation-workflow-using-large-language-models-connor-beveridge-et-al-2024>(1/1 | 246/259) Comprehensive Lipidomic Automation Workflow using Large Language Models (Connor Beveridge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Connor Beveridge, Sanjay Iyer, Caitlin E. Randolph, Matthew Muhoberac, Palak Manchanda, Amy C. Clingenpeel, Shane Tichy, Gaurav Chopra. (2024)<br><strong>Comprehensive Lipidomic Automation Workflow using Large Language Models</strong><br><button class=copy-to-clipboard title="Comprehensive Lipidomic Automation Workflow using Large Language Models" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, q-bio-BM, q-bio-QM, q-bio-SC, q-bio.QM<br>Keyword Score: 20<br>Keywords: Chatbot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15076v1.pdf filename=2403.15076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lipidomics generates <b>large</b> <b>data</b> <b>that</b> makes manual annotation and interpretation challenging. Lipid chemical and structural diversity with structural isomers further complicates annotation. Although, several commercial and open-source software for targeted lipid identification exists, it lacks automated method generation workflows and integration with statistical and bioinformatics tools. We have developed the Comprehensive Lipidomic Automated Workflow (CLAW) platform with integrated workflow for parsing, detailed statistical analysis and lipid annotations based on custom multiple reaction monitoring (MRM) precursor and product ion pair transitions. CLAW contains several modules including identification of carbon-carbon double bond position(s) in unsaturated lipids when combined with ozone electrospray ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated workflow in CLAW, <b>large-scale</b> <b>lipidomics</b> <b>data</b> was collected with traditional and OzESI-MRM profiling on biological and non-biological samples. Specifically, a total of 1497 transitions organized into 10 MRM-based mass spectrometry methods were used to profile lipid droplets isolated from different brain regions of 18-24 month-old Alzheimer&rsquo;s disease mice and age-matched wild-type controls. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon double bond specificity were generated from canola oil samples using OzESI-MRM profiling. We also developed an integrated language user interface with <b>large</b> <b>language</b> <b>models</b> using artificially intelligent (AI) agents that permits users to interact with the CLAW platform using a <b>chatbot</b> terminal to perform statistical and bioinformatic analyses. We envision CLAW pipeline to be used in high-throughput lipid structural identification tasks aiding users to generate automated lipidomics workflows ranging from data acquisition to AI agent-based bioinformatic analysis.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--247259-modeling-distributed-computing-infrastructures-for-hep-applications-maximilian-horzela-et-al-2024>(1/2 | 247/259) Modeling Distributed Computing Infrastructures for HEP Applications (Maximilian Horzela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Horzela, Henri Casanova, Manuel Giffels, Artur Gottmann, Robin Hofsaess, Günter Quast, Simone Rossi Tisbeni, Achim Streit, Frédéric Suter. (2024)<br><strong>Modeling Distributed Computing Infrastructures for HEP Applications</strong><br><button class=copy-to-clipboard title="Modeling Distributed Computing Infrastructures for HEP Applications" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC, hep-ex<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14903v1.pdf filename=2403.14903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the performance of various infrastructure design options in complex federated infrastructures with computing sites distributed over a wide area network that support a plethora of users and workflows, such as the Worldwide LHC Computing Grid (WLCG), is not trivial. Due to the complexity and size of these infrastructures, it is not feasible to deploy experimental test-beds at large scales merely for the purpose of comparing and evaluating alternate designs. An alternative is to study the behaviours of these systems using <b>simulation.</b> This approach has been used successfully in the past to identify efficient and practical infrastructure designs for High Energy Physics (HEP). A prominent example is the Monarc <b>simulation</b> framework, which was used to study the initial structure of the WLCG. New <b>simulation</b> capabilities are needed to simulate large-scale heterogeneous computing systems with complex networks, data access and caching patterns. A modern tool to simulate HEP workloads that execute on distributed computing infrastructures based on the SimGrid and WRENCH <b>simulation</b> frameworks is outlined. Studies of its accuracy and scalability are presented using HEP as a case-study. Hypothetical adjustments to prevailing computing architectures in HEP are studied providing insights into the dynamics of a part of the WLCG and candidates for improvements.</p></p class="citation"></blockquote><h3 id=22--248259-fsd-inference-fully-serverless-distributed-inference-with-scalable-cloud-communication-joe-oakley-et-al-2024>(2/2 | 248/259) FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication (Joe Oakley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joe Oakley, Hakan Ferhatosmanoglu. (2024)<br><strong>FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication</strong><br><button class=copy-to-clipboard title="FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-DC, cs-LG, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15195v1.pdf filename=2403.15195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Serverless computing offers attractive scalability, elasticity and cost-effectiveness. However, constraints on memory, CPU and function runtime have hindered its adoption for data-intensive applications and machine learning (ML) workloads. Traditional &lsquo;server-ful&rsquo; platforms enable distributed computation via fast networks and well-established inter-process communication (IPC) mechanisms such as MPI and shared memory. In the absence of such solutions in the serverless domain, parallel computation with significant IPC requirements is challenging. We present FSD-Inference, the first fully serverless and highly scalable system for distributed ML inference. We explore potential communication channels, in conjunction with Function-as-a-Service (FaaS) compute, to design a state-of-the-art solution for distributed ML within the context of serverless data-intensive computing. We introduce novel fully serverless communication schemes for ML inference workloads, leveraging both cloud-based publish-subscribe/queueing and object storage offerings. We demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC with comparable performance to object storage, while offering significantly reduced cost at high parallelism levels. We conduct in-depth experiments on <b>benchmark</b> DNNs of various sizes. The results show that when compared to server-based alternatives, FSD-Inference is significantly more cost-effective and scalable, and can even achieve competitive performance against optimized HPC solutions. Experiments also confirm that our serverless solution can handle large distributed workloads and leverage high degrees of FaaS parallelism.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--249259-cross-layer-modeling-and-design-of-content-addressable-memories-in-advanced-technology-nodes-for-similarity-search-siri-narla-et-al-2024>(1/1 | 249/259) Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search (Siri Narla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siri Narla, Piyush Kumar, Mohammad Adnaan, Azad Naeemi. (2024)<br><strong>Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search</strong><br><button class=copy-to-clipboard title="Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-AR, cs-ET, cs.ET<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15328v1.pdf filename=2403.15328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we present a comprehensive design and <b>benchmarking</b> study of Content Addressable Memory (CAM) at the 7nm technology node in the context of similarity search applications. We design CAM cells based on SRAM, spin-orbit torque, and ferroelectric field effect transistor devices and from their layouts extract cell parasitics using state of the art EDA tools. These parasitics are used to develop SPICE netlists to model search operations. We use a CAM-based dataset search and a sequential <b>recommendation</b> system to highlight the application-level performance degradation due to interconnect parasitics. We propose and evaluate two solutions to mitigate interconnect effects.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--250259-image-classification-with-rotation-invariant-variational-quantum-circuits-paul-san-sebastian-et-al-2024>(1/1 | 250/259) Image Classification with Rotation-Invariant Variational Quantum Circuits (Paul San Sebastian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul San Sebastian, Mikel Cañizo, Román Orús. (2024)<br><strong>Image Classification with Rotation-Invariant Variational Quantum Circuits</strong><br><button class=copy-to-clipboard title="Image Classification with Rotation-Invariant Variational Quantum Circuits" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CV, cs-LG, quant-ph, quant-ph<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15031v1.pdf filename=2403.15031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is <b>benchmarked</b> against two different architectures, and it is experimentally observed that the geometric approach boosts the model&rsquo;s performance. Finally, a classical equivariant <b>convolution</b> operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.</p></p class="citation"></blockquote><h2 id=csds-3>cs.DS (3)</h2><h3 id=13--251259-approximation-algorithms-for-school-assignment-group-fairness-and-multi-criteria-optimization-santhini-k-a-et-al-2024>(1/3 | 251/259) Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization (Santhini K. A. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santhini K. A., Kamesh Munagala, Meghana Nasre, Govind S. Sankar. (2024)<br><strong>Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization</strong><br><button class=copy-to-clipboard title="Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15623v1.pdf filename=2403.15623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of assigning students to schools, when students have different utilities for schools and schools have capacity. There are additional group <b>fairness</b> considerations over students that can be captured either by concave objectives, or additional constraints on the groups. We present approximation algorithms for this problem via convex program rounding that achieve various trade-offs between utility violation, capacity violation, and running time. We also show that our techniques easily extend to the setting where there are arbitrary covering constraints on the feasible assignment, capturing multi-criteria and ranking optimization.</p></p class="citation"></blockquote><h3 id=23--252259-approximation-algorithms-for-network-design-in-non-uniform-fault-models-chandra-chekuri-et-al-2024>(2/3 | 252/259) Approximation Algorithms for Network Design in Non-Uniform Fault Models (Chandra Chekuri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chandra Chekuri, Rhea Jain. (2024)<br><strong>Approximation Algorithms for Network Design in Non-Uniform Fault Models</strong><br><button class=copy-to-clipboard title="Approximation Algorithms for Network Design in Non-Uniform Fault Models" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15547v1.pdf filename=2403.15547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Survivable Network Design problem (SNDP) is a well-studied problem, motivated by the design of networks that are robust to faults under the assumption that any subset of edges up to a specific number can fail. We consider non-uniform fault models where the subset of edges that fail can be specified in different ways. Our primary interest is in the flexible <b>graph</b> connectivity model, in which the edge set is partitioned into safe and unsafe edges. The goal is to design a network that has desired connectivity properties under the assumption that only unsafe edges up to a specific number can fail. We also discuss the bulk-robust model and the relative survivable network design model. While SNDP admits a 2-approximation, the approximability of problems in these more complex models is much less understood even in special cases. We make two contributions. Our first set of results are in the flexible <b>graph</b> connectivity model. Motivated by a conjecture that a constant factor approximation is feasible when the robustness parameters are fixed constants, we consider two important special cases, namely the single pair case, and the global connectivity case. For both these, we obtain constant factor approximations in several parameter ranges of interest. These are based on an augmentation framework and via decomposing the families of cuts that need to be covered into a small number of uncrossable families. Our second set of results are poly-logarithmic approximations for the bulk-robust model when the &ldquo;width&rdquo; of the given instance (the maximum number of edges that can fail in any particular scenario) is fixed. Via this, we derive corresponding approximations for the flexible <b>graph</b> connectivity model and the relative survivable network design model. The results are obtained via two algorithmic approaches and they have different tradeoffs in terms of the approximation ratio and generality.</p></p class="citation"></blockquote><h3 id=33--253259-fourier-transform-based-estimators-for-data-sketches-seth-pettie-et-al-2024>(3/3 | 253/259) Fourier Transform-based Estimators for Data Sketches (Seth Pettie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seth Pettie, Dingyu Wang. (2024)<br><strong>Fourier Transform-based Estimators for Data Sketches</strong><br><button class=copy-to-clipboard title="Fourier Transform-based Estimators for Data Sketches" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DB, cs-DC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15366v1.pdf filename=2403.15366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we consider the problem of estimating the $f$-moment ($\sum_{v\in [n]} (f(\mathbf{x}(v))-f(0))$) of a dynamic vector $\mathbf{x}\in \mathbb{G}^n$ over some abelian group $(\mathbb{G},+)$, where the $|f|_\infty$ norm is bounded. We propose a simple sketch and new estimation framework based on the \emph{Fourier transform} of $f$. I.e., we decompose $f$ into a linear combination of homomorphisms $f_1,f_2,\ldots$ from $(\mathbb{G},+)$ to $(\mathbb{C},\times)$, estimate the $f_k$-moment for each $f_k$, and synthesize them to obtain an estimate of the $f$-moment. Our estimators are asymptotically unbiased and have variance asymptotic to $|\mathbf{x}|<em>0^2 (|f|</em>\infty^2 m^{-1} + |\hat{f}|_1^2 m^{-2})$, where the size of the sketch is $O(m\log n\log|\mathbb{G}|)$ bits. When $\mathbb{G}=\mathbb{Z}$ this problem can also be solved using off-the-shelf $\ell_0$-samplers with space $O(m\log^2 n)$ bits, which does not obviously generalize to finite groups. As a concrete <b>benchmark,</b> we extend Ganguly, Garofalakis, and Rastogi&rsquo;s singleton-detector-based sampler to work over $\mathbb{G}$ using $O(m\log n\log|\mathbb{G}|\log(m\log n))$ bits. We give some experimental evidence that the Fourier-based estimation framework is significantly more accurate than sampling-based approaches at the same memory footprint.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--254259-music-to-dance-as-language-translation-using-sequence-models-andré-correia-et-al-2024>(1/1 | 254/259) Music to Dance as Language Translation using Sequence Models (André Correia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Correia, Luís A. Alexandre. (2024)<br><strong>Music to Dance as Language Translation using Sequence Models</strong><br><button class=copy-to-clipboard title="Music to Dance as Language Translation using Sequence Models" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-RO, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15569v1.pdf filename=2403.15569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthesising appropriate choreographies from music remains an open problem. We introduce MDLT, a novel approach that frames the choreography generation problem as a translation task. Our method leverages an existing data set to learn to translate sequences of audio into corresponding dance poses. We present two variants of MDLT: one utilising the <b>Transformer</b> architecture and the other employing the Mamba architecture. We train our method on AIST++ and PhantomDance data sets to teach a robotic arm to dance, but our method can be applied to a full humanoid robot. Evaluation metrics, including Average Joint Error and Frechet Inception Distance, consistently demonstrate that, when given a piece of music, MDLT excels at producing realistic and high-quality choreography. The code can be found at github.com/meowatthemoon/MDLT.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--255259-a-gradient-enhanced-univariate-dimension-reduction-method-for-uncertainty-propagation-bingran-wang-et-al-2024>(1/2 | 255/259) A gradient-enhanced univariate dimension reduction method for uncertainty propagation (Bingran Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingran Wang, Nicholas C. Orndorff, Mark Sperry, John T. Hwang. (2024)<br><strong>A gradient-enhanced univariate dimension reduction method for uncertainty propagation</strong><br><button class=copy-to-clipboard title="A gradient-enhanced univariate dimension reduction method for uncertainty propagation" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15622v1.pdf filename=2403.15622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The univariate dimension reduction (UDR) method stands as a way to estimate the statistical moments of the output that is effective in a large class of uncertainty quantification (UQ) problems. UDR&rsquo;s fundamental strategy is to approximate the original function using univariate functions so that the UQ cost only scales linearly with the dimension of the problem. Nonetheless, UDR&rsquo;s effectiveness can diminish when uncertain inputs have high variance, particularly when assessing the output&rsquo;s second and higher-order statistical moments. This paper proposes a new method, gradient-enhanced univariate dimension reduction (GUDR), that enhances the accuracy of UDR by incorporating univariate gradient function terms into the UDR approximation function. Theoretical results indicate that the GUDR approximation is expected to be one order more accurate than UDR in approximating the original function, and it is expected to generate more accurate results in computing the output&rsquo;s second and higher-order statistical moments. Our proposed method uses a computational <b>graph</b> transformation strategy to efficiently evaluate the GUDR approximation function on tensor-grid quadrature inputs, and use the tensor-grid input-output data to compute the statistical moments of the output. With an efficient automatic differentiation method to compute the gradients, our method preserves UDR&rsquo;s linear scaling of computation time with problem dimension. Numerical results show that the GUDR is more accurate than UDR in estimating the standard deviation of the output and has a performance comparable to the method of moments using a third-order Taylor series expansion.</p></p class="citation"></blockquote><h3 id=22--256259-graph-accelerated-non-intrusive-polynomial-chaos-expansion-using-partially-tensor-structured-quadrature-rules-bingran-wang-et-al-2024>(2/2 | 256/259) Graph-accelerated non-intrusive polynomial chaos expansion using partially tensor-structured quadrature rules (Bingran Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingran Wang, Nicholas C. Orndorff, John T. Hwang. (2024)<br><strong>Graph-accelerated non-intrusive polynomial chaos expansion using partially tensor-structured quadrature rules</strong><br><button class=copy-to-clipboard title="Graph-accelerated non-intrusive polynomial chaos expansion using partially tensor-structured quadrature rules" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15614v1.pdf filename=2403.15614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the <b>graph-accelerated</b> non-intrusive polynomial chaos (NIPC) method has been proposed for solving uncertainty quantification (UQ) problems. This method leverages the full-grid integration-based NIPC method to address UQ problems while employing the computational <b>graph</b> transformation approach, AMTC, to accelerate the tensor-grid evaluations. This method exhibits remarkable efficacy on a broad range of low-dimensional (three dimensions or less) UQ problems featuring multidisciplinary models. However, it often does not scale well with problem dimensions due to the exponential increase in the number of quadrature points when using the full-grid quadrature rule. To expand the applicability of this method to a broader range of UQ problems, this paper introduces a new framework for generating a tailored, partially tensor-structured quadrature rule to use with the <b>graph-accelerated</b> NIPC method. This quadrature rule, generated through the designed quadrature approach, possesses a tensor structure that is tailored for the computational model. The selection of the tensor structure is guided by an analysis of the computational <b>graph,</b> ensuring that the quadrature rule effectively capitalizes on the sparsity within the computational <b>graph</b> when paired with the AMTC method. This method has been tested on one 4D and one 6D UQ problem, both originating from aircraft design scenarios and featuring multidisciplinary models. Numerical results show that, when using with <b>graph-accelerated</b> NIPC method, our approach generates a partially tensor-structured quadrature rule that outperforms the full-grid Gauss quadrature and the designed quadrature methods (more than 40% reduction in computational costs) in both of the test problems.</p></p class="citation"></blockquote><h2 id=cond-matdis-nn-1>cond-mat.dis-nn (1)</h2><h3 id=11--257259-spectral-initialization-for-high-dimensional-phase-retrieval-with-biased-spatial-directions-pierre-bousseyroux-et-al-2024>(1/1 | 257/259) Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions (Pierre Bousseyroux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Bousseyroux, Marc Potters. (2024)<br><strong>Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions</strong><br><button class=copy-to-clipboard title="Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.dis-nn<br>Categories: cond-mat-dis-nn, cond-mat.dis-nn, cs-IR, math-PR<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15548v1.pdf filename=2403.15548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore a spectral initialization method that plays a central role in contemporary research on signal estimation in nonconvex scenarios. In a noiseless phase retrieval framework, we precisely analyze the method&rsquo;s performance in the high-dimensional limit when sensing vectors follow a multivariate Gaussian distribution for two rotationally invariant models of the covariance matrix C. In the first model C is a projector on a lower dimensional space while in the second it is a Wishart matrix. Our analytical results extend the well-established case when C is the identity matrix. Our examination shows that the introduction of biased spatial directions leads to a substantial improvement in the spectral method&rsquo;s effectiveness, particularly when the number of measurements is less than the signal&rsquo;s dimension. This extension also consistently reveals a phase transition phenomenon dependent on the ratio between <b>sample</b> <b>size</b> and signal dimension. Surprisingly, both of these models share the same threshold value.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--258259-flip-breakability-a-combinatorial-dichotomy-for-monadically-dependent-graph-classes-jan-dreier-et-al-2024>(1/1 | 258/259) Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes (Jan Dreier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Dreier, Nikolas Mählmann, Szymon Toruńczyk. (2024)<br><strong>Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes</strong><br><button class=copy-to-clipboard title="Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, cs-LO, math-CO, math-LO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.15201v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.15201v2.pdf filename=2403.15201v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A conjecture in algorithmic model theory predicts that the model-checking problem for first-order logic is fixed-parameter tractable on a hereditary <b>graph</b> class if and only if the class is monadically dependent. Originating in model theory, this notion is defined in terms of logic, and encompasses nowhere dense classes, monadically stable classes, and classes of bounded twin-width. Working towards this conjecture, we provide the first two combinatorial characterizations of monadically dependent <b>graph</b> classes. This yields the following dichotomy. On the structure side, we characterize monadic dependence by a Ramsey-theoretic property called flip-breakability. This notion generalizes the notions of uniform quasi-wideness, flip-flatness, and bounded grid rank, which characterize nowhere denseness, monadic stability, and bounded twin-width, respectively, and played a key role in their respective model checking algorithms. Natural restrictions of flip-breakability additionally characterize bounded treewidth and cliquewidth and bounded treedepth and shrubdepth. On the non-structure side, we characterize monadic dependence by explicitly listing few families of forbidden induced subgraphs. This result is analogous to the characterization of nowhere denseness via forbidden subdivided cliques, and allows us to resolve one half of the motivating conjecture: First-order model checking is AW[$*$]-hard on every hereditary <b>graph</b> class that is monadically independent. The result moreover implies that hereditary <b>graph</b> classes which are small, have almost bounded twin-width, or have almost bounded flip-width, are monadically dependent. Lastly, we lift our result to also obtain a combinatorial dichotomy in the more general setting of monadically dependent classes of binary structures.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--259259-reconstructing-the-evolution-history-of-networked-complex-systems-junya-wang-et-al-2024>(1/1 | 259/259) Reconstructing the evolution history of networked complex systems (Junya Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junya Wang, Yi-Jiao Zhang, Cong Xu, Jiaze Li, Jiachen Sun, Jiarong Xie, Ling Feng, Tianshou Zhou, Yanqing Hu. (2024)<br><strong>Reconstructing the evolution history of networked complex systems</strong><br><button class=copy-to-clipboard title="Reconstructing the evolution history of networked complex systems" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-SI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.14983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.14983v1.pdf filename=2403.14983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution processes of complex systems carry key information in the systems&rsquo; functional properties. Applying machine learning algorithms, we demonstrate that the historical formation process of various networked complex systems can be extracted, including protein-protein interaction, ecology, and social network systems. The recovered evolution process has demonstrations of immense scientific values, such as interpreting the evolution of protein-protein interaction network, facilitating structure prediction, and particularly revealing the key co-evolution features of network structures such as preferential attachment, community structure, local <b>clustering,</b> degree-degree correlation that could not be explained collectively by previous theories. Intriguingly, we discover that for large networks, if the performance of the machine learning model is slightly better than a random guess on the pairwise order of links, reliable restoration of the overall network formation process can be achieved. This suggests that evolution history restoration is generally highly feasible on empirical networks.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.23</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.25</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-33>cs.LG (33)</a><ul><li><a href=#133--1259-can-large-language-models-explore-in-context-akshay-krishnamurthy-et-al-2024>(1/33 | 1/259) Can large language models explore in-context? (Akshay Krishnamurthy et al., 2024)</a></li><li><a href=#233--2259-gtc-gnn-transformer-co-contrastive-learning-for-self-supervised-heterogeneous-graph-representation-yundong-sun-et-al-2024>(2/33 | 2/259) GTC: GNN-Transformer Co-contrastive Learning for Self-supervised Heterogeneous Graph Representation (Yundong Sun et al., 2024)</a></li><li><a href=#333--3259-gtagcn-generalized-topology-adaptive-graph-convolutional-networks-sukhdeep-singh-et-al-2024>(3/33 | 3/259) GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks (Sukhdeep Singh et al., 2024)</a></li><li><a href=#433--4259-improved-long-short-term-memory-based-wastewater-treatment-simulators-for-deep-reinforcement-learning-esmaeel-mohammadi-et-al-2024>(4/33 | 4/259) Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning (Esmaeel Mohammadi et al., 2024)</a></li><li><a href=#533--5259-magic-for-the-age-of-quantized-dnns-yoshihide-sawada-et-al-2024>(5/33 | 5/259) Magic for the Age of Quantized DNNs (Yoshihide Sawada et al., 2024)</a></li><li><a href=#633--6259-robust-optimization-for-adversarial-learning-with-finite-sample-complexity-guarantees-andré-bertolace-et-al-2024>(6/33 | 6/259) Robust optimization for adversarial learning with finite sample complexity guarantees (André Bertolace et al., 2024)</a></li><li><a href=#733--7259-self-improvement-for-neural-combinatorial-optimization-sample-without-replacement-but-improvement-jonathan-pirnay-et-al-2024>(7/33 | 7/259) Self-Improvement for Neural Combinatorial Optimization: Sample without Replacement, but Improvement (Jonathan Pirnay et al., 2024)</a></li><li><a href=#833--8259-dp-dueling-learning-from-preference-feedback-without-compromising-user-privacy-aadirupa-saha-et-al-2024>(8/33 | 8/259) DP-Dueling: Learning from Preference Feedback without Compromising User Privacy (Aadirupa Saha et al., 2024)</a></li><li><a href=#933--9259-simple-graph-condensation-zhenbang-xiao-et-al-2024>(9/33 | 9/259) Simple Graph Condensation (Zhenbang Xiao et al., 2024)</a></li><li><a href=#1033--10259-deep-learning-based-method-for-weather-forecasting-a-case-study-in-itoshima-yuzhong-cheng-et-al-2024>(10/33 | 10/259) Deep learning-based method for weather forecasting: A case study in Itoshima (Yuzhong Cheng et al., 2024)</a></li><li><a href=#1133--11259-pde-cnns-axiomatic-derivations-and-applications-gijs-bellaard-et-al-2024>(11/33 | 11/259) PDE-CNNs: Axiomatic Derivations and Applications (Gijs Bellaard et al., 2024)</a></li><li><a href=#1233--12259-addressing-concept-shift-in-online-time-series-forecasting-detect-then-adapt-yifan-zhang-et-al-2024>(12/33 | 12/259) Addressing Concept Shift in Online Time Series Forecasting: Detect-then-Adapt (YiFan Zhang et al., 2024)</a></li><li><a href=#1333--13259-multiple-input-auto-encoder-guided-feature-selection-for-iot-intrusion-detection-systems-phai-vu-dinh-et-al-2024>(13/33 | 13/259) Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems (Phai Vu Dinh et al., 2024)</a></li><li><a href=#1433--14259-coda-a-cost-efficient-test-time-domain-adaptation-mechanism-for-har-minghui-qiu-et-al-2024>(14/33 | 14/259) CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR (Minghui Qiu et al., 2024)</a></li><li><a href=#1533--15259-exploring-the-task-agnostic-trait-of-self-supervised-learning-in-the-context-of-detecting-mental-disorders-rohan-kumar-gupta-et-al-2024>(15/33 | 15/259) Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders (Rohan Kumar Gupta et al., 2024)</a></li><li><a href=#1633--16259-quantification-using-permutation-invariant-networks-based-on-histograms-olaya-pérez-mon-et-al-2024>(16/33 | 16/259) Quantification using Permutation-Invariant Networks based on Histograms (Olaya Pérez-Mon et al., 2024)</a></li><li><a href=#1733--17259-adapprox-adaptive-approximation-in-adam-optimization-via-randomized-low-rank-matrices-pengxiang-zhao-et-al-2024>(17/33 | 17/259) Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices (Pengxiang Zhao et al., 2024)</a></li><li><a href=#1833--18259-do-not-trust-what-you-trust-miscalibration-in-semi-supervised-learning-shambhavi-mishra-et-al-2024>(18/33 | 18/259) Do not trust what you trust: Miscalibration in Semi-supervised Learning (Shambhavi Mishra et al., 2024)</a></li><li><a href=#1933--19259-parametric-encoding-with-attention-and-convolution-mitigate-spectral-bias-of-neural-partial-differential-equation-solvers-mehdi-shishehbor-et-al-2024>(19/33 | 19/259) Parametric Encoding with Attention and Convolution Mitigate Spectral Bias of Neural Partial Differential Equation Solvers (Mehdi Shishehbor et al., 2024)</a></li><li><a href=#2033--20259-planning-with-a-learned-policy-basis-to-optimally-solve-complex-tasks-guillermo-infante-et-al-2024>(20/33 | 20/259) Planning with a Learned Policy Basis to Optimally Solve Complex Tasks (Guillermo Infante et al., 2024)</a></li><li><a href=#2133--21259-parametric-pde-control-with-deep-reinforcement-learning-and-differentiable-l0-sparse-polynomial-policies-nicolò-botteghi-et-al-2024>(21/33 | 21/259) Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies (Nicolò Botteghi et al., 2024)</a></li><li><a href=#2233--22259-federated-bayesian-deep-learning-the-application-of-statistical-aggregation-methods-to-bayesian-models-john-fischer-et-al-2024>(22/33 | 22/259) Federated Bayesian Deep Learning: The Application of Statistical Aggregation Methods to Bayesian Models (John Fischer et al., 2024)</a></li><li><a href=#2333--23259-early-period-of-training-impacts-out-of-distribution-generalization-chen-cecilia-liu-et-al-2024>(23/33 | 23/259) Early Period of Training Impacts Out-of-Distribution Generalization (Chen Cecilia Liu et al., 2024)</a></li><li><a href=#2433--24259-an-in-depth-analysis-of-data-reduction-methods-for-sustainable-deep-learning-víctor-toscano-durán-et-al-2024>(24/33 | 24/259) An In-Depth Analysis of Data Reduction Methods for Sustainable Deep Learning (Víctor Toscano-Durán et al., 2024)</a></li><li><a href=#2533--25259-on-the-convergence-of-adam-under-non-uniform-smoothness-separability-from-sgdm-and-beyond-bohan-wang-et-al-2024>(25/33 | 25/259) On the Convergence of Adam under Non-uniform Smoothness: Separability from SGDM and Beyond (Bohan Wang et al., 2024)</a></li><li><a href=#2633--26259-improving-forward-compatibility-in-class-incremental-learning-by-increasing-representation-rank-and-feature-richness-jaeill-kim-et-al-2024>(26/33 | 26/259) Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness (Jaeill Kim et al., 2024)</a></li><li><a href=#2733--27259-active-learning-for-regression-based-on-wasserstein-distance-and-groupsort-neural-networks-benjamin-bobbia-et-al-2024>(27/33 | 27/259) Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks (Benjamin Bobbia et al., 2024)</a></li><li><a href=#2833--28259-automated-feature-selection-for-inverse-reinforcement-learning-daulet-baimukashev-et-al-2024>(28/33 | 28/259) Automated Feature Selection for Inverse Reinforcement Learning (Daulet Baimukashev et al., 2024)</a></li><li><a href=#2933--29259-robust-conformal-prediction-under-distribution-shift-via-physics-informed-structural-causal-model-rui-xu-et-al-2024>(29/33 | 29/259) Robust Conformal Prediction under Distribution Shift via Physics-Informed Structural Causal Model (Rui Xu et al., 2024)</a></li><li><a href=#3033--30259-insights-into-the-lottery-ticket-hypothesis-and-iterative-magnitude-pruning-tausifa-jan-saleem-et-al-2024>(30/33 | 30/259) Insights into the Lottery Ticket Hypothesis and Iterative Magnitude Pruning (Tausifa Jan Saleem et al., 2024)</a></li><li><a href=#3133--31259-unifying-lane-level-traffic-prediction-from-a-graph-structural-perspective-benchmark-and-baseline-shuhao-li-et-al-2024>(31/33 | 31/259) Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline (Shuhao Li et al., 2024)</a></li><li><a href=#3233--32259-grey-informed-neural-network-for-time-series-forecasting-wanli-xie-et-al-2024>(32/33 | 32/259) Grey-informed neural network for time-series forecasting (Wanli Xie et al., 2024)</a></li><li><a href=#3333--33259-transition-graph-properties-of-target-class-classification-levon-aslanyan-et-al-2024>(33/33 | 33/259) Transition Graph Properties of Target Class Classification (Levon Aslanyan et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#17--34259-comprehensive-evaluation-and-insights-into-the-use-of-large-language-models-in-the-automation-of-behavior-driven-development-acceptance-test-formulation-shanthi-karpurapu-et-al-2024>(1/7 | 34/259) Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation (Shanthi Karpurapu et al., 2024)</a></li><li><a href=#27--35259-allhands-ask-me-anything-on-large-scale-verbatim-feedback-via-large-language-models-chaoyun-zhang-et-al-2024>(2/7 | 35/259) AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large Language Models (Chaoyun Zhang et al., 2024)</a></li><li><a href=#37--36259-just-another-copy-and-paste-comparing-the-security-vulnerabilities-of-chatgpt-generated-code-and-stackoverflow-answers-sivana-hamer-et-al-2024>(3/7 | 36/259) Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers (Sivana Hamer et al., 2024)</a></li><li><a href=#47--37259-enhancing-testing-at-meta-with-rich-state-simulated-populations-nadia-alshahwan-et-al-2024>(4/7 | 37/259) Enhancing Testing at Meta with Rich-State Simulated Populations (Nadia Alshahwan et al., 2024)</a></li><li><a href=#57--38259-an-exploratory-investigation-into-code-license-infringements-in-large-language-model-training-datasets-jonathan-katzy-et-al-2024>(5/7 | 38/259) An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets (Jonathan Katzy et al., 2024)</a></li><li><a href=#67--39259-on-the-generalizability-of-deep-learning-based-code-completion-across-programming-language-versions-matteo-ciniselli-et-al-2024>(6/7 | 39/259) On the Generalizability of Deep Learning-based Code Completion Across Programming Language Versions (Matteo Ciniselli et al., 2024)</a></li><li><a href=#77--40259-testing-for-fault-diversity-in-reinforcement-learning-quentin-mazouni-et-al-2024>(7/7 | 40/259) Testing for Fault Diversity in Reinforcement Learning (Quentin Mazouni et al., 2024)</a></li></ul></li><li><a href=#csir-2>cs.IR (2)</a><ul><li><a href=#12--41259-bilateral-unsymmetrical-graph-contrastive-learning-for-recommendation-jiaheng-yu-et-al-2024>(1/2 | 41/259) Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation (Jiaheng Yu et al., 2024)</a></li><li><a href=#22--42259-followir-evaluating-and-teaching-information-retrieval-models-to-follow-instructions-orion-weller-et-al-2024>(2/2 | 42/259) FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions (Orion Weller et al., 2024)</a></li></ul></li><li><a href=#cscl-27>cs.CL (27)</a><ul><li><a href=#127--43259-masontigers-at-semeval-2024-task-9-solving-puzzles-with-an-ensemble-of-chain-of-thoughts-md-nishat-raihan-et-al-2024>(1/27 | 43/259) MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts (Md Nishat Raihan et al., 2024)</a></li><li><a href=#227--44259-on-zero-shot-counterspeech-generation-by-llms-punyajoy-saha-et-al-2024>(2/27 | 44/259) On Zero-Shot Counterspeech Generation by LLMs (Punyajoy Saha et al., 2024)</a></li><li><a href=#327--45259-towards-knowledge-grounded-natural-language-understanding-and-generation-chenxi-whitehouse-2024>(3/27 | 45/259) Towards Knowledge-Grounded Natural Language Understanding and Generation (Chenxi Whitehouse, 2024)</a></li><li><a href=#427--46259-college-concept-embedding-generation-for-large-language-models-ryan-teehan-et-al-2024>(4/27 | 46/259) CoLLEGe: Concept Embedding Generation for Large Language Models (Ryan Teehan et al., 2024)</a></li><li><a href=#527--47259-evidence-driven-retrieval-augmented-response-generation-for-online-misinformation-zhenrui-yue-et-al-2024>(5/27 | 47/259) Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation (Zhenrui Yue et al., 2024)</a></li><li><a href=#627--48259-stance-reasoner-zero-shot-stance-detection-on-social-media-with-explicit-reasoning-maksym-taranukhin-et-al-2024>(6/27 | 48/259) Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning (Maksym Taranukhin et al., 2024)</a></li><li><a href=#727--49259-knowla-enhancing-parameter-efficient-finetuning-with-knowledgeable-adaptation-xindi-luo-et-al-2024>(7/27 | 49/259) KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation (Xindi Luo et al., 2024)</a></li><li><a href=#827--50259-imagination-augmented-generation-learning-to-imagine-richer-context-for-question-answering-over-large-language-models-huanxuan-liao-et-al-2024>(8/27 | 50/259) Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models (Huanxuan Liao et al., 2024)</a></li><li><a href=#927--51259-chisiec-an-information-extraction-corpus-for-ancient-chinese-history-xuemei-tang-et-al-2024>(9/27 | 51/259) CHisIEC: An Information Extraction Corpus for Ancient Chinese History (Xuemei Tang et al., 2024)</a></li><li><a href=#1027--52259-esg-classification-by-implicit-rule-learning-via-gpt-4-hyo-jeong-yun-et-al-2024>(10/27 | 52/259) ESG Classification by Implicit Rule Learning via GPT-4 (Hyo Jeong Yun et al., 2024)</a></li><li><a href=#1127--53259-masontigers-at-semeval-2024-task-8-performance-analysis-of-transformer-based-models-on-machine-generated-text-detection-sadiya-sayara-chowdhury-puspo-et-al-2024>(11/27 | 53/259) MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection (Sadiya Sayara Chowdhury Puspo et al., 2024)</a></li><li><a href=#1227--54259-event-temporal-relation-extraction-based-on-retrieval-augmented-on-llms-xiaobin-zhang-et-al-2024>(12/27 | 54/259) Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs (Xiaobin Zhang et al., 2024)</a></li><li><a href=#1327--55259-llm2llm-boosting-llms-with-novel-iterative-data-enhancement-nicholas-lee-et-al-2024>(13/27 | 55/259) LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement (Nicholas Lee et al., 2024)</a></li><li><a href=#1427--56259-text-clustering-with-llm-embeddings-alina-petukhova-et-al-2024>(14/27 | 56/259) Text clustering with LLM embeddings (Alina Petukhova et al., 2024)</a></li><li><a href=#1527--57259-masontigers-at-semeval-2024-task-1-an-ensemble-approach-for-semantic-textual-relatedness-dhiman-goswami-et-al-2024>(15/27 | 57/259) MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness (Dhiman Goswami et al., 2024)</a></li><li><a href=#1627--58259-multi-review-fusion-in-context-aviv-slobodkin-et-al-2024>(16/27 | 58/259) Multi-Review Fusion-in-Context (Aviv Slobodkin et al., 2024)</a></li><li><a href=#1727--59259-investigating-the-performance-of-language-models-for-completing-code-in-functional-programming-languages-a-haskell-case-study-tim-van-dam-et-al-2024>(17/27 | 59/259) Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study (Tim van Dam et al., 2024)</a></li><li><a href=#1827--60259-risk-and-response-in-large-language-models-evaluating-key-threat-categories-bahareh-harandizadeh-et-al-2024>(18/27 | 60/259) Risk and Response in Large Language Models: Evaluating Key Threat Categories (Bahareh Harandizadeh et al., 2024)</a></li><li><a href=#1927--61259-enhancing-effectiveness-and-robustness-in-a-low-resource-regime-via-decision-boundary-aware-data-augmentation-kyohoon-jin-et-al-2024>(19/27 | 61/259) Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation (Kyohoon Jin et al., 2024)</a></li><li><a href=#2027--62259-attention-driven-reasoning-unlocking-the-potential-of-large-language-models-bingli-liao-et-al-2024>(20/27 | 62/259) Attention-Driven Reasoning: Unlocking the Potential of Large Language Models (Bingli Liao et al., 2024)</a></li><li><a href=#2127--63259-limgen-probing-the-llms-for-generating-suggestive-limitations-of-research-papers-abdur-rahman-bin-md-faizullah-et-al-2024>(21/27 | 63/259) LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers (Abdur Rahman Bin Md Faizullah et al., 2024)</a></li><li><a href=#2227--64259-co-fun-a-german-dataset-on-company-outsourcing-in-fund-prospectuses-for-named-entity-recognition-and-relation-extraction-neda-foroutan-et-al-2024>(22/27 | 64/259) CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction (Neda Foroutan et al., 2024)</a></li><li><a href=#2327--65259-hierarchical-skip-decoding-for-efficient-autoregressive-text-generation-yunqi-zhu-et-al-2024>(23/27 | 65/259) Hierarchical Skip Decoding for Efficient Autoregressive Text Generation (Yunqi Zhu et al., 2024)</a></li><li><a href=#2427--66259-comprehensive-reassessment-of-large-scale-evaluation-outcomes-in-llms-a-multifaceted-statistical-approach-kun-sun-et-al-2024>(24/27 | 66/259) Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach (Kun Sun et al., 2024)</a></li><li><a href=#2527--67259-language-models-in-dialogue-conversational-maxims-for-human-ai-interactions-erik-miehling-et-al-2024>(25/27 | 67/259) Language Models in Dialogue: Conversational Maxims for Human-AI Interactions (Erik Miehling et al., 2024)</a></li><li><a href=#2627--68259-ctsm-combining-trait-and-state-emotions-for-empathetic-response-model-wang-yufeng-et-al-2024>(26/27 | 68/259) CTSM: Combining Trait and State Emotions for Empathetic Response Model (Wang Yufeng et al., 2024)</a></li><li><a href=#2727--69259-a-single-linear-layer-yields-task-adapted-low-rank-matrices-hwichan-kim-et-al-2024>(27/27 | 69/259) A Single Linear Layer Yields Task-Adapted Low-Rank Matrices (Hwichan Kim et al., 2024)</a></li></ul></li><li><a href=#cscv-72>cs.CV (72)</a><ul><li><a href=#172--70259-medpromptx-grounded-multimodal-prompting-for-chest-x-ray-diagnosis-mai-a-shaaban-et-al-2024>(1/72 | 70/259) MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis (Mai A. Shaaban et al., 2024)</a></li><li><a href=#272--71259-controlled-training-data-generation-with-diffusion-models-teresa-yeo-et-al-2024>(2/72 | 71/259) Controlled Training Data Generation with Diffusion Models (Teresa Yeo et al., 2024)</a></li><li><a href=#372--72259-internvideo2-scaling-video-foundation-models-for-multimodal-video-understanding-yi-wang-et-al-2024>(3/72 | 72/259) InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding (Yi Wang et al., 2024)</a></li><li><a href=#472--73259-llava-prumerge-adaptive-token-reduction-for-efficient-large-multimodal-models-yuzhang-shang-et-al-2024>(4/72 | 73/259) LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models (Yuzhang Shang et al., 2024)</a></li><li><a href=#572--74259-clip-vqdiffusion--langauge-free-training-of-text-to-image-generation-using-clip-and-vector-quantized-diffusion-model-seungdae-han-et-al-2024>(5/72 | 74/259) CLIP-VQDiffusion : Langauge Free Training of Text To Image generation using CLIP and vector quantized diffusion model (Seungdae Han et al., 2024)</a></li><li><a href=#672--75259-cell-variational-information-bottleneck-network-zhonghua-zhai-et-al-2024>(6/72 | 75/259) Cell Variational Information Bottleneck Network (Zhonghua Zhai et al., 2024)</a></li><li><a href=#772--76259-cartoon-hallucinations-detection-pose-aware-in-context-visual-learning-bumsoo-kim-et-al-2024>(7/72 | 76/259) Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning (Bumsoo Kim et al., 2024)</a></li><li><a href=#872--77259-bsnet-box-supervised-simulation-assisted-mean-teacher-for-3d-instance-segmentation-jiahao-lu-et-al-2024>(8/72 | 77/259) BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (Jiahao Lu et al., 2024)</a></li><li><a href=#972--78259-parformer-vision-transformer-baseline-with-parallel-local-global-token-mixer-and-convolution-attention-patch-embedding-novendra-setyawan-et-al-2024>(9/72 | 78/259) ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding (Novendra Setyawan et al., 2024)</a></li><li><a href=#1072--79259-geometric-generative-models-based-on-morphological-equivariant-pdes-and-gans-el-hadji-s-diop-et-al-2024>(10/72 | 79/259) Geometric Generative Models based on Morphological Equivariant PDEs and GANs (El Hadji S. Diop et al., 2024)</a></li><li><a href=#1172--80259-trajectory-regularization-enhances-self-supervised-geometric-representation-jiayun-wang-et-al-2024>(11/72 | 80/259) Trajectory Regularization Enhances Self-Supervised Geometric Representation (Jiayun Wang et al., 2024)</a></li><li><a href=#1272--81259-selectively-informative-description-can-reduce-undesired-embedding-entanglements-in-text-to-image-personalization-jimyeong-kim-et-al-2024>(12/72 | 81/259) Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization (Jimyeong Kim et al., 2024)</a></li><li><a href=#1372--82259-gcn-devlstm-path-development-for-skeleton-based-action-recognition-lei-jiang-et-al-2024>(13/72 | 82/259) GCN-DevLSTM: Path Development for Skeleton-Based Action Recognition (Lei Jiang et al., 2024)</a></li><li><a href=#1472--83259-mscotdet-language-driven-multi-modal-fusion-for-improved-multispectral-pedestrian-detection-taeheon-kim-et-al-2024>(14/72 | 83/259) MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection (Taeheon Kim et al., 2024)</a></li><li><a href=#1572--84259-long-clip-unlocking-the-long-text-capability-of-clip-beichen-zhang-et-al-2024>(15/72 | 84/259) Long-CLIP: Unlocking the Long-Text Capability of CLIP (Beichen Zhang et al., 2024)</a></li><li><a href=#1672--85259-point-detr3d-leveraging-imagery-data-with-spatial-point-prior-for-weakly-semi-supervised-3d-object-detection-hongzhi-gao-et-al-2024>(16/72 | 85/259) Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection (Hongzhi Gao et al., 2024)</a></li><li><a href=#1772--86259-dreamflow-high-quality-text-to-3d-generation-by-approximating-probability-flow-kyungmin-lee-et-al-2024>(17/72 | 86/259) DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow (Kyungmin Lee et al., 2024)</a></li><li><a href=#1872--87259-stag4d-spatial-temporal-anchored-generative-4d-gaussians-yifei-zeng-et-al-2024>(18/72 | 87/259) STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians (Yifei Zeng et al., 2024)</a></li><li><a href=#1972--88259-self-supervised-backbone-framework-for-diverse-agricultural-vision-tasks-sudhir-sornapudi-et-al-2024>(19/72 | 88/259) Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks (Sudhir Sornapudi et al., 2024)</a></li><li><a href=#2072--89259-fairerclip-debiasing-clips-zero-shot-predictions-using-functions-in-rkhss-sepehr-dehdashtian-et-al-2024>(20/72 | 89/259) FairerCLIP: Debiasing CLIP&rsquo;s Zero-Shot Predictions using Functions in RKHSs (Sepehr Dehdashtian et al., 2024)</a></li><li><a href=#2172--90259-simba-simplified-mamba-based-architecture-for-vision-and-multivariate-time-series-badri-n-patro-et-al-2024>(21/72 | 90/259) SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series (Badri N. Patro et al., 2024)</a></li><li><a href=#2272--91259-piecewise-linear-manifolds-for-deep-metric-learning-shubhang-bhatnagar-et-al-2024>(22/72 | 91/259) Piecewise-Linear Manifolds for Deep Metric Learning (Shubhang Bhatnagar et al., 2024)</a></li><li><a href=#2372--92259-augmented-reality-based-simulated-data-arsim-with-multi-view-consistency-for-av-perception-networks-aqeel-anwar-et-al-2024>(23/72 | 92/259) Augmented Reality based Simulated Data (ARSim) with multi-view consistency for AV perception networks (Aqeel Anwar et al., 2024)</a></li><li><a href=#2472--93259-shadow-generation-for-composite-image-using-diffusion-model-qingyang-liu-et-al-2024>(24/72 | 93/259) Shadow Generation for Composite Image Using Diffusion model (Qingyang Liu et al., 2024)</a></li><li><a href=#2572--94259-transfer-clip-for-generalizable-image-denoising-jun-cheng-et-al-2024>(25/72 | 94/259) Transfer CLIP for Generalizable Image Denoising (Jun Cheng et al., 2024)</a></li><li><a href=#2672--95259-continual-vision-and-language-navigation-seongjun-jeong-et-al-2024>(26/72 | 95/259) Continual Vision-and-Language Navigation (Seongjun Jeong et al., 2024)</a></li><li><a href=#2772--96259-vehicle-detection-performance-in-nordic-region-hamam-mokayed-et-al-2024>(27/72 | 96/259) Vehicle Detection Performance in Nordic Region (Hamam Mokayed et al., 2024)</a></li><li><a href=#2872--97259-improve-cross-domain-mixed-sampling-with-guidance-training-for-adaptive-segmentation-wenlve-zhou-et-al-2024>(28/72 | 97/259) Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation (Wenlve Zhou et al., 2024)</a></li><li><a href=#2972--98259-gpt-connect-interaction-between-text-driven-human-motion-generator-and-3d-scenes-in-a-training-free-manner-haoxuan-qu-et-al-2024>(29/72 | 98/259) GPT-Connect: Interaction between Text-Driven Human Motion Generator and 3D Scenes in a Training-free Manner (Haoxuan Qu et al., 2024)</a></li><li><a href=#3072--99259-is-fusion-instance-scene-collaborative-fusion-for-multimodal-3d-object-detection-junbo-yin-et-al-2024>(30/72 | 99/259) IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection (Junbo Yin et al., 2024)</a></li><li><a href=#3172--100259-neural-plasticity-inspired-foundation-model-for-observing-the-earth-crossing-modalities-zhitong-xiong-et-al-2024>(31/72 | 100/259) Neural Plasticity-Inspired Foundation Model for Observing the Earth Crossing Modalities (Zhitong Xiong et al., 2024)</a></li><li><a href=#3272--101259-mm-diff-high-fidelity-image-personalization-via-multi-modal-condition-integration-zhichao-wei-et-al-2024>(32/72 | 101/259) MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration (Zhichao Wei et al., 2024)</a></li><li><a href=#3372--102259-forward-learning-for-gradient-based-black-box-saliency-map-generation-zeliang-zhang-et-al-2024>(33/72 | 102/259) Forward Learning for Gradient-based Black-box Saliency Map Generation (Zeliang Zhang et al., 2024)</a></li><li><a href=#3472--103259-hyperbolic-metric-learning-for-visual-outlier-detection-alvaro-gonzalez-jimenez-et-al-2024>(34/72 | 103/259) Hyperbolic Metric Learning for Visual Outlier Detection (Alvaro Gonzalez-Jimenez et al., 2024)</a></li><li><a href=#3572--104259-lsk3dnet-towards-effective-and-efficient-3d-perception-with-large-sparse-kernels-tuo-feng-et-al-2024>(35/72 | 104/259) LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels (Tuo Feng et al., 2024)</a></li><li><a href=#3672--105259-fastcad-real-time-cad-retrieval-and-alignment-from-scans-and-videos-florian-langer-et-al-2024>(36/72 | 105/259) FastCAD: Real-Time CAD Retrieval and Alignment from Scans and Videos (Florian Langer et al., 2024)</a></li><li><a href=#3772--106259-avt2-dwf-improving-deepfake-detection-with-audio-visual-fusion-and-dynamic-weighting-strategies-rui-wang-et-al-2024>(37/72 | 106/259) AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies (Rui Wang et al., 2024)</a></li><li><a href=#3872--107259-an-optimization-framework-to-enforce-multi-view-consistency-for-texturing-3d-meshes-using-pre-trained-text-to-image-models-zhengyi-zhao-et-al-2024>(38/72 | 107/259) An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models (Zhengyi Zhao et al., 2024)</a></li><li><a href=#3972--108259-wscloc-weakly-supervised-sparse-view-camera-relocalization-jialu-wang-et-al-2024>(39/72 | 108/259) WSCLoc: Weakly-Supervised Sparse-View Camera Relocalization (Jialu Wang et al., 2024)</a></li><li><a href=#4072--109259-spectral-motion-alignment-for-video-motion-transfer-using-diffusion-models-geon-yeong-park-et-al-2024>(40/72 | 109/259) Spectral Motion Alignment for Video Motion Transfer using Diffusion Models (Geon Yeong Park et al., 2024)</a></li><li><a href=#4172--110259-reasoning-enhanced-object-centric-learning-for-videos-jian-li-et-al-2024>(41/72 | 110/259) Reasoning-Enhanced Object-Centric Learning for Videos (Jian Li et al., 2024)</a></li><li><a href=#4272--111259-anytime-anywhere-anyone-investigating-the-feasibility-of-segment-anything-model-for-crowd-sourcing-medical-image-annotations-pranav-kulkarni-et-al-2024>(42/72 | 111/259) Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations (Pranav Kulkarni et al., 2024)</a></li><li><a href=#4372--112259-syncs-synthetic-data-and-contrastive-self-supervised-training-for-central-sulcus-segmentation-vladyslav-zalevskyi-et-al-2024>(43/72 | 112/259) SYNCS: Synthetic Data and Contrastive Self-Supervised Training for Central Sulcus Segmentation (Vladyslav Zalevskyi et al., 2024)</a></li><li><a href=#4472--113259-towards-a-comprehensive-efficient-and-promptable-anatomic-structure-segmentation-model-using-3d-whole-body-ct-scans-heng-guo-et-al-2024>(44/72 | 113/259) Towards a Comprehensive, Efficient and Promptable Anatomic Structure Segmentation Model using 3D Whole-body CT Scans (Heng Guo et al., 2024)</a></li><li><a href=#4572--114259-toward-tiny-and-high-quality-facial-makeup-with-data-amplify-learning-qiaoqiao-jin-et-al-2024>(45/72 | 114/259) Toward Tiny and High-quality Facial Makeup with Data Amplify Learning (Qiaoqiao Jin et al., 2024)</a></li><li><a href=#4672--115259-multimodal-fusion-with-pre-trained-model-features-in-affective-behaviour-analysis-in-the-wild-zhuofan-wen-et-al-2024>(46/72 | 115/259) Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild (Zhuofan Wen et al., 2024)</a></li><li><a href=#4772--116259-latte3d-large-scale-amortized-text-to-enhanced3d-synthesis-kevin-xie-et-al-2024>(47/72 | 116/259) LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis (Kevin Xie et al., 2024)</a></li><li><a href=#4872--117259-tri-perspective-view-decomposition-for-geometry-aware-depth-completion-zhiqiang-yan-et-al-2024>(48/72 | 117/259) Tri-Perspective View Decomposition for Geometry-Aware Depth Completion (Zhiqiang Yan et al., 2024)</a></li><li><a href=#4972--118259-efficiently-assemble-normalization-layers-and-regularization-for-federated-domain-generalization-khiem-le-et-al-2024>(49/72 | 118/259) Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization (Khiem Le et al., 2024)</a></li><li><a href=#5072--119259-diffusionmtl-learning-multi-task-denoising-diffusion-model-from-partially-annotated-data-hanrong-ye-et-al-2024>(50/72 | 119/259) DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data (Hanrong Ye et al., 2024)</a></li><li><a href=#5172--120259-cr3dt-camera-radar-fusion-for-3d-detection-and-tracking-nicolas-baumann-et-al-2024>(51/72 | 120/259) CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking (Nicolas Baumann et al., 2024)</a></li><li><a href=#5272--121259-semantic-gaussians-open-vocabulary-scene-understanding-with-3d-gaussian-splatting-jun-guo-et-al-2024>(52/72 | 121/259) Semantic Gaussians: Open-Vocabulary Scene Understanding with 3D Gaussian Splatting (Jun Guo et al., 2024)</a></li><li><a href=#5372--122259-interfusion-text-driven-generation-of-3d-human-object-interaction-sisi-dai-et-al-2024>(53/72 | 122/259) InterFusion: Text-Driven Generation of 3D Human-Object Interaction (Sisi Dai et al., 2024)</a></li><li><a href=#5472--123259-pixel-gs-density-control-with-pixel-aware-gradient-for-3d-gaussian-splatting-zheng-zhang-et-al-2024>(54/72 | 123/259) Pixel-GS: Density Control with Pixel-aware Gradient for 3D Gaussian Splatting (Zheng Zhang et al., 2024)</a></li><li><a href=#5572--124259-themestation-generating-theme-aware-3d-assets-from-few-exemplars-zhenwei-wang-et-al-2024>(55/72 | 124/259) ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars (Zhenwei Wang et al., 2024)</a></li><li><a href=#5672--125259-dragapart-learning-a-part-level-motion-prior-for-articulated-objects-ruining-li-et-al-2024>(56/72 | 125/259) DragAPart: Learning a Part-Level Motion Prior for Articulated Objects (Ruining Li et al., 2024)</a></li><li><a href=#5772--126259-lego-leveraging-a-surface-deformation-network-for-animatable-stylized-face-generation-with-one-example-soyeon-yoon-et-al-2024>(57/72 | 126/259) LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example (Soyeon Yoon et al., 2024)</a></li><li><a href=#5872--127259-your-image-is-my-video-reshaping-the-receptive-field-via-image-to-video-differentiable-autoaugmentation-and-fusion-sofia-casarin-et-al-2024>(58/72 | 127/259) Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion (Sofia Casarin et al., 2024)</a></li><li><a href=#5972--128259-sfod-spiking-fusion-object-detector-yimeng-fan-et-al-2024>(59/72 | 128/259) SFOD: Spiking Fusion Object Detector (Yimeng Fan et al., 2024)</a></li><li><a href=#6072--129259-modular-deep-active-learning-framework-for-image-annotation-a-technical-report-for-the-ophthalmo-ai-project-md-abdul-kadir-et-al-2024>(60/72 | 129/259) Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project (Md Abdul Kadir et al., 2024)</a></li><li><a href=#6172--130259-gradient-based-sampling-for-class-imbalanced-semi-supervised-object-detection-jiaming-li-et-al-2024>(61/72 | 130/259) Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection (Jiaming Li et al., 2024)</a></li><li><a href=#6272--131259-ifsenet--harnessing-sparse-iterations-for-interactive-few-shot-segmentation-excellence-shreyas-chandgothia-et-al-2024>(62/72 | 131/259) IFSENet : Harnessing Sparse Iterations for Interactive Few-shot Segmentation Excellence (Shreyas Chandgothia et al., 2024)</a></li><li><a href=#6372--132259-an-integrated-neighborhood-and-scale-information-network-for-open-pit-mine-change-detection-in-high-resolution-remote-sensing-images-zilin-xie-et-al-2024>(63/72 | 132/259) An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images (Zilin Xie et al., 2024)</a></li><li><a href=#6472--133259-vrso-visual-centric-reconstruction-for-static-object-annotation-chenyao-yu-et-al-2024>(64/72 | 133/259) VRSO: Visual-Centric Reconstruction for Static Object Annotation (Chenyao Yu et al., 2024)</a></li><li><a href=#6572--134259-cell-tracking-according-to-biological-needs----strong-mitosis-aware-random-finite-sets-tracker-with-aleatoric-uncertainty-timo-kaiser-et-al-2024>(65/72 | 134/259) Cell Tracking according to Biological Needs &ndash; Strong Mitosis-aware Random-finite Sets Tracker with Aleatoric Uncertainty (Timo Kaiser et al., 2024)</a></li><li><a href=#6672--135259-clean-image-backdoor-attacks-dazhong-rong-et-al-2024>(66/72 | 135/259) Clean-image Backdoor Attacks (Dazhong Rong et al., 2024)</a></li><li><a href=#6772--136259-generative-active-learning-for-image-synthesis-personalization-xulu-zhang-et-al-2024>(67/72 | 136/259) Generative Active Learning for Image Synthesis Personalization (Xulu Zhang et al., 2024)</a></li><li><a href=#6872--137259-a-multimodal-approach-for-cross-domain-image-retrieval-lucas-iijima-et-al-2024>(68/72 | 137/259) A Multimodal Approach for Cross-Domain Image Retrieval (Lucas Iijima et al., 2024)</a></li><li><a href=#6972--138259-gani-global-and-near-field-illumination-aware-neural-inverse-rendering-jiaye-wu-et-al-2024>(69/72 | 138/259) GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering (Jiaye Wu et al., 2024)</a></li><li><a href=#7072--139259-recent-trends-in-3d-reconstruction-of-general-non-rigid-scenes-raza-yunus-et-al-2024>(70/72 | 139/259) Recent Trends in 3D Reconstruction of General Non-Rigid Scenes (Raza Yunus et al., 2024)</a></li><li><a href=#7172--140259-survey-on-modeling-of-articulated-objects-jiayi-liu-et-al-2024>(71/72 | 140/259) Survey on Modeling of Articulated Objects (Jiayi Liu et al., 2024)</a></li><li><a href=#7272--141259-an-open-world-diverse-cross-spatial-temporal-benchmark-for-dynamic-wild-person-re-identification-lei-zhang-et-al-2024>(72/72 | 141/259) An Open-World, Diverse, Cross-Spatial-Temporal Benchmark for Dynamic Wild Person Re-Identification (Lei Zhang et al., 2024)</a></li></ul></li><li><a href=#eessiv-8>eess.IV (8)</a><ul><li><a href=#18--142259-integrating-multiscale-topology-in-digital-pathology-with-pyramidal-graph-convolutional-networks-victor-ibañez-et-al-2024>(1/8 | 142/259) Integrating multiscale topology in digital pathology with pyramidal graph convolutional networks (Victor Ibañez et al., 2024)</a></li><li><a href=#28--143259-weep-a-method-for-spatial-interpretation-of-weakly-supervised-cnn-models-in-computational-pathology-abhinav-sharma-et-al-2024>(2/8 | 143/259) WEEP: A method for spatial interpretation of weakly supervised CNN models in computational pathology (Abhinav Sharma et al., 2024)</a></li><li><a href=#38--144259-evaluating-gpt-4-with-vision-on-detection-of-radiological-findings-on-chest-radiographs-yiliang-zhou-et-al-2024>(3/8 | 144/259) Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs (Yiliang Zhou et al., 2024)</a></li><li><a href=#48--145259-ultrasound-imaging-based-on-the-variance-of-a-diffusion-restoration-model-yuxin-zhang-et-al-2024>(4/8 | 145/259) Ultrasound Imaging based on the Variance of a Diffusion Restoration Model (Yuxin Zhang et al., 2024)</a></li><li><a href=#58--146259-a2dmn-anatomy-aware-dilated-multiscale-network-for-breast-ultrasound-semantic-segmentation-kyle-lucke-et-al-2024>(5/8 | 146/259) A2DMN: Anatomy-Aware Dilated Multiscale Network for Breast Ultrasound Semantic Segmentation (Kyle Lucke et al., 2024)</a></li><li><a href=#68--147259-latent-neural-cellular-automata-for-resource-efficient-image-restoration-andrea-menta-et-al-2024>(6/8 | 147/259) Latent Neural Cellular Automata for Resource-Efficient Image Restoration (Andrea Menta et al., 2024)</a></li><li><a href=#78--148259-improving-cross-domain-brain-tissue-segmentation-in-fetal-mri-with-synthetic-data-vladyslav-zalevskyi-et-al-2024>(7/8 | 148/259) Improving cross-domain brain tissue segmentation in fetal MRI with synthetic data (Vladyslav Zalevskyi et al., 2024)</a></li><li><a href=#88--149259-subjective-quality-assessment-of-compressed-tone-mapped-high-dynamic-range-videos-abhinau-k-venkataramanan-et-al-2024>(8/8 | 149/259) Subjective Quality Assessment of Compressed Tone-Mapped High Dynamic Range Videos (Abhinau K. Venkataramanan et al., 2024)</a></li></ul></li><li><a href=#statml-2>stat.ML (2)</a><ul><li><a href=#12--150259-contrastive-learning-on-multimodal-analysis-of-electronic-health-records-tianxi-cai-et-al-2024>(1/2 | 150/259) Contrastive Learning on Multimodal Analysis of Electronic Health Records (Tianxi Cai et al., 2024)</a></li><li><a href=#22--151259-conformal-online-model-aggregation-matteo-gasparin-et-al-2024>(2/2 | 151/259) Conformal online model aggregation (Matteo Gasparin et al., 2024)</a></li></ul></li><li><a href=#cscr-6>cs.CR (6)</a><ul><li><a href=#16--152259-differentially-private-next-token-prediction-of-large-language-models-james-flemings-et-al-2024>(1/6 | 152/259) Differentially Private Next-Token Prediction of Large Language Models (James Flemings et al., 2024)</a></li><li><a href=#26--153259-privacy-preserving-end-to-end-spoken-language-understanding-yinggui-wang-et-al-2024>(2/6 | 153/259) Privacy-Preserving End-to-End Spoken Language Understanding (Yinggui Wang et al., 2024)</a></li><li><a href=#36--154259-attacking-with-something-that-does-not-exist-low-rate-flood-with-proof-of-non-existence-can-exhaust-dns-resolver-cpu-olivia-gruza-et-al-2024>(3/6 | 154/259) Attacking with Something That Does Not Exist: Low-Rate Flood with &lsquo;Proof of Non-Existence&rsquo; Can Exhaust DNS Resolver CPU (Olivia Gruza et al., 2024)</a></li><li><a href=#46--155259-differentially-private-ad-conversion-measurement-john-delaney-et-al-2024>(4/6 | 155/259) Differentially Private Ad Conversion Measurement (John Delaney et al., 2024)</a></li><li><a href=#56--156259-a-transfer-attack-to-image-watermarks-yuepeng-hu-et-al-2024>(5/6 | 156/259) A Transfer Attack to Image Watermarks (Yuepeng Hu et al., 2024)</a></li><li><a href=#66--157259-twin-auto-encoder-model-for-learning-separable-representation-in-cyberattack-detection-phai-vu-dinh-et-al-2024>(6/6 | 157/259) Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection (Phai Vu Dinh et al., 2024)</a></li></ul></li><li><a href=#csai-11>cs.AI (11)</a><ul><li><a href=#111--158259-large-language-models-for-crowd-decision-making-based-on-prompt-design-strategies-using-chatgpt-models-analysis-and-challenges-cristina-zuheros-et-al-2024>(1/11 | 158/259) Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges (Cristina Zuheros et al., 2024)</a></li><li><a href=#211--159259-sphere-neural-networks-for-rational-reasoning-tiansi-dong-et-al-2024>(2/11 | 159/259) Sphere Neural-Networks for Rational Reasoning (Tiansi Dong et al., 2024)</a></li><li><a href=#311--160259-a-picture-is-worth-a-graph-blueprint-debate-on-graph-for-multimodal-reasoning-changmeng-zheng-et-al-2024>(3/11 | 160/259) A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning (Changmeng Zheng et al., 2024)</a></li><li><a href=#411--161259-contextual-restless-multi-armed-bandits-with-application-to-demand-response-decision-making-xin-chen-et-al-2024>(4/11 | 161/259) Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making (Xin Chen et al., 2024)</a></li><li><a href=#511--162259-generative-ai-in-education-a-study-of-educators-awareness-sentiments-and-influencing-factors-aashish-ghimire-et-al-2024>(5/11 | 162/259) Generative AI in Education: A Study of Educators&rsquo; Awareness, Sentiments, and Influencing Factors (Aashish Ghimire et al., 2024)</a></li><li><a href=#611--163259-caca-agent-capability-collaboration-based-ai-agent-peng-xu-et-al-2024>(6/11 | 163/259) CACA Agent: Capability Collaboration based AI Agent (Peng Xu et al., 2024)</a></li><li><a href=#711--164259-symboslam-semantic-map-generation-in-a-multi-agent-system-brandon-curtis-colelough-2024>(7/11 | 164/259) SymboSLAM: Semantic Map Generation in a Multi-Agent System (Brandon Curtis Colelough, 2024)</a></li><li><a href=#811--165259-sensoryt5-infusing-sensorimotor-norms-into-t5-for-enhanced-fine-grained-emotion-classification-yuhan-xia-et-al-2024>(8/11 | 165/259) SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification (Yuhan Xia et al., 2024)</a></li><li><a href=#911--166259-autonomous-driving-with-perception-uncertainties-deep-ensemble-based-adaptive-cruise-control-xiao-li-et-al-2024>(9/11 | 166/259) Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control (Xiao Li et al., 2024)</a></li><li><a href=#1011--167259-collaborative-ai-teaming-in-unknown-environments-via-active-goal-deduction-zuyuan-zhang-et-al-2024>(10/11 | 167/259) Collaborative AI Teaming in Unknown Environments via Active Goal Deduction (Zuyuan Zhang et al., 2024)</a></li><li><a href=#1111--168259-safe-learning-of-pddl-domains-with-conditional-effects----extended-version-argaman-mordoch-et-al-2024>(11/11 | 168/259) Safe Learning of PDDL Domains with Conditional Effects &ndash; Extended Version (Argaman Mordoch et al., 2024)</a></li></ul></li><li><a href=#q-bioot-1>q-bio.OT (1)</a><ul><li><a href=#11--169259-bioinformatics-and-biomedical-informatics-with-chatgpt-year-one-review-jinge-wang-et-al-2024>(1/1 | 169/259) Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review (Jinge Wang et al., 2024)</a></li></ul></li><li><a href=#cscy-8>cs.CY (8)</a><ul><li><a href=#18--170259-instasynth-opportunities-and-challenges-in-generating-synthetic-instagram-data-with-chatgpt-for-sponsored-content-detection-thales-bertaglia-et-al-2024>(1/8 | 170/259) InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection (Thales Bertaglia et al., 2024)</a></li><li><a href=#28--171259-investigating-bias-in-llm-based-bias-detection-disparities-between-llms-and-human-perception-luyang-lin-et-al-2024>(2/8 | 171/259) Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception (Luyang Lin et al., 2024)</a></li><li><a href=#38--172259-from-guidelines-to-governance-a-study-of-ai-policies-in-education-aashish-ghimire-et-al-2024>(3/8 | 172/259) From Guidelines to Governance: A Study of AI Policies in Education (Aashish Ghimire et al., 2024)</a></li><li><a href=#48--173259-ai-teaches-the-art-of-elegant-coding-timely-fair-and-helpful-style-feedback-in-a-global-course-juliette-woodrow-et-al-2024>(4/8 | 173/259) AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course (Juliette Woodrow et al., 2024)</a></li><li><a href=#58--174259-analyzing-male-domestic-violence-through-exploratory-data-analysis-and-explainable-machine-learning-insights-md-abrar-jahin-et-al-2024>(5/8 | 174/259) Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights (Md Abrar Jahin et al., 2024)</a></li><li><a href=#68--175259-learners-teaching-novices-an-uplifting-alternative-assessment-ali-malik-et-al-2024>(6/8 | 175/259) Learners Teaching Novices: An Uplifting Alternative Assessment (Ali Malik et al., 2024)</a></li><li><a href=#78--176259-analyzing-potential-solutions-involving-regulation-to-escape-some-of-ais-ethical-concerns-jay-nemec-2024>(7/8 | 176/259) Analyzing Potential Solutions Involving Regulation to Escape Some of AI&rsquo;s Ethical Concerns (Jay Nemec, 2024)</a></li><li><a href=#88--177259-ktbench-a-novel-data-leakage-free-framework-for-knowledge-tracing-yahya-badran-et-al-2024>(8/8 | 177/259) KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing (Yahya Badran et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--178259-adaptive-coded-federated-learning-privacy-preservation-and-straggler-mitigation-chengxi-li-et-al-2024>(1/2 | 178/259) Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation (Chengxi Li et al., 2024)</a></li><li><a href=#22--179259-learning-to-walk-on-new-ground-calibration-free-decoding-for-c-vep-bci-j-thielen-et-al-2024>(2/2 | 179/259) Learning to walk on new ground: Calibration-free decoding for c-VEP BCI (J. Thielen et al., 2024)</a></li></ul></li><li><a href=#cssi-6>cs.SI (6)</a><ul><li><a href=#16--180259-hierarchical-information-enhancement-network-for-cascade-prediction-in-social-networks-fanrui-zhang-et-al-2024>(1/6 | 180/259) Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks (Fanrui Zhang et al., 2024)</a></li><li><a href=#26--181259-llm-driven-agents-for-influencer-selection-in-digital-advertising-campaigns-xiaoqing-zhang-et-al-2024>(2/6 | 181/259) LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns (Xiaoqing Zhang et al., 2024)</a></li><li><a href=#36--182259-multi-perspective-memory-enhanced-network-for-identifying-key-nodes-in-social-networks-qiang-zhang-et-al-2024>(3/6 | 182/259) Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks (Qiang Zhang et al., 2024)</a></li><li><a href=#46--183259-ellipsoidal-embeddings-of-graphs-michaël-fanuel-et-al-2024>(4/6 | 183/259) Ellipsoidal embeddings of graphs (Michaël Fanuel et al., 2024)</a></li><li><a href=#56--184259-nonparametric-inference-of-higher-order-interaction-patterns-in-networks-anatol-e-wegner-et-al-2024>(5/6 | 184/259) Nonparametric inference of higher order interaction patterns in networks (Anatol E. Wegner et al., 2024)</a></li><li><a href=#66--185259-unraveling-contagion-origins-optimal-estimation-through-maximum-likelihood-and-starlike-tree-approximation-in-markovian-spreading-models-pei-duo-yu-et-al-2024>(6/6 | 185/259) Unraveling Contagion Origins: Optimal Estimation through Maximum-Likelihood and Starlike Tree Approximation in Markovian Spreading Models (Pei-Duo Yu et al., 2024)</a></li></ul></li><li><a href=#csro-18>cs.RO (18)</a><ul><li><a href=#118--186259-convoi-context-aware-navigation-using-vision-language-models-in-outdoor-and-indoor-environments-adarsh-jagan-sathyamoorthy-et-al-2024>(1/18 | 186/259) CoNVOI: Context-aware Navigation using Vision Language Models in Outdoor and Indoor Environments (Adarsh Jagan Sathyamoorthy et al., 2024)</a></li><li><a href=#218--187259-srlm-human-in-loop-interactive-social-robot-navigation-with-large-language-model-and-deep-reinforcement-learning-weizheng-wang-et-al-2024>(2/18 | 187/259) SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning (Weizheng Wang et al., 2024)</a></li><li><a href=#318--188259-safe-and-stable-teleoperation-of-quadrotor-uavs-under-haptic-shared-autonomy-dawei-zhang-et-al-2024>(3/18 | 188/259) Safe and Stable Teleoperation of Quadrotor UAVs under Haptic Shared Autonomy (Dawei Zhang et al., 2024)</a></li><li><a href=#418--189259-guided-decoding-for-robot-motion-generation-and-adaption-nutan-chen-et-al-2024>(4/18 | 189/259) Guided Decoding for Robot Motion Generation and Adaption (Nutan Chen et al., 2024)</a></li><li><a href=#518--190259-boundary-aware-value-function-generation-for-safe-stochastic-motion-planning-junhong-xu-et-al-2024>(5/18 | 190/259) Boundary-Aware Value Function Generation for Safe Stochastic Motion Planning (Junhong Xu et al., 2024)</a></li><li><a href=#618--191259-global-games-with-negative-feedback-for-autonomous-colony-maintenance-using-robot-teams-logan-e-beaver-2024>(6/18 | 191/259) Global Games with Negative Feedback for Autonomous Colony Maintenance using Robot Teams (Logan E. Beaver, 2024)</a></li><li><a href=#718--192259-gesture-controlled-aerial-robot-formation-for-human-swarm-interaction-in-safety-monitoring-applications-vít-krátký-et-al-2024>(7/18 | 192/259) Gesture-Controlled Aerial Robot Formation for Human-Swarm Interaction in Safety Monitoring Applications (Vít Krátký et al., 2024)</a></li><li><a href=#818--193259-infrastructure-assisted-collaborative-perception-in-automated-valet-parking-a-safety-perspective-yukuan-jia-et-al-2024>(8/18 | 193/259) Infrastructure-Assisted Collaborative Perception in Automated Valet Parking: A Safety Perspective (Yukuan Jia et al., 2024)</a></li><li><a href=#918--194259-rhino-vr-experience-teaching-mobile-robotics-concepts-in-an-interactive-museum-exhibit-erik-schlachhoff-et-al-2024>(9/18 | 194/259) RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive Museum Exhibit (Erik Schlachhoff et al., 2024)</a></li><li><a href=#1018--195259-a-twin-delayed-deep-deterministic-policy-gradient-algorithm-for-autonomous-ground-vehicle-navigation-via-digital-twin-perception-awareness-kabirat-olayemi-et-al-2024>(10/18 | 195/259) A Twin Delayed Deep Deterministic Policy Gradient Algorithm for Autonomous Ground Vehicle Navigation via Digital Twin Perception Awareness (Kabirat Olayemi et al., 2024)</a></li><li><a href=#1118--196259-linear-quadratic-guidance-law-for-joint-motion-planning-of-a-pursuer-turret-assembly-bhargav-jha-et-al-2024>(11/18 | 196/259) Linear Quadratic Guidance Law for Joint Motion Planning of a Pursuer-Turret Assembly (Bhargav Jha et al., 2024)</a></li><li><a href=#1218--197259-oceanplan-hierarchical-planning-and-replanning-for-natural-language-auv-piloting-in-large-scale-unexplored-ocean-environments-ruochu-yang-et-al-2024>(12/18 | 197/259) OceanPlan: Hierarchical Planning and Replanning for Natural Language AUV Piloting in Large-scale Unexplored Ocean Environments (Ruochu Yang et al., 2024)</a></li><li><a href=#1318--198259-hortibot-an-adaptive-multi-arm-system-for-robotic-horticulture-of-sweet-peppers-christian-lenz-et-al-2024>(13/18 | 198/259) HortiBot: An Adaptive Multi-Arm System for Robotic Horticulture of Sweet Peppers (Christian Lenz et al., 2024)</a></li><li><a href=#1418--199259-trihelper-zero-shot-object-navigation-with-dynamic-assistance-lingfeng-zhang-et-al-2024>(14/18 | 199/259) TriHelper: Zero-Shot Object Navigation with Dynamic Assistance (Lingfeng Zhang et al., 2024)</a></li><li><a href=#1518--200259-crplace-camera-radar-fusion-with-bev-representation-for-place-recognition-shaowei-fu-et-al-2024>(15/18 | 200/259) CRPlace: Camera-Radar Fusion with BEV Representation for Place Recognition (Shaowei Fu et al., 2024)</a></li><li><a href=#1618--201259-alpine-a-climbing-robot-for-operations-in-mountain-environments-michele-focchi-et-al-2024>(16/18 | 201/259) ALPINE: a climbing robot for operations in mountain environments (Michele Focchi et al., 2024)</a></li><li><a href=#1718--202259-subequivariant-reinforcement-learning-framework-for-coordinated-motion-control-haoyu-wang-et-al-2024>(17/18 | 202/259) Subequivariant Reinforcement Learning Framework for Coordinated Motion Control (Haoyu Wang et al., 2024)</a></li><li><a href=#1818--203259-rethinking-6-dof-grasp-detection-a-flexible-framework-for-high-quality-grasping-wei-tang-et-al-2024>(18/18 | 203/259) Rethinking 6-Dof Grasp Detection: A Flexible Framework for High-Quality Grasping (Wei Tang et al., 2024)</a></li></ul></li><li><a href=#q-fincp-2>q-fin.CP (2)</a><ul><li><a href=#12--204259-construction-of-a-japanese-financial-benchmark-for-large-language-models-masanori-hirano-2024>(1/2 | 204/259) Construction of a Japanese Financial Benchmark for Large Language Models (Masanori Hirano, 2024)</a></li><li><a href=#22--205259-robust-utility-optimization-via-a-gan-approach-florian-krach-et-al-2024>(2/2 | 205/259) Robust Utility Optimization via a GAN Approach (Florian Krach et al., 2024)</a></li></ul></li><li><a href=#csit-8>cs.IT (8)</a><ul><li><a href=#18--206259-mutual-information-of-a-class-of-poisson-type-channels-using-markov-renewal-theory-maximilian-gehri-et-al-2024>(1/8 | 206/259) Mutual Information of a class of Poisson-type Channels using Markov Renewal Theory (Maximilian Gehri et al., 2024)</a></li><li><a href=#28--207259-information-rates-of-successive-interference-cancellation-for-optical-fiber-alex-jäger-et-al-2024>(2/8 | 207/259) Information Rates of Successive Interference Cancellation for Optical Fiber (Alex Jäger et al., 2024)</a></li><li><a href=#38--208259-uplink-soft-handover-for-leo-constellations-how-strong-the-inter-satellite-link-should-be-houcem-ben-salem-et-al-2024>(3/8 | 208/259) Uplink soft handover for LEO constellations: how strong the inter-satellite link should be (Houcem Ben Salem et al., 2024)</a></li><li><a href=#48--209259-range-angle-estimation-for-fda-mimo-system-with-frequency-offset-mengjiang-sun-et-al-2024>(4/8 | 209/259) Range-Angle Estimation for FDA-MIMO System With Frequency Offset (Mengjiang Sun et al., 2024)</a></li><li><a href=#58--210259-secure-outage-analysis-for-ris-aided-miso-systems-with-randomly-located-eavesdroppers-wei-shi-et-al-2024>(5/8 | 210/259) Secure Outage Analysis for RIS-Aided MISO Systems with Randomly Located Eavesdroppers (Wei Shi et al., 2024)</a></li><li><a href=#68--211259-ris-assisted-cell-free-massive-mimo-systems-with-two-timescale-design-and-hardware-impairments-jianxin-dai-et-al-2024>(6/8 | 211/259) RIS-assisted Cell-Free Massive MIMO Systems With Two-Timescale Design and Hardware Impairments (Jianxin Dai et al., 2024)</a></li><li><a href=#78--212259-robust-resource-allocation-for-star-ris-assisted-swipt-systems-guangyu-zhu-et-al-2024>(7/8 | 212/259) Robust Resource Allocation for STAR-RIS Assisted SWIPT Systems (Guangyu Zhu et al., 2024)</a></li><li><a href=#88--213259-coexisting-passive-ris-and-active-relay-assisted-noma-systems-ao-huang-et-al-2024>(8/8 | 213/259) Coexisting Passive RIS and Active Relay Assisted NOMA Systems (Ao Huang et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--214259-allspark-workload-orchestration-for-visual-transformers-on-processing-in-memory-systems-mengke-ge-et-al-2024>(1/1 | 214/259) Allspark: Workload Orchestration for Visual Transformers on Processing In-Memory Systems (Mengke Ge et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#11--215259-digital-twin-model-of-colon-electromechanics-for-manometry-prediction-of-laser-tissue-soldering-rené-thierry-djoumessi-et-al-2024>(1/1 | 215/259) Digital twin model of colon electromechanics for manometry prediction of laser tissue soldering (René Thierry Djoumessi et al., 2024)</a></li></ul></li><li><a href=#eesssy-6>eess.SY (6)</a><ul><li><a href=#16--216259-cascading-blackout-severity-prediction-with-statistically-augmented-graph-neural-networks-joe-gorka-et-al-2024>(1/6 | 216/259) Cascading Blackout Severity Prediction with Statistically-Augmented Graph Neural Networks (Joe Gorka et al., 2024)</a></li><li><a href=#26--217259-transactive-local-energy-markets-enable-community-level-resource-coordination-using-individual-rewards-daniel-c-may-et-al-2024>(2/6 | 217/259) Transactive Local Energy Markets Enable Community-Level Resource Coordination Using Individual Rewards (Daniel C. May et al., 2024)</a></li><li><a href=#36--218259-control-designs-for-critical-continegency-responsible-grid-following-inverters-and-seamless-transitions-to-and-from-grid-forming-modes-jaesang-park-et-al-2024>(3/6 | 218/259) Control Designs for Critical-Continegency Responsible Grid-Following Inverters and Seamless Transitions To and From Grid-Forming Modes (Jaesang Park et al., 2024)</a></li><li><a href=#46--219259-optimal-data-driven-prediction-and-predictive-control-using-signal-matrix-models-roy-s-smith-et-al-2024>(4/6 | 219/259) Optimal Data-Driven Prediction and Predictive Control using Signal Matrix Models (Roy S. Smith et al., 2024)</a></li><li><a href=#56--220259-uncertainty-propagation-in-stochastic-systems-via-mixture-models-with-error-quantification-eduardo-figueiredo-et-al-2024>(5/6 | 220/259) Uncertainty Propagation in Stochastic Systems via Mixture Models with Error Quantification (Eduardo Figueiredo et al., 2024)</a></li><li><a href=#66--221259-event-triggered-state-estimation-through-confidence-level-wei-liu-2024>(6/6 | 221/259) Event-Triggered State Estimation Through Confidence Level (Wei Liu, 2024)</a></li></ul></li><li><a href=#csgt-5>cs.GT (5)</a><ul><li><a href=#15--222259-ppa-game-characterizing-and-learning-competitive-dynamics-among-online-content-creators-renzhe-xu-et-al-2024>(1/5 | 222/259) PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators (Renzhe Xu et al., 2024)</a></li><li><a href=#25--223259-balancing-fairness-and-efficiency-in-energy-resource-allocations-jiayi-li-et-al-2024>(2/5 | 223/259) Balancing Fairness and Efficiency in Energy Resource Allocations (Jiayi Li et al., 2024)</a></li><li><a href=#35--224259-on-the-weighted-top-difference-distance-axioms-aggregation-and-approximation-andrea-aveni-et-al-2024>(3/5 | 224/259) On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation (Andrea Aveni et al., 2024)</a></li><li><a href=#45--225259-on-the-variational-interpretation-of-mirror-play-in-monotone-games-yunian-pan-et-al-2024>(4/5 | 225/259) On the Variational Interpretation of Mirror Play in Monotone Games (Yunian Pan et al., 2024)</a></li><li><a href=#55--226259-strategic-network-creation-for-enabling-greedy-routing-julian-berger-et-al-2024>(5/5 | 226/259) Strategic Network Creation for Enabling Greedy Routing (Julian Berger et al., 2024)</a></li></ul></li><li><a href=#physicsgeo-ph-1>physics.geo-ph (1)</a><ul><li><a href=#11--227259-end-to-end-mineral-exploration-with-artificial-intelligence-and-ambient-noise-tomography-jack-muir-et-al-2024>(1/1 | 227/259) End-to-End Mineral Exploration with Artificial Intelligence and Ambient Noise Tomography (Jack Muir et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--228259-augmented-reality-warnings-in-roadway-work-zones-evaluating-the-effect-of-modality-on-worker-reaction-times-sepehr-sabeti-et-al-2024>(1/1 | 228/259) Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect of Modality on Worker Reaction Times (Sepehr Sabeti et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--229259-not-all-attention-is-needed-parameter-and-computation-efficient-transfer-learning-for-multi-modal-large-language-models-qiong-wu-et-al-2024>(1/2 | 229/259) Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models (Qiong Wu et al., 2024)</a></li><li><a href=#22--230259-experimental-studies-of-metaverse-streaming-haopeng-wang-et-al-2024>(2/2 | 230/259) Experimental Studies of Metaverse Streaming (Haopeng Wang et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--231259-brain-grounding-of-semantic-vectors-improves-neural-decoding-of-visual-stimuli-shirin-vafaei-et-al-2024>(1/1 | 231/259) Brain-grounding of semantic vectors improves neural decoding of visual stimuli (Shirin Vafaei et al., 2024)</a></li></ul></li><li><a href=#cond-matquant-gas-1>cond-mat.quant-gas (1)</a><ul><li><a href=#11--232259-fast-real-time-arbitrary-waveform-generation-using-graphic-processing-units-juntian-tu-et-al-2024>(1/1 | 232/259) Fast real-time arbitrary waveform generation using graphic processing units (Juntian Tu et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--233259-towards-a-statistical-probabilistic-lazy-lambda-calculus-radha-jagadeesan-2024>(1/1 | 233/259) (Towards a) Statistical Probabilistic Lazy Lambda Calculus (Radha Jagadeesan, 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--234259-efficiently-estimating-mutual-information-between-attributes-across-tables-aécio-santos-et-al-2024>(1/1 | 234/259) Efficiently Estimating Mutual Information Between Attributes Across Tables (Aécio Santos et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--235259-random-vortex-dynamics-and-monte-carlo-simulations-for-wall-bounded-viscous-flows-vladislav-cherepanov-et-al-2024>(1/4 | 235/259) Random vortex dynamics and Monte-Carlo simulations for wall-bounded viscous flows (Vladislav Cherepanov et al., 2024)</a></li><li><a href=#24--236259-accelerating-aeroelastic-uvlm-simulations-by-inexact-newton-algorithms-jenny-schubert-et-al-2024>(2/4 | 236/259) Accelerating Aeroelastic UVLM Simulations by Inexact Newton Algorithms (Jenny Schubert et al., 2024)</a></li><li><a href=#34--237259-two-scale-analysis-for-multiscale-landau-lifshitz-gilbert-equation-theory-and-numerical-methods-xiaofei-guan-et-al-2024>(3/4 | 237/259) Two-scale Analysis for Multiscale Landau-Lifshitz-Gilbert Equation: Theory and Numerical Methods (Xiaofei Guan et al., 2024)</a></li><li><a href=#44--238259-sparse-additive-function-decompositions-facing-basis-transforms-fatima-antarou-ba-et-al-2024>(4/4 | 238/259) Sparse additive function decompositions facing basis transforms (Fatima Antarou Ba et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--239259-a-modular-end-to-end-next-generation-network-testbed-towards-a-fully-automated-network-management-platform-ali-chouman-et-al-2024>(1/2 | 239/259) A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully Automated Network Management Platform (Ali Chouman et al., 2024)</a></li><li><a href=#22--240259-blockchain-based-pseudonym-management-for-vehicle-twin-migrations-in-vehicular-edge-metaverse-jiawen-kang-et-al-2024>(2/2 | 240/259) Blockchain-based Pseudonym Management for Vehicle Twin Migrations in Vehicular Edge Metaverse (Jiawen Kang et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--241259-a-wasserstein-perspective-of-vanilla-gans-lea-kunkel-et-al-2024>(1/1 | 241/259) A Wasserstein perspective of Vanilla GANs (Lea Kunkel et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#13--242259-pursuit-evasion-on-a-sphere-and-when-it-can-be-considered-flat-dejan-milutinovic-et-al-2024>(1/3 | 242/259) Pursuit-Evasion on a Sphere and When It Can Be Considered Flat (Dejan Milutinovic et al., 2024)</a></li><li><a href=#23--243259-optimal-contract-design-for-end-of-life-care-payments-muyan-jiang-et-al-2024>(2/3 | 243/259) Optimal Contract Design for End-of-Life Care Payments (Muyan Jiang et al., 2024)</a></li><li><a href=#33--244259-network-learning-with-directional-sign-patterns-anqi-dong-et-al-2024>(3/3 | 244/259) Network Learning with Directional Sign Patterns (Anqi Dong et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--245259-a-hybrid-approach-to-semi-automated-rust-verification-sacha-élie-ayoun-et-al-2024>(1/1 | 245/259) A hybrid approach to semi-automated Rust verification (Sacha-Élie Ayoun et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--246259-comprehensive-lipidomic-automation-workflow-using-large-language-models-connor-beveridge-et-al-2024>(1/1 | 246/259) Comprehensive Lipidomic Automation Workflow using Large Language Models (Connor Beveridge et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--247259-modeling-distributed-computing-infrastructures-for-hep-applications-maximilian-horzela-et-al-2024>(1/2 | 247/259) Modeling Distributed Computing Infrastructures for HEP Applications (Maximilian Horzela et al., 2024)</a></li><li><a href=#22--248259-fsd-inference-fully-serverless-distributed-inference-with-scalable-cloud-communication-joe-oakley-et-al-2024>(2/2 | 248/259) FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication (Joe Oakley et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--249259-cross-layer-modeling-and-design-of-content-addressable-memories-in-advanced-technology-nodes-for-similarity-search-siri-narla-et-al-2024>(1/1 | 249/259) Cross-layer Modeling and Design of Content Addressable Memories in Advanced Technology Nodes for Similarity Search (Siri Narla et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--250259-image-classification-with-rotation-invariant-variational-quantum-circuits-paul-san-sebastian-et-al-2024>(1/1 | 250/259) Image Classification with Rotation-Invariant Variational Quantum Circuits (Paul San Sebastian et al., 2024)</a></li></ul></li><li><a href=#csds-3>cs.DS (3)</a><ul><li><a href=#13--251259-approximation-algorithms-for-school-assignment-group-fairness-and-multi-criteria-optimization-santhini-k-a-et-al-2024>(1/3 | 251/259) Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization (Santhini K. A. et al., 2024)</a></li><li><a href=#23--252259-approximation-algorithms-for-network-design-in-non-uniform-fault-models-chandra-chekuri-et-al-2024>(2/3 | 252/259) Approximation Algorithms for Network Design in Non-Uniform Fault Models (Chandra Chekuri et al., 2024)</a></li><li><a href=#33--253259-fourier-transform-based-estimators-for-data-sketches-seth-pettie-et-al-2024>(3/3 | 253/259) Fourier Transform-based Estimators for Data Sketches (Seth Pettie et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--254259-music-to-dance-as-language-translation-using-sequence-models-andré-correia-et-al-2024>(1/1 | 254/259) Music to Dance as Language Translation using Sequence Models (André Correia et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--255259-a-gradient-enhanced-univariate-dimension-reduction-method-for-uncertainty-propagation-bingran-wang-et-al-2024>(1/2 | 255/259) A gradient-enhanced univariate dimension reduction method for uncertainty propagation (Bingran Wang et al., 2024)</a></li><li><a href=#22--256259-graph-accelerated-non-intrusive-polynomial-chaos-expansion-using-partially-tensor-structured-quadrature-rules-bingran-wang-et-al-2024>(2/2 | 256/259) Graph-accelerated non-intrusive polynomial chaos expansion using partially tensor-structured quadrature rules (Bingran Wang et al., 2024)</a></li></ul></li><li><a href=#cond-matdis-nn-1>cond-mat.dis-nn (1)</a><ul><li><a href=#11--257259-spectral-initialization-for-high-dimensional-phase-retrieval-with-biased-spatial-directions-pierre-bousseyroux-et-al-2024>(1/1 | 257/259) Spectral Initialization for High-Dimensional Phase Retrieval with Biased Spatial Directions (Pierre Bousseyroux et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--258259-flip-breakability-a-combinatorial-dichotomy-for-monadically-dependent-graph-classes-jan-dreier-et-al-2024>(1/1 | 258/259) Flip-Breakability: A Combinatorial Dichotomy for Monadically Dependent Graph Classes (Jan Dreier et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--259259-reconstructing-the-evolution-history-of-networked-complex-systems-junya-wang-et-al-2024>(1/1 | 259/259) Reconstructing the evolution history of networked complex systems (Junya Wang et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>