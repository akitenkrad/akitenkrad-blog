<!doctype html><html><head><title>arXiv @ 2024.03.11</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.11"><meta property="og:description" content="Primary Categories cs.AI (1) cs.AR (1) cs.CE (2) cs.CL (17) cs.CR (4) cs.CV (29) cs.DC (1) cs.DS (2) cs.HC (5) cs.IR (1) cs.IT (6) cs.LG (27) cs.MA (1) cs.MM (1) cs.NE (1) cs.NI (1) cs.RO (8) cs.SD (3) cs.SE (3) eess.AS (1) eess.IV (4) eess.SP (1) eess.SY (9) math.OC (1) physics.soc-ph (1) q-bio.QM (1) q-bio.TO (1) quant-ph (2) stat.ME (2) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG Adversarial Attack 1 Anomaly Detection 1 Autoencoder 1 Benchmarking 5 8 8 Black Box 1 1 Clustering 1 2 Code Generation 1 Continual Learning 1 Contrastive Learning 3 Convolution 1 Convolutional Neural Network 3 Counter-factual 1 Data Augmentation 1 Dialogue System 1 Diffusion Model 2 1 Document Ranking 1 Domain Adaptation 1 Emotion Recognition 2 Fairness 1 2 Federated Learning 1 Few-shot 2 Few-shot Learning 1 Fine-tuning 4 5 Foundation Model 2 GPT 1 1 1 GPT-4 1 1 1 Generative Adversarial Network 1 Geometry 3 Graph 2 1 5 Graph Neural Network 5 Image2text 1 In-context Learning 2 1 Information Retrieval 1 Knowledge Distillation 6 Knowledge Graph 3 2 Knowledge Transfer 1 LLaMA 2 LSTM 1 2 Language Generation 2 Large Language Model 16 2 7 Low-Resource 1 Markov Decision Process 1 Masked Language Model 1 Multi-modal 5 2 Multiple Instance Learning 1 Named Entity Recognition 1 Natural Language Generation 2 Neural Machine Translation 3 Node Classification 1 Object Detection 2 Out-of-distribution 3 1 Pre-trained Language Model 2 Probabilistic Model 1 Prompt 3 2 1 Prompt Learning 1 Question Answering 4 Reasoning 1 1 Recurrent Neural Network 1 Reinforcement Learning 1 4 Representation Learning 2 Rouge 1 Rouge-L 1 Sample Size 1 Scaling Law 1 Self-Attention 1 Self-supervised Learning 8 Sentiment Analysis 1 Simulation 1 1 Simulator 1 1 Stemming 1 Stochastic Gradient Descent 2 Summarization 2 Supervised Learning 1 3 Tensor Decomposition 1 Text Summarization 1 Text2image 1 Tokenization 1 Transfer Learning 1 Transformer 2 4 Unsupervised Learning 1 1 Vision Transformer 2 Vision-and-Language 1 Weakly-supervised Learning 1 Zero-shot 3 Zero-shot Learning 1 human-in-the-loop 1 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240311000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-11T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.11"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240311000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Mar 11, 2024</p></div><div class=title><h1>arXiv @ 2024.03.11</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csai-1>cs.AI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cscl-17>cs.CL (17)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cscv-29>cs.CV (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csir-1>cs.IR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cslg-27>cs.LG (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csro-8>cs.RO (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#eessas-1>eess.AS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#eessiv-4>eess.IV (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#eesssy-9>eess.SY (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#q-bioto-1>q-bio.TO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#statme-2>stat.ME (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>5</td><td>8</td><td>8</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td></tr><tr><td>Clustering</td><td></td><td>1</td><td>2</td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td>3</td><td></td></tr><tr><td>Convolution</td><td></td><td>1</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>2</td><td>1</td></tr><tr><td>Document Ranking</td><td>1</td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>2</td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td>2</td></tr><tr><td>Federated Learning</td><td></td><td></td><td>1</td></tr><tr><td>Few-shot</td><td>2</td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>4</td><td>5</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>2</td><td></td></tr><tr><td>GPT</td><td>1</td><td>1</td><td>1</td></tr><tr><td>GPT-4</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>1</td></tr><tr><td>Geometry</td><td></td><td>3</td><td></td></tr><tr><td>Graph</td><td>2</td><td>1</td><td>5</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>5</td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>2</td><td>1</td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>6</td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td>2</td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td></td></tr><tr><td>LSTM</td><td>1</td><td>2</td><td></td></tr><tr><td>Language Generation</td><td>2</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>16</td><td>2</td><td>7</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td></tr><tr><td>Masked Language Model</td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td></td><td>5</td><td>2</td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>2</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>3</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>2</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>3</td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>2</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>3</td><td>2</td><td>1</td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>4</td><td></td><td></td></tr><tr><td>Reasoning</td><td>1</td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>4</td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td></tr><tr><td>Rouge-L</td><td>1</td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>1</td></tr><tr><td>Scaling Law</td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>8</td><td></td></tr><tr><td>Sentiment Analysis</td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>1</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>1</td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Summarization</td><td>2</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>3</td><td></td></tr><tr><td>Tensor Decomposition</td><td></td><td></td><td>1</td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Tokenization</td><td></td><td></td><td>1</td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td></tr><tr><td>Transformer</td><td>2</td><td>4</td><td></td></tr><tr><td>Unsupervised Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>3</td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td>1</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-17>cs.CL (17)</h2><h3 id=117--1138-few-shot-cross-lingual-transfer-for-prompting-large-language-models-in-low-resource-languages-christopher-toukmaji-2024>(1/17 | 1/138) Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages (Christopher Toukmaji, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Toukmaji. (2024)<br><strong>Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages</strong><br><button class=copy-to-clipboard title="Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 140<br>Keywords: Few-shot, Fine-tuning, Low-Resource, LLaMA, Named Entity Recognition, Neural Machine Translation, Neural Machine Translation, In-context Learning, In-context Learning, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06018v1.pdf filename=2403.06018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> are at the forefront of advances in Natural Language Processing. One widespread use case of <b>PLMs</b> is <b>&ldquo;prompting&rdquo;</b> - or <b>in-context</b> <b>learning</b> - where a user provides a description of a task and some completed examples of the task to a <b>PLM</b> as context before <b>prompting</b> the <b>PLM</b> to perform the task on a new example. Only the largest, most capable <b>PLMs</b> are able to perform <b>in-context</b> <b>learning</b> effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific <b>PLMs</b> capable of <b>prompting.</b> Albeit the surge in work of <b>prompting</b> settings, it is still unclear how <b>PLMs</b> should be adapted cross-lingually specifically for <b>prompting.</b> We evaluate the possible methods to adapt <b>LLaMa,</b> a 7B parameter open-source <b>PLM</b> mainly trained in English, for <b>prompting</b> in <b>low-resource</b> languages, namely for Kinyarwanda, Hausa, and Luganda. We consider three methods: <b>few-shot</b> <b>prompting</b> <b>(prompt),</b> language-adaptive <b>fine-tuning</b> (LAFT), and <b>neural</b> <b>machine</b> <b>translation</b> (translate), and evaluate on abstractive <b>summarization,</b> multi-class topic classification, and <b>named-entity</b> <b>recognition.</b> <b>Although</b> LAFT carries the greatest compute cost and intuitively should lead to the best results, our experiments exhibit that LAFT is only occasionally the optimal choice for adapting <b>PLMs</b> for <b>prompting.</b> Rather, the translate and <b>prompt</b> settings are a compute-efficient and cost-effective method of <b>few-shot</b> <b>prompting</b> for the selected <b>low-resource</b> languages. We find that the results are task and language dependent but find that the <b>prompting</b> method is the best on average across all tasks and languages. Results show that the <b>prompt</b> setting performs better than both translating and LAFT with statistical significance for all shots when aggregated across all tasks and languages.</p></p class="citation"></blockquote><h3 id=217--2138-kg-rank-enhancing-large-language-models-for-medical-qa-with-knowledge-graphs-and-ranking-techniques-rui-yang-et-al-2024>(2/17 | 2/138) KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques (Rui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Yang, Haoran Liu, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li. (2024)<br><strong>KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques</strong><br><button class=copy-to-clipboard title="KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Question Answering, Question Answering, Large Language Model, Large Language Model, Rouge, Rouge-L<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05881v1.pdf filename=2403.05881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented <b>LLM</b> framework, <b>KG-Rank,</b> which leverages a medical <b>knowledge</b> <b>graph</b> <b>(KG)</b> with ranking and re-ranking techniques, aiming to improve free-text <b>question-answering</b> <b>(QA)</b> in the medical domain. Specifically, upon receiving a <b>question,</b> <b>we</b> initially retrieve triplets from a medical <b>KG</b> to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our <b>knowledge,</b> <b>KG-Rank</b> is the first application of ranking models combined with <b>KG</b> in medical <b>QA</b> specifically for generating long answers. Evaluation of four selected medical <b>QA</b> datasets shows that <b>KG-Rank</b> achieves an improvement of over 18% in the <b>ROUGE-L</b> score. Moreover, we extend <b>KG-Rank</b> to open domains, where it realizes a 14% improvement in <b>ROUGE-L,</b> showing the effectiveness and potential of <b>KG-Rank.</b></p></p class="citation"></blockquote><h3 id=317--3138-clinicalmamba-a-generative-clinical-language-model-on-longitudinal-clinical-notes-zhichao-yang-et-al-2024>(3/17 | 3/138) ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes (Zhichao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichao Yang, Avijit Mitra, Sunjae Kwon, Hong Yu. (2024)<br><strong>ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes</strong><br><button class=copy-to-clipboard title="ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, GPT, GPT-4, LLaMA, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05795v1.pdf filename=2403.05795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of natural language processing (NLP) systems in healthcare hinges on language model ability to interpret the intricate <b>information</b> <b>contained</b> within clinical notes. This process often requires integrating <b>information</b> <b>from</b> various time points in a patient&rsquo;s medical history. However, most earlier clinical language models were pretrained with a context length limited to roughly one clinical document. In this study, We introduce ClinicalMamba, a specialized version of the Mamba language model, pretrained on a vast corpus of longitudinal clinical notes to address the unique linguistic characteristics and <b>information</b> <b>processing</b> needs of the medical domain. ClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a superior performance in modeling clinical language across extended text lengths compared to Mamba and clinical <b>Llama.</b> With <b>few-shot</b> <b>learning,</b> ClinicalMamba achieves notable <b>benchmarks</b> in speed and accuracy, outperforming existing clinical language models and general domain large models like <b>GPT-4</b> in longitudinal clinical notes <b>information</b> <b>extraction</b> tasks.</p></p class="citation"></blockquote><h3 id=417--4138-mp2d-an-automated-topic-shift-dialogue-generation-framework-leveraging-knowledge-graphs-yerin-hwang-et-al-2024>(4/17 | 4/138) MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs (Yerin Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yerin Hwang, Yongil Kim, Yunah Jang, Jeesoo Bang, Hyunkyung Bae, Kyomin Jung. (2024)<br><strong>MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs</strong><br><button class=copy-to-clipboard title="MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 51<br>Keywords: Graph, Benchmarking, Knowledge Graph, Dialogue System, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05814v1.pdf filename=2403.05814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite advancements in on-topic <b>dialogue</b> <b>systems,</b> effectively managing topic shifts within <b>dialogues</b> <b>remains</b> a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to <b>Dialogue</b> <b>(MP2D),</b> a data generation framework that automatically creates conversational <b>question-answering</b> <b>datasets</b> with natural topic transitions. By leveraging the relationships between entities in a <b>knowledge</b> <b>graph,</b> MP2D maps the flow of topics within a <b>dialogue,</b> <b>effectively</b> mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into <b>dialogues</b> <b>through</b> the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D&rsquo;s efficacy in generating <b>dialogue</b> <b>with</b> natural topic shifts. Furthermore, this study introduces a novel <b>benchmark</b> for topic shift <b>dialogues,</b> <b>TS-WikiDialog.</b> Utilizing the dataset, we demonstrate that even <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> struggle to handle topic shifts in <b>dialogue</b> <b>effectively,</b> and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift <b>dialogue</b> <b>tasks.</b></p></p class="citation"></blockquote><h3 id=517--5138-decoding-the-ai-pen-techniques-and-challenges-in-detecting-ai-generated-text-sara-abdali-et-al-2024>(5/17 | 5/138) Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text (Sara Abdali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Abdali, Richard Anarfi, CJ Barberan, Jia He. (2024)<br><strong>Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text</strong><br><button class=copy-to-clipboard title="Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Language Generation, Natural Language Generation, Natural Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05750v1.pdf filename=2403.05750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have revolutionized the field of <b>Natural</b> <b>Language</b> <b>Generation</b> <b>(NLG)</b> by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.</p></p class="citation"></blockquote><h3 id=617--6138-calibrating-large-language-models-using-their-generations-only-dennis-ulmer-et-al-2024>(6/17 | 6/138) Calibrating Large Language Models Using Their Generations Only (Dennis Ulmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, Seong Joon Oh. (2024)<br><strong>Calibrating Large Language Models Using Their Generations Only</strong><br><button class=copy-to-clipboard title="Calibrating Large Language Models Using Their Generations Only" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 45<br>Keywords: Black Box, Language Generation, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05973v1.pdf filename=2403.05973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are increasingly deployed in user-facing applications, building trust and maintaining safety by accurately quantifying a model&rsquo;s confidence in its prediction becomes even more important. However, finding effective ways to calibrate <b>LLMs</b> - especially when the only interface to the models is their generated text - remains a challenge. We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an <b>LLM&rsquo;s</b> confidence based on its textual input and output alone. This approach has several advantages: It is conceptually simple, does not require access to the target model beyond its output, does not interfere with the <b>language</b> <b>generation,</b> and has a multitude of potential usages, for instance by verbalizing the predicted confidence or adjusting the given answer based on the confidence. We show how our approach performs competitively in terms of calibration error for white-box and <b>black-box</b> <b>LLMs</b> on closed-book <b>question-answering</b> <b>to</b> detect incorrect <b>LLM</b> answers.</p></p class="citation"></blockquote><h3 id=717--7138-persian-slang-text-conversion-to-formal-and-deep-learning-of-persian-short-texts-on-social-media-for-sentiment-classification-mohsen-khazeni-et-al-2024>(7/17 | 7/138) Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification (Mohsen Khazeni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohsen Khazeni, Mohammad Heydari, Amir Albadvi. (2024)<br><strong>Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification</strong><br><button class=copy-to-clipboard title="Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Supervised Learning, Unsupervised Learning, LSTM, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06023v1.pdf filename=2403.06023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The lack of a suitable tool for the analysis of conversational texts in the Persian language has made various analyses of these texts, including <b>Sentiment</b> <b>Analysis,</b> difficult. In this research, we tried to make the understanding of these texts easier for the machine by providing PSC, Persian Slang Converter, a tool for converting conversational texts into formal ones, and by using the most up-to-date and best deep learning methods along with the PSC, the <b>sentiment</b> <b>learning</b> of short Persian language texts for the machine in a better way. be made More than 10 million unlabeled texts from various social networks and movie subtitles (as Conversational texts) and about 10 million news texts (as formal texts) have been used for training <b>unsupervised</b> models and formal implementation of the tool. 60,000 texts from the comments of Instagram social network users with positive, negative, and neutral labels are considered <b>supervised</b> data for training the emotion classification model of short texts. Using the formal tool, 57% of the words of the corpus of conversation were converted. Finally, by using the formalizer, FastText model, and deep <b>LSTM</b> network, an accuracy of 81.91 was obtained on the test data.</p></p class="citation"></blockquote><h3 id=817--8138-thread-detection-and-response-generation-using-transformers-with-prompt-optimisation-kevin-joshua-t-et-al-2024>(8/17 | 8/138) Thread Detection and Response Generation using Transformers with Prompt Optimisation (Kevin Joshua T et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Joshua T, Arnav Agarwal, Shriya Sanjay, Yash Sarda, John Sahaya Rani Alex, Saurav Gupta, Sushant Kumar, Vishwanath Kamath. (2024)<br><strong>Thread Detection and Response Generation using Transformers with Prompt Optimisation</strong><br><button class=copy-to-clipboard title="Thread Detection and Response Generation using Transformers with Prompt Optimisation" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-2-6, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Transformer, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05931v1.pdf filename=2403.05931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational systems are crucial for human-computer interaction, managing complex dialogues by identifying threads and prioritising responses. This is especially vital in multi-party conversations, where precise identification of threads and strategic response prioritisation ensure efficient dialogue management. To address these challenges an end-to-end model that identifies threads and prioritises their response generation based on the importance was developed, involving a systematic decomposition of the problem into discrete components - thread detection, prioritisation, and performance optimisation which was meticulously analysed and optimised. These refined components seamlessly integrate into a unified framework, in conversational systems. Llama2 7b is used due to its high level of generalisation but the system can be updated with any open source Large Language Model(LLM). The computational capabilities of the Llama2 model was augmented by using fine tuning methods and strategic <b>prompting</b> techniques to optimise the model&rsquo;s performance, reducing computational time and increasing the accuracy of the model. The model achieves up to 10x speed improvement, while generating more coherent results compared to existing models.</p></p class="citation"></blockquote><h3 id=917--9138-flap-flow-adhering-planning-with-constrained-decoding-in-llms-shamik-roy-et-al-2024>(9/17 | 9/138) FLAP: Flow Adhering Planning with Constrained Decoding in LLMs (Shamik Roy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, Arshit Gupta. (2024)<br><strong>FLAP: Flow Adhering Planning with Constrained Decoding in LLMs</strong><br><button class=copy-to-clipboard title="FLAP: Flow Adhering Planning with Constrained Decoding in LLMs" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05766v1.pdf filename=2403.05766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require <b>reasoning</b> and planning. With the recent advances in <b>LLMs,</b> there have been increasing attempts to use <b>LLMs</b> for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with <b>LLMs</b> because of their bias towards pretraining data. Moreover, in real life, workflows are custom-defined and prone to change, hence, quickly adapting agents to the changes is desirable. In this paper, we study faithful planning in TODs to resolve user intents by following predefined flows and preserving API dependencies. We propose a constrained decoding algorithm based on lookahead heuristic for faithful planning. Our algorithm alleviates the need for <b>finetuning</b> <b>LLMs</b> using domain specific data, outperforms other decoding and <b>prompting-based</b> baselines, and applying our algorithm on smaller <b>LLMs</b> (7B) we achieve comparable performance to larger <b>LLMs</b> (30B-40B).</p></p class="citation"></blockquote><h3 id=1017--10138-itd-large-language-models-can-teach-themselves-induction-through-deduction-wangtao-sun-et-al-2024>(10/17 | 10/138) ItD: Large Language Models Can Teach Themselves Induction through Deduction (Wangtao Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wangtao Sun, Haotian Xu, Xuanqing Yu, Pei Chen, Shizhu He, Jun Zhao, Kang Liu. (2024)<br><strong>ItD: Large Language Models Can Teach Themselves Induction through Deduction</strong><br><button class=copy-to-clipboard title="ItD: Large Language Models Can Teach Themselves Induction through Deduction" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05789v1.pdf filename=2403.05789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt ``post processes&rsquo;&rsquo; paradigms to improve the performance of <b>LLMs</b> on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the <b>LLMs.</b> In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the <b>LLMs</b> to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the <b>fine-tuning</b> and decoding of <b>LLMs.</b> Our empirical results showcase the effectiveness of ItD on two induction <b>benchmarks,</b> achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different <b>LLMs</b> and deductors. The data and code of this paper can be found at <a href=https://anonymous.4open.science/r/ItD-E844>https://anonymous.4open.science/r/ItD-E844</a>.</p></p class="citation"></blockquote><h3 id=1117--11138-algorithmic-progress-in-language-models-anson-ho-et-al-2024>(11/17 | 11/138) Algorithmic progress in language models (Anson Ho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan Carl Guo, David Atkinson, Neil Thompson, Jaime Sevilla. (2024)<br><strong>Algorithmic progress in language models</strong><br><button class=copy-to-clipboard title="Algorithmic progress in language models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05812v1.pdf filename=2403.05812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the rate at which algorithms for pre-training language models have improved since the advent of deep learning. Using a dataset of over 200 language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we find that the compute required to reach a set performance threshold has halved approximately every 8 months, with a 95% confidence interval of around 5 to 14 months, substantially faster than hardware gains per Moore&rsquo;s Law. We estimate augmented <b>scaling</b> <b>laws,</b> which enable us to quantify algorithmic progress and determine the relative contributions of <b>scaling</b> <b>models</b> versus innovations in training algorithms. Despite the rapid pace of algorithmic progress and the development of new architectures such as the <b>transformer,</b> our analysis reveals that the increase in compute made an even larger contribution to overall performance improvements over this time period. Though limited by noisy <b>benchmark</b> data, our analysis quantifies the rapid progress in language modeling, shedding light on the relative contributions from compute and algorithms.</p></p class="citation"></blockquote><h3 id=1217--12138-measuring-bias-in-a-ranked-list-using-term-based-representations-amin-abolghasemi-et-al-2024>(12/17 | 12/138) Measuring Bias in a Ranked List using Term-based Representations (Amin Abolghasemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Abolghasemi, Leif Azzopardi, Arian Askari, Maarten de Rijke, Suzan Verberne. (2024)<br><strong>Measuring Bias in a Ranked List using Term-based Representations</strong><br><button class=copy-to-clipboard title="Measuring Bias in a Ranked List using Term-based Representations" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fairness, Document Ranking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05975v1.pdf filename=2403.05975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In most recent studies, gender bias in <b>document</b> <b>ranking</b> is evaluated with the NFaiRR metric, which measures bias in a ranked list based on an aggregation over the unbiasedness scores of each ranked <b>document.</b> <b>This</b> perspective in measuring the bias of a ranked list has a key limitation: individual <b>documents</b> <b>of</b> a ranked list might be biased while the ranked list as a whole balances the groups&rsquo; representations. To address this issue, we propose a novel metric called TExFAIR (term exposure-based <b>fairness),</b> which is based on two new extensions to a generic <b>fairness</b> evaluation framework, attention-weighted ranking <b>fairness</b> (AWRF). TExFAIR assesses <b>fairness</b> based on the term-based representation of groups in a ranked list: (i) an explicit definition of associating <b>documents</b> <b>to</b> groups based on probabilistic term-level associations, and (ii) a rank-biased discounting factor (RBDF) for counting non-representative <b>documents</b> <b>towards</b> the measurement of the <b>fairness</b> of a ranked list. We assess TExFAIR on the task of measuring gender bias in passage ranking, and study the relationship between TExFAIR and NFaiRR. Our experiments show that there is no strong correlation between TExFAIR and NFaiRR, which indicates that TExFAIR measures a different dimension of <b>fairness</b> than NFaiRR. With TExFAIR, we extend the AWRF framework to allow for the evaluation of <b>fairness</b> in settings with term-based representations of groups in <b>documents</b> <b>in</b> a ranked list.</p></p class="citation"></blockquote><h3 id=1317--13138-reverse-that-number-decoding-order-matters-in-arithmetic-learning-daniel-zhang-li-et-al-2024>(13/17 | 13/138) Reverse That Number! Decoding Order Matters in Arithmetic Learning (Daniel Zhang-Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Zhang-Li, Nianyi Lin, Jifan Yu, Zheyuan Zhang, Zijun Yao, Xiaokang Zhang, Lei Hou, Jing Zhang, Juanzi Li. (2024)<br><strong>Reverse That Number! Decoding Order Matters in Arithmetic Learning</strong><br><button class=copy-to-clipboard title="Reverse That Number! Decoding Order Matters in Arithmetic Learning" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05845v1.pdf filename=2403.05845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in pretraining have demonstrated that modern <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching <b>LLMs</b> arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of in accuracy while requiring only a third of the tokens typically used during training. For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at \url{https://anonymous.4open.science/r/RAIT-9FB7/}.</p></p class="citation"></blockquote><h3 id=1417--14138-on-the-benefits-of-fine-grained-loss-truncation-a-case-study-on-factuality-in-summarization-lorenzo-jaime-yu-flores-et-al-2024>(14/17 | 14/138) On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization (Lorenzo Jaime Yu Flores et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Jaime Yu Flores, Arman Cohan. (2024)<br><strong>On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization</strong><br><button class=copy-to-clipboard title="On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05788v1.pdf filename=2403.05788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>summarization</b> and simplification are among the most widely used applications of AI. However, models developed for such tasks are often prone to hallucination, which can result from training on unaligned data. One efficient approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto, 2020), an approach to modify the standard log loss to adaptively remove noisy examples during training. However, we find that LT alone yields a considerable number of hallucinated entities on various datasets. We study the behavior of the underlying losses between factual and non-factual examples, to understand and refine the performance of LT. We demonstrate that LT&rsquo;s performance is limited when the underlying assumption that noisy targets have higher NLL loss is not satisfied, and find that word-level NLL among entities provides better signal for distinguishing factuality. We then leverage this to propose a fine-grained NLL loss and fine-grained data cleaning strategies, and observe improvements in hallucination reduction across some datasets. Our work is available at https://https://github.com/yale-nlp/fine-grained-lt.</p></p class="citation"></blockquote><h3 id=1517--15138-enhanced-auto-language-prediction-with-dictionary-capsule----a-novel-approach-pinni-venkata-abhiram-et-al-2024>(15/17 | 15/138) Enhanced Auto Language Prediction with Dictionary Capsule &ndash; A Novel Approach (Pinni Venkata Abhiram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pinni Venkata Abhiram, Ananya Rathore, Abhir Mirikar, Hari Krishna S, Sheena Christabel Pravin, Vishwanath Kamath Pethri, Manjunath Lokanath Belgod, Reetika Gupta, K Muthukumaran. (2024)<br><strong>Enhanced Auto Language Prediction with Dictionary Capsule &ndash; A Novel Approach</strong><br><button class=copy-to-clipboard title="Enhanced Auto Language Prediction with Dictionary Capsule -- A Novel Approach" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05982v1.pdf filename=2403.05982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper presents a novel Auto Language Prediction Dictionary Capsule (ALPDC) framework for language prediction and <b>machine</b> <b>translation.</b> The model uses a combination of neural networks and symbolic representations to predict the language of a given input text and then translate it to a target language using pre-built dictionaries. This research work also aims to translate the text of various languages to its literal meaning in English. The proposed model achieves state-of-the-art results on several <b>benchmark</b> datasets and significantly improves translation accuracy compared to existing methods. The results show the potential of the proposed method for practical use in multilingual communication and natural language processing tasks.</p></p class="citation"></blockquote><h3 id=1617--16138-high-throughput-phenotyping-of-physician-notes-with-large-language-and-hybrid-nlp-models-syed-i-munzir-et-al-2024>(16/17 | 16/138) High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models (Syed I. Munzir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed I. Munzir, Daniel B. Hier, Michael D. Carrithers. (2024)<br><strong>High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models</strong><br><button class=copy-to-clipboard title="High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2; J-2, cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05920v1.pdf filename=2403.05920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past thirty years, progress toward making high throughput phenotyping feasible. In this study, we demonstrate that a <b>large</b> <b>language</b> <b>model</b> and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. <b>Large</b> <b>language</b> <b>models</b> will likely emerge as the preferred method for high throughput deep phenotyping of physician notes.</p></p class="citation"></blockquote><h3 id=1717--17138-unisparse-an-intermediate-language-for-general-sparse-format-customization-jie-liu-et-al-2024>(17/17 | 17/138) UniSparse: An Intermediate Language for General Sparse Format Customization (Jie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, Zhiru Zhang. (2024)<br><strong>UniSparse: An Intermediate Language for General Sparse Format Customization</strong><br><button class=copy-to-clipboard title="UniSparse: An Intermediate Language for General Sparse Format Customization" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05802v1.pdf filename=2403.05802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. As a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic <b>code</b> <b>generation</b> of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device.</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--18138-ham-tts-hierarchical-acoustic-modeling-for-token-based-zero-shot-text-to-speech-with-model-and-data-scaling-chunhui-wang-et-al-2024>(1/3 | 18/138) HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling (Chunhui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunhui Wang, Chang Zeng, Bowen Zhang, Ziyang Ma, Yefan Zhu, Zifeng Cai, Jian Zhao, Zhonglin Jiang, Yong Chen. (2024)<br><strong>HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling</strong><br><button class=copy-to-clipboard title="HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Data Augmentation, Few-shot, Self-supervised Learning, Self-supervised Learning, Zero-shot, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05989v1.pdf filename=2403.05989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Token-based <b>text-to-speech</b> <b>(TTS)</b> models have emerged as a promising avenue for generating natural and realistic speech, yet they grapple with low pronunciation accuracy, speaking style and timbre inconsistency, and a substantial need for diverse training <b>data.</b> <b>In</b> response, we introduce a novel hierarchical acoustic modeling approach complemented by a tailored <b>data</b> <b>augmentation</b> strategy and train it on the combination of real and synthetic <b>data,</b> <b>scaling</b> the <b>data</b> <b>size</b> up to 650k hours, leading to the <b>zero-shot</b> <b>TTS</b> model with 0.8B parameters. Specifically, our method incorporates a latent variable sequence containing supplementary acoustic information based on refined <b>self-supervised</b> <b>learning</b> (SSL) discrete units into the <b>TTS</b> model by a predictor. This significantly mitigates pronunciation errors and style mutations in synthesized speech. During training, we strategically replace and duplicate segments of the <b>data</b> <b>to</b> enhance timbre uniformity. Moreover, a pretrained <b>few-shot</b> voice conversion model is utilized to generate a plethora of voices with identical content yet varied timbres. This facilitates the explicit learning of utterance-level one-to-many mappings, enriching speech diversity and also ensuring consistency in timbre. Comparative experiments (Demo page: <a href=https://anonymous.4open.science/w/ham-tts/)demonstrate>https://anonymous.4open.science/w/ham-tts/)demonstrate</a> our model&rsquo;s superiority over VALL-E in pronunciation precision and maintaining speaking style, as well as timbre continuity.</p></p class="citation"></blockquote><h3 id=23--19138-an-audio-textual-diffusion-model-for-converting-speech-signals-into-ultrasound-tongue-imaging-data-yudong-yang-et-al-2024>(2/3 | 19/138) An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data (Yudong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, Lan Wang. (2024)<br><strong>An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data</strong><br><button class=copy-to-clipboard title="An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Diffusion Model, BERT, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05820v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05820v2.pdf filename=2403.05820v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator movements, such as ultrasound tongue imaging (UTI) data. An issue of existing AAI methods is only using the personalized acoustic information to derive the general patterns of tongue motions, and thus the quality of generated UTI data is limited. To address this issue, this paper proposes an audio-textual <b>diffusion</b> <b>model</b> for the UTI data generation task. In this model, the inherent acoustic characteristics of individuals related to the tongue motion details are encoded by using wav2vec 2.0, while the <b>ASR</b> transcriptions related to the universality of tongue motions are encoded by using <b>BERT.</b> UTI data are then generated by using a <b>diffusion</b> <b>module.</b> Experimental results showed that the proposed <b>diffusion</b> <b>model</b> could generate high-quality UTI data with clear tongue contour that is crucial for the linguistic analysis and clinical assessment. The project can be found on the website\footnote{https://yangyudong2020.github.io/wav2uti/</p></p class="citation"></blockquote><h3 id=33--20138-svad-a-robust-low-power-and-light-weight-voice-activity-detection-with-spiking-neural-networks-qu-yang-et-al-2024>(3/3 | 20/138) sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks (Qu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qu Yang, Qianhui Liu, Nan Li, Meng Ge, Zeyang Song, Haizhou Li. (2024)<br><strong>sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-NE, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Convolution, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05772v1.pdf filename=2403.05772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech applications are expected to be low-power and robust under noisy conditions. An effective Voice Activity Detection (VAD) front-end lowers the computational need. Spiking Neural Networks (SNNs) are known to be biologically plausible and power-efficient. However, SNN-based VADs have yet to achieve noise robustness and often require large models for high performance. This paper introduces a novel SNN-based VAD model, referred to as sVAD, which features an auditory encoder with an SNN-based attention mechanism. Particularly, it provides effective auditory feature representation through SincNet and 1D <b>convolution,</b> and improves noise robustness with attention mechanisms. The classifier utilizes Spiking <b>Recurrent</b> <b>Neural</b> <b>Networks</b> (sRNN) to exploit temporal speech information. Experimental results demonstrate that our sVAD achieves remarkable noise robustness and meanwhile maintains low power consumption and a small footprint, making it a promising solution for real-world VAD applications.</p></p class="citation"></blockquote><h2 id=csai-1>cs.AI (1)</h2><h3 id=11--21138-enhancing-multi-hop-knowledge-graph-reasoning-through-reward-shaping-techniques-chen-li-et-al-2024>(1/1 | 21/138) Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques (Chen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Li, Haotian Zheng, Yiping Sun, Cangqing Wang, Liqiang Yu, Che Chang, Xinyu Tian, Bo Liu. (2024)<br><strong>Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques</strong><br><button class=copy-to-clipboard title="Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 66<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Reinforcement Learning, BERT, Reasoning, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05801v1.pdf filename=2403.05801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of computational <b>knowledge</b> <b>representation,</b> <b>Knowledge</b> <b>Graph</b> <b>Reasoning</b> <b>(KG-R)</b> stands at the forefront of facilitating sophisticated inferential capabilities across multifarious domains. The quintessence of this research elucidates the employment of <b>reinforcement</b> <b>learning</b> (RL) strategies, notably the REINFORCE algorithm, to navigate the intricacies inherent in multi-hop <b>KG-R.</b> This investigation critically addresses the prevalent challenges introduced by the inherent incompleteness of <b>Knowledge</b> <b>Graphs</b> <b>(KGs),</b> which frequently results in erroneous inferential outcomes, manifesting as both false negatives and misleading positives. By partitioning the Unified Medical Language System (UMLS) <b>benchmark</b> dataset into rich and sparse subsets, we investigate the efficacy of pre-trained <b>BERT</b> embeddings and <b>Prompt</b> <b>Learning</b> methodologies to refine the reward shaping process. This approach not only enhances the precision of multi-hop <b>KG-R</b> but also sets a new precedent for future research in the field, aiming to improve the robustness and accuracy of <b>knowledge</b> <b>inference</b> within complex <b>KG</b> frameworks. Our work contributes a novel perspective to the discourse on <b>KG</b> <b>reasoning,</b> offering a methodological advancement that aligns with the academic rigor and scholarly aspirations of the Natural journal, promising to invigorate further advancements in the realm of computational <b>knowledge</b> <b>representation.</b></p></p class="citation"></blockquote><h2 id=cscv-29>cs.CV (29)</h2><h3 id=129--22138-learned-3d-volumetric-recovery-of-clouds-and-its-uncertainty-for-climate-analysis-roi-ronen-et-al-2024>(1/29 | 22/138) Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis (Roi Ronen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roi Ronen, Ilan Koren, Aviad Levis, Eshkol Eytan, Vadim Holodovsky, Yoav Y. Schechner. (2024)<br><strong>Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis</strong><br><button class=copy-to-clipboard title="Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Out-of-distribution, Self-supervised Learning, Self-supervised Learning, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05932v1.pdf filename=2403.05932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant uncertainty in climate prediction and cloud physics is tied to observational gaps relating to shallow scattered clouds. Addressing these challenges requires remote sensing of their three-dimensional (3D) heterogeneous volumetric scattering content. This calls for passive scattering computed tomography (CT). We design a learning-based model (ProbCT) to achieve CT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers - for the first time - the posterior probability distribution of the heterogeneous extinction coefficient, per 3D location. This yields arbitrary valuable statistics, e.g., the 3D field of the most probable extinction and its uncertainty. ProbCT uses a neural-field representation, making essentially real-time inference. ProbCT undergoes <b>supervised</b> training by a new labeled multi-class database of physics-based volumetric fields of clouds and their corresponding images. To improve <b>out-of-distribution</b> inference, we incorporate <b>self-supervised</b> <b>learning</b> through differential rendering. We demonstrate the approach in <b>simulations</b> and on real-world data, and indicate the relevance of 3D recovery and uncertainty to precipitation and renewable energy.</p></p class="citation"></blockquote><h3 id=229--23138-gpt-as-psychologist-preliminary-evaluations-for-gpt-4v-on-visual-affective-computing-hao-lu-et-al-2024>(2/29 | 23/138) GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing (Hao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, Dengbo He, Shuiguang Deng, Hao Chen, Yingcong Chen, Shiguang Shan. (2024)<br><strong>GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing</strong><br><button class=copy-to-clipboard title="GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4, Emotion Recognition, Reasoning, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05916v1.pdf filename=2403.05916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> language models <b>(MLMs)</b> are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of <b>MLMs</b> with 5 crucial abilities for affective computing, spanning from visual affective tasks and <b>reasoning</b> tasks. The results show that <b>GPT4</b> has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of <b>GPT4</b> for handling advanced tasks in <b>emotion</b> <b>recognition</b> and related fields by integrating with task-related agents for more complex tasks, such as heart rate estimation through signal processing. In conclusion, this paper provides valuable insights into the potential applications and challenges of <b>MLMs</b> in human-centric computing. The interesting samples are available at \url{https://github.com/LuPaoPao/GPT4Affectivity}.</p></p class="citation"></blockquote><h3 id=329--24138-frequency-attention-for-knowledge-distillation-cuong-pham-et-al-2024>(3/29 | 24/138) Frequency Attention for Knowledge Distillation (Cuong Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cuong Pham, Van-Anh Nguyen, Trung Le, Dinh Phung, Gustavo Carneiro, Thanh-Toan Do. (2024)<br><strong>Frequency Attention for Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Frequency Attention for Knowledge Distillation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Object Detection, Benchmarking, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05894v1.pdf filename=2403.05894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> is an attractive approach for learning compact deep neural networks, which learns a lightweight student model by <b>distilling</b> <b>knowledge</b> <b>from</b> a complex teacher model. Attention-based <b>knowledge</b> <b>distillation</b> is a specific form of intermediate feature-based <b>knowledge</b> <b>distillation</b> that uses attention mechanisms to encourage the student to better mimic the teacher. However, most of the previous attention-based <b>distillation</b> approaches perform attention in the spatial domain, which primarily affects local regions in the input image. This may not be sufficient when we need to capture the broader context or global information necessary for effective <b>knowledge</b> <b>transfer.</b> In frequency domain, since each frequency is determined from all pixels of the image in spatial domain, it can contain global information about the image. Inspired by the benefits of the frequency domain, we propose a novel module that functions as an attention mechanism in the frequency domain. The module consists of a learnable global filter that can adjust the frequencies of student&rsquo;s features under the guidance of the teacher&rsquo;s features, which encourages the student&rsquo;s features to have patterns similar to the teacher&rsquo;s features. We then propose an enhanced <b>knowledge</b> <b>review-based</b> <b>distillation</b> model by leveraging the proposed frequency attention module. The extensive experiments with various teacher and student architectures on image classification and <b>object</b> <b>detection</b> <b>benchmark</b> datasets show that the proposed approach outperforms other <b>knowledge</b> <b>distillation</b> methods.</p></p class="citation"></blockquote><h3 id=429--25138-general-surgery-vision-transformer-a-video-pre-trained-foundation-model-for-general-surgery-samuel-schmidgall-et-al-2024>(4/29 | 25/138) General surgery vision transformer: A video pre-trained foundation model for general surgery (Samuel Schmidgall et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger. (2024)<br><strong>General surgery vision transformer: A video pre-trained foundation model for general surgery</strong><br><button class=copy-to-clipboard title="General surgery vision transformer: A video pre-trained foundation model for general surgery" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, q-bio-TO<br>Keyword Score: 50<br>Keywords: Vision Transformer, Fine-tuning, Foundation Model, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05949v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05949v2.pdf filename=2403.05949v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The absence of openly accessible data and specialized <b>foundation</b> <b>models</b> is a major barrier for computational research in surgery. Toward this, (i) we open-source the largest dataset of general surgery videos to-date, consisting of 680 hours of surgical videos, including data from robotic and laparoscopic techniques across 28 procedures; (ii) we propose a technique for video pre-training a general surgery <b>vision</b> <b>transformer</b> (GSViT) on surgical videos based on forward video prediction that can run in real-time for surgical applications, toward which we open-source the code and weights of GSViT; (iii) we also release code and weights for procedure-specific <b>fine-tuned</b> versions of GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the Cholec80 phase annotation task, displaying improved performance over state-of-the-art single frame predictors.</p></p class="citation"></blockquote><h3 id=529--26138-a-self-supervised-cnn-for-image-watermark-removal-chunwei-tian-et-al-2024>(5/29 | 26/138) A self-supervised CNN for image watermark removal (Chunwei Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunwei Tian, Menghua Zheng, Tiancai Jiao, Wangmeng Zuo, Yanning Zhang, Chia-Wen Lin. (2024)<br><strong>A self-supervised CNN for image watermark removal</strong><br><button class=copy-to-clipboard title="A self-supervised CNN for image watermark removal" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05807v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05807v1.pdf filename=2403.05807v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Popular <b>convolutional</b> <b>neural</b> <b>networks</b> mainly use paired images in a <b>supervised</b> way for image watermark removal. However, watermarked images do not have reference images in the real world, which results in poor robustness of image watermark removal techniques. In this paper, we propose a <b>self-supervised</b> <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> in image watermark removal (SWCNN). SWCNN uses a <b>self-supervised</b> way to construct reference watermarked images rather than given paired training samples, according to watermark distribution. A heterogeneous U-Net architecture is used to extract more complementary structural information via simple components for image watermark removal. Taking into account texture information, a mixed loss is exploited to improve visual effects of image watermark removal. Besides, a watermark dataset is conducted. Experimental results show that the proposed SWCNN is superior to popular <b>CNNs</b> in image watermark removal.</p></p class="citation"></blockquote><h3 id=629--27138-weakly-supervised-change-detection-via-knowledge-distillation-and-multiscale-sigmoid-inference-binghao-lu-et-al-2024>(6/29 | 27/138) Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference (Binghao Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binghao Lu, Caiwen Ding, Jinbo Bi, Dongjin Song. (2024)<br><strong>Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference</strong><br><button class=copy-to-clipboard title="Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05796v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05796v1.pdf filename=2403.05796v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Change detection, which aims to detect spatial changes from a pair of multi-temporal images due to natural or man-made causes, has been widely applied in remote sensing, disaster management, urban management, etc. Most existing change detection approaches, however, are fully <b>supervised</b> and require labor-intensive pixel-level labels. To address this, we develop a novel weakly <b>supervised</b> change detection technique via <b>Knowledge</b> <b>Distillation</b> and Multiscale Sigmoid Inference <b>(KD-MSI)</b> that leverages image-level labels. In our approach, the Class Activation Maps (CAM) are utilized not only to derive a change probability map but also to serve as a foundation for the <b>knowledge</b> <b>distillation</b> process. This is done through a joint training strategy of the teacher and student networks, enabling the student network to highlight potential change areas more accurately than teacher network based on image-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI) module as a post processing step to further refine the change probability map from the trained student network. Empirical results on three public datasets, i.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique, with its integrated training strategy, significantly outperforms the state-of-the-art.</p></p class="citation"></blockquote><h3 id=729--28138-ltgc-long-tail-recognition-via-leveraging-llms-driven-generated-content-qihao-zhao-et-al-2024>(7/29 | 28/138) LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content (Qihao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihao Zhao, Yalun Dai, Hao Li, Wei Hu, Fan Zhang, Jun Liu. (2024)<br><strong>LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content</strong><br><button class=copy-to-clipboard title="LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05854v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05854v3.pdf filename=2403.05854v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-tail recognition is challenging because it requires the model to learn good representations from tail categories and address imbalances across all categories. In this paper, we propose a novel generative and <b>fine-tuning</b> framework, LTGC, to handle long-tail recognition via leveraging generated content. Firstly, inspired by the rich implicit knowledge in <b>large-scale</b> <b>models</b> <b>(e.g.,</b> <b>large</b> <b>language</b> <b>models,</b> <b>LLMs),</b> LTGC leverages the power of these models to parse and reason over the original tail data to produce diverse tail-class content. We then propose several novel designs for LTGC to ensure the quality of the generated data and to efficiently <b>fine-tune</b> the model using both the generated and original data. The visualization demonstrates the effectiveness of the generation module in LTGC, which produces accurate and diverse tail data. Additionally, the experimental results demonstrate that our LTGC outperforms existing state-of-the-art methods on popular long-tailed <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=829--29138-unigradicon-a-foundation-model-for-medical-image-registration-lin-tian-et-al-2024>(8/29 | 29/138) uniGradICON: A Foundation Model for Medical Image Registration (Lin Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Tian, Hastings Greer, Roland Kwitt, Francois-Xavier Vialard, Raul San Jose Estepar, Sylvain Bouix, Richard Rushmore, Marc Niethammer. (2024)<br><strong>uniGradICON: A Foundation Model for Medical Image Registration</strong><br><button class=copy-to-clipboard title="uniGradICON: A Foundation Model for Medical Image Registration" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Out-of-distribution, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05780v1.pdf filename=2403.05780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional medical image registration approaches directly optimize over the parameters of a transformation model. These approaches have been highly successful and are used generically for registrations of different anatomical regions. Recent deep registration networks are incredibly fast and accurate but are only trained for specific tasks. Hence, they are no longer generic registration approaches. We therefore propose uniGradICON, a first step toward a <b>foundation</b> <b>model</b> for registration providing 1) great performance \emph{across} multiple datasets which is not feasible for current learning-based registration methods, 2) <b>zero-shot</b> capabilities for new registration tasks suitable for different acquisitions, anatomical regions, and modalities compared to the training dataset, and 3) a strong initialization for <b>finetuning</b> on <b>out-of-distribution</b> registration tasks. UniGradICON unifies the speed and accuracy benefits of learning-based registration algorithms with the generic applicability of conventional non-deep-learning approaches. We extensively trained and evaluated uniGradICON on twelve different public datasets. Our code and the uniGradICON model are available at <a href=https://github.com/uncbiag/uniGradICON>https://github.com/uncbiag/uniGradICON</a>.</p></p class="citation"></blockquote><h3 id=929--30138-carbonnet-how-computer-vision-plays-a-role-in-climate-change-application-learning-geomechanics-from-subsurface-geometry-of-ccs-to-mitigate-global-warming-wei-chen-et-al-2024>(9/29 | 30/138) CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming (Wei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Chen, Yunan Li, Yuan Tian. (2024)<br><strong>CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming</strong><br><button class=copy-to-clipboard title="CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Convolutional Neural Network, Geometry, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06025v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06025v2.pdf filename=2403.06025v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new approach using computer vision to predict the land surface displacement from subsurface <b>geometry</b> images for Carbon Capture and Sequestration (CCS). CCS has been proved to be a key component for a carbon neutral society. However, scientists see there are challenges along the way including the high computational cost due to the large model scale and limitations to generalize a pre-trained model with complex physics. We tackle those challenges by training models directly from the subsurface <b>geometry</b> images. The goal is to understand the respons of land surface displacement due to carbon injection and utilize our trained models to inform decision making in CCS projects. We implement multiple models <b>(CNN,</b> ResNet, and ResNetUNet) for static mechanics problem, which is a image prediction problem. Next, we use the <b>LSTM</b> and <b>transformer</b> for transient mechanics scenario, which is a video prediction problem. It shows ResNetUNet outperforms the others thanks to its architecture in static mechanics problem, and <b>LSTM</b> shows comparable performance to <b>transformer</b> in transient problem. This report proceeds by outlining our dataset in detail followed by model descriptions in method section. Result and discussion state the key learning, observations, and conclusion with future work rounds out the paper.</p></p class="citation"></blockquote><h3 id=1029--31138-can-generative-models-improve-self-supervised-representation-learning-arash-afkanpour-et-al-2024>(10/29 | 31/138) Can Generative Models Improve Self-Supervised Representation Learning? (Arash Afkanpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arash Afkanpour, Vahid Reza Khazaie, Sana Ayromlou, Fereshteh Forghani. (2024)<br><strong>Can Generative Models Improve Self-Supervised Representation Learning?</strong><br><button class=copy-to-clipboard title="Can Generative Models Improve Self-Supervised Representation Learning?" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 35<br>Keywords: Data Augmentation, Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05966v1.pdf filename=2403.05966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement in <b>self-supervised</b> <b>learning</b> (SSL) has highlighted its potential to leverage unlabeled <b>data</b> <b>for</b> learning powerful visual <b>representations.</b> <b>However,</b> existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined <b>data</b> <b>augmentations.</b> This constrains the diversity and quality of transformations, which leads to sub-optimal <b>representations.</b> <b>In</b> this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image <b>representation,</b> <b>our</b> method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of <b>data</b> <b>for</b> <b>self-supervised</b> <b>learning.</b> Our experimental results demonstrate that our framework significantly enhances the quality of learned visual <b>representations.</b> <b>This</b> research demonstrates that incorporating generative models into the SSL workflow opens new avenues for exploring the potential of unlabeled visual <b>data.</b> <b>This</b> development paves the way for more robust and versatile <b>representation</b> <b>learning</b> techniques.</p></p class="citation"></blockquote><h3 id=1129--32138-robust-emotion-recognition-in-context-debiasing-dingkang-yang-et-al-2024>(11/29 | 32/138) Robust Emotion Recognition in Context Debiasing (Dingkang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang, Lihua Zhang. (2024)<br><strong>Robust Emotion Recognition in Context Debiasing</strong><br><button class=copy-to-clipboard title="Robust Emotion Recognition in Context Debiasing" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Graph, Counter-factual, Emotion Recognition, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05963v1.pdf filename=2403.05963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context-aware <b>emotion</b> <b>recognition</b> (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person&rsquo;s <b>emotional</b> <b>state.</b> Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and <b>emotion</b> <b>labels</b> in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a <b>counterfactual</b> <b>emotion</b> <b>inference</b> (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal <b>graph</b> to decouple the causal relationships among the variables in CAER. Following the causal <b>graph,</b> CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and <b>counterfactual</b> outcomes, resulting in bias mitigation and robust prediction. As a model-agnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.</p></p class="citation"></blockquote><h3 id=1229--33138-generalizing-to-out-of-sample-degradations-via-model-reprogramming-runhua-jiang-et-al-2024>(12/29 | 33/138) Generalizing to Out-of-Sample Degradations via Model Reprogramming (Runhua Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runhua Jiang, Yahong Han. (2024)<br><strong>Generalizing to Out-of-Sample Degradations via Model Reprogramming</strong><br><button class=copy-to-clipboard title="Generalizing to Out-of-Sample Degradations via Model Reprogramming" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Zero-shot, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05886v1.pdf filename=2403.05886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>image</b> <b>restoration</b> models are typically designed for specific tasks and struggle to generalize to out-of-sample degradations not encountered during training. While <b>zero-shot</b> methods can address this limitation by <b>fine-tuning</b> model parameters on testing samples, their effectiveness relies on predefined natural priors and physical models of specific degradations. Nevertheless, determining out-of-sample degradations faced in real-world scenarios is always impractical. As a result, it is more desirable to train restoration models with inherent generalization ability. To this end, this work introduces the Out-of-Sample Restoration (OSR) task, which aims to develop restoration models capable of handling out-of-sample degradations. An intuitive solution involves pre-translating out-of-sample degradations to known degradations of restoration models. However, directly translating them in the <b>image</b> <b>space</b> could lead to complex <b>image</b> <b>translation</b> issues. To address this issue, we propose a model reprogramming framework, which translates out-of-sample degradations by quantum mechanic and wave functions. Specifically, input <b>images</b> <b>are</b> decoupled as wave functions of amplitude and phase terms. The translation of out-of-sample degradation is performed by adapting the phase term. Meanwhile, the <b>image</b> <b>content</b> is maintained and enhanced in the amplitude term. By taking these two terms as inputs, restoration models are able to handle out-of-sample degradations without <b>fine-tuning.</b> Through extensive experiments across multiple evaluation cases, we demonstrate the effectiveness and flexibility of our proposed framework. Our codes are available at \href{https://github.com/ddghjikle/Out-of-sample-restoration}{Github}.</p></p class="citation"></blockquote><h3 id=1329--34138-diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-michael-toker-et-al-2024>(13/29 | 34/138) Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines (Michael Toker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, Yonatan Belinkov. (2024)<br><strong>Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines</strong><br><button class=copy-to-clipboard title="Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-2-7; I-4-0, cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05846v1.pdf filename=2403.05846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> (T2I) use a latent representation of a text <b>prompt</b> to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the <b>Diffusion</b> <b>Lens,</b> a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the <b>Diffusion</b> <b>Lens,</b> we perform an extensive analysis of two recent T2I models. Exploring compound <b>prompts,</b> we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.</p></p class="citation"></blockquote><h3 id=1429--35138-do3d-self-supervised-learning-of-decomposed-object-aware-3d-motion-and-depth-from-monocular-videos-xiuzhe-wu-et-al-2024>(14/29 | 35/138) DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and Depth from Monocular Videos (Xiuzhe Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiuzhe Wu, Xiaoyang Lyu, Qihao Huang, Yong Liu, Yang Wu, Ying Shan, Xiaojuan Qi. (2024)<br><strong>DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and Depth from Monocular Videos</strong><br><button class=copy-to-clipboard title="DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and Depth from Monocular Videos" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Benchmarking, Geometry, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05895v1.pdf filename=2403.05895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although considerable advancements have been attained in <b>self-supervised</b> <b>depth</b> estimation from monocular videos, most existing methods often treat all objects in a video as static entities, which however violates the dynamic nature of real-world scenes and fails to model the <b>geometry</b> and motion of moving objects. In this paper, we propose a <b>self-supervised</b> <b>method</b> to jointly learn 3D motion and depth from monocular videos. Our system contains a depth estimation module to predict depth, and a new decomposed object-wise 3D motion (DO3D) estimation module to predict ego-motion and 3D object motion. Depth and motion networks work collaboratively to faithfully model the <b>geometry</b> and dynamics of real-world scenes, which, in turn, benefits both depth and 3D motion estimation. Their predictions are further combined to synthesize a novel video frame for <b>self-supervised</b> <b>training.</b> As a core component of our framework, DO3D is a new motion disentanglement module that learns to predict camera ego-motion and instance-aware 3D object motion separately. To alleviate the difficulties in estimating non-rigid 3D object motions, they are decomposed to object-wise 6-DoF global transformations and a pixel-wise local 3D motion deformation field. Qualitative and quantitative experiments are conducted on three <b>benchmark</b> datasets, including KITTI, Cityscapes, and VKITTI2, where our model delivers superior performance in all evaluated settings. For the depth estimation task, our model outperforms all compared research works in the high-resolution setting, attaining an absolute relative depth error (abs rel) of 0.099 on the KITTI <b>benchmark.</b> Besides, our optical flow estimation results (an overall EPE of 7.09 on KITTI) also surpass state-of-the-art methods and largely improve the estimation of dynamic regions, demonstrating the effectiveness of our motion model. Our code will be available.</p></p class="citation"></blockquote><h3 id=1529--36138-cscnet-class-specified-cascaded-network-for-compositional-zero-shot-learning-yanyi-zhang-et-al-2024>(15/29 | 36/138) CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning (Yanyi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanyi Zhang, Qi Jia, Xin Fan, Yu Liu, Ran He. (2024)<br><strong>CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning</strong><br><button class=copy-to-clipboard title="CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Representation Learning, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05924v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05924v2.pdf filename=2403.05924v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attribute and object (A-O) disentanglement is a fundamental and critical problem for Compositional <b>Zero-shot</b> <b>Learning</b> (CZSL), whose aim is to recognize novel A-O compositions based on foregone knowledge. Existing methods based on disentangled <b>representation</b> <b>learning</b> lose sight of the contextual dependency between the A-O primitive pairs. Inspired by this, we propose a novel A-O disentangled framework for CZSL, namely Class-specified Cascaded Network (CSCNet). The key insight is to firstly classify one primitive and then specifies the predicted class as a priori for guiding another primitive recognition in a cascaded fashion. To this end, CSCNet constructs Attribute-to-Object and Object-to-Attribute cascaded branches, in addition to a composition branch modeling the two primitives as a whole. Notably, we devise a parametric classifier (ParamCls) to improve the matching between visual and semantic embeddings. By improving the A-O disentanglement, our framework achieves superior results than previous competitive methods.</p></p class="citation"></blockquote><h3 id=1629--37138-realnet-a-feature-selection-network-with-realistic-synthetic-anomaly-for-anomaly-detection-ximiao-zhang-et-al-2024>(16/29 | 37/138) RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection (Ximiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ximiao Zhang, Min Xu, Xiuzhuang Zhou. (2024)<br><strong>RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection</strong><br><button class=copy-to-clipboard title="RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Anomaly Detection, Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05897v1.pdf filename=2403.05897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> feature reconstruction methods have shown promising advances in industrial image <b>anomaly</b> <b>detection</b> and localization. Despite this progress, these methods still face challenges in synthesizing realistic and diverse <b>anomaly</b> <b>samples,</b> as well as addressing the feature redundancy and pre-training bias of pre-trained feature. In this work, we introduce RealNet, a feature reconstruction network with realistic synthetic <b>anomaly</b> <b>and</b> adaptive feature selection. It is incorporated with three key innovations: First, we propose Strength-controllable Diffusion <b>Anomaly</b> <b>Synthesis</b> (SDAS), a diffusion process-based synthesis strategy capable of generating samples with varying <b>anomaly</b> <b>strengths</b> that mimic the distribution of real anomalous samples. Second, we develop <b>Anomaly-aware</b> <b>Features</b> Selection (AFS), a method for selecting representative and discriminative pre-trained feature subsets to improve <b>anomaly</b> <b>detection</b> performance while controlling computational costs. Third, we introduce Reconstruction Residuals Selection (RRS), a strategy that adaptively selects discriminative residuals for comprehensive identification of anomalous regions across multiple levels of granularity. We assess RealNet on four <b>benchmark</b> datasets, and our results demonstrate significant improvements in both Image AUROC and Pixel AUROC compared to the current state-o-the-art methods. The code, data, and models are available at <a href=https://github.com/cnulab/RealNet>https://github.com/cnulab/RealNet</a>.</p></p class="citation"></blockquote><h3 id=1729--38138-long-term-frame-event-visual-tracking-benchmark-dataset-and-baseline-xiao-wang-et-al-2024>(17/29 | 38/138) Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline (Xiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Wang, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong Tian, Jin Tang, Bin Luo. (2024)<br><strong>Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline</strong><br><button class=copy-to-clipboard title="Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-NE, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05839v1.pdf filename=2403.05839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory <b>Transformer</b> network as a unified backbone by introducing modern Hopfield layers into multi-head <b>self-attention</b> blocks to fuse both RGB and event data. Extensive experiments on both FELT and RGB-T tracking dataset LasHeR fully validated the effectiveness of our model. The dataset and source code can be found at \url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.</p></p class="citation"></blockquote><h3 id=1829--39138-adaptive-multi-modal-fusion-of-spatially-variant-kernel-refinement-with-diffusion-model-for-blind-image-super-resolution-junxiong-lin-et-al-2024>(18/29 | 39/138) Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution (Junxiong Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junxiong Lin, Yan Wang, Zeng Tao, Boyang Wang, Qing Zhao, Haorang Wang, Xuan Tong, Xinji Mai, Yuxuan Lin, Wei Song, Jiawen Yu, Shaoqi Yan, Wenqiang Zhang. (2024)<br><strong>Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution</strong><br><button class=copy-to-clipboard title="Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Multi-modal, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05808v1.pdf filename=2403.05808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>diffusion</b> <b>models</b> utilized for image generation encapsulate a substantial reservoir of a priori knowledge pertaining to intricate textures. Harnessing the potential of leveraging this a priori knowledge in the context of image super-resolution presents a compelling avenue. Nonetheless, prevailing <b>diffusion-based</b> <b>methodologies</b> presently overlook the constraints imposed by degradation information on the <b>diffusion</b> <b>process.</b> Furthermore, these methods fail to consider the spatial variability inherent in the estimated blur kernel, <b>stemming</b> from factors such as motion jitter and out-of-focus elements in open-environment scenarios. This oversight results in a notable deviation of the image super-resolution effect from fundamental realities. To address these concerns, we introduce a framework known as Adaptive <b>Multi-modal</b> Fusion of \textbf{S}patially Variant Kernel Refinement with <b>Diffusion</b> <b>Model</b> for Blind Image \textbf{S}uper-\textbf{R}esolution (SSR). Within the SSR framework, we propose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a Depth-Informed Kernel, which takes the depth information into account and is spatially variant. Additionally, SVKR enhance the accuracy of depth information acquired from LR images, allowing for mutual enhancement between the depth map and blur kernel estimates. Finally, we introduce the Adaptive <b>Multi-Modal</b> Fusion (AMF) module to align the information from three modalities: low-resolution images, depth maps, and blur kernels. This alignment can constrain the <b>diffusion</b> <b>model</b> to generate more authentic SR results. Quantitative and qualitative experiments affirm the superiority of our approach, while ablation experiments corroborate the effectiveness of the modules we have proposed.</p></p class="citation"></blockquote><h3 id=1929--40138-classifying-objects-in-3d-point-clouds-using-recurrent-neural-network-a-gru-lstm-hybrid-approach-ramin-mousa-et-al-2024>(19/29 | 40/138) Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach (Ramin Mousa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ramin Mousa, Mitra Khezli, Saba Hesaraki. (2024)<br><strong>Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach</strong><br><button class=copy-to-clipboard title="Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05950v1.pdf filename=2403.05950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate classification of objects in 3D point clouds is a significant problem in several applications, such as autonomous navigation and augmented/virtual reality scenarios, which has become a research hot spot. In this paper, we presented a deep learning strategy for 3D object classification in augmented reality. The proposed approach is a combination of the GRU and <b>LSTM.</b> <b>LSTM</b> networks learn longer dependencies well, but due to the number of gates, it takes longer to train; on the other hand, GRU networks have a weaker performance than <b>LSTM,</b> but their training speed is much higher than GRU, which is The speed is due to its fewer gates. The proposed approach used the combination of speed and accuracy of these two networks. The proposed approach achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes eight classes (unlabeled, man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the traditional machine learning approaches could achieve a maximum accuracy of 0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality, Hybrid Model, GRULSTM, GRU, <b>LSTM</b></p></p class="citation"></blockquote><h3 id=2029--41138-dynamic-policy-driven-adaptive-multi-instance-learning-for-whole-slide-image-classification-tingting-zheng-et-al-2024>(20/29 | 41/138) Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification (Tingting Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tingting Zheng, Kui Jiang, Hongxun Yao. (2024)<br><strong>Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification</strong><br><button class=copy-to-clipboard title="Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07939v1.pdf filename=2403.07939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Instance Learning (MIL) has shown impressive performance for histopathology whole slide image (WSI) analysis using bags or pseudo-bags. It involves instance sampling, feature representation, and decision-making. However, existing MIL-based technologies at least suffer from one or more of the following problems: 1) requiring high storage and intensive pre-processing for numerous instances (sampling); 2) potential over-fitting with limited knowledge to predict bag labels (feature representation); 3) pseudo-bag counts and prior biases affect model robustness and generalizability (decision-making). Inspired by clinical diagnostics, using the past sampling instances can facilitate the final WSI analysis, but it is barely explored in prior technologies. To break free these limitations, we integrate the dynamic instance sampling and <b>reinforcement</b> <b>learning</b> into a unified framework to improve the instance selection and feature aggregation, forming a novel Dynamic Policy Instance Selection (DPIS) scheme for better and more credible decision-making. Specifically, the measurement of feature distance and reward function are employed to boost continuous instance sampling. To alleviate the over-fitting, we explore the latent global relations among instances for more robust and discriminative feature representation while establishing reward and punishment mechanisms to correct biases in pseudo-bags using <b>contrastive</b> <b>learning.</b> These strategies form the final Dynamic Policy-Driven Adaptive Multi-Instance Learning (PAMIL) method for WSI tasks. Extensive experiments reveal that our PAMIL method outperforms the state-of-the-art by 3.8% on CAMELYON16 and 4.4% on TCGA lung cancer datasets.</p></p class="citation"></blockquote><h3 id=2129--42138-towards-deviation-robust-agent-navigation-via-perturbation-aware-contrastive-learning-bingqian-lin-et-al-2024>(21/29 | 42/138) Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning (Bingqian Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Liang Lin. (2024)<br><strong>Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning</strong><br><button class=copy-to-clipboard title="Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05770v1.pdf filename=2403.05770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-language</b> navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Progressive Perturbation-aware <b>Contrastive</b> <b>Learning</b> (PROPER) to enhance the generalization ability of existing VLN agents, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction. Since directly enforcing the agent to learn perturbed trajectories may lead to inefficient training, a progressively perturbed trajectory augmentation strategy is designed, where the agent can self-adaptively learn to navigate under perturbation with the improvement of its navigation performance for each specific trajectory. For encouraging the agent to well capture the difference brought by perturbation, a perturbation-aware <b>contrastive</b> <b>learning</b> mechanism is further developed by contrasting perturbation-free trajectory encodings and perturbation-based counterparts. Extensive experiments on R2R show that PROPER can benefit multiple VLN baselines in perturbation-free scenarios. We further collect the perturbed path data to construct an introspection subset based on the R2R, called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying robustness of popular VLN agents and the capability of PROPER in improving the navigation robustness.</p></p class="citation"></blockquote><h3 id=2229--43138-semi-supervised-multimodal-multi-instance-learning-for-aortic-stenosis-diagnosis-zhe-huang-et-al-2024>(22/29 | 43/138) Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis (Zhe Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Huang, Xiaowei Yu, Benjamin S. Wessler, Michael C. Hughes. (2024)<br><strong>Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis</strong><br><button class=copy-to-clipboard title="Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-ET, cs-LG, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Multiple Instance Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06024v1.pdf filename=2403.06024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated interpretation of ultrasound imaging of the heart (echocardiograms) could improve the detection and treatment of aortic stenosis (AS), a deadly heart disease. However, existing deep learning pipelines for assessing AS from echocardiograms have two key limitations. First, most methods rely on limited 2D cineloops, thereby ignoring widely available Doppler imaging that contains important complementary information about pressure gradients and blood flow abnormalities associated with AS. Second, obtaining labeled data is difficult. There are often far more unlabeled echocardiogram recordings available, but these remain underutilized by existing methods. To overcome these limitations, we introduce Semi-supervised <b>Multimodal</b> <b>Multiple-Instance</b> <b>Learning</b> <b>(SMMIL),</b> a new deep learning framework for automatic interpretation for structural heart diseases like AS. When deployed, SMMIL can combine information from two input modalities, spectral Dopplers and 2D cineloops, to produce a study-level AS diagnosis. During training, SMMIL can combine a smaller labeled set and an abundant unlabeled set of both modalities to improve its classifier. Experiments demonstrate that SMMIL outperforms recent alternatives at 3-level AS severity classification as well as several clinically relevant AS detection tasks.</p></p class="citation"></blockquote><h3 id=2329--44138-spaformer-sequential-3d-part-assembly-with-transformers-boshen-xu-et-al-2024>(23/29 | 44/138) SPAFormer: Sequential 3D Part Assembly with Transformers (Boshen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boshen Xu, Sipeng Zheng, Qin Jin. (2024)<br><strong>SPAFormer: Sequential 3D Part Assembly with Transformers</strong><br><button class=copy-to-clipboard title="SPAFormer: Sequential 3D Part Assembly with Transformers" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05874v1.pdf filename=2403.05874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SPAFormer, an innovative model designed to overcome the combinatorial explosion challenge in the 3D Part Assembly (3D-PA) task. This task requires accurate prediction of each part&rsquo;s pose and shape in sequential steps, and as the number of parts increases, the possible assembly combinations increase exponentially, leading to a combinatorial explosion that severely hinders the efficacy of 3D-PA. SPAFormer addresses this problem by leveraging weak constraints from assembly sequences, effectively reducing the solution space&rsquo;s complexity. Since assembly part sequences convey construction rules similar to sentences being structured through words, our model explores both parallel and autoregressive generation. It further enhances assembly through knowledge enhancement strategies that utilize the attributes of parts and their sequence information, enabling it to capture the inherent assembly pattern and relationships among sequentially ordered parts. We also construct a more challenging <b>benchmark</b> named PartNet-Assembly covering 21 varied categories to more comprehensively validate the effectiveness of SPAFormer. Extensive experiments demonstrate the superior generalization capabilities of SPAFormer, particularly with multi-tasking and in scenarios requiring long-horizon assembly. Codes and model weights will be released at \url{https://github.com/xuboshen/SPAFormer}.</p></p class="citation"></blockquote><h3 id=2429--45138-pov-prompt-oriented-view-agnostic-learning-for-egocentric-hand-object-interaction-in-the-multi-view-world-boshen-xu-et-al-2024>(24/29 | 45/138) POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World (Boshen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boshen Xu, Sipeng Zheng, Qin Jin. (2024)<br><strong>POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World</strong><br><button class=copy-to-clipboard title="POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05856v1.pdf filename=2403.05856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We humans are good at translating third-person observations of hand-object interactions (HOI) into an egocentric view. However, current methods struggle to replicate this ability of view adaptation from third-person to first-person. Although some approaches attempt to learn view-agnostic representation from large-scale video datasets, they ignore the relationships among multiple third-person views. To this end, we propose a <b>Prompt-Oriented</b> View-agnostic learning (POV) framework in this paper, which enables this view adaptation with few egocentric videos. Specifically, We introduce interactive masking <b>prompts</b> at the frame level to capture fine-grained action information, and view-aware <b>prompts</b> at the token level to learn view-agnostic representation. To verify our method, we establish two <b>benchmarks</b> for transferring from multiple third-person views to the egocentric view. Our extensive experiments on these <b>benchmarks</b> demonstrate the efficiency and effectiveness of our POV framework and <b>prompt</b> tuning techniques in terms of view adaptation and view generalization. Our code is available at \url{https://github.com/xuboshen/pov_acmmm2023}.</p></p class="citation"></blockquote><h3 id=2529--46138-recurrent-aligned-network-for-generalized-pedestrian-trajectory-prediction-yonghao-dong-et-al-2024>(25/29 | 46/138) Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction (Yonghao Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonghao Dong, Le Wang, Sanping Zhou, Gang Hua, Changyin Sun. (2024)<br><strong>Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05810v1.pdf filename=2403.05810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pedestrian trajectory prediction is a crucial component in computer vision and robotics, but remains challenging due to the <b>domain</b> <b>shift</b> problem. Previous studies have tried to tackle this problem by leveraging a portion of the trajectory data from the target <b>domain</b> <b>to</b> adapt the model. However, such <b>domain</b> <b>adaptation</b> methods are impractical in real-world scenarios, as it is infeasible to collect trajectory data from all potential target <b>domains.</b> <b>In</b> this paper, we study a task named generalized pedestrian trajectory prediction, with the aim of generalizing the model to unseen <b>domains</b> <b>without</b> accessing their trajectories. To tackle this task, we introduce a Recurrent Aligned Network~(RAN) to minimize the <b>domain</b> <b>gap</b> through <b>domain</b> <b>alignment.</b> Specifically, we devise a recurrent alignment module to effectively align the trajectory feature spaces at both time-state and time-sequence levels by the recurrent alignment strategy.Furthermore, we introduce a pre-aligned representation module to combine social interactions with the recurrent alignment strategy, which aims to consider social interactions during the alignment process instead of just target trajectories. We extensively evaluate our method and compare it with state-of-the-art methods on three widely used <b>benchmarks.</b> The experimental results demonstrate the superior generalization capability of our method. Our work not only fills the gap in the generalization setting for practical pedestrian trajectory prediction but also sets strong baselines in this field.</p></p class="citation"></blockquote><h3 id=2629--47138-deep-contrastive-multi-view-clustering-under-semantic-feature-guidance-siwen-liu-et-al-2024>(26/29 | 47/138) Deep Contrastive Multi-view Clustering under Semantic Feature Guidance (Siwen Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siwen Liu, Jinyan Liu, Hanning Yuan, Qi Li, Jing Geng, Ziqiang Yuan, Huaxu Han. (2024)<br><strong>Deep Contrastive Multi-view Clustering under Semantic Feature Guidance</strong><br><button class=copy-to-clipboard title="Deep Contrastive Multi-view Clustering under Semantic Feature Guidance" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05768v1.pdf filename=2403.05768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>learning</b> has achieved promising performance in the field of multi-view <b>clustering</b> recently. However, the positive and negative sample construction mechanisms ignoring semantic consistency lead to false negative pairs, limiting the performance of existing algorithms from further improvement. To solve this problem, we propose a multi-view <b>clustering</b> framework named Deep <b>Contrastive</b> <b>Multi-view</b> <b>Clustering</b> under Semantic feature guidance (DCMCS) to alleviate the influence of false negative pairs. Specifically, view-specific features are firstly extracted from raw features and fused to obtain fusion view features according to view importance. To mitigate the interference of view-private information, specific view and fusion view semantic features are learned by cluster-level <b>contrastive</b> <b>learning</b> and concatenated to measure the semantic similarity of instances. By minimizing instance-level <b>contrastive</b> <b>loss</b> weighted by semantic similarity, DCMCS adaptively weakens <b>contrastive</b> <b>leaning</b> between false negative pairs. Experimental results on several public datasets demonstrate the proposed framework outperforms the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2729--48138-fast-kernel-scene-flow-xueqian-li-et-al-2024>(27/29 | 48/138) Fast Kernel Scene Flow (Xueqian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueqian Li, Simon Lucey. (2024)<br><strong>Fast Kernel Scene Flow</strong><br><button class=copy-to-clipboard title="Fast Kernel Scene Flow" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05896v1.pdf filename=2403.05896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contrast to current state-of-the-art methods, such as NSFP [25], which employ deep implicit neural functions for modeling scene flow, we present a novel approach that utilizes classical kernel representations. This representation enables our approach to effectively handle dense lidar points while demonstrating exceptional computational efficiency &ndash; compared to recent deep approaches &ndash; achieved through the solution of a linear system. As a runtime optimization-based method, our model exhibits impressive generalizability across various <b>out-of-distribution</b> scenarios, achieving competitive performance on large-scale lidar datasets. We propose a new positional encoding-based kernel that demonstrates state-of-the-art performance in efficient lidar scene flow estimation on large-scale point clouds. An important highlight of our method is its near real-time performance (~150-170 ms) with dense lidar data (~8k-144k points), enabling a variety of practical applications in robotics and autonomous driving scenarios.</p></p class="citation"></blockquote><h3 id=2829--49138-safdnet-a-simple-and-effective-network-for-fully-sparse-3d-object-detection-gang-zhang-et-al-2024>(28/29 | 49/138) SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection (Gang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Si Liu, Xiaolin Hu. (2024)<br><strong>SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection</strong><br><button class=copy-to-clipboard title="SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05817v1.pdf filename=2403.05817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR-based 3D <b>object</b> <b>detection</b> plays an essential role in autonomous driving. Existing high-performing 3D <b>object</b> <b>detectors</b> usually build dense feature maps in the backbone network and prediction head. However, the computational costs introduced by the dense feature maps grow quadratically as the perception range increases, making these models hard to scale up to long-range detection. Some recent works have attempted to construct fully sparse detectors to solve this issue; nevertheless, the resulting models either rely on a complex multi-stage pipeline or exhibit inferior performance. In this work, we propose SAFDNet, a straightforward yet highly effective architecture, tailored for fully sparse 3D <b>object</b> <b>detection.</b> In SAFDNet, an adaptive feature diffusion strategy is designed to address the center feature missing problem. We conducted extensive experiments on Waymo Open, nuScenes, and Argoverse2 datasets. SAFDNet performed slightly better than the previous SOTA on the first two datasets but much better on the last dataset, which features long-range detection, verifying the efficacy of SAFDNet in scenarios where long-range detection is required. Notably, on Argoverse2, SAFDNet surpassed the previous best hybrid detector HEDNet by 2.6% mAP while being 2.1x faster, and yielded 2.1% mAP gains over the previous best sparse detector FSDv2 while being 1.3x faster. The code will be available at <a href=https://github.com/zhanggang001/HEDNet>https://github.com/zhanggang001/HEDNet</a>.</p></p class="citation"></blockquote><h3 id=2929--50138-lightning-nerf-efficient-hybrid-scene-representation-for-autonomous-driving-junyi-cao-et-al-2024>(29/29 | 50/138) Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving (Junyi Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyi Cao, Zhichao Li, Naiyan Wang, Chao Ma. (2024)<br><strong>Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05907v1.pdf filename=2403.05907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have highlighted the promising application of NeRF in autonomous driving contexts. However, the complexity of outdoor environments, combined with the restricted viewpoints in driving scenarios, complicates the task of precisely reconstructing scene <b>geometry.</b> Such challenges often lead to diminished quality in reconstructions and extended durations for both training and rendering. To tackle these challenges, we present Lightning NeRF. It uses an efficient hybrid scene representation that effectively utilizes the <b>geometry</b> prior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly improves the novel view synthesis performance of NeRF and reduces computational overheads. Through evaluations on real-world datasets, such as KITTI-360, Argoverse2, and our private dataset, we demonstrate that our approach not only exceeds the current state-of-the-art in novel view synthesis quality but also achieves a five-fold increase in training speed and a ten-fold improvement in rendering speed. Codes are available at <a href=https://github.com/VISION-SJTU/Lightning-NeRF>https://github.com/VISION-SJTU/Lightning-NeRF</a> .</p></p class="citation"></blockquote><h2 id=eessiv-4>eess.IV (4)</h2><h3 id=14--51138-segmentation-guided-sparse-transformer-for-under-display-camera-image-restoration-jingyun-xue-et-al-2024>(1/4 | 51/138) Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration (Jingyun Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyun Xue, Tao Wang, Jun Wang, Kaihao Zhang, Wenhan Luo, Wenqi Ren, Zikun Liu, Hyunhee Park, Xiaochun Cao. (2024)<br><strong>Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration</strong><br><button class=copy-to-clipboard title="Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05906v1.pdf filename=2403.05906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Under-Display Camera (UDC) is an emerging technology that achieves full-screen display via hiding the camera under the display panel. However, the current implementation of UDC causes serious degradation. The incident light required for camera imaging undergoes attenuation and diffraction when passing through the display panel, leading to various artifacts in UDC imaging. Presently, the prevailing UDC image restoration methods predominantly utilize <b>convolutional</b> <b>neural</b> <b>network</b> architectures, whereas <b>Transformer-based</b> methods have exhibited superior performance in the majority of image restoration tasks. This is attributed to the <b>Transformer&rsquo;s</b> capability to sample global features for the local reconstruction of images, thereby achieving high-quality image restoration. In this paper, we observe that when using the <b>Vision</b> <b>Transformer</b> for UDC degraded image restoration, the global attention samples a large amount of redundant information and noise. Furthermore, compared to the ordinary <b>Transformer</b> employing dense attention, the <b>Transformer</b> utilizing sparse attention can alleviate the adverse impact of redundant information and noise. Building upon this discovery, we propose a Segmentation Guided Sparse <b>Transformer</b> method (SGSFormer) for the task of restoring high-quality images from UDC degraded images. Specifically, we utilize sparse <b>self-attention</b> to filter out redundant information and noise, directing the model&rsquo;s attention to focus on the features more relevant to the degraded regions in need of reconstruction. Moreover, we integrate the instance segmentation map as prior information to guide the sparse <b>self-attention</b> in filtering and focusing on the correct regions.</p></p class="citation"></blockquote><h3 id=24--52138-hair-and-scalp-disease-detection-using-deep-learning-kavita-sultanpure-et-al-2024>(2/4 | 52/138) Hair and scalp disease detection using deep learning (Kavita Sultanpure et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kavita Sultanpure, Bhairavi Shirsath, Bhakti Bhande, Harshada Sawai, Srushti Gawade, Suraj Samgir. (2024)<br><strong>Hair and scalp disease detection using deep learning</strong><br><button class=copy-to-clipboard title="Hair and scalp disease detection using deep learning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07940v1.pdf filename=2403.07940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a notable advancement in the integration of healthcare and technology, particularly evident in the field of medical image analysis. This paper introduces a pioneering approach in dermatology, presenting a robust method for the detection of hair and scalp diseases using state-of-the-art deep learning techniques. Our methodology relies on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> well-known for their efficacy in image recognition, to meticulously analyze images for various dermatological conditions affecting the hair and scalp. Our proposed system represents a significant advancement in dermatological diagnostics, offering a non-invasive and highly efficient means of early detection and diagnosis. By leveraging the capabilities of <b>CNNs,</b> our model holds the potential to revolutionize dermatology, providing accessible and timely healthcare solutions. Furthermore, the seamless integration of our trained model into a web-based platform developed with the Django framework ensures broad accessibility and usability, democratizing advanced medical diagnostics. The integration of machine learning algorithms into web applications marks a pivotal moment in healthcare delivery, promising empowerment for both healthcare providers and patients. Through the synergy between technology and healthcare, our paper outlines the meticulous methodology, technical intricacies, and promising future prospects of our system. With a steadfast commitment to advancing healthcare frontiers, our goal is to significantly contribute to leveraging technology for improved healthcare outcomes globally. This endeavor underscores the profound impact of technological innovation in shaping the future of healthcare delivery and patient care, highlighting the transformative potential of our approach.</p></p class="citation"></blockquote><h3 id=34--53138-udcr-unsupervised-aortic-dsacta-rigid-registration-using-deep-reinforcement-learning-and-overlap-degree-calculation-wentao-liu-et-al-2024>(3/4 | 53/138) UDCR: Unsupervised Aortic DSA/CTA Rigid Registration Using Deep Reinforcement Learning and Overlap Degree Calculation (Wentao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Liu, Bowen Liang, Weijin Xu, Tong Tian, Qingsheng Lu, Xipeng Pan, Haoyuan Li, Siyu Tian, Huihua Yang, Ruisheng Su. (2024)<br><strong>UDCR: Unsupervised Aortic DSA/CTA Rigid Registration Using Deep Reinforcement Learning and Overlap Degree Calculation</strong><br><button class=copy-to-clipboard title="UDCR: Unsupervised Aortic DSA/CTA Rigid Registration Using Deep Reinforcement Learning and Overlap Degree Calculation" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05753v1.pdf filename=2403.05753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rigid registration of aortic Digital Subtraction Angiography (DSA) and Computed Tomography Angiography (CTA) can provide 3D anatomical details of the vasculature for the interventional surgical treatment of conditions such as aortic dissection and aortic aneurysms, holding significant value for clinical research. However, the current methods for 2D/3D image registration are dependent on manual annotations or synthetic data, as well as the extraction of landmarks, which is not suitable for cross-modal registration of aortic DSA/CTA. In this paper, we propose an <b>unsupervised</b> method, UDCR, for aortic DSA/CTA rigid registration based on deep <b>reinforcement</b> <b>learning.</b> Leveraging the imaging principles and characteristics of DSA and CTA, we have constructed a cross-dimensional registration environment based on spatial transformations. Specifically, we propose an overlap degree calculation reward function that measures the intensity difference between the foreground and background, aimed at assessing the accuracy of registration between segmentation maps and DSA images. This method is highly flexible, allowing for the loading of pre-trained models to perform registration directly or to seek the optimal spatial transformation parameters through online learning. We manually annotated 61 pairs of aortic DSA/CTA for algorithm evaluation. The results indicate that the proposed UDCR achieved a Mean Absolute Error (MAE) of 2.85 mm in translation and 4.35{\deg} in rotation, showing significant potential for clinical applications.</p></p class="citation"></blockquote><h3 id=44--54138-ioi-invisible-one-iteration-adversarial-attack-on-no-reference-image--and-video-quality-metrics-ekaterina-shumitskaya-et-al-2024>(4/4 | 54/138) IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics (Ekaterina Shumitskaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy Vatolin. (2024)<br><strong>IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics</strong><br><button class=copy-to-clipboard title="IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05955v1.pdf filename=2403.05955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>No-reference image- and video-quality metrics are widely used in video processing <b>benchmarks.</b> The robustness of learning-based metrics under video attacks has not been widely studied. In addition to having success, attacks that can be employed in video processing <b>benchmarks</b> must be fast and imperceptible. This paper introduces an Invisible One-Iteration (IOI) <b>adversarial</b> <b>attack</b> on no reference image and video quality metrics. We compared our method alongside eight prior approaches using image and video datasets via objective and subjective tests. Our method exhibited superior visual quality across various attacked metric architectures while maintaining comparable attack success and speed. We made the code available on GitHub.</p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=14--55138-hufu-a-modality-agnositc-watermarking-system-for-pre-trained-transformers-via-permutation-equivariance-hengyuan-xu-et-al-2024>(1/4 | 55/138) Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance (Hengyuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengyuan Xu, Liyao Xiang, Xingjun Ma, Borui Yang, Baochun Li. (2024)<br><strong>Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance</strong><br><button class=copy-to-clipboard title="Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: Vision Transformer, Fine-tuning, BERT, GPT-2, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05842v1.pdf filename=2403.05842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained <b>Transformer-based</b> models, relying on the permutation equivariance property of <b>Transformers.</b> Hufu embeds watermark by <b>fine-tuning</b> the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights &ndash; one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downstream tasks. Since our method only depends on the model itself, it is naturally modality-agnostic, task-independent, and trigger-sample-free. Extensive experiments on the state-of-the-art <b>vision</b> <b>Transformers,</b> <b>BERT,</b> and <b>GPT2</b> have demonstrated Hufu&rsquo;s superiority in meeting watermarking requirements including effectiveness, efficiency, fidelity, and robustness, showing its great potential to be deployed as a uniform ownership verification service for various <b>Transformers.</b></p></p class="citation"></blockquote><h3 id=24--56138-mirrorattack-backdoor-attack-on-3d-point-cloud-with-a-distorting-mirror-yuhao-bian-et-al-2024>(2/4 | 56/138) MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror (Yuhao Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Bian, Shengjing Tian, Xiuping Liu. (2024)<br><strong>MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror</strong><br><button class=copy-to-clipboard title="MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 20<br>Keywords: Reconstruction Loss, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05847v1.pdf filename=2403.05847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud processing starkly contrasts with their susceptibility to security breaches, notably backdoor attacks. These attacks hijack DNNs during training, embedding triggers in the data that, once activated, cause the network to make predetermined errors while maintaining normal performance on unaltered data. This vulnerability poses significant risks, especially given the insufficient research on robust defense mechanisms for 3D point cloud networks against such sophisticated threats. Existing attacks either struggle to resist basic point cloud pre-processing methods, or rely on delicate manual design. Exploring simple, effective, imperceptible, and difficult-to-defend triggers in 3D point clouds is still challenging.To address these challenges, we introduce MirrorAttack, a novel effective 3D backdoor attack method, which implants the trigger by simply reconstructing a clean point cloud with an auto-encoder. The data-driven nature of the MirrorAttack obviates the need for complex manual design. Minimizing the <b>reconstruction</b> <b>loss</b> automatically improves imperceptibility. Simultaneously, the <b>reconstruction</b> <b>network</b> endows the trigger with pronounced nonlinearity and sample specificity, rendering traditional preprocessing techniques ineffective in eliminating it. A trigger smoothing module based on spherical harmonic transformation is also attached to regulate the intensity of the attack.Both quantitive and qualitative results verify the effectiveness of our method. We achieve state-of-the-art <b>ASR</b> on different types of victim models with the intervention of defensive techniques. Moreover, the minimal perturbation introduced by our trigger, as assessed by various metrics, attests to the method&rsquo;s stealth, ensuring its imperceptibility.</p></p class="citation"></blockquote><h3 id=34--57138-contemplating-secure-and-optimal-design-practices-for-information-infrastructure-from-a-human-factors-perspective-niroop-sugunaraj-2024>(3/4 | 57/138) Contemplating Secure and Optimal Design Practices for Information Infrastructure From a Human Factors Perspective (Niroop Sugunaraj, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niroop Sugunaraj. (2024)<br><strong>Contemplating Secure and Optimal Design Practices for Information Infrastructure From a Human Factors Perspective</strong><br><button class=copy-to-clipboard title="Contemplating Secure and Optimal Design Practices for Information Infrastructure From a Human Factors Perspective" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07018v1.pdf filename=2403.07018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing secure information infrastructure is a function of design and usability. However, security is seldom given priority when systems are being developed. Secure design practices should balance between functionality (i.e., proper design) to meet minimum requirements and user-friendliness. Design <b>recommendations</b> such as those with a user-centric approach (i.e., inclusive of only relevant information, user liberty) and presenting information within its proper context in a clear and engaging manner has been scientifically shown to improve user response and experience.</p></p class="citation"></blockquote><h3 id=44--58138-privacy-preserving-diffusion-model-using-homomorphic-encryption-yaojian-chen-et-al-2024>(4/4 | 58/138) Privacy-Preserving Diffusion Model Using Homomorphic Encryption (Yaojian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaojian Chen, Qiben Yan. (2024)<br><strong>Privacy-Preserving Diffusion Model Using Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Diffusion Model Using Homomorphic Encryption" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05794v1.pdf filename=2403.05794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a privacy-preserving stable <b>diffusion</b> <b>framework</b> leveraging homomorphic encryption, called HE-Diffusion, which primarily focuses on protecting the denoising phase of the <b>diffusion</b> <b>process.</b> HE-Diffusion is a tailored encryption framework specifically designed to align with the unique architecture of stable <b>diffusion,</b> <b>ensuring</b> both privacy and functionality. To address the inherent computational challenges, we propose a novel min-distortion method that enables efficient partial image encryption, significantly reducing the overhead without compromising the model&rsquo;s output quality. Furthermore, we adopt a sparse tensor representation to expedite computational operations, enhancing the overall efficiency of the privacy-preserving <b>diffusion</b> <b>process.</b> We successfully implement HE-based privacy-preserving stable <b>diffusion</b> <b>inference.</b> The experimental results show that HE-Diffusion achieves 500 times speedup compared with the baseline method, and reduces time cost of the homomorphically encrypted inference to the minute level. Both the performance and accuracy of the HE-Diffusion are on par with the plaintext counterpart. Our approach marks a significant step towards integrating advanced cryptographic techniques with state-of-the-art generative models, paving the way for privacy-preserving and efficient image generation in critical applications.</p></p class="citation"></blockquote><h2 id=cslg-27>cs.LG (27)</h2><h3 id=127--59138-textbfs2ip-llm-semantic-space-informed-prompt-learning-with-llm-for-time-series-forecasting-zijie-pan-et-al-2024>(1/27 | 59/138) $\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting (Zijie Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, Dongjin Song. (2024)<br><strong>$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting</strong><br><button class=copy-to-clipboard title="$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Tokenization, Large Language Model, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05798v1.pdf filename=2403.05798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing interest in leveraging pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for various time series applications. However, the semantic space of <b>LLMs,</b> established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed <b>Prompt</b> <b>learning</b> with <b>LLM</b> ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned <b>prompts</b> <b>from</b> the joint space. We first design a <b>tokenization</b> module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maximizing the cosine similarity in the joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as <b>prompts</b> <b>to</b> provide strong indicators (context) for time series that exhibit different temporal dynamics. With thorough empirical studies on multiple <b>benchmark</b> datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve superior forecasting performance over state-of-the-art baselines. Furthermore, our ablation studies and visualizations verify the necessity of <b>prompt</b> <b>learning</b> informed by semantic space.</p></p class="citation"></blockquote><h3 id=227--60138-task-oriented-gnns-training-on-large-knowledge-graphs-for-accurate-and-efficient-modeling-hussein-abdallah-et-al-2024>(2/27 | 60/138) Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling (Hussein Abdallah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour. (2024)<br><strong>Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling</strong><br><button class=copy-to-clipboard title="Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Knowledge Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05752v1.pdf filename=2403.05752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>Knowledge</b> <b>Graph</b> <b>(KG)</b> <b>is</b> a heterogeneous <b>graph</b> <b>encompassing</b> <b>a</b> diverse range of <b>node</b> <b>and</b> edge types. Heterogeneous <b>Graph</b> <b>Neural</b> <b>Networks</b> (HGNNs) are popular for training machine learning tasks like <b>node</b> <b>classification</b> and link prediction on <b>KGs.</b> However, HGNN methods exhibit excessive complexity influenced by the <b>KG&rsquo;s</b> size, density, and the number of <b>node</b> <b>and</b> edge types. AI practitioners handcraft a subgraph of a <b>KG</b> G relevant to a specific task. We refer to this subgraph as a task-oriented subgraph (TOSG), which contains a subset of task-related <b>node</b> <b>and</b> edge types in G. Training the task using TOSG instead of G alleviates the excessive computation required for a large <b>KG.</b> Crafting the TOSG demands a deep understanding of the <b>KG&rsquo;s</b> structure and the task&rsquo;s objectives. Hence, it is challenging and time-consuming. This paper proposes <b>KG-TOSA,</b> an approach to automate the TOSG extraction for task-oriented HGNN training on a large <b>KG.</b> In <b>KG-TOSA,</b> we define a generic <b>graph</b> <b>pattern</b> <b>that</b> captures the <b>KG&rsquo;s</b> local and global structure relevant to a specific task. We explore different techniques to extract subgraphs matching our <b>graph</b> <b>pattern:</b> <b>namely</b> (i) two techniques sampling around targeted <b>nodes</b> <b>using</b> biased random walk or influence scores, and (ii) a SPARQL-based extraction method leveraging RDF engines&rsquo; built-in indices. Hence, it achieves negligible preprocessing overhead compared to the sampling techniques. We develop a <b>benchmark</b> of real <b>KGs</b> of large sizes and various tasks for <b>node</b> <b>classification</b> and link prediction. Our experiments show that <b>KG-TOSA</b> helps state-of-the-art HGNN methods reduce training time and memory usage by up to 70% while improving the model performance, e.g., accuracy and inference time.</p></p class="citation"></blockquote><h3 id=327--61138-addressing-shortcomings-in-fair-graph-learning-datasets-towards-a-new-benchmark-xiaowei-qian-et-al-2024>(3/27 | 61/138) Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark (Xiaowei Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma. (2024)<br><strong>Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark</strong><br><button class=copy-to-clipboard title="Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 39<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Benchmarking, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06017v1.pdf filename=2403.06017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fair <b>graph</b> <b>learning</b> <b>plays</b> a pivotal role in numerous practical applications. Recently, many fair <b>graph</b> <b>learning</b> <b>methods</b> have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> in both utility and <b>fairness.</b> In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using <b>graph</b> <b>structures</b> <b>in</b> these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant <b>graph</b> <b>structures</b> <b>and</b> bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to create data with controllable bias parameters, thereby enabling the generation of desired datasets with user-defined bias values with ease. Moreover, we conduct systematic evaluations of these proposed datasets and establish a unified evaluation approach for fair <b>graph</b> <b>learning</b> <b>models.</b> Our extensive experimental results with fair <b>graph</b> <b>learning</b> <b>methods</b> across our datasets demonstrate their effectiveness in <b>benchmarking</b> the performance of these methods. Our datasets and the code for reproducing our experiments are available at <a href=https://github.com/XweiQ/Benchmark-GraphFairness>https://github.com/XweiQ/Benchmark-GraphFairness</a>.</p></p class="citation"></blockquote><h3 id=427--62138-non-intrusive-load-monitoring-with-missing-data-imputation-based-on-tensor-decomposition-dengyu-shi-2024>(4/27 | 62/138) Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition (DengYu Shi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>DengYu Shi. (2024)<br><strong>Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition</strong><br><button class=copy-to-clipboard title="Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07012v1.pdf filename=2403.07012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in building energy management, ensuring the high quality of NILM data has become imperative. However, practical applications of NILM face challenges associated with data loss, significantly impacting accuracy and reliability in energy management. This paper addresses the issue of NILM data loss by introducing an innovative <b>tensor</b> <b>completion(TC)</b> model- Proportional-Integral-Derivative (PID)-incorporated Non-negative Latent Factorization of <b>Tensors</b> <b>(PNLFT)</b> with twofold ideas: 1) To tackle the issue of slow convergence in Latent Factorization of <b>Tensors</b> <b>(LFT)</b> using <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD),</b> a Proportional-Integral-Derivative controller is introduced during the learning process. The PID controller utilizes historical and current information to control learning residuals. 2) Considering the characteristics of NILM data, non-negative update rules are proposed in the model&rsquo;s learning scheme. Experimental results on three datasets demonstrate that, compared to state-of-the-art models, the proposed model exhibits noteworthy enhancements in both convergence speed and accuracy.</p></p class="citation"></blockquote><h3 id=527--63138-provable-policy-gradient-methods-for-average-reward-markov-potential-games-min-cheng-et-al-2024>(5/27 | 63/138) Provable Policy Gradient Methods for Average-Reward Markov Potential Games (Min Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Min Cheng, Ruida Zhou, P. R. Kumar, Chao Tian. (2024)<br><strong>Provable Policy Gradient Methods for Average-Reward Markov Potential Games</strong><br><button class=copy-to-clipboard title="Provable Policy Gradient Methods for Average-Reward Markov Potential Games" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05738v1.pdf filename=2403.05738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study <b>Markov</b> <b>potential</b> <b>games</b> under the infinite horizon average reward criterion. Most previous studies have been for discounted rewards. We prove that both algorithms based on independent policy gradient and independent natural policy gradient converge globally to a Nash equilibrium for the average reward criterion. To set the stage for gradient-based methods, we first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity and the second largest eigenvalue of the underlying <b>Markov</b> <b>decision</b> <b>process</b> (MDP). We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a gradient/differential Q function oracle. When policy gradients have to be estimated, we propose an algorithm with $\tilde{O}(\frac{1}{\min_{s,a}\pi(a|s)\delta})$ sample complexity to achieve $\delta$ approximation error w.r.t~the $\ell_2$ norm. Equipped with the estimator, we derive the first sample complexity analysis for a policy gradient ascent algorithm, featuring a sample complexity of $\tilde{O}(1/\epsilon^5)$. <b>Simulation</b> studies are presented.</p></p class="citation"></blockquote><h3 id=627--64138-optimizing-llm-queries-in-relational-workloads-shu-liu-et-al-2024>(6/27 | 64/138) Optimizing LLM Queries in Relational Workloads (Shu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E. Gonzalez, Ion Stoica, Matei Zaharia. (2024)<br><strong>Optimizing LLM Queries in Relational Workloads</strong><br><button class=copy-to-clipboard title="Optimizing LLM Queries in Relational Workloads" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DB, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05821v1.pdf filename=2403.05821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, <b>LLM</b> inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize <b>LLM</b> inference for analytical workloads that invoke <b>LLMs</b> within relational queries. We show that relational queries present novel opportunities for accelerating <b>LLM</b> inference, including reordering rows to maximize key-value (KV) cache reuse within the <b>LLM</b> inference engine, reordering columns within a row to further increase cache reuse, and deduplicating redundant inference requests. We implement these optimizations in Apache Spark, with vLLM as the model serving backend and achieve up to 4.4x improvement in end-to-end latency on a <b>benchmark</b> of diverse <b>LLM-based</b> queries on real datasets. To the best of our knowledge, this is the first work to explicitly address the problem of optimizing <b>LLM</b> invocations within SQL queries.</p></p class="citation"></blockquote><h3 id=727--65138-autoeval-done-right-using-synthetic-data-for-model-evaluation-pierre-boyeau-et-al-2024>(7/27 | 65/138) AutoEval Done Right: Using Synthetic Data for Model Evaluation (Pierre Boyeau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik, Michael I. Jordan. (2024)<br><strong>AutoEval Done Right: Using Synthetic Data for Model Evaluation</strong><br><button class=copy-to-clipboard title="AutoEval Done Right: Using Synthetic Data for Model Evaluation" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ME<br>Keyword Score: 23<br>Keywords: Sample Size, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07008v1.pdf filename=2403.07008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve <b>sample</b> <b>efficiency</b> while remaining unbiased. These algorithms increase the effective human-labeled <b>sample</b> <b>size</b> by up to 50% on experiments with <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=827--66138-detectors-for-safe-and-reliable-llms-implementations-uses-and-limitations-swapnaja-achintalwar-et-al-2024>(8/27 | 66/138) Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations (Swapnaja Achintalwar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor, Ioana Baldini, Sara E. Berger, Bishwaranjan Bhattacharjee, Djallel Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen, Lamogha Chiazor, Elizabeth M. Daly, Rogério Abreu de Paula, Pierre Dognin, Eitan Farchi, Soumya Ghosh, Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling, Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski, Ambrish Rawat, Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna Swaminathan, Christoph Tillmann, Aashka Trivedi, Kush R. Varshney, Dennis Wei, Shalisha Witherspooon, Marcel Zalmanovici. (2024)<br><strong>Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations</strong><br><button class=copy-to-clipboard title="Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06009v1.pdf filename=2403.06009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding <b>LLMs</b> (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.</p></p class="citation"></blockquote><h3 id=927--67138-dissecting-deep-rl-with-high-update-ratios-combatting-value-overestimation-and-divergence-marcel-hussing-et-al-2024>(9/27 | 67/138) Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence (Marcel Hussing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud Farahmand, Eric Eaton. (2024)<br><strong>Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence</strong><br><button class=copy-to-clipboard title="Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05996v1.pdf filename=2403.05996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that deep <b>reinforcement</b> <b>learning</b> can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on <b>out-of-distribution</b> but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios, show its efficacy on the widely used dm_control suite, and obtain strong performance on the challenging dog tasks, competitive with model-based approaches. Our results question, in parts, the prior explanation for sub-optimal learning due to overfitting on early data.</p></p class="citation"></blockquote><h3 id=1027--68138-paper-hilt-personalized-and-adaptive-privacy-aware-early-exit-for-reinforcement-learning-in-human-in-the-loop-systems-mojtaba-taherisadr-et-al-2024>(10/27 | 68/138) PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems (Mojtaba Taherisadr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mojtaba Taherisadr, Salma Elmalaki. (2024)<br><strong>PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems</strong><br><button class=copy-to-clipboard title="PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: F-2-2, I-2-7, cs-CR, cs-HC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05864v1.pdf filename=2403.05864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has increasingly become a preferred method over traditional rule-based systems in diverse <b>human-in-the-loop</b> (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT&rsquo;s effectiveness is evaluated through its application in two distinct contexts: Smart Home environments and Virtual Reality (VR) Smart Classrooms. The empirical results demonstrate PAPER-HILT&rsquo;s capability to provide a personalized equilibrium between user privacy and application utility, adapting effectively to individual user needs and preferences. On average for both experiments, utility (performance) drops by 24%, and privacy (state prediction) improves by 31%.</p></p class="citation"></blockquote><h3 id=1127--69138-mg-tsd-multi-granularity-time-series-diffusion-models-with-guided-learning-process-xinyao-fan-et-al-2024>(11/27 | 69/138) MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (Xinyao Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, Jiang Bian. (2024)<br><strong>MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process</strong><br><button class=copy-to-clipboard title="MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05751v1.pdf filename=2403.05751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>diffusion</b> <b>probabilistic</b> <b>models</b> have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the <b>probabilistic</b> <b>time</b> series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series <b>Diffusion</b> <b>(MG-TSD)</b> model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate <b>diffusion</b> <b>steps</b> to guide the learning process of <b>diffusion</b> <b>models.</b> The way to construct the targets is motivated by the observation that the forward process of the <b>diffusion</b> <b>model,</b> which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the process of smoothing fine-grained data into a coarse-grained representation, both of which result in a gradual loss of fine distribution features. In the study, we derive a novel multi-granularity guidance <b>diffusion</b> <b>loss</b> function and propose a concise implementation method to effectively utilize coarse-grained data across various granularity levels. More importantly, our approach does not rely on additional external data, making it versatile and applicable across various domains. Extensive experiments conducted on real-world datasets demonstrate that our MG-TSD model outperforms existing time series prediction methods.</p></p class="citation"></blockquote><h3 id=1227--70138-hard-label-based-small-query-black-box-adversarial-attack-jeonghwan-park-et-al-2024>(12/27 | 70/138) Hard-label based Small Query Black-box Adversarial Attack (Jeonghwan Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeonghwan Park, Paul Miller, Niall McLaughlin. (2024)<br><strong>Hard-label based Small Query Black-box Adversarial Attack</strong><br><button class=copy-to-clipboard title="Hard-label based Small Query Black-box Adversarial Attack" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06014v1.pdf filename=2403.06014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the hard label based <b>black</b> <b>box</b> <b>adversarial</b> <b>attack</b> setting which solely observes predicted classes from the target model. Most of the attack methods in this setting suffer from impractical number of queries required to achieve a successful attack. One approach to tackle this drawback is utilising the <b>adversarial</b> <b>transferability</b> between white box surrogate models and <b>black</b> <b>box</b> target model. However, the majority of the methods adopting this approach are soft label based to take the full advantage of zeroth order optimisation. Unlike mainstream methods, we propose a new practical setting of hard label based attack with an optimisation process guided by a pretrained surrogate model. Experiments show the proposed method significantly improves the query efficiency of the hard label based <b>black-box</b> <b>attack</b> across various target model architectures. We find the proposed method achieves approximately 5 times higher attack success rate compared to the <b>benchmarks,</b> especially at the small query budgets as 100 and 250.</p></p class="citation"></blockquote><h3 id=1327--71138-multimodal-deep-learning-approach-to-predicting-neurological-recovery-from-coma-after-cardiac-arrest-felix-h-krones-et-al-2024>(13/27 | 71/138) Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest (Felix H. Krones et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix H. Krones, Ben Walker, Guy Parsons, Terry Lyons, Adam Mahdi. (2024)<br><strong>Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest</strong><br><button class=copy-to-clipboard title="Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06027v1.pdf filename=2403.06027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work showcases our team&rsquo;s (The BEEGees) contributions to the 2023 George B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from coma following cardiac arrest using clinical data and time-series such as multi-channel EEG and ECG signals. Our modelling approach is <b>multimodal,</b> based on two-dimensional spectrogram representations derived from numerous EEG channels, alongside the integration of clinical data and features extracted directly from EEG recordings. Our submitted model achieved a Challenge score of $0.53$ on the hidden test set for predictions made $72$ hours after return of spontaneous circulation. Our study shows the efficacy and limitations of employing <b>transfer</b> <b>learning</b> in medical classification. With regard to prospective implementation, our analysis reveals that the performance of the model is strongly linked to the selection of a decision threshold and exhibits strong variability across data splits.</p></p class="citation"></blockquote><h3 id=1427--72138-towards-a-generic-representation-of-combinatorial-problems-for-learning-based-approaches-léo-boisvert-et-al-2024>(14/27 | 72/138) Towards a Generic Representation of Combinatorial Problems for Learning-Based Approaches (Léo Boisvert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Léo Boisvert, Hélène Verhaeghe, Quentin Cappart. (2024)<br><strong>Towards a Generic Representation of Combinatorial Problems for Learning-Based Approaches</strong><br><button class=copy-to-clipboard title="Towards a Generic Representation of Combinatorial Problems for Learning-Based Approaches" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06026v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06026v2.pdf filename=2403.06026v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a <b>graph,</b> <b>to</b> <b>leverage</b> the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves constructing a <b>graph</b> <b>by</b> <b>breaking</b> down any constraint of a combinatorial problem into an abstract syntax tree and expressing relationships (e.g., a variable involved in a constraint) through the edges. Furthermore, we introduce a <b>graph</b> <b>neural</b> <b>network</b> architecture capable of efficiently learning from this representation. The tool provided operates on combinatorial problems expressed in the XCSP3 format, handling all the constraints available in the 2023 mini-track competition. Experimental results on four combinatorial problems demonstrate that our architecture achieves performance comparable to dedicated architectures while maintaining generality. Our code and trained models are publicly available at \url{https://github.com/corail-research/learning-generic-csp}.</p></p class="citation"></blockquote><h3 id=1527--73138-spatial-clustering-approach-for-vessel-path-identification-mohamed-abuella-et-al-2024>(15/27 | 73/138) Spatial Clustering Approach for Vessel Path Identification (Mohamed Abuella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Abuella, M. Amine Atoui, Slawomir Nowaczyk, Simon Johansson, Ethan Faghan. (2024)<br><strong>Spatial Clustering Approach for Vessel Path Identification</strong><br><button class=copy-to-clipboard title="Spatial Clustering Approach for Vessel Path Identification" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T10 (Primary), 68T20 (Secondary), G-1-3; H-3-4, cs-CE, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05778v1.pdf filename=2403.05778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenge of identifying the paths for vessels with operating routes of repetitive paths, partially repetitive paths, and new paths. We propose a spatial <b>clustering</b> approach for labeling the vessel paths by using only position information. We develop a path <b>clustering</b> framework employing two methods: a distance-based path modeling and a likelihood estimation method. The former enhances the accuracy of path <b>clustering</b> through the integration of <b>unsupervised</b> machine learning techniques, while the latter focuses on likelihood-based path modeling and introduces segmentation for a more detailed analysis. The result findings highlight the superior performance and efficiency of the developed approach, as both methods for <b>clustering</b> vessel paths into five classes achieve a perfect F1-score. The approach aims to offer valuable insights for route planning, ultimately contributing to improving safety and efficiency in maritime transportation.</p></p class="citation"></blockquote><h3 id=1627--74138-fairtargetsim-an-interactive-simulator-for-understanding-and-explaining-the-fairness-effects-of-target-variable-definition-dalia-gala-et-al-2024>(16/27 | 74/138) FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition (Dalia Gala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dalia Gala, Milo Phillips-Brown, Naman Goel, Carinal Prunkl, Laura Alvarez Jubete, medb corcoran, Ray Eitel-Porter. (2024)<br><strong>FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition</strong><br><button class=copy-to-clipboard title="FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06031v1.pdf filename=2403.06031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning requires defining one&rsquo;s target variable for predictions or decisions, a process that can have profound implications on <b>fairness:</b> biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts <b>fairness.</b> FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: <a href=http://tinyurl.com/ftsinterface>http://tinyurl.com/ftsinterface</a>. The video accompanying this paper is here: <a href=http://tinyurl.com/ijcaifts>http://tinyurl.com/ijcaifts</a>.</p></p class="citation"></blockquote><h3 id=1727--75138-reinforcement-learning-paycheck-optimization-for-multivariate-financial-goals-melda-alaluf-et-al-2024>(17/27 | 75/138) Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals (Melda Alaluf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melda Alaluf, Giulia Crippa, Sinong Geng, Zijian Jing, Nikhil Krishnan, Sanjeev Kulkarni, Wyatt Navarro, Ronnie Sircar, Jonathan Tang. (2024)<br><strong>Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals</strong><br><button class=copy-to-clipboard title="Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06011v1.pdf filename=2403.06011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study paycheck optimization, which examines how to allocate income in order to achieve several competing financial goals. For paycheck optimization, a quantitative methodology is missing, due to a lack of a suitable problem formulation. To deal with this issue, we formulate the problem as a utility maximization problem. The proposed formulation is able to (i) unify different financial goals; (ii) incorporate user preferences regarding the goals; (iii) handle stochastic interest rates. The proposed formulation also facilitates an end-to-end <b>reinforcement</b> <b>learning</b> solution, which is implemented on a variety of problem settings.</p></p class="citation"></blockquote><h3 id=1827--76138-enhancing-classification-performance-via-reinforcement-learning-for-feature-selection-younes-ghazagh-jahed-et-al-2024>(18/27 | 76/138) Enhancing Classification Performance via Reinforcement Learning for Feature Selection (Younes Ghazagh Jahed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Younes Ghazagh Jahed, Seyyed Ali Sadat Tavana. (2024)<br><strong>Enhancing Classification Performance via Reinforcement Learning for Feature Selection</strong><br><button class=copy-to-clipboard title="Enhancing Classification Performance via Reinforcement Learning for Feature Selection" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T99, E-1, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05979v1.pdf filename=2403.05979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature selection plays a crucial role in improving predictive accuracy by identifying relevant features while filtering out irrelevant ones. This study investigates the importance of effective feature selection in enhancing the performance of classification models. By employing <b>reinforcement</b> <b>learning</b> (RL) algorithms, specifically Q-learning (QL) and SARSA learning, this paper addresses the feature selection challenge. Using the Breast Cancer Coimbra dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the study evaluates the performance of these algorithms. Results show that QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching 87% and 88%, respectively. This highlights the effectiveness of RL-based feature selection methods in optimizing classification tasks, contributing to improved model accuracy and efficiency.</p></p class="citation"></blockquote><h3 id=1927--77138-adaptive-hyperparameter-optimization-for-continual-learning-scenarios-rudy-semola-et-al-2024>(19/27 | 77/138) Adaptive Hyperparameter Optimization for Continual Learning Scenarios (Rudy Semola et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rudy Semola, Julio Hurtado, Vincenzo Lomonaco, Davide Bacciu. (2024)<br><strong>Adaptive Hyperparameter Optimization for Continual Learning Scenarios</strong><br><button class=copy-to-clipboard title="Adaptive Hyperparameter Optimization for Continual Learning Scenarios" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07015v1.pdf filename=2403.07015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperparameter selection in <b>continual</b> <b>learning</b> scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in <b>continual</b> <b>learning</b> and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to <b>continual</b> <b>scenarios</b> and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit robustness even in the face of varying sequential task orders. We believe that our findings can contribute to the advancement of <b>continual</b> <b>learning</b> methodologies towards more efficient, robust and adaptable models for real-world applications.</p></p class="citation"></blockquote><h3 id=2027--78138-semres-ddpm-residual-network-based-diffusion-modelling-applied-to-imbalanced-data-ming-zheng-et-al-2024>(20/27 | 78/138) SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data (Ming Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Zheng, Yang Yang, Zhi-Hang Zhao, Shan-Chao Gan, Yang Chen, Si-Kai Ni, Yang Lu. (2024)<br><strong>SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data</strong><br><button class=copy-to-clipboard title="SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05918v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05918v2.pdf filename=2403.05918v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of data mining and machine learning, commonly used classification models cannot effectively learn in unbalanced data. In order to balance the data distribution before model training, oversampling methods are often used to generate data for a small number of classes to solve the problem of classifying unbalanced data. Most of the classical oversampling methods are based on the SMOTE technique, which only focuses on the local information of the data, and therefore the generated data may have the problem of not being realistic enough. In the current oversampling methods based on generative networks, the methods based on <b>GANs</b> can capture the true distribution of data, but there is the problem of pattern collapse and training instability in training; in the oversampling methods based on denoising diffusion probability models, the neural network of the inverse diffusion process using the U-Net is not applicable to tabular data, and although the MLP can be used to replace the U-Net, the problem exists due to the simplicity of the structure and the poor effect of removing noise. problem of poor noise removal. In order to overcome the above problems, we propose a novel oversampling method SEMRes-DDPM.In the SEMRes-DDPM backward diffusion process, a new neural network structure SEMST-ResNet is used, which is suitable for tabular data and has good noise removal effect, and it can generate tabular data with higher quality. Experiments show that the SEMResNet network removes noise better than MLP; SEMRes-DDPM generates data distributions that are closer to the real data distributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular datasets with 9 classification models, SEMRes-DDPM improves the quality of the generated tabular data in terms of three evaluation metrics (F1, G-mean, AUC) with better classification performance than other SOTA oversampling methods.</p></p class="citation"></blockquote><h3 id=2127--79138-towards-efficient-replay-in-federated-incremental-learning-yichen-li-et-al-2024>(21/27 | 79/138) Towards Efficient Replay in Federated Incremental Learning (Yichen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong, Guannan Zhang. (2024)<br><strong>Towards Efficient Replay in Federated Incremental Learning</strong><br><button class=copy-to-clipboard title="Towards Efficient Replay in Federated Incremental Learning" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05890v1.pdf filename=2403.05890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in <b>Federated</b> <b>Incremental</b> Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2227--80138-tlasdi-thermodynamics-informed-latent-space-dynamics-identification-jun-sur-richard-park-et-al-2024>(22/27 | 80/138) tLaSDI: Thermodynamics-informed latent space dynamics identification (Jun Sur Richard Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, Yeonjong Shin. (2024)<br><strong>tLaSDI: Thermodynamics-informed latent space dynamics identification</strong><br><button class=copy-to-clipboard title="tLaSDI: Thermodynamics-informed latent space dynamics identification" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05848v1.pdf filename=2403.05848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a data-driven latent space dynamics identification method (tLaSDI) that embeds the first and second principles of thermodynamics. The latent variables are learned through an <b>autoencoder</b> as a nonlinear dimension reduction model. The dynamics of the latent variables are constructed by a neural network-based model that preserves certain structures to respect the thermodynamic laws through the GENERIC formalism. An abstract error estimate of the approximation is established, which provides a new loss formulation involving the Jacobian computation of <b>autoencoder.</b> Both the <b>autoencoder</b> and the latent dynamics are trained to minimize the new loss. Numerical examples are presented to demonstrate the performance of tLaSDI, which exhibits robust generalization ability, even in extrapolation. In addition, an intriguing correlation is empirically observed between the entropy production rates in the latent space and the behaviors of the full-state solution.</p></p class="citation"></blockquote><h3 id=2327--81138-extending-activation-steering-to-broad-skills-and-multiple-behaviours-teun-van-der-weij-et-al-2024>(23/27 | 81/138) Extending Activation Steering to Broad Skills and Multiple Behaviours (Teun van der Weij et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teun van der Weij, Massimo Poesio, Nandi Schoots. (2024)<br><strong>Extending Activation Steering to Broad Skills and Multiple Behaviours</strong><br><button class=copy-to-clipboard title="Extending Activation Steering to Broad Skills and Multiple Behaviours" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05767v1.pdf filename=2403.05767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>large</b> <b>language</b> <b>models</b> have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.</p></p class="citation"></blockquote><h3 id=2427--82138-multi-conditioned-graph-diffusion-for-neural-architecture-search-rohan-asthana-et-al-2024>(24/27 | 82/138) Multi-conditioned Graph Diffusion for Neural Architecture Search (Rohan Asthana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohan Asthana, Joschua Conrad, Youssef Dawoud, Maurits Ortmanns, Vasileios Belagiannis. (2024)<br><strong>Multi-conditioned Graph Diffusion for Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Multi-conditioned Graph Diffusion for Neural Architecture Search" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06020v1.pdf filename=2403.06020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a <b>graph</b> diffusion-based NAS approach that uses discrete conditional <b>graph</b> diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to <b>graph</b> diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard <b>benchmarks,</b> yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset.</p></p class="citation"></blockquote><h3 id=2527--83138-are-classification-robustness-and-explanation-robustness-really-strongly-correlated-an-analysis-through-input-loss-landscape-tiejin-chen-et-al-2024>(25/27 | 83/138) Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape (Tiejin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiejin Chen, Wenwang Huang, Linsey Pang, Dongsheng Luo, Hua Wei. (2024)<br><strong>Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape</strong><br><button class=copy-to-clipboard title="Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06013v1.pdf filename=2403.06013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into the critical area of deep learning robustness, challenging the conventional belief that classification robustness and explanation robustness in image classification systems are inherently correlated. Through a novel evaluation approach leveraging <b>clustering</b> for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To deeply investigate this contradiction, a groundbreaking training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the prevailing assumption of a strong correlation between the two forms of robustness but also pave new pathways for understanding relationship between loss landscape and explanation loss.</p></p class="citation"></blockquote><h3 id=2627--84138-optimistic-safety-for-linearly-constrained-online-convex-optimization-spencer-hutchinson-et-al-2024>(26/27 | 84/138) Optimistic Safety for Linearly-Constrained Online Convex Optimization (Spencer Hutchinson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spencer Hutchinson, Tianyi Chen, Mahnoosh Alizadeh. (2024)<br><strong>Optimistic Safety for Linearly-Constrained Online Convex Optimization</strong><br><button class=copy-to-clipboard title="Optimistic Safety for Linearly-Constrained Online Convex Optimization" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05786v1.pdf filename=2403.05786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The setting of online convex optimization (OCO) under unknown constraints has garnered significant attention in recent years. In this work, we consider a version of this problem with static linear constraints that the player receives noisy feedback of and must always satisfy. By leveraging our novel design paradigm of optimistic safety, we give an algorithm for this problem that enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly stronger assumptions of independent noise and an oblivious adversary. Then, by recasting this problem as OCO under time-varying stochastic linear constraints, we show that our algorithm enjoys the same regret guarantees in such a setting and never violates the constraints in expectation. This contributes to the literature on OCO under time-varying stochastic constraints, where the state-of-the-art algorithms enjoy $\tilde{\mathcal{O}}(\sqrt{T})$ regret and $\tilde{\mathcal{O}}(\sqrt{T})$ violation when the constraints are convex and the player receives full feedback. Additionally, we provide a version of our algorithm that is more computationally efficient and give numerical experiments comparing it with <b>benchmark</b> algorithms.</p></p class="citation"></blockquote><h3 id=2727--85138-membership-testing-in-markov-equivalence-classes-via-independence-query-oracles-jiaqi-zhang-et-al-2024>(27/27 | 85/138) Membership Testing in Markov Equivalence Classes via Independence Query Oracles (Jiaqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Zhang, Kirankumar Shiragur, Caroline Uhler. (2024)<br><strong>Membership Testing in Markov Equivalence Classes via Independence Query Oracles</strong><br><button class=copy-to-clipboard title="Membership Testing in Markov Equivalence Classes via Independence Query Oracles" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05759v1.pdf filename=2403.05759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding causal relationships between variables is a fundamental problem with broad impact in numerous scientific fields. While extensive research has been dedicated to learning causal <b>graphs</b> from data, its complementary concept of testing causal relationships has remained largely unexplored. While learning involves the task of recovering the Markov equivalence class (MEC) of the underlying causal <b>graph</b> from observational data, the testing counterpart addresses the following critical question: Given a specific MEC and observational data from some causal <b>graph,</b> can we determine if the data-generating causal <b>graph</b> belongs to the given MEC? We explore constraint-based testing methods by establishing bounds on the required number of conditional independence tests. Our bounds are in terms of the size of the maximum undirected clique ($s$) of the given MEC. In the worst case, we show a lower bound of $\exp(\Omega(s))$ independence tests. We then give an algorithm that resolves the task with $\exp(O(s))$ tests, matching our lower bound. Compared to the learning problem, where algorithms often use a number of independence tests that is exponential in the maximum in-degree, this shows that testing is relatively easier. In particular, it requires exponentially less independence tests in <b>graphs</b> featuring high in-degrees and small clique sizes. Additionally, using the DAG associahedron, we provide a geometric interpretation of testing versus learning and discuss how our testing result can aid learning.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--86138-hdreason-algorithm-hardware-codesign-for-hyperdimensional-knowledge-graph-reasoning-hanning-chen-et-al-2024>(1/1 | 86/138) HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning (Hanning Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanning Chen, Yang Ni, Ali Zakeri, Zhuowen Zou, Sanggeon Yun, Fei Wen, Behnam Khaleghi, Narayan Srinivasa, Hugo Latapie, Mohsen Imani. (2024)<br><strong>HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning</strong><br><button class=copy-to-clipboard title="HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-LG, cs.AR<br>Keyword Score: 53<br>Keywords: Graph Convolutional Network, Graph Classification, Graph, Convolution, Knowledge Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05763v1.pdf filename=2403.05763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent times, a plethora of hardware accelerators have been put forth for <b>graph</b> <b>learning</b> applications such as vertex classification and <b>graph</b> <b>classification.</b> However, previous works have paid little attention to <b>Knowledge</b> <b>Graph</b> <b>Completion</b> (KGC), a task that is well-known for its significantly higher algorithm complexity. The state-of-the-art KGC solutions based on <b>graph</b> <b>convolution</b> neural network <b>(GCN)</b> involve extensive vertex/relation embedding updates and complicated score functions, which are inherently cumbersome for acceleration. As a result, existing accelerator designs are no longer optimal, and a novel algorithm-hardware co-design for <b>KG</b> <b>reasoning</b> is needed. Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced as a promising solution for lightweight machine learning, particularly for <b>graph</b> <b>learning</b> applications. In this paper, we leverage HDC for an intrinsically more efficient and acceleration-friendly KGC algorithm. We also co-design an acceleration framework named HDReason targeting FPGA platforms. On the algorithm level, HDReason achieves a balance between high <b>reasoning</b> accuracy, strong model interpretability, and less computation complexity. In terms of architecture, HDReason offers reconfigurability, high training throughput, and low energy consumption. When compared with NVIDIA RTX 4090 GPU, the proposed accelerator achieves an average 10.6x speedup and 65x energy efficiency improvement. When conducting cross-models and cross-platforms comparison, HDReason yields an average 4.2x higher performance and 3.4x better energy efficiency with similar accuracy versus the state-of-the-art FPGA-based <b>GCN</b> training platform.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--87138-interest-aware-joint-caching-computing-and-communication-optimization-for-mobile-vr-delivery-in-mec-networks-baojie-fu-et-al-2024>(1/1 | 87/138) Interest-Aware Joint Caching, Computing, and Communication Optimization for Mobile VR Delivery in MEC Networks (Baojie Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baojie Fu, Tong Tang, Dapeng Wu, Ruyan Wang. (2024)<br><strong>Interest-Aware Joint Caching, Computing, and Communication Optimization for Mobile VR Delivery in MEC Networks</strong><br><button class=copy-to-clipboard title="Interest-Aware Joint Caching, Computing, and Communication Optimization for Mobile VR Delivery in MEC Networks" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-ET, cs-MM, cs.MM<br>Keyword Score: 50<br>Keywords: Fairness, Simulation, Simulator, BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05851v1.pdf filename=2403.05851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the upcoming B5G/6G era, virtual reality (VR) over wireless has become a typical application, which is an inevitable trend in the development of video. However, in immersive and interactive VR experiences, VR services typically exhibit high delay, while simultaneously posing challenges for the energy consumption of local devices. To address these issues, this paper aims to improve the performance of the VR service in the edge-terminal cooperative system. Specifically, we formulate a problem of joint caching, computing, and communication VR service policy, by optimizing the weighted sum of overall VR delivery delay and energy consumption of local devices. For the purpose of designing the optimal VR service policy, the optimization problem is decoupled into three independent subproblems to be solved separately. To enhance the caching efficiency within the network, a bidirectional encoder representations from <b>transformers</b> <b>(Bert)-based</b> user interest analysis method is first proposed to characterize the content requesting behavior accurately. On the basis of this, a service cost minimum-maximization problem is formulated with consideration of performance <b>fairness</b> among users. Thereafter, the joint caching and computing scheme is derived for each user with given allocation of communication resources while a bisection-based communication scheme is acquired with the given information on joint caching and computing policy. With alternative optimization, an optimal policy for joint caching, computing and communication based on user interest can be finally obtained. <b>Simulation</b> results are presented to demonstrate the superiority of the proposed user interest-aware caching scheme and the effective of the joint caching, computing and communication optimization policy with consideration of user <b>fairness.</b></p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--88138-cached-model-as-a-resource-provisioning-large-language-model-agents-for-edge-intelligence-in-space-air-ground-integrated-networks-minrui-xu-et-al-2024>(1/1 | 88/138) Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks (Minrui Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minrui Xu, Dusit Niyato, Hongliang Zhang, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han. (2024)<br><strong>Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks</strong><br><button class=copy-to-clipboard title="Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 50<br>Keywords: Few-shot, Few-shot Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05826v1.pdf filename=2403.05826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space-air-ground integrated networks (SAGINs) enable worldwide network coverage beyond geographical limitations for users to access ubiquitous intelligence services. {\color{black}Facing global coverage and complex environments in SAGINs, edge intelligence can provision AI agents based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for users via edge servers at ground base stations (BSs) or cloud data centers relayed by satellites.} As <b>LLMs</b> with billions of parameters are pre-trained on vast datasets, <b>LLM</b> agents have <b>few-shot</b> <b>learning</b> capabilities, e.g., chain-of-thought (CoT) <b>prompting</b> for complex tasks, which are challenged by limited resources in SAGINs. In this paper, we propose a joint caching and inference framework for edge intelligence to provision sustainable and ubiquitous <b>LLM</b> agents in SAGINs. We introduce &ldquo;cached model-as-a-resource&rdquo; for offering <b>LLMs</b> with limited context windows and propose a novel optimization framework, i.e., joint model caching and inference, to utilize cached model resources for provisioning <b>LLM</b> agent services along with communication, computing, and storage resources. We design &ldquo;age of thought&rdquo; (AoT) considering the CoT <b>prompting</b> of <b>LLMs,</b> and propose the least AoT cached model replacement algorithm for optimizing the provisioning cost. We propose a deep Q-network-based modified second-bid (DQMSB) auction to incentivize these network operators, which can enhance allocation efficiency while guaranteeing strategy-proofness and free from adverse selection.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--89138-large-generative-model-assisted-3d-semantic-communication-feibo-jiang-et-al-2024>(1/6 | 89/138) Large Generative Model Assisted 3D Semantic Communication (Feibo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You. (2024)<br><strong>Large Generative Model Assisted 3D Semantic Communication</strong><br><button class=copy-to-clipboard title="Large Generative Model Assisted 3D Semantic Communication" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-LG, cs.IT, math-IT<br>Keyword Score: 50<br>Keywords: Diffusion Model, Generative AI, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05783v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05783v1.pdf filename=2403.05783v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Communication (SC) is a novel paradigm for data transmission in 6G. However, there are several challenges posed when performing SC in 3D scenarios: 1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain channel estimation. To address these issues, we propose a <b>Generative</b> <b>AI</b> <b>Model</b> assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor (3DSE), which employs <b>generative</b> <b>AI</b> <b>models,</b> including Segment Anything Model (SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D scenario based on user requirements. The extracted 3D semantics are represented as multi-perspective images of the goal-oriented 3D object. Then, we present an Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective images, in which we use a semantic encoder with two output heads to perform semantic encoding and mask redundant semantics in the latent semantic space, respectively. Next, we design a conditional <b>Generative</b> <b>adversarial</b> <b>network</b> and <b>Diffusion</b> <b>model</b> aided-Channel Estimation (GDCE) to estimate and refine the Channel State Information (CSI) of physical channels. Finally, <b>simulation</b> results demonstrate the advantages of the proposed GAM-3DSC system in effectively transmitting the goal-oriented 3D scenario.</p></p class="citation"></blockquote><h3 id=26--90138-deep-reinforcement-learning-enhanced-rate-splitting-multiple-access-for-interference-mitigation-osman-nuri-irkicatal-et-al-2024>(2/6 | 90/138) Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation (Osman Nuri Irkicatal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Osman Nuri Irkicatal, Elif Tugce Ceran, Melda Yuksel. (2024)<br><strong>Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-MA, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05974v1.pdf filename=2403.05974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems. It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep <b>reinforcement</b> <b>learning.</b> The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers. A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space. We also explore the challenges posed by imperfect channel side information at the transmitter. Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences. <b>Simulation</b> results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios. Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods. These findings highlight the potential of deep <b>reinforcement</b> <b>learning-driven</b> RSMA in reducing interference and enhancing system performance in communication systems.</p></p class="citation"></blockquote><h3 id=36--91138-electromagnetic-hybrid-beamforming-for-holographic-communications-ran-ji-et-al-2024>(3/6 | 91/138) Electromagnetic Hybrid Beamforming for Holographic Communications (Ran Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Ji, Chongwen Huang, Xiaoming Chen, Wei E. I. Sha, Linglong Dai, Jiguang He, Zhaoyang Zhang, Chau Yuen, Mérouane Debbah. (2024)<br><strong>Electromagnetic Hybrid Beamforming for Holographic Communications</strong><br><button class=copy-to-clipboard title="Electromagnetic Hybrid Beamforming for Holographic Communications" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05970v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05970v1.pdf filename=2403.05970v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that there is inherent radiation pattern distortion for the commercial base station antenna array, which usually needs three antenna sectors to cover the whole space. To eliminate pattern distortion and further enhance beamforming performance, we propose an electromagnetic hybrid beamforming (EHB) scheme based on a three-dimensional (3D) superdirective holographic antenna array. Specifically, EHB consists of antenna excitation current vectors (analog beamforming) and digital precoding matrices, where the implementation of analog beamforming involves the real-time adjustment of the radiation pattern to adapt it to the dynamic wireless environment. Meanwhile, the digital beamforming is optimized based on the channel characteristics of analog beamforming to further improve the achievable rate of communication systems. An electromagnetic channel model incorporating array radiation patterns and the mutual coupling effect is also developed to evaluate the benefits of our proposed scheme. <b>Simulation</b> results demonstrate that our proposed EHB scheme with a 3D holographic array achieves a relatively flat superdirective beamforming gain and allows for programmable focusing directions throughout the entire spatial domain. Furthermore, they also verify that the proposed scheme achieves a sum rate gain of over 150% compared to traditional beamforming algorithms.</p></p class="citation"></blockquote><h3 id=46--92138-stacked-intelligent-metasurface-enabled-leo-satellite-communications-relying-on-statistical-csi-shining-lin-et-al-2024>(4/6 | 92/138) Stacked Intelligent Metasurface Enabled LEO Satellite Communications Relying on Statistical CSI (Shining Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shining Lin, Jiancheng An, Lu Gan, Mérouane Debbah, Chau Yuen. (2024)<br><strong>Stacked Intelligent Metasurface Enabled LEO Satellite Communications Relying on Statistical CSI</strong><br><button class=copy-to-clipboard title="Stacked Intelligent Metasurface Enabled LEO Satellite Communications Relying on Statistical CSI" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05892v1.pdf filename=2403.05892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low earth orbit (LEO) satellite communication systems have gained increasing attention as a crucial supplement to terrestrial wireless networks due to their extensive coverage area. This letter presents a novel system design for LEO satellite systems by leveraging stacked intelligent metasurface (SIM) technology. Specifically, the lightweight and energy-efficient SIM is mounted on a satellite to achieve multiuser beamforming directly in the electromagnetic wave domain, which substantially reduces the processing delay and computational load of the satellite compared to the traditional digital beamforming scheme. To overcome the challenges of obtaining instantaneous channel state information (CSI) at the transmitter and maximize the system&rsquo;s performance, a joint power allocation and SIM phase shift optimization problem for maximizing the ergodic sum rate is formulated based on statistical CSI, and an alternating optimization (AO) algorithm is customized to solve it efficiently. Additionally, a user grouping method based on channel correlation and an antenna selection algorithm are proposed to further improve the system performance. <b>Simulation</b> results demonstrate the effectiveness of the proposed SIM-based LEO satellite system design and statistical CSI-based AO algorithm.</p></p class="citation"></blockquote><h3 id=56--93138-derivation-of-mutual-information-and-linear-minimum-mean-square-error-for-viterbi-decoding-of-convolutional-codes-using-the-innovations-method-masato-tajima-2024>(5/6 | 93/138) Derivation of Mutual Information and Linear Minimum Mean-Square Error for Viterbi Decoding of Convolutional Codes Using the Innovations Method (Masato Tajima, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masato Tajima. (2024)<br><strong>Derivation of Mutual Information and Linear Minimum Mean-Square Error for Viterbi Decoding of Convolutional Codes Using the Innovations Method</strong><br><button class=copy-to-clipboard title="Derivation of Mutual Information and Linear Minimum Mean-Square Error for Viterbi Decoding of Convolutional Codes Using the Innovations Method" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Convolution, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05740v1.pdf filename=2403.05740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We see that <b>convolutional</b> coding/Viterbi decoding has the structure of the Kalman filter (or the linear minimum variance filter). First, we calculate the covariance matrix of the innovation (i.e., the soft-decision input to the main decoder in a Scarce-State-Transition (SST) Viterbi decoder). Then a covariance matrix corresponding to that of the one-step prediction error in the Kalman filter is obtained. Furthermore, from that matrix, a covariance matrix corresponding to that of the filtering error in the Kalman filter is derived using the formula in the Kalman filter. As a result, the average <b>mutual</b> <b>information</b> per branch for Viterbi decoding of <b>convolutional</b> codes is given using these covariance matrices. Also, the trace of the latter matrix represents the linear minimum mean-square error (LMMSE). We show that an approximate value of the average <b>mutual</b> <b>information</b> is sandwiched between half the SNR times the average filtering and one-step prediction LMMSEs. In the case of QLI codes, from the covariance matrix of the soft-decision input to the main decoder, we can get a matrix. We show that the trace of this matrix has some connection with the linear smoothing error.</p></p class="citation"></blockquote><h3 id=66--94138-channel-estimation-for-stacked-intelligent-metasurface-assisted-wireless-networks-xianghao-yao-et-al-2024>(6/6 | 94/138) Channel Estimation for Stacked Intelligent Metasurface-Assisted Wireless Networks (Xianghao Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianghao Yao, Jiancheng An, Lu Gan, Marco Di Renzo, Chau Yuen. (2024)<br><strong>Channel Estimation for Stacked Intelligent Metasurface-Assisted Wireless Networks</strong><br><button class=copy-to-clipboard title="Channel Estimation for Stacked Intelligent Metasurface-Assisted Wireless Networks" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05870v1.pdf filename=2403.05870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging technologies, such as holographic multiple-input multiple-output (HMIMO) and stacked intelligent metasurface (SIM), are driving the development of wireless communication systems. Specifically, the SIM is physically constructed by stacking multiple layers of metasurfaces and has an architecture similar to an artificial neural network (ANN), which can flexibly manipulate the electromagnetic waves that propagate through it at the speed of light. This architecture enables the SIM to achieve HMIMO precoding and combining in the wave domain, thus significantly reducing the hardware cost and energy consumption. In this letter, we investigate the channel estimation problem in SIM-assisted multi-user HMIMO communication systems. Since the number of antennas at the base station (BS) is much smaller than the number of meta-atoms per layer of the SIM, it is challenging to acquire the channel state information (CSI) in SIM-assisted multi-user systems. To address this issue, we collect multiple copies of the uplink pilot signals that propagate through the SIM. Furthermore, we leverage the array <b>geometry</b> to identify the subspace that spans arbitrary spatial correlation matrices. Based on partial CSI about the channel statistics, a pair of subspace-based channel estimators are proposed. Additionally, we compute the mean square error (MSE) of the proposed channel estimators and optimize the phase shifts of the SIM to minimize the MSE. Numerical results are illustrated to analyze the effectiveness of the proposed channel estimation schemes.</p></p class="citation"></blockquote><h2 id=csro-8>cs.RO (8)</h2><h3 id=18--95138-image-guided-autonomous-guidewire-navigation-in-robot-assisted-endovascular-interventions-using-reinforcement-learning-wentao-liu-et-al-2024>(1/8 | 95/138) Image-Guided Autonomous Guidewire Navigation in Robot-Assisted Endovascular Interventions using Reinforcement Learning (Wentao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Liu, Tong Tian, Weijin Xu, Bowen Liang, Qingsheng Lu, Xipeng Pan, Wenyi Zhao, Huihua Yang, Ruisheng Su. (2024)<br><strong>Image-Guided Autonomous Guidewire Navigation in Robot-Assisted Endovascular Interventions using Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Image-Guided Autonomous Guidewire Navigation in Robot-Assisted Endovascular Interventions using Reinforcement Learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05748v1.pdf filename=2403.05748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous robots in endovascular interventions possess the potential to navigate guidewires with safety and reliability, while reducing human error and shortening surgical time. However, current methods of guidewire navigation based on <b>Reinforcement</b> <b>Learning</b> (RL) depend on manual demonstration data or magnetic guidance. In this work, we propose an Image-guided Autonomous Guidewire Navigation (IAGN) method. Specifically, we introduce BDA-star, a path planning algorithm with boundary distance constraints, for the trajectory planning of guidewire navigation. We established an IAGN-RL environment where the observations are real-time guidewire feeding images highlighting the position of the guidewire tip and the planned path. We proposed a reward function based on the distances from both the guidewire tip to the planned path and the target to evaluate the agent&rsquo;s actions. Furthermore, in policy network, we employ a pre-trained <b>convolutional</b> <b>neural</b> <b>network</b> to extract features, mitigating stability issues and slow convergence rates associated with direct learning from raw pixels. Experiments conducted on the aortic <b>simulation</b> IAGN platform demonstrated that the proposed method, targeting the left subclavian artery and the brachiocephalic artery, achieved a 100% guidewire navigation success rate, along with reduced movement and retraction distances and trajectories tend to the center of the vessels.</p></p class="citation"></blockquote><h3 id=28--96138-imu-as-an-input-vs-a-measurement-of-the-state-in-inertial-aided-state-estimation-keenan-burnett-et-al-2024>(2/8 | 96/138) IMU as an Input vs. a Measurement of the State in Inertial-Aided State Estimation (Keenan Burnett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keenan Burnett, Angela P. Schoellig, Timothy D. Barfoot. (2024)<br><strong>IMU as an Input vs. a Measurement of the State in Inertial-Aided State Estimation</strong><br><button class=copy-to-clipboard title="IMU as an Input vs. a Measurement of the State in Inertial-Aided State Estimation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05968v1.pdf filename=2403.05968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this technical report, we compare treating an IMU as an input to a motion model against treating it as a measurement of the state in a <b>continuous-time</b> <b>state</b> estimation framework. Treating IMU measurements as inputs to a motion model and then preintegrating these measurements has almost become a de-facto standard in many robotics applications. However, this approach has a few shortcomings. First, it conflates the IMU measurement noise with the underlying process noise. Second, it is unclear how the state will be propagated in the case of IMU measurement dropout. Third, it does not lend itself well to dealing with multiple high-rate sensors such as a lidar and an IMU or multiple IMUs. In this work, we methodically compare the performance of these two approaches on a 1D <b>simulation</b> and show that they perform identically, assuming that each method&rsquo;s hyperparameters have been tuned on a training set. We show how to preintegrate heterogeneous factors using <b>Gaussian</b> <b>process</b> interpolation. We also provide results for our <b>continuous-time</b> <b>lidar-inertial</b> odometry in <b>simulation</b> and on the Newer College Dataset. Code for our lidar-inertial odometry can be found at: <a href=https://github.com/utiasASRL/steam_icp>https://github.com/utiasASRL/steam_icp</a></p></p class="citation"></blockquote><h3 id=38--97138-c3d-cascade-control-with-change-point-detection-and-deep-koopman-learning-for-autonomous-surface-vehicles-jianwen-li-et-al-2024>(3/8 | 97/138) C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles (Jianwen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwen Li, Hyunsang Park, Wenjian Hao, Lei Xin, Jalil Chavez-Galaviz, Ajinkya Chaudhary, Meredith Bloss, Kyle Pattison, Christopher Vo, Devesh Upadhyay, Shreyas Sundaram, Shaoshuai Mou, Nina Mahmoudian. (2024)<br><strong>C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles</strong><br><button class=copy-to-clipboard title="C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05972v1.pdf filename=2403.05972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we discuss the development and deployment of a robust autonomous system capable of performing various tasks in the maritime domain under unknown dynamic conditions. We investigate a data-driven approach based on modular design for ease of transfer of autonomy across different maritime surface vessel platforms. The data-driven approach alleviates issues related to a priori identification of system models that may become deficient under evolving system behaviors or shifting, unanticipated, environmental influences. Our proposed learning-based platform comprises a deep Koopman system model and a change point detector that provides guidance on domain shifts <b>prompting</b> relearning under severe exogenous and endogenous perturbations. Motion control of the autonomous system is achieved via an optimal controller design. The Koopman linearized model naturally lends itself to a linear-quadratic regulator (LQR) control design. We propose the C3D control architecture Cascade Control with Change Point Detection and Deep Koopman Learning. The framework is verified in station keeping task on an ASV in both <b>simulation</b> and real experiments. The approach achieved at least 13.9 percent improvement in mean distance error in all test cases compared to the methods that do not consider system changes.</p></p class="citation"></blockquote><h3 id=48--98138-cease-collision-evaluation-based-active-sense-system-for-collaborative-robotic-arms-xian-huang-et-al-2024>(4/8 | 98/138) CEASE: Collision-Evaluation-based Active Sense System for Collaborative Robotic Arms (Xian Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xian Huang, Yuanjiong Ying, Wei Dong. (2024)<br><strong>CEASE: Collision-Evaluation-based Active Sense System for Collaborative Robotic Arms</strong><br><button class=copy-to-clipboard title="CEASE: Collision-Evaluation-based Active Sense System for Collaborative Robotic Arms" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05761v1.pdf filename=2403.05761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collision detection via visual fences can significantly enhance the safety of collaborative robotic arms. Existing work typically performs such detection based on pre-deployed stationary cameras outside the robotic arm&rsquo;s workspace. These stationary cameras can only provide a restricted detection range and constrain the mobility of the robotic system. To cope with this issue, we propose an active sense method enabling a wide range of collision risk evaluation in dynamic scenarios. First, an active vision mechanism is implemented by equipping cameras with additional degrees of rotation. Considering the uncertainty in the active sense, we design a state confidence envelope to uniformly characterize both known and potential dynamic obstacles. Subsequently, using the observation-based uncertainty evolution, collision risk is evaluated by the prediction of obstacle envelopes. On this basis, a <b>Markov</b> <b>decision</b> <b>process</b> was employed to search for an optimal observation sequence of the active sense system, which enlarges the field of observation and reduces uncertainties in the state estimation of surrounding obstacles. <b>Simulation</b> and real-world experiments consistently demonstrate a 168% increase in the observation time coverage of typical dynamic humanoid obstacles compared to the method using stationary cameras, which underscores our system&rsquo;s effectiveness in collision risk tracking and enhancing the safety of robotic arms.</p></p class="citation"></blockquote><h3 id=58--99138-scaling-team-coordination-on-graphs-with-reinforcement-learning-manshi-limbu-et-al-2024>(5/8 | 99/138) Scaling Team Coordination on Graphs with Reinforcement Learning (Manshi Limbu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manshi Limbu, Zechen Hu, Xuan Wang, Daigo Shishika, Xuesu Xiao. (2024)<br><strong>Scaling Team Coordination on Graphs with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Scaling Team Coordination on Graphs with Reinforcement Learning" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05787v1.pdf filename=2403.05787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies <b>Reinforcement</b> <b>Learning</b> (RL) techniques to enable team coordination behaviors in <b>graph</b> environments with support actions among teammates to reduce the costs of traversing certain risky edges in a centralized manner. While classical approaches can solve this non-standard multi-agent path planning problem by converting the original Environment <b>Graph</b> (EG) into a Joint State <b>Graph</b> (JSG) to implicitly incorporate the support actions, those methods do not scale well to large <b>graphs</b> and teams. To address this curse of dimensionality, we propose to use RL to enable agents to learn such <b>graph</b> traversal and teammate supporting behaviors in a data-driven manner. Specifically, through a new formulation of the team coordination on <b>graphs</b> with risky edges problem into Markov Decision Processes <b>(MDPs)</b> with a novel state and action space, we investigate how RL can solve it in two paradigms: First, we use RL for a team of agents to learn how to coordinate and reach the goal with minimal cost on a single EG. We show that RL efficiently solves problems with up to 20/4 or 25/3 nodes/agents, using a fraction of the time needed for JSG to solve such complex problems; Second, we learn a general RL policy for any $N$-node EGs to produce efficient supporting behaviors. We present extensive experiments and compare our RL approaches against their classical counterparts.</p></p class="citation"></blockquote><h3 id=68--100138-physics-informed-neural-motion-planning-on-constraint-manifolds-ruiqi-ni-et-al-2024>(6/8 | 100/138) Physics-informed Neural Motion Planning on Constraint Manifolds (Ruiqi Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Ni, Ahmed H. Qureshi. (2024)<br><strong>Physics-informed Neural Motion Planning on Constraint Manifolds</strong><br><button class=copy-to-clipboard title="Physics-informed Neural Motion Planning on Constraint Manifolds" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05765v1.pdf filename=2403.05765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constrained Motion Planning (CMP) aims to find a collision-free path between the given start and goal configurations on the kinematic constraint manifolds. These problems appear in various scenarios ranging from object manipulation to legged-robot locomotion. However, the zero-volume nature of manifolds makes the CMP problem challenging, and the state-of-the-art methods still take several seconds to find a path and require a computationally expansive path dataset for imitation learning. Recently, physics-informed motion planning methods have emerged that directly solve the Eikonal equation through neural networks for motion planning and do not require expert demonstrations for learning. Inspired by these approaches, we propose the first physics-informed CMP framework that solves the Eikonal equation on the constraint manifolds and trains neural function for CMP without expert data. Our results show that the proposed approach efficiently solves various CMP problems in both <b>simulation</b> and real-world, including object manipulation under orientation constraints and door opening with a high-dimensional 6-DOF robot manipulator. In these complex settings, our method exhibits high success rates and finds paths in sub-seconds, which is many times faster than the state-of-the-art CMP methods.</p></p class="citation"></blockquote><h3 id=78--101138-matrix-multi-agent-trajectory-generation-with-diverse-contexts-zhuo-xu-et-al-2024>(7/8 | 101/138) MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts (Zhuo Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, Jiachen Li. (2024)<br><strong>MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts</strong><br><button class=copy-to-clipboard title="MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-MA, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06041v1.pdf filename=2403.06041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data-driven</b> <b>methods</b> have great advantages in modeling complicated human behavioral dynamics and dealing with many human-robot interaction applications. However, collecting massive and annotated real-world human datasets has been a laborious task, especially for highly interactive scenarios. On the other hand, algorithmic <b>data</b> <b>generation</b> methods are usually limited by their model capacities, making them unable to offer realistic and diverse <b>data</b> <b>needed</b> by various application users. In this work, we study trajectory-level <b>data</b> <b>generation</b> for multi-human or human-robot interaction scenarios and propose a learning-based automatic trajectory generation model, which we call Multi-Agent TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of generating interactive human behaviors in realistic diverse contexts. We achieve this goal by modeling the explicit and interpretable objectives so that MATRIX can generate human motions based on diverse destinations and heterogeneous behaviors. We carried out extensive comparison and ablation studies to illustrate the effectiveness of our approach across various metrics. We also presented experiments that demonstrate the capability of MATRIX to serve as <b>data</b> <b>augmentation</b> for imitation-based motion planning.</p></p class="citation"></blockquote><h3 id=88--102138-multi-robot-communication-aware-cooperative-belief-space-planning-with-inconsistent-beliefs-an-action-consistent-approach-tanmoy-kundu-et-al-2024>(8/8 | 102/138) Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach (Tanmoy Kundu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmoy Kundu, Moshe Rafaeli, Vadim Indelman. (2024)<br><strong>Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach</strong><br><button class=copy-to-clipboard title="Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05962v1.pdf filename=2403.05962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot belief space planning (MR-BSP) is essential for reliable and safe autonomy. While planning, each robot maintains a belief over the state of the environment and reasons how the belief would evolve in the future for different candidate actions. Yet, existing MR-BSP works have a common assumption that the beliefs of different robots are consistent at planning time. Such an assumption is often highly unrealistic, as it requires prohibitively extensive and frequent communication capabilities. In practice, each robot may have a different belief about the state of the environment. Crucially, when the beliefs of different robots are inconsistent, state-of-the-art MR-BSP approaches could result in a lack of coordination between the robots, and in general, could yield dangerous, unsafe and sub-optimal decisions. In this paper, we tackle this crucial gap. We develop a novel decentralized algorithm that is guaranteed to find a consistent joint action. For a given robot, our algorithm reasons for action preferences about 1) its local information, 2) what it perceives about the <b>reasoning</b> of the other robot, and 3) what it perceives about the <b>reasoning</b> of itself perceived by the other robot. This algorithm finds a consistent joint action whenever these steps yield the same best joint action obtained by <b>reasoning</b> about action preferences; otherwise, it self-triggers communication between the robots. Experimental results show efficacy of our algorithm in comparison with two baseline algorithms.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--103138-legion-harnessing-pre-trained-language-models-for-github-topic-recommendations-with-distribution-balance-loss-yen-trang-dang-et-al-2024>(1/3 | 103/138) LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss (Yen-Trang Dang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yen-Trang Dang, Thanh-Le Cong, Phuc-Thanh Nguyen, Anh M. T. Bui, Phuong T. Nguyen, Bach Le, Quyet-Thang Huynh. (2024)<br><strong>LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss</strong><br><button class=copy-to-clipboard title="LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-IR, cs-LG, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Recommendation, Pre-trained Language Model, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05873v1.pdf filename=2403.05873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-source development has revolutionized the software industry by promoting collaboration, transparency, and community-driven innovation. Today, a vast amount of various kinds of open-source software, which form networks of repositories, is often hosted on GitHub - a popular software development platform. To enhance the discoverability of the repository networks, i.e., groups of similar repositories, GitHub introduced repository topics in 2017 that enable users to more easily explore relevant projects by type, technology, and more. It is thus crucial to accurately assign topics for each GitHub repository. Current methods for automatic topic <b>recommendation</b> rely heavily on <b>TF-IDF</b> for encoding textual data, presenting challenges in understanding semantic nuances. This paper addresses the limitations of existing techniques by proposing Legion, a novel approach that leverages <b>Pre-trained</b> <b>Language</b> <b>Models</b> (PTMs) for recommending topics for GitHub repositories. The key novelty of Legion is three-fold. First, Legion leverages the extensive capabilities of PTMs in language understanding to capture contextual information and semantic meaning in GitHub repositories. Second, Legion overcomes the challenge of long-tailed distribution, which results in a bias toward popular topics in PTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the PTMs. Third, Legion employs a filter to eliminate vague <b>recommendations,</b> thereby improving the precision of PTMs. Our empirical evaluation on a <b>benchmark</b> dataset of real-world GitHub repositories shows that Legion can improve vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also can suggest GitHub topics more precisely and effectively than the state-of-the-art baseline with an average improvement of 20% and 5% in terms of Precision and F1-score, respectively.</p></p class="citation"></blockquote><h3 id=23--104138-a-tool-for-automated-reasoning-about-traces-based-on-configurable-formal-semantics-ferhat-erata-et-al-2024>(2/3 | 104/138) A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics (Ferhat Erata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ferhat Erata, Arda Goknil, Bedir Tekinerdogan, Geylani Kardas. (2024)<br><strong>A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics</strong><br><button class=copy-to-clipboard title="A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-PL, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06012v1.pdf filename=2403.06012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Tarski, a tool for specifying configurable trace semantics to facilitate automated <b>reasoning</b> about traces. Software development projects require that various types of traces be modeled between and within development artifacts. For any given artifact (e.g., requirements, architecture models and source code), Tarski allows the user to specify new trace types and their configurable semantics, while, using the semantics, it automatically infers new traces based on existing traces provided by the user, and checks the consistency of traces. It has been evaluated on three industrial case studies in the automotive domain (<a href=https://modelwriter.github.io/Tarski/)>https://modelwriter.github.io/Tarski/)</a>.</p></p class="citation"></blockquote><h3 id=33--105138-a-novel-refactoring-and-semantic-aware-abstract-syntax-tree-differencing-tool-and-a-benchmark-for-evaluating-the-accuracy-of-diff-tools-pouria-alikhanifard-et-al-2024>(3/3 | 105/138) A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools (Pouria Alikhanifard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouria Alikhanifard, Nikolaos Tsantalis. (2024)<br><strong>A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools</strong><br><button class=copy-to-clipboard title="A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05939v1.pdf filename=2403.05939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. Thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. Abstract Syntax Tree (AST) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. Despite the notable improvements brought by AST diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible AST nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. We propose a novel AST diff tool based on RefactoringMiner that resolves all aforementioned limitations. First, we improved RefactoringMiner to increase its statement mapping accuracy, and then we developed an algorithm that generates AST diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by RefactoringMiner. To evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first <b>benchmark</b> of AST node mappings, including 800 bug-fixing commits and 188 refactoring commits. Our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--106138-a-preliminary-exploration-of-youtubers-use-of-generative-ai-in-content-creation-yao-lyu-et-al-2024>(1/5 | 106/138) A Preliminary Exploration of YouTubers&rsquo; Use of Generative-AI in Content Creation (Yao Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Lyu, He Zhang, Shuo Niu, Jie Cai. (2024)<br><strong>A Preliminary Exploration of YouTubers&rsquo; Use of Generative-AI in Content Creation</strong><br><button class=copy-to-clipboard title="A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Generative AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06039v1.pdf filename=2403.06039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Content creators increasingly utilize <b>generative</b> <b>artificial</b> intelligence (Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging sites to produce imaginative images, AI-generated videos, and articles using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Despite its growing popularity, there remains an underexplored area concerning the specific domains where AI-generated content is being applied, and the methodologies content creators employ with Gen-AI tools during the creation process. This study initially explores this emerging area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI usage. Our research focuses on identifying the content domains, the variety of tools used, the activities performed, and the nature of the final products generated by Gen-AI in the context of user-generated content.</p></p class="citation"></blockquote><h3 id=25--107138-towards-optimizing-human-centric-objectives-in-ai-assisted-decision-making-with-offline-reinforcement-learning-zana-buçinca-et-al-2024>(2/5 | 107/138) Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning (Zana Buçinca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zana Buçinca, Siddharth Swaroop, Amanda E. Paluch, Susan A. Murphy, Krzysztof Z. Gajos. (2024)<br><strong>Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05911v1.pdf filename=2403.05911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans &ndash; the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N = 964), our results consistently demonstrate that people interacting with policies optimized for accuracy achieve significantly better accuracy &ndash; and even human-AI complementarity &ndash; compared to those interacting with any other type of AI support. Our results further indicate that human learning is more difficult to optimize than accuracy, with participants who interacted with learning-optimized policies showing significant learning improvement only at times. Our research (1) demonstrates <b>offline</b> <b>RL</b> <b>to</b> be a promising approach to model dynamics of human-AI decision-making, leading to policies that may optimize various human-centric objectives and provide novel insights about the AI-assisted decision-making space, and (2) emphasizes the importance of considering human-centric objectives beyond decision accuracy in AI-assisted decision-making, while also opening up the novel research challenge of optimizing such objectives.</p></p class="citation"></blockquote><h3 id=35--108138-leva-using-large-language-models-to-enhance-visual-analytics-yuheng-zhao-et-al-2024>(3/5 | 108/138) LEVA: Using Large Language Models to Enhance Visual Analytics (Yuheng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuheng Zhao, Yixing Zhang, Yu Zhang, Xinyi Zhao, Junjie Wang, Zekai Shao, Cagatay Turkay, Siming Chen. (2024)<br><strong>LEVA: Using Large Language Models to Enhance Visual Analytics</strong><br><button class=copy-to-clipboard title="LEVA: Using Large Language Models to Enhance Visual Analytics" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05816v1.pdf filename=2403.05816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual analytics supports data analysis tasks within complex domain problems. However, due to the richness of data types, visual designs, and interaction designs, users need to recall and process a significant amount of information when they visually analyze data. These challenges emphasize the need for more intelligent visual analytics methods. <b>Large</b> <b>language</b> <b>models</b> have demonstrated the ability to interpret various forms of textual data, offering the potential to facilitate intelligent support for visual analytics. We propose LEVA, a framework that uses <b>large</b> <b>language</b> <b>models</b> to enhance users&rsquo; VA workflows at multiple stages: onboarding, exploration, and <b>summarization.</b> To support onboarding, we use <b>large</b> <b>language</b> <b>models</b> to interpret visualization designs and view relationships based on system specifications. For exploration, we use <b>large</b> <b>language</b> <b>models</b> to recommend insights based on the analysis of system status and data to facilitate mixed-initiative exploration. For <b>summarization,</b> we present a selective reporting strategy to retrace analysis history through a stream visualization and generate insight reports with the help of <b>large</b> <b>language</b> <b>models.</b> We demonstrate how LEVA can be integrated into existing visual analytics systems. Two usage scenarios and a user study suggest that LEVA effectively aids users in conducting visual analytics.</p></p class="citation"></blockquote><h3 id=45--109138-what-motivates-people-to-trust-ai-systems-nanna-inie-2024>(4/5 | 109/138) What Motivates People to Trust &lsquo;AI&rsquo; Systems? (Nanna Inie, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nanna Inie. (2024)<br><strong>What Motivates People to Trust &lsquo;AI&rsquo; Systems?</strong><br><button class=copy-to-clipboard title="What Motivates People to Trust 'AI' Systems?" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 15<br>Keywords: Black Box, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05957v1.pdf filename=2403.05957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Companies, organizations, and governments across the world are eager to employ so-called &lsquo;AI&rsquo; (artificial intelligence) technology in a broad range of different products and systems. The promise of this cause c'el`ebre is that the technologies offer increased automation, efficiency, and productivity - meanwhile, critics sound warnings of illusions of objectivity, pollution of our information ecosystems, and reproduction of biases and discriminatory outcomes. This paper explores patterns of motivation in the general population for trusting (or distrusting) &lsquo;AI&rsquo; systems. Based on a survey with more than 450 respondents from more than 30 different countries (and about 3000 open text answers), this paper presents a qualitative analysis of current opinions and thoughts about &lsquo;AI&rsquo; technology, focusing on reasons for trusting such systems. The different reasons are synthesized into four rationales (lines of <b>reasoning):</b> the Human favoritism rationale, the <b>Black</b> <b>box</b> rationale, the OPSEC rationale, and the &lsquo;Wicked world, tame computers&rsquo; rationale. These rationales provide insights into human motivation for trusting &lsquo;AI&rsquo; which could be relevant for developers and designers of such systems, as well as for scholars developing measures of trust in technological systems.</p></p class="citation"></blockquote><h3 id=55--110138-content-moderation-justice-and-fairness-on-social-media-comparisons-across-different-contexts-and-platforms-jie-cai-et-al-2024>(5/5 | 110/138) Content Moderation Justice and Fairness on Social Media: Comparisons Across Different Contexts and Platforms (Jie Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Cai, Aashka Patel, Azadeh Naderi, Donghee Yvette Wohn. (2024)<br><strong>Content Moderation Justice and Fairness on Social Media: Comparisons Across Different Contexts and Platforms</strong><br><button class=copy-to-clipboard title="Content Moderation Justice and Fairness on Social Media: Comparisons Across Different Contexts and Platforms" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06034v1.pdf filename=2403.06034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media users may perceive moderation decisions by the platform differently, which can lead to frustration and dropout. This study investigates users&rsquo; perceived justice and <b>fairness</b> of online moderation decisions when they are exposed to various illegal versus legal scenarios, retributive versus restorative moderation strategies, and user-moderated versus commercially moderated platforms. We conduct an online experiment on 200 American social media users of Reddit and Twitter. Results show that retributive moderation delivers higher justice and <b>fairness</b> for commercially moderated than for user-moderated platforms in illegal violations; restorative moderation delivers higher <b>fairness</b> for legal violations than illegal ones. We discuss the opportunities for platform policymaking to improve moderation system design.</p></p class="citation"></blockquote><h2 id=statme-2>stat.ME (2)</h2><h3 id=12--111138-online-identification-of-stochastic-continuous-time-wiener-models-using-sampled-data-mohamed-abdalmoaty-et-al-2024>(1/2 | 111/138) Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data (Mohamed Abdalmoaty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Abdalmoaty, Efe C. Balta, John Lygeros, Roy S. Smith. (2024)<br><strong>Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data</strong><br><button class=copy-to-clipboard title="Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, cs-SY, eess-SP, eess-SY, stat-ME, stat.ME<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05899v1.pdf filename=2403.05899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that ignoring the presence of stochastic disturbances in the identification of stochastic Wiener models leads to asymptotically biased estimators. On the other hand, optimal statistical identification, via likelihood-based methods, is sensitive to the assumptions on the data distribution and is usually based on relatively complex sequential Monte Carlo algorithms. We develop a simple recursive online estimation algorithm based on an output-error predictor, for the identification of <b>continuous-time</b> <b>stochastic</b> parametric Wiener models through stochastic approximation. The method is applicable to generic model parameterizations and, as demonstrated in the numerical <b>simulation</b> examples, it is robust with respect to the assumptions on the spectrum of the disturbance process.</p></p class="citation"></blockquote><h3 id=22--112138-model-free-local-recalibration-of-neural-networks-r-torres-et-al-2024>(2/2 | 112/138) Model-Free Local Recalibration of Neural Networks (R. Torres et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Torres, D. J. Nott, S. A. Sisson, T. Rodrigues, J. G. Reis, G. S. Rodrigues. (2024)<br><strong>Model-Free Local Recalibration of Neural Networks</strong><br><button class=copy-to-clipboard title="Model-Free Local Recalibration of Neural Networks" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: 62G07 (Primary), 68T07, 68T37 (Secondary), 68Q10, G-3; I-5-1; I-6-4, cs-LG, stat-ME, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05756v1.pdf filename=2403.05756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial neural networks (ANNs) are highly flexible predictive models. However, reliably quantifying uncertainty for their predictions is a continuing challenge. There has been much recent work on &ldquo;recalibration&rdquo; of predictive distributions for ANNs, so that forecast probabilities for events of interest are consistent with certain frequency evaluations of them. Uncalibrated probabilistic forecasts are of limited use for many important decision-making tasks. To address this issue, we propose a localized recalibration of ANN predictive distributions using the dimension-reduced representation of the input provided by the ANN hidden layers. Our novel method draws inspiration from recalibration techniques used in the literature on approximate Bayesian computation and likelihood-free inference methods. Most existing calibration methods for ANNs can be thought of as calibrating either on the input layer, which is difficult when the input is high-dimensional, or the output layer, which may not be sufficiently flexible. Through a <b>simulation</b> study, we demonstrate that our method has good performance compared to alternative approaches, and explore the benefits that can be achieved by localizing the calibration based on different layers of the network. Finally, we apply our proposed method to a diamond price prediction problem, demonstrating the potential of our approach to improve prediction and uncertainty quantification in real-world applications.</p></p class="citation"></blockquote><h2 id=eesssy-9>eess.SY (9)</h2><h3 id=19--113138-measuring-robustness-in-cyber-physical-systems-under-sensor-attacks-jian-xiang-et-al-2024>(1/9 | 113/138) Measuring Robustness in Cyber-Physical Systems under Sensor Attacks (Jian Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Xiang, Ruggero Lanotte, Simone Tini, Stephen Chong, Massimo Merro. (2024)<br><strong>Measuring Robustness in Cyber-Physical Systems under Sensor Attacks</strong><br><button class=copy-to-clipboard title="Measuring Robustness in Cyber-Physical Systems under Sensor Attacks" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CR, cs-ET, cs-LO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05829v1.pdf filename=2403.05829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper contributes a formal framework for quantitative analysis of bounded sensor attacks on cyber-physical systems, using the formalism of differential dynamic logic. Given a precondition and postcondition of a system, we formalize two quantitative safety notions, quantitative forward and backward safety, which respectively express (1) how strong the strongest postcondition of the system is with respect to the specified postcondition, and (2) how strong the specified precondition is with respect to the weakest precondition of the system needed to ensure the specified postcondition holds. We introduce two notions, forward and backward robustness, to characterize the robustness of a system against sensor attacks as the loss of safety. To reason about robustness, we introduce two <b>simulation</b> distances, forward and backward <b>simulation</b> distances, which are defined based on the behavioral distances between the original system and the system with compromised sensors. Forward and backward distances, respectively, characterize upper bounds of the degree of forward and backward safety loss caused by the sensor attacks. We verify the two <b>simulation</b> distances by expressing them as modalities, i.e., formulas of differential dynamic logic, and develop an ad-hoc proof system to reason with such formulas. We showcase our formal notions and <b>reasoning</b> techniques on two non-trivial case studies: an autonomous vehicle that needs to avoid collision and a water tank system.</p></p class="citation"></blockquote><h3 id=29--114138-bounding-stochastic-safety-leveraging-freedmans-inequality-with-discrete-time-control-barrier-functions-ryan-k-cosner-et-al-2024>(2/9 | 114/138) Bounding Stochastic Safety: Leveraging Freedman&rsquo;s Inequality with Discrete-Time Control Barrier Functions (Ryan K. Cosner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan K. Cosner, Preston Culbertson, Aaron D. Ames. (2024)<br><strong>Bounding Stochastic Safety: Leveraging Freedman&rsquo;s Inequality with Discrete-Time Control Barrier Functions</strong><br><button class=copy-to-clipboard title="Bounding Stochastic Safety: Leveraging Freedman's Inequality with Discrete-Time Control Barrier Functions" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05745v1.pdf filename=2403.05745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When deployed in the real world, safe control methods must be robust to unstructured uncertainties such as modeling error and external disturbances. Typical approaches either leverage worst-case uncertainty bounds to provide safety guarantees for an expanded (overly conservative) safe set or synthesize controllers that always assume the worst-case disturbance will occur. In contrast, this paper utilizes Freedman&rsquo;s inequality in the context of <b>discrete-time</b> <b>control</b> barrier functions (DTCBFs) to provide stronger (less conservative) safety guarantees for stochastic systems. Our approach accounts for the underlying disturbance distribution instead of relying exclusively on its worst-case bound and does not require the barrier function to be upper-bounded, which makes the resulting safety probability bounds more directly useful for intuitive constraints such as signed distance. We compare our results with existing safety guarantees, such as Input-to-State-Safety (ISSf) and martingale results that rely on Ville&rsquo;s inequality. When the assumptions for all methods hold, we provide a range of parameters for which our guarantee is less conservative. Finally, we present <b>simulation</b> examples, including a bipedal walking robot, that demonstrate the utility and tightness of our safety guarantee.</p></p class="citation"></blockquote><h3 id=39--115138-efficient-fault-detection-and-categorization-in-electrical-distribution-systems-using-hessian-locally-linear-embedding-on-measurement-data-victor-sam-moses-babu-k-et-al-2024>(3/9 | 115/138) Efficient Fault Detection and Categorization in Electrical Distribution Systems Using Hessian Locally Linear Embedding on Measurement Data (Victor Sam Moses Babu K. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Sam Moses Babu K., Sidharthenee Nayak, Divyanshi Dwivedi, Pratyush Chakraborty, Chandrashekhar Narayan Bhende, Pradeep Kumar Yemula, Mayukha Pal. (2024)<br><strong>Efficient Fault Detection and Categorization in Electrical Distribution Systems Using Hessian Locally Linear Embedding on Measurement Data</strong><br><button class=copy-to-clipboard title="Efficient Fault Detection and Categorization in Electrical Distribution Systems Using Hessian Locally Linear Embedding on Measurement Data" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05995v1.pdf filename=2403.05995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Faults on electrical power lines could severely compromise both the reliability and safety of power systems, leading to unstable power delivery and increased outage risks. They pose significant safety hazards, necessitating swift detection and mitigation to maintain electrical infrastructure integrity and ensure continuous power supply. Hence, accurate detection and categorization of electrical faults are pivotal for optimized power system maintenance and operation. In this work, we propose a novel approach for detecting and categorizing electrical faults using the Hessian locally linear embedding (HLLE) technique and subsequent <b>clustering</b> with t-SNE (t-distributed stochastic neighbor embedding) and Gaussian mixture model (GMM). First, we employ HLLE to transform high-dimensional (HD) electrical data into low-dimensional (LD) embedding coordinates. This technique effectively captures the inherent variations and patterns in the data, enabling robust feature extraction. Next, we perform the Mann-Whitney U test based on the feature space of the embedding coordinates for fault detection. This statistical approach allows us to detect electrical faults providing an efficient means of system monitoring and control. Furthermore, to enhance fault categorization, we employ t-SNE with GMM to cluster the detected faults into various categories. To evaluate the performance of the proposed method, we conduct extensive <b>simulations</b> on an electrical system integrated with solar farm. Our results demonstrate that the proposed approach exhibits effective fault detection and <b>clustering</b> across a range of fault types with different variations of the same fault. Overall, this research presents an effective methodology for robust fault detection and categorization in electrical systems, contributing to the advancement of fault management practices and the prevention of system failures.</p></p class="citation"></blockquote><h3 id=49--116138-beacon-a-bayesian-evolutionary-approach-for-counterexample-generation-of-control-systems-joshua-yancosek-et-al-2024>(4/9 | 116/138) BEACON: A Bayesian Evolutionary Approach for Counterexample Generation of Control Systems (Joshua Yancosek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Yancosek, Ali Baheri. (2024)<br><strong>BEACON: A Bayesian Evolutionary Approach for Counterexample Generation of Control Systems</strong><br><button class=copy-to-clipboard title="BEACON: A Bayesian Evolutionary Approach for Counterexample Generation of Control Systems" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05925v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05925v2.pdf filename=2403.05925v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rigorous safety verification of control systems in critical applications is essential, given their increasing complexity and integration into everyday life. <b>Simulation-based</b> falsification approaches play a pivotal role in the safety verification of control systems, particularly within critical applications. These methods systematically explore the operational space of systems to identify configurations that result in violations of safety specifications. However, the effectiveness of traditional <b>simulation-based</b> falsification is frequently limited by the high dimensionality of the search space and the substantial computational resources required for exhaustive exploration. This paper presents BEACON, a novel framework that enhances the falsification process through a combination of Bayesian optimization and covariance matrix adaptation evolutionary strategy. By exploiting quantitative metrics to evaluate how closely a system adheres to safety specifications, BEACON advances the state-of-the-art in testing methodologies. It employs a model-based test point selection approach, designed to facilitate exploration across dynamically evolving search zones to efficiently uncover safety violations. Our findings demonstrate that BEACON not only locates a higher percentage of counterexamples compared to standalone BO but also achieves this with significantly fewer <b>simulations</b> than required by CMA-ES, highlighting its potential to optimize the verification process of control systems. This framework offers a promising direction for achieving thorough and resource-efficient safety evaluations, ensuring the reliability of control systems in critical applications.</p></p class="citation"></blockquote><h3 id=59--117138-safe-merging-in-mixed-traffic-with-confidence-heeseung-bang-et-al-2024>(5/9 | 117/138) Safe Merging in Mixed Traffic with Confidence (Heeseung Bang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heeseung Bang, Aditya Dave, Andreas A. Malikopoulos. (2024)<br><strong>Safe Merging in Mixed Traffic with Confidence</strong><br><button class=copy-to-clipboard title="Safe Merging in Mixed Traffic with Confidence" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05742v1.pdf filename=2403.05742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we present an approach for learning human driving behavior, without relying on specific model structures or prior distributions, in a mixed-traffic environment where connected and automated vehicles (CAVs) coexist with human-driven vehicles (HDVs). We employ conformal prediction to obtain theoretical safety guarantees and use real-world traffic data to validate our approach. Then, we design a controller that ensures effective merging of CAVs with HDVs with safety guarantees. We provide numerical <b>simulations</b> to illustrate the efficacy of the control approach.</p></p class="citation"></blockquote><h3 id=69--118138-sample-optimal-zero-violation-safety-for-continuous-control-ritabrata-ray-et-al-2024>(6/9 | 118/138) Sample-Optimal Zero-Violation Safety For Continuous Control (Ritabrata Ray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ritabrata Ray, Yorie Nakahira, Soummya Kar. (2024)<br><strong>Sample-Optimal Zero-Violation Safety For Continuous Control</strong><br><button class=copy-to-clipboard title="Sample-Optimal Zero-Violation Safety For Continuous Control" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06045v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06045v2.pdf filename=2403.06045v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the problem of ensuring safety with a few shots of samples for partially unknown systems. We first characterize a fundamental limit when producing safe actions is not possible due to insufficient information or samples. Then, we develop a technique that can generate provably safe actions and recovery behaviors using a minimum number of samples. In the performance analysis, we also establish Nagumos theorem - like results with relaxed assumptions, which is potentially useful in other contexts. Finally, we discuss how the proposed method can be integrated into a policy gradient algorithm to assure safety and stability with a handful of samples without stabilizing initial policies or generative models to probe safe actions.</p></p class="citation"></blockquote><h3 id=79--119138-frequency-domain-auto-tuning-of-structured-lpv-controllers-for-high-precision-motion-control-yorick-broens-et-al-2024>(7/9 | 119/138) Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control (Yorick Broens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yorick Broens, Hans Butler, Roland Tóth. (2024)<br><strong>Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control</strong><br><button class=copy-to-clipboard title="Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: 37M15, J-2; G-2-0, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05878v1.pdf filename=2403.05878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motion systems are a vital part of many industrial processes. However, meeting the increasingly stringent demands of these systems, especially concerning precision and throughput, requires novel control design methods that can go beyond the capabilities of traditional solutions. Traditional control methods often struggle with the complexity and position-dependent effects inherent in modern motion systems, leading to compromises in performance and a laborious task of controller design. This paper addresses these challenges by introducing a novel structured feedback control auto-tuning approach for multiple-input multiple-output (MIMO) motion systems. By leveraging frequency response function (FRF) estimates and the linear-parameter-varying (LPV) control framework, the proposed approach automates the controller design, while providing local stability and performance guarantees. Key innovations include norm-based magnitude optimization of the sensitivity functions, an automated stability check through a novel extended factorized Nyquist criterion, a modular structured MIMO LPV controller parameterization, and a controller discretization approach which preserves the <b>continuous-time</b> <b>(CT)</b> controller parameterization. The proposed approach is validated through experiments using a state-of-the-art moving-magnet planar actuator prototype.</p></p class="citation"></blockquote><h3 id=89--120138-fault-classification-in-electrical-distribution-systems-using-grassmann-manifold-victor-sam-moses-babu-k-et-al-2024>(8/9 | 120/138) Fault Classification in Electrical Distribution Systems using Grassmann Manifold (Victor Sam Moses Babu K. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Victor Sam Moses Babu K., Sidharthenee Nayak, Divyanshi Dwivedi, Pratyush Chakraborty, Chandrashekhar Narayan Bhende, Pradeep Kumar Yemula, Mayukha Pal. (2024)<br><strong>Fault Classification in Electrical Distribution Systems using Grassmann Manifold</strong><br><button class=copy-to-clipboard title="Fault Classification in Electrical Distribution Systems using Grassmann Manifold" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05991v1.pdf filename=2403.05991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electrical fault classification is vital for ensuring the reliability and safety of power systems. Accurate and efficient fault classification methods are essential for timely and effective maintenance. In this paper, we propose a novel approach for effective fault classification through Grassmann manifolds, which is a non-Euclidean space that captures the intrinsic structure of high-dimensional data and offers a robust framework for feature extraction. We use simulated data for electrical distribution systems with various types of electrical faults. The proposed method involves transforming the measurement fault data into Grassmann manifold space using techniques from differential <b>geometry.</b> This transformation aids in uncovering the underlying fault patterns and reducing the computational complexity of subsequent classification steps. To achieve fault classification, we employ a machine learning technique optimized for the Grassmann manifold. The support vector machine classifier is adapted to operate within the Grassmann manifold space, enabling effective discrimination between different fault classes. The results illustrate the efficacy of the proposed Grassmann manifold-based approach for electrical fault classification which showcases its ability to accurately differentiate between various fault types.</p></p class="citation"></blockquote><h3 id=99--121138-characterizing-flow-complexity-in-transportation-networks-using-graph-homology-shashank-a-deshpande-et-al-2024>(9/9 | 121/138) Characterizing Flow Complexity in Transportation Networks using Graph Homology (Shashank A Deshpande et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashank A Deshpande, Hamsa Balakrishnan. (2024)<br><strong>Characterizing Flow Complexity in Transportation Networks using Graph Homology</strong><br><button class=copy-to-clipboard title="Characterizing Flow Complexity in Transportation Networks using Graph Homology" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-DM, cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05749v1.pdf filename=2403.05749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Series-parallel network topologies generally exhibit simplified dynamical behavior and avoid high combinatorial complexity. A comprehensive analysis of how flow complexity emerges with a <b>graph&rsquo;s</b> deviation from series-parallel topology is therefore of fundamental interest. We introduce the notion of a robust $k$-path on a directed acycylic <b>graph,</b> with increasing values of the length $k$ reflecting increasing deviations. We propose a <b>graph</b> homology with robust $k$-paths as the bases of its chain spaces. In this framework, the topological simplicity of series-parallel <b>graphs</b> translates into a triviality of higher-order chain spaces. We discuss a correspondence between the space of order-three chains and sites within the network that are susceptible to the Braess paradox, a well-known phenomenon in transportation networks. In this manner, we illustrate the utility of the proposed <b>graph</b> homology in sytematically studying the complexity of flow networks.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--122138-near-minimax-optimal-distributional-temporal-difference-algorithms-and-the-freedman-inequality-in-hilbert-spaces-yang-peng-et-al-2024>(1/1 | 122/138) Near Minimax-Optimal Distributional Temporal Difference Algorithms and The Freedman Inequality in Hilbert Spaces (Yang Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Peng, Liangyu Zhang, Zhihua Zhang. (2024)<br><strong>Near Minimax-Optimal Distributional Temporal Difference Algorithms and The Freedman Inequality in Hilbert Spaces</strong><br><button class=copy-to-clipboard title="Near Minimax-Optimal Distributional Temporal Difference Algorithms and The Freedman Inequality in Hilbert Spaces" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Distributional Reinforcement Learning, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05811v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05811v2.pdf filename=2403.05811v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Distributional</b> <b>reinforcement</b> <b>learning</b> (DRL) has achieved empirical success in various domains. One of the core tasks in the field of DRL is <b>distributional</b> <b>policy</b> <b>evaluation,</b> which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. The <b>distributional</b> <b>temporal</b> <b>difference</b> (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of <b>distributional</b> <b>TD,</b> <b>namely</b> categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of <b>distributional</b> <b>TD.</b> <b>To</b> facilitate theoretical analysis, we propose a non-parametric <b>distributional</b> <b>TD</b> <b>algorithm</b> (NTD). For a $\gamma$-discounted infinite-horizon tabular <b>Markov</b> <b>decision</b> <b>process,</b> we show that for NTD we need $\tilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)$ iterations to achieve an $\varepsilon$-optimal estimator with high probability, when the estimation error is measured by the $p$-Wasserstein distance. This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance. To achieve this, we establish a novel Freedman&rsquo;s inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$-Wasserstein distance.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--123138-quantum-hpc-framework-with-multi-gpu-enabled-hybrid-quantum-classical-workflow-applications-in-quantum-simulations-kuan-cheng-chen-et-al-2024>(1/2 | 123/138) Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations (Kuan-Cheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang, Chen-Yu Liu. (2024)<br><strong>Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations</strong><br><button class=copy-to-clipboard title="Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-AR, cs-DC, quant-ph, quant-ph<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05828v1.pdf filename=2403.05828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving high-performance computation on quantum systems presents a formidable challenge that necessitates bridging the capabilities between quantum hardware and classical computing resources. This study introduces an innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture, which integrates cutting-edge quantum software framework works with high-performance classical computing resources to address challenges in quantum <b>simulation</b> for materials and condensed matter physics. At the heart of this architecture is the seamless integration of VQE algorithms running on QPUs for efficient quantum state preparation, Tensor Network states, and QCNNs for classifying quantum states on classical hardware. For <b>benchmarking</b> quantum simulators, the QCQ architecture utilizes the cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane&rsquo;s Lightning plugin, demonstrating up to tenfold increases in computational speed for complex phase transition classification tasks compared to traditional CPU-based methods. This significant acceleration enables models such as the transverse field Ising and XXZ systems to accurately predict phase transitions with a 99.5% accuracy. The architecture&rsquo;s ability to distribute computation between QPUs and classical resources addresses critical bottlenecks in Quantum-HPC, paving the way for scalable quantum <b>simulation.</b> The QCQ framework embodies a synergistic combination of quantum algorithms, machine learning, and Quantum-HPC capabilities, enhancing its potential to provide transformative insights into the behavior of quantum systems across different scales. As quantum hardware continues to improve, this hybrid distribution-aware framework will play a crucial role in realizing the full potential of quantum computing by seamlessly integrating distributed quantum resources with the state-of-the-art classical computing infrastructure.</p></p class="citation"></blockquote><h3 id=22--124138-investigation-into-the-potential-of-parallel-quantum-annealing-for-simultaneous-optimization-of-multiple-problems-a-comprehensive-study-arit-kumar-bishwas-et-al-2024>(2/2 | 124/138) Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study (Arit Kumar Bishwas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arit Kumar Bishwas, Anuraj Som, Saurabh Choudhary. (2024)<br><strong>Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study</strong><br><button class=copy-to-clipboard title="Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05764v1.pdf filename=2403.05764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parallel Quantum Annealing is a technique to solve multiple optimization problems simultaneously. Parallel quantum annealing aims to optimize the utilization of available qubits on a quantum topology by addressing multiple independent problems in a single annealing cycle. This study provides insights into the potential and the limitations of this parallelization method. The experiments consisting of two different problems are integrated, and various problem dimensions are explored including normalization techniques using specific methods such as DWaveSampler with Default Embedding, DWaveSampler with Custom Embedding and LeapHybridSampler. This method minimizes idle qubits and holds promise for substantial speed-up, as indicated by the Time-to-Solution <b>(TTS)</b> metric, compared to traditional quantum annealing, which solves problems sequentially and may leave qubits unutilized.</p></p class="citation"></blockquote><h2 id=eessas-1>eess.AS (1)</h2><h3 id=11--125138-asynchronous-microphone-array-calibration-using-hybrid-tdoa-information-chengjie-zhang-et-al-2024>(1/1 | 125/138) Asynchronous Microphone Array Calibration using Hybrid TDOA Information (Chengjie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengjie Zhang, Jiang Wang, He Kong. (2024)<br><strong>Asynchronous Microphone Array Calibration using Hybrid TDOA Information</strong><br><button class=copy-to-clipboard title="Asynchronous Microphone Array Calibration using Hybrid TDOA Information" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05791v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05791v2.pdf filename=2403.05791v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Asynchronous Microphone array calibration is a prerequisite for most audition robot applications. In practice, the calibration requires estimating microphone positions, time offsets, clock drift rates, and sound event locations simultaneously. The existing method proposed <b>Graph-based</b> Simultaneous Localisation and Mapping <b>(Graph-SLAM)</b> utilizing common TDOA, time difference of arrival between two microphones (TDOA-M), and odometry measurement, however, it heavily depends on the initial value. In this paper, we propose a novel TDOA, time difference of arrival between adjacent sound events (TDOA-S), combine it with TDOA-M, called hybrid TDOA, and add odometry measurement to construct <b>Graph-SLAM</b> and use the Gauss-Newton (GN) method to solve. TDOA-S is simple and efficient because it eliminates time offset without generating new variables. <b>Simulation</b> and real-world experiment results consistently show that our method is independent of microphone number, insensitive to initial values, and has better calibration accuracy and stability under various TDOA noises. In addition, the <b>simulation</b> result demonstrates that our method has a lower Cram'er-Rao lower bound (CRLB) for microphone parameters, which explains the advantages of my method.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=12--126138-deciphering-crypto-twitter-inwon-kang-et-al-2024>(1/2 | 126/138) Deciphering Crypto Twitter (Inwon Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inwon Kang, Maruf Ahmed Mridul, Abraham Sanders, Yao Ma, Thilanka Munasinghe, Aparna Gupta, Oshani Seneviratne. (2024)<br><strong>Deciphering Crypto Twitter</strong><br><button class=copy-to-clipboard title="Deciphering Crypto Twitter" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-SI, cs.CE<br>Keyword Score: 20<br>Keywords: Autoencoder, Botnet Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06036v1.pdf filename=2403.06036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year. However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology. This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within Crypto Twitter, a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions. We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape. We collected 40 million tweets using cryptocurrency-related keywords and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network. We used sentence-level embeddings and <b>autoencoders</b> to create K-means clusters of tweets and identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time. Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022. We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network. Our networks reveal a structure of <b>bot</b> <b>activity</b> in Crypto Twitter and suggest that they can be detected and handled using a network-based approach. Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for <b>bot</b> <b>detection</b> in Crypto Twitter using a network-based approach.</p></p class="citation"></blockquote><h3 id=22--127138-research-progress-on-intelligent-optimization-techniques-for-energy-efficient-design-of-ship-hull-forms-shuwei-zhu-et-al-2024>(2/2 | 127/138) Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms (Shuwei Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuwei Zhu, Siying Lv, Kaifeng Chen, Wei Fang, Leilei Cao. (2024)<br><strong>Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms</strong><br><button class=copy-to-clipboard title="Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: 41C99, J-6; I-2-8, cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05832v1.pdf filename=2403.05832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The design optimization of ship hull form based on hydrodynamics theory and <b>simulation-based</b> design (SBD) technologies generally considers ship performance and energy efficiency performance as the design objective, which plays an important role in smart design and manufacturing of green ship. An optimal design of sustainable energy system requires multidisciplinary tools to build ships with the least resistance and energy consumption. Through a systematic approach, this paper presents the research progress of energy-efficient design of ship hull forms based on intelligent optimization techniques. We discuss different methods involved in the optimization procedure, especially the latest developments of intelligent optimization algorithms and surrogate models. Moreover, current development trends and technical challenges of multidisciplinary design optimization and surrogate-assisted evolutionary algorithms for ship design are further analyzed. We explore the gaps and potential future directions, so as to paving the way towards the design of the next generation of more energy-efficient ship hull form.</p></p class="citation"></blockquote><h2 id=q-bioto-1>q-bio.TO (1)</h2><h3 id=11--128138-an-in-silico-approach-to-meniscus-tissue-regeneration-modeling-numerical-simulation-and-experimental-analysis-elise-grosjean-et-al-2024>(1/1 | 128/138) An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis (Elise Grosjean et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elise Grosjean, Alex Keilmann, Henry Jäger, Shimi Mohanan, Claudia Redenbach, Bernd Simeon, Christina Surulescu, Luisa de Roy, Andreas Seitz, Graciosa Teixeira, Martin Dauner, Carsten Linti, Günter Schmidt. (2024)<br><strong>An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis</strong><br><button class=copy-to-clipboard title="An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.TO<br>Categories: cs-NA, math-AP, math-NA, q-bio-CB, q-bio-QM, q-bio-TO, q-bio.TO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05909v1.pdf filename=2403.05909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium. The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion. The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress. The model takes deformations of ECM and PET scaffold into account. The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images. The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure. Numerical <b>simulations</b> show its influence on the overall cell and tissue dynamics.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--129138-deepvm-integrating-spot-and-on-demand-vms-for-cost-efficient-deep-learning-clusters-in-the-cloud-yoochan-kim-et-al-2024>(1/1 | 129/138) DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud (Yoochan Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoochan Kim, Kihyun Kim, Yonghyeon Cho, Jinwoo Kim, Awais Khan, Ki-Dong Kang, Baik-Song An, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim. (2024)<br><strong>DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud</strong><br><button class=copy-to-clipboard title="DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05861v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05861v2.pdf filename=2403.05861v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based clusters as the optimal infrastructure for training large-scale Deep Neural Networks (DNNs). However, the high cost of such resources makes them inaccessible to many users. Public cloud services, particularly Spot Virtual Machines (VMs), offer a cost-effective alternative, but their unpredictable availability poses a significant challenge to the crucial checkpointing process in DDL. To address this, we introduce DeepVM, a novel solution that recommends cost-effective cluster configurations by intelligently balancing the use of Spot and On-Demand VMs. DeepVM leverages a four-stage process that analyzes instance performance using the FLOPP (FLoating-point Operations Per Price) metric, performs architecture-level analysis with linear programming, and identifies the optimal configuration for the user-specific needs. Extensive <b>simulations</b> and real-world deployments in the AWS environment demonstrate that DeepVM consistently outperforms other policies, reducing training costs and overall makespan. By enabling cost-effective checkpointing with Spot VMs, DeepVM opens up DDL to a wider range of users and facilitates a more efficient training of complex DNNs.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--130138-a-feasibility-analysis-at-signal-free-intersections-filippos-n-tzortzoglou-et-al-2024>(1/1 | 130/138) A Feasibility Analysis at Signal-Free Intersections (Filippos N. Tzortzoglou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filippos N. Tzortzoglou, Logan E. Beaver, Andreas A. Malikopoulos. (2024)<br><strong>A Feasibility Analysis at Signal-Free Intersections</strong><br><button class=copy-to-clipboard title="A Feasibility Analysis at Signal-Free Intersections" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05739v1.pdf filename=2403.05739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we address the problem of improving the feasible domain of the solution of a decentralized control framework for coordinating connected and automated vehicles (CAVs) at signal-free intersections as the traffic volume increases. The framework provides the optimal trajectories of CAVs to cross the intersection safely without stop-and-go driving. However, as the traffic volume increases, the domain of the feasible trajectories decreases. We use concepts of numerical interpolation to identify appropriate polynomials that can serve as alternative trajectories of the CAVs, expanding the domain of the feasible CAV trajectories. We provide the conditions under which such polynomials exist. Finally, we demonstrate the efficacy of our approach through numerical <b>simulations.</b></p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--131138-adanovo-adaptive-emphde-novo-peptide-sequencing-with-conditional-mutual-information-jun-xia-et-al-2024>(1/1 | 131/138) AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information (Jun Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Lin, Wenjie Du, Sizhe Liu, Stan Z. Li. (2024)<br><strong>AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information</strong><br><button class=copy-to-clipboard title="AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-BM, q-bio-QM, q-bio.QM<br>Keyword Score: 13<br>Keywords: Benchmarking, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07013v1.pdf filename=2403.07013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological samples. Despite the development of various deep learning methods for identifying amino acid sequences (peptides) responsible for observed spectra, challenges persist in \emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify amino acids with post-translational modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in decreased peptide-level identification precision. Secondly, diverse types of noise and missing peaks in mass spectra reduce the reliability of training data (peptide-spectrum matches, PSMs). To address these challenges, we propose AdaNovo, a novel framework that calculates conditional <b>mutual</b> <b>information</b> (CMI) between the spectrum and each amino acid/peptide, using CMI for adaptive model training. Extensive experiments demonstrate AdaNovo&rsquo;s state-of-the-art performance on a 9-species <b>benchmark,</b> where the peptides in the training set are almost completely disjoint from the peptides of the test sets. Moreover, AdaNovo excels in identifying amino acids with PTMs and exhibits robustness against data noise. The supplementary materials contain the official code.</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--132138-invariant-properties-of-linear-iterative-distributed-averaging-algorithms-and-application-to-error-detection-christoforos-n-hadjicostis-et-al-2024>(1/1 | 132/138) Invariant Properties of Linear-Iterative Distributed Averaging Algorithms and Application to Error Detection (Christoforos N. Hadjicostis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoforos N. Hadjicostis, Alejandro D. Dominguez-Garcia. (2024)<br><strong>Invariant Properties of Linear-Iterative Distributed Averaging Algorithms and Application to Error Detection</strong><br><button class=copy-to-clipboard title="Invariant Properties of Linear-Iterative Distributed Averaging Algorithms and Application to Error Detection" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06007v1.pdf filename=2403.06007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of average consensus in a distributed system comprising a set of nodes that can exchange information among themselves. We focus on a class of algorithms for solving such a problem whereby each node maintains a state and updates it iteratively as a linear combination of the states maintained by its in-neighbors, i.e., nodes from which it receives information directly. Averaging algorithms within this class can be thought of as <b>discrete-time</b> <b>linear</b> time-varying systems without external driving inputs and whose state matrix is column stochastic. As a result, the algorithms exhibit a global invariance property in that the sum of the state variables remains constant at all times. In this paper, we report on another invariance property for the aforementioned class of averaging algorithms. This property is local to each node and reflects the conservation of certain quantities capturing an aggregate of all the values received by a node from its in-neighbors and all the values sent by said node to its out-neighbors (i.e., nodes to which it sends information directly) throughout the execution of the averaging algorithm. We show how this newly-discovered invariant can be leveraged for detecting errors while executing the averaging algorithm.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--133138-mathematics-of-multi-agent-learning-systems-at-the-interface-of-game-theory-and-artificial-intelligence-long-wang-et-al-2024>(1/1 | 133/138) Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence (Long Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Wang, Feng Fu, Xingru Chen. (2024)<br><strong>Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence</strong><br><button class=copy-to-clipboard title="Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, cs-GT, cs-MA, physics-soc-ph, physics.soc-ph<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07017v1.pdf filename=2403.07017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two fields that, at first glance, might seem distinct, but they have notable connections and intersections. The former focuses on the evolution of behaviors (or strategies) in a population, where individuals interact with others and update their strategies based on imitation (or social learning). The more successful a strategy is, the more prevalent it becomes over time. The latter, meanwhile, is centered on machine learning algorithms and (deep) neural networks. It is often from a single-agent perspective but increasingly involves multi-agent environments, in which intelligent agents adjust their strategies based on feedback and experience, somewhat akin to the evolutionary process yet distinct in their self-learning capacities. In light of the key components necessary to address real-world problems, including (i) learning and adaptation, (ii) cooperation and competition, (iii) robustness and stability, and altogether (iv) population dynamics of individual agents whose strategies evolve, the cross-fertilization of ideas between both fields will contribute to the advancement of mathematics of multi-agent learning systems, in particular, to the nascent domain of ``collective cooperative intelligence&rsquo;&rsquo; bridging evolutionary dynamics and multi-agent <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--134138-generative-probabilistic-forecasting-with-applications-in-market-operations-xinyi-wang-et-al-2024>(1/1 | 134/138) Generative Probabilistic Forecasting with Applications in Market Operations (Xinyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Wang, Lang Tong. (2024)<br><strong>Generative Probabilistic Forecasting with Applications in Market Operations</strong><br><button class=copy-to-clipboard title="Generative Probabilistic Forecasting with Applications in Market Operations" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, econ-GN, eess-SP, eess.SP, q-fin-EC<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05743v1.pdf filename=2403.05743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an <b>autoencoder</b> that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching <b>autoencoder</b> input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations are considered: (i) locational marginal price forecasting for merchant storage participants, {(ii) interregional price spread forecasting for interchange markets,} and (iii) area control error forecasting for frequency regulations. Numerical studies based on market data from multiple independent system operators demonstrate superior performance against leading traditional and machine learning-based forecasting techniques under both probabilistic and point forecast metrics.</p></p class="citation"></blockquote><h2 id=csir-1>cs.IR (1)</h2><h3 id=11--135138-hierarchical-query-classification-in-e-commerce-search-bing-he-et-al-2024>(1/1 | 135/138) Hierarchical Query Classification in E-commerce Search (Bing He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang. (2024)<br><strong>Hierarchical Query Classification in E-commerce Search</strong><br><button class=copy-to-clipboard title="Hierarchical Query Classification in E-commerce Search" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06021v1.pdf filename=2403.06021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced <b>representation</b> <b>learning</b> that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called &lsquo;&lsquo;instance hierarchy&rsquo;&rsquo;, and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named &lsquo;&rsquo;label hierarchy&rsquo;&rsquo;. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--136138-hamiltonicity-path-cover-and-independence-number-an-fpt-perspective-fedor-v-fomin-et-al-2024>(1/2 | 136/138) Hamiltonicity, Path Cover, and Independence Number: An FPT Perspective (Fedor V. Fomin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fedor V. Fomin, Petr A. Golovach, Danil Sagunov, Kirill Simonov. (2024)<br><strong>Hamiltonicity, Path Cover, and Independence Number: An FPT Perspective</strong><br><button class=copy-to-clipboard title="Hamiltonicity, Path Cover, and Independence Number: An FPT Perspective" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05943v1.pdf filename=2403.05943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The connection between Hamiltonicity and the independence numbers of <b>graphs</b> has been a fundamental aspect of <b>Graph</b> Theory since the seminal works of the 1960s. This paper presents a novel algorithmic perspective on these classical problems. Our contributions are twofold. First, we establish that a wide array of problems in undirected <b>graphs,</b> encompassing problems such as Hamiltonian Path and Cycle, Path Cover, Largest Linkage, and Topological Minor Containment are fixed-parameter tractable (FPT) parameterized by the independence number of a <b>graph.</b> To the best of our knowledge, these results mark the first instances of FPT problems for such parameterization. Second, we extend the algorithmic scope of the Gallai-Milgram theorem. The original theorem by Gallai and Milgram, asserts that for a <b>graph</b> G with the independence number \alpha(G), the vertex set of G can be covered by at most \alpha(G) vertex-disjoint paths. We show that determining whether a <b>graph</b> can be covered by fewer than \alpha(G) - k vertex-disjoint paths is FPT parameterized by k. Notably, the independence number parameterization, which describes <b>graph&rsquo;s</b> density, departs from the typical flow of research in parameterized complexity, which focuses on parameters describing <b>graph&rsquo;s</b> sparsity, like treewidth or vertex cover.</p></p class="citation"></blockquote><h3 id=22--137138-approximate-bipartite-b-matching-using-multiplicative-auction-bhargav-samineni-et-al-2024>(2/2 | 137/138) Approximate Bipartite $b$-Matching using Multiplicative Auction (Bhargav Samineni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhargav Samineni, S M Ferdous, Mahantesh Halappanavar, Bala Krishnamoorthy. (2024)<br><strong>Approximate Bipartite $b$-Matching using Multiplicative Auction</strong><br><button class=copy-to-clipboard title="Approximate Bipartite $b$-Matching using Multiplicative Auction" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05781v1.pdf filename=2403.05781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a bipartite <b>graph</b> $G(V= (A \cup B),E)$ with $n$ vertices and $m$ edges and a function $b \colon V \to \mathbb{Z}_+$, a $b$-matching is a subset of edges such that every vertex $v \in V$ is incident to at most $b(v)$ edges in the subset. When we are also given edge weights, the Max Weight $b$-Matching problem is to find a $b$-matching of maximum weight, which is a fundamental combinatorial optimization problem with many applications. Extending on the recent work of Zheng and Henzinger (IPCO, 2023) on standard bipartite matching problems, we develop a simple auction algorithm to approximately solve Max Weight $b$-Matching. Specifically, we present a multiplicative auction algorithm that gives a $(1 - \varepsilon)$-approximation in $O(m \varepsilon^{-1} \log \varepsilon^{-1} \log \beta)$ worst case time, where $\beta$ the maximum $b$-value. Although this is a $\log \beta$ factor greater than the current best approximation algorithm by Huang and Pettie (Algorithmica, 2022), it is considerably simpler to present, analyze, and implement.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--138138-a-performance-analysis-of-basin-hopping-compared-to-established-metaheuristics-for-global-optimization-marco-baioletti-et-al-2024>(1/1 | 138/138) A Performance Analysis of Basin Hopping Compared to Established Metaheuristics for Global Optimization (Marco Baioletti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Baioletti, Valentino Santucci, Marco Tomassini. (2024)<br><strong>A Performance Analysis of Basin Hopping Compared to Established Metaheuristics for Global Optimization</strong><br><button class=copy-to-clipboard title="A Performance Analysis of Basin Hopping Compared to Established Metaheuristics for Global Optimization" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: G-4, cs-NE, cs-PF, cs.NE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.05877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.05877v1.pdf filename=2403.05877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>During the last decades many metaheuristics for global numerical optimization have been proposed. Among them, Basin Hopping is very simple and straightforward to implement, although rarely used outside its original Physical Chemistry community. In this work, our aim is to compare Basin Hopping, and two population variants of it, with readily available implementations of the well known metaheuristics Differential Evolution, Particle Swarm Optimization, and Covariance Matrix Adaptation Evolution Strategy. We perform numerical experiments using the IOH profiler environment with the BBOB test function set and two difficult real-world problems. The experiments were carried out in two different but complementary ways: by measuring the performance under a fixed budget of function evaluations and by considering a fixed target value. The general conclusion is that Basin Hopping and its newly introduced population variant are almost as good as Covariance Matrix Adaptation on the synthetic <b>benchmark</b> functions and better than it on the two hard cluster energy minimization problems. Thus, the proposed analyses show that Basin Hopping can be considered a good candidate for global numerical optimization problems along with the more established metaheuristics, especially if one wants to obtain quick and reliable results on an unknown problem.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.10</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.12</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-17>cs.CL (17)</a><ul><li><a href=#117--1138-few-shot-cross-lingual-transfer-for-prompting-large-language-models-in-low-resource-languages-christopher-toukmaji-2024>(1/17 | 1/138) Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages (Christopher Toukmaji, 2024)</a></li><li><a href=#217--2138-kg-rank-enhancing-large-language-models-for-medical-qa-with-knowledge-graphs-and-ranking-techniques-rui-yang-et-al-2024>(2/17 | 2/138) KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques (Rui Yang et al., 2024)</a></li><li><a href=#317--3138-clinicalmamba-a-generative-clinical-language-model-on-longitudinal-clinical-notes-zhichao-yang-et-al-2024>(3/17 | 3/138) ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes (Zhichao Yang et al., 2024)</a></li><li><a href=#417--4138-mp2d-an-automated-topic-shift-dialogue-generation-framework-leveraging-knowledge-graphs-yerin-hwang-et-al-2024>(4/17 | 4/138) MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs (Yerin Hwang et al., 2024)</a></li><li><a href=#517--5138-decoding-the-ai-pen-techniques-and-challenges-in-detecting-ai-generated-text-sara-abdali-et-al-2024>(5/17 | 5/138) Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text (Sara Abdali et al., 2024)</a></li><li><a href=#617--6138-calibrating-large-language-models-using-their-generations-only-dennis-ulmer-et-al-2024>(6/17 | 6/138) Calibrating Large Language Models Using Their Generations Only (Dennis Ulmer et al., 2024)</a></li><li><a href=#717--7138-persian-slang-text-conversion-to-formal-and-deep-learning-of-persian-short-texts-on-social-media-for-sentiment-classification-mohsen-khazeni-et-al-2024>(7/17 | 7/138) Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification (Mohsen Khazeni et al., 2024)</a></li><li><a href=#817--8138-thread-detection-and-response-generation-using-transformers-with-prompt-optimisation-kevin-joshua-t-et-al-2024>(8/17 | 8/138) Thread Detection and Response Generation using Transformers with Prompt Optimisation (Kevin Joshua T et al., 2024)</a></li><li><a href=#917--9138-flap-flow-adhering-planning-with-constrained-decoding-in-llms-shamik-roy-et-al-2024>(9/17 | 9/138) FLAP: Flow Adhering Planning with Constrained Decoding in LLMs (Shamik Roy et al., 2024)</a></li><li><a href=#1017--10138-itd-large-language-models-can-teach-themselves-induction-through-deduction-wangtao-sun-et-al-2024>(10/17 | 10/138) ItD: Large Language Models Can Teach Themselves Induction through Deduction (Wangtao Sun et al., 2024)</a></li><li><a href=#1117--11138-algorithmic-progress-in-language-models-anson-ho-et-al-2024>(11/17 | 11/138) Algorithmic progress in language models (Anson Ho et al., 2024)</a></li><li><a href=#1217--12138-measuring-bias-in-a-ranked-list-using-term-based-representations-amin-abolghasemi-et-al-2024>(12/17 | 12/138) Measuring Bias in a Ranked List using Term-based Representations (Amin Abolghasemi et al., 2024)</a></li><li><a href=#1317--13138-reverse-that-number-decoding-order-matters-in-arithmetic-learning-daniel-zhang-li-et-al-2024>(13/17 | 13/138) Reverse That Number! Decoding Order Matters in Arithmetic Learning (Daniel Zhang-Li et al., 2024)</a></li><li><a href=#1417--14138-on-the-benefits-of-fine-grained-loss-truncation-a-case-study-on-factuality-in-summarization-lorenzo-jaime-yu-flores-et-al-2024>(14/17 | 14/138) On the Benefits of Fine-Grained Loss Truncation: A Case Study on Factuality in Summarization (Lorenzo Jaime Yu Flores et al., 2024)</a></li><li><a href=#1517--15138-enhanced-auto-language-prediction-with-dictionary-capsule----a-novel-approach-pinni-venkata-abhiram-et-al-2024>(15/17 | 15/138) Enhanced Auto Language Prediction with Dictionary Capsule &ndash; A Novel Approach (Pinni Venkata Abhiram et al., 2024)</a></li><li><a href=#1617--16138-high-throughput-phenotyping-of-physician-notes-with-large-language-and-hybrid-nlp-models-syed-i-munzir-et-al-2024>(16/17 | 16/138) High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models (Syed I. Munzir et al., 2024)</a></li><li><a href=#1717--17138-unisparse-an-intermediate-language-for-general-sparse-format-customization-jie-liu-et-al-2024>(17/17 | 17/138) UniSparse: An Intermediate Language for General Sparse Format Customization (Jie Liu et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--18138-ham-tts-hierarchical-acoustic-modeling-for-token-based-zero-shot-text-to-speech-with-model-and-data-scaling-chunhui-wang-et-al-2024>(1/3 | 18/138) HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot Text-to-Speech with Model and Data Scaling (Chunhui Wang et al., 2024)</a></li><li><a href=#23--19138-an-audio-textual-diffusion-model-for-converting-speech-signals-into-ultrasound-tongue-imaging-data-yudong-yang-et-al-2024>(2/3 | 19/138) An Audio-textual Diffusion Model For Converting Speech Signals Into Ultrasound Tongue Imaging Data (Yudong Yang et al., 2024)</a></li><li><a href=#33--20138-svad-a-robust-low-power-and-light-weight-voice-activity-detection-with-spiking-neural-networks-qu-yang-et-al-2024>(3/3 | 20/138) sVAD: A Robust, Low-Power, and Light-Weight Voice Activity Detection with Spiking Neural Networks (Qu Yang et al., 2024)</a></li></ul></li><li><a href=#csai-1>cs.AI (1)</a><ul><li><a href=#11--21138-enhancing-multi-hop-knowledge-graph-reasoning-through-reward-shaping-techniques-chen-li-et-al-2024>(1/1 | 21/138) Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques (Chen Li et al., 2024)</a></li></ul></li><li><a href=#cscv-29>cs.CV (29)</a><ul><li><a href=#129--22138-learned-3d-volumetric-recovery-of-clouds-and-its-uncertainty-for-climate-analysis-roi-ronen-et-al-2024>(1/29 | 22/138) Learned 3D volumetric recovery of clouds and its uncertainty for climate analysis (Roi Ronen et al., 2024)</a></li><li><a href=#229--23138-gpt-as-psychologist-preliminary-evaluations-for-gpt-4v-on-visual-affective-computing-hao-lu-et-al-2024>(2/29 | 23/138) GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing (Hao Lu et al., 2024)</a></li><li><a href=#329--24138-frequency-attention-for-knowledge-distillation-cuong-pham-et-al-2024>(3/29 | 24/138) Frequency Attention for Knowledge Distillation (Cuong Pham et al., 2024)</a></li><li><a href=#429--25138-general-surgery-vision-transformer-a-video-pre-trained-foundation-model-for-general-surgery-samuel-schmidgall-et-al-2024>(4/29 | 25/138) General surgery vision transformer: A video pre-trained foundation model for general surgery (Samuel Schmidgall et al., 2024)</a></li><li><a href=#529--26138-a-self-supervised-cnn-for-image-watermark-removal-chunwei-tian-et-al-2024>(5/29 | 26/138) A self-supervised CNN for image watermark removal (Chunwei Tian et al., 2024)</a></li><li><a href=#629--27138-weakly-supervised-change-detection-via-knowledge-distillation-and-multiscale-sigmoid-inference-binghao-lu-et-al-2024>(6/29 | 27/138) Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference (Binghao Lu et al., 2024)</a></li><li><a href=#729--28138-ltgc-long-tail-recognition-via-leveraging-llms-driven-generated-content-qihao-zhao-et-al-2024>(7/29 | 28/138) LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content (Qihao Zhao et al., 2024)</a></li><li><a href=#829--29138-unigradicon-a-foundation-model-for-medical-image-registration-lin-tian-et-al-2024>(8/29 | 29/138) uniGradICON: A Foundation Model for Medical Image Registration (Lin Tian et al., 2024)</a></li><li><a href=#929--30138-carbonnet-how-computer-vision-plays-a-role-in-climate-change-application-learning-geomechanics-from-subsurface-geometry-of-ccs-to-mitigate-global-warming-wei-chen-et-al-2024>(9/29 | 30/138) CarbonNet: How Computer Vision Plays a Role in Climate Change? Application: Learning Geomechanics from Subsurface Geometry of CCS to Mitigate Global Warming (Wei Chen et al., 2024)</a></li><li><a href=#1029--31138-can-generative-models-improve-self-supervised-representation-learning-arash-afkanpour-et-al-2024>(10/29 | 31/138) Can Generative Models Improve Self-Supervised Representation Learning? (Arash Afkanpour et al., 2024)</a></li><li><a href=#1129--32138-robust-emotion-recognition-in-context-debiasing-dingkang-yang-et-al-2024>(11/29 | 32/138) Robust Emotion Recognition in Context Debiasing (Dingkang Yang et al., 2024)</a></li><li><a href=#1229--33138-generalizing-to-out-of-sample-degradations-via-model-reprogramming-runhua-jiang-et-al-2024>(12/29 | 33/138) Generalizing to Out-of-Sample Degradations via Model Reprogramming (Runhua Jiang et al., 2024)</a></li><li><a href=#1329--34138-diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-michael-toker-et-al-2024>(13/29 | 34/138) Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines (Michael Toker et al., 2024)</a></li><li><a href=#1429--35138-do3d-self-supervised-learning-of-decomposed-object-aware-3d-motion-and-depth-from-monocular-videos-xiuzhe-wu-et-al-2024>(14/29 | 35/138) DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and Depth from Monocular Videos (Xiuzhe Wu et al., 2024)</a></li><li><a href=#1529--36138-cscnet-class-specified-cascaded-network-for-compositional-zero-shot-learning-yanyi-zhang-et-al-2024>(15/29 | 36/138) CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot Learning (Yanyi Zhang et al., 2024)</a></li><li><a href=#1629--37138-realnet-a-feature-selection-network-with-realistic-synthetic-anomaly-for-anomaly-detection-ximiao-zhang-et-al-2024>(16/29 | 37/138) RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection (Ximiao Zhang et al., 2024)</a></li><li><a href=#1729--38138-long-term-frame-event-visual-tracking-benchmark-dataset-and-baseline-xiao-wang-et-al-2024>(17/29 | 38/138) Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline (Xiao Wang et al., 2024)</a></li><li><a href=#1829--39138-adaptive-multi-modal-fusion-of-spatially-variant-kernel-refinement-with-diffusion-model-for-blind-image-super-resolution-junxiong-lin-et-al-2024>(18/29 | 39/138) Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with Diffusion Model for Blind Image Super-Resolution (Junxiong Lin et al., 2024)</a></li><li><a href=#1929--40138-classifying-objects-in-3d-point-clouds-using-recurrent-neural-network-a-gru-lstm-hybrid-approach-ramin-mousa-et-al-2024>(19/29 | 40/138) Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach (Ramin Mousa et al., 2024)</a></li><li><a href=#2029--41138-dynamic-policy-driven-adaptive-multi-instance-learning-for-whole-slide-image-classification-tingting-zheng-et-al-2024>(20/29 | 41/138) Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification (Tingting Zheng et al., 2024)</a></li><li><a href=#2129--42138-towards-deviation-robust-agent-navigation-via-perturbation-aware-contrastive-learning-bingqian-lin-et-al-2024>(21/29 | 42/138) Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning (Bingqian Lin et al., 2024)</a></li><li><a href=#2229--43138-semi-supervised-multimodal-multi-instance-learning-for-aortic-stenosis-diagnosis-zhe-huang-et-al-2024>(22/29 | 43/138) Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis Diagnosis (Zhe Huang et al., 2024)</a></li><li><a href=#2329--44138-spaformer-sequential-3d-part-assembly-with-transformers-boshen-xu-et-al-2024>(23/29 | 44/138) SPAFormer: Sequential 3D Part Assembly with Transformers (Boshen Xu et al., 2024)</a></li><li><a href=#2429--45138-pov-prompt-oriented-view-agnostic-learning-for-egocentric-hand-object-interaction-in-the-multi-view-world-boshen-xu-et-al-2024>(24/29 | 45/138) POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object Interaction in the Multi-View World (Boshen Xu et al., 2024)</a></li><li><a href=#2529--46138-recurrent-aligned-network-for-generalized-pedestrian-trajectory-prediction-yonghao-dong-et-al-2024>(25/29 | 46/138) Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction (Yonghao Dong et al., 2024)</a></li><li><a href=#2629--47138-deep-contrastive-multi-view-clustering-under-semantic-feature-guidance-siwen-liu-et-al-2024>(26/29 | 47/138) Deep Contrastive Multi-view Clustering under Semantic Feature Guidance (Siwen Liu et al., 2024)</a></li><li><a href=#2729--48138-fast-kernel-scene-flow-xueqian-li-et-al-2024>(27/29 | 48/138) Fast Kernel Scene Flow (Xueqian Li et al., 2024)</a></li><li><a href=#2829--49138-safdnet-a-simple-and-effective-network-for-fully-sparse-3d-object-detection-gang-zhang-et-al-2024>(28/29 | 49/138) SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection (Gang Zhang et al., 2024)</a></li><li><a href=#2929--50138-lightning-nerf-efficient-hybrid-scene-representation-for-autonomous-driving-junyi-cao-et-al-2024>(29/29 | 50/138) Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving (Junyi Cao et al., 2024)</a></li></ul></li><li><a href=#eessiv-4>eess.IV (4)</a><ul><li><a href=#14--51138-segmentation-guided-sparse-transformer-for-under-display-camera-image-restoration-jingyun-xue-et-al-2024>(1/4 | 51/138) Segmentation Guided Sparse Transformer for Under-Display Camera Image Restoration (Jingyun Xue et al., 2024)</a></li><li><a href=#24--52138-hair-and-scalp-disease-detection-using-deep-learning-kavita-sultanpure-et-al-2024>(2/4 | 52/138) Hair and scalp disease detection using deep learning (Kavita Sultanpure et al., 2024)</a></li><li><a href=#34--53138-udcr-unsupervised-aortic-dsacta-rigid-registration-using-deep-reinforcement-learning-and-overlap-degree-calculation-wentao-liu-et-al-2024>(3/4 | 53/138) UDCR: Unsupervised Aortic DSA/CTA Rigid Registration Using Deep Reinforcement Learning and Overlap Degree Calculation (Wentao Liu et al., 2024)</a></li><li><a href=#44--54138-ioi-invisible-one-iteration-adversarial-attack-on-no-reference-image--and-video-quality-metrics-ekaterina-shumitskaya-et-al-2024>(4/4 | 54/138) IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics (Ekaterina Shumitskaya et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#14--55138-hufu-a-modality-agnositc-watermarking-system-for-pre-trained-transformers-via-permutation-equivariance-hengyuan-xu-et-al-2024>(1/4 | 55/138) Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance (Hengyuan Xu et al., 2024)</a></li><li><a href=#24--56138-mirrorattack-backdoor-attack-on-3d-point-cloud-with-a-distorting-mirror-yuhao-bian-et-al-2024>(2/4 | 56/138) MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror (Yuhao Bian et al., 2024)</a></li><li><a href=#34--57138-contemplating-secure-and-optimal-design-practices-for-information-infrastructure-from-a-human-factors-perspective-niroop-sugunaraj-2024>(3/4 | 57/138) Contemplating Secure and Optimal Design Practices for Information Infrastructure From a Human Factors Perspective (Niroop Sugunaraj, 2024)</a></li><li><a href=#44--58138-privacy-preserving-diffusion-model-using-homomorphic-encryption-yaojian-chen-et-al-2024>(4/4 | 58/138) Privacy-Preserving Diffusion Model Using Homomorphic Encryption (Yaojian Chen et al., 2024)</a></li></ul></li><li><a href=#cslg-27>cs.LG (27)</a><ul><li><a href=#127--59138-textbfs2ip-llm-semantic-space-informed-prompt-learning-with-llm-for-time-series-forecasting-zijie-pan-et-al-2024>(1/27 | 59/138) $\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting (Zijie Pan et al., 2024)</a></li><li><a href=#227--60138-task-oriented-gnns-training-on-large-knowledge-graphs-for-accurate-and-efficient-modeling-hussein-abdallah-et-al-2024>(2/27 | 60/138) Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and Efficient Modeling (Hussein Abdallah et al., 2024)</a></li><li><a href=#327--61138-addressing-shortcomings-in-fair-graph-learning-datasets-towards-a-new-benchmark-xiaowei-qian-et-al-2024>(3/27 | 61/138) Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark (Xiaowei Qian et al., 2024)</a></li><li><a href=#427--62138-non-intrusive-load-monitoring-with-missing-data-imputation-based-on-tensor-decomposition-dengyu-shi-2024>(4/27 | 62/138) Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition (DengYu Shi, 2024)</a></li><li><a href=#527--63138-provable-policy-gradient-methods-for-average-reward-markov-potential-games-min-cheng-et-al-2024>(5/27 | 63/138) Provable Policy Gradient Methods for Average-Reward Markov Potential Games (Min Cheng et al., 2024)</a></li><li><a href=#627--64138-optimizing-llm-queries-in-relational-workloads-shu-liu-et-al-2024>(6/27 | 64/138) Optimizing LLM Queries in Relational Workloads (Shu Liu et al., 2024)</a></li><li><a href=#727--65138-autoeval-done-right-using-synthetic-data-for-model-evaluation-pierre-boyeau-et-al-2024>(7/27 | 65/138) AutoEval Done Right: Using Synthetic Data for Model Evaluation (Pierre Boyeau et al., 2024)</a></li><li><a href=#827--66138-detectors-for-safe-and-reliable-llms-implementations-uses-and-limitations-swapnaja-achintalwar-et-al-2024>(8/27 | 66/138) Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations (Swapnaja Achintalwar et al., 2024)</a></li><li><a href=#927--67138-dissecting-deep-rl-with-high-update-ratios-combatting-value-overestimation-and-divergence-marcel-hussing-et-al-2024>(9/27 | 67/138) Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence (Marcel Hussing et al., 2024)</a></li><li><a href=#1027--68138-paper-hilt-personalized-and-adaptive-privacy-aware-early-exit-for-reinforcement-learning-in-human-in-the-loop-systems-mojtaba-taherisadr-et-al-2024>(10/27 | 68/138) PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems (Mojtaba Taherisadr et al., 2024)</a></li><li><a href=#1127--69138-mg-tsd-multi-granularity-time-series-diffusion-models-with-guided-learning-process-xinyao-fan-et-al-2024>(11/27 | 69/138) MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process (Xinyao Fan et al., 2024)</a></li><li><a href=#1227--70138-hard-label-based-small-query-black-box-adversarial-attack-jeonghwan-park-et-al-2024>(12/27 | 70/138) Hard-label based Small Query Black-box Adversarial Attack (Jeonghwan Park et al., 2024)</a></li><li><a href=#1327--71138-multimodal-deep-learning-approach-to-predicting-neurological-recovery-from-coma-after-cardiac-arrest-felix-h-krones-et-al-2024>(13/27 | 71/138) Multimodal deep learning approach to predicting neurological recovery from coma after cardiac arrest (Felix H. Krones et al., 2024)</a></li><li><a href=#1427--72138-towards-a-generic-representation-of-combinatorial-problems-for-learning-based-approaches-léo-boisvert-et-al-2024>(14/27 | 72/138) Towards a Generic Representation of Combinatorial Problems for Learning-Based Approaches (Léo Boisvert et al., 2024)</a></li><li><a href=#1527--73138-spatial-clustering-approach-for-vessel-path-identification-mohamed-abuella-et-al-2024>(15/27 | 73/138) Spatial Clustering Approach for Vessel Path Identification (Mohamed Abuella et al., 2024)</a></li><li><a href=#1627--74138-fairtargetsim-an-interactive-simulator-for-understanding-and-explaining-the-fairness-effects-of-target-variable-definition-dalia-gala-et-al-2024>(16/27 | 74/138) FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition (Dalia Gala et al., 2024)</a></li><li><a href=#1727--75138-reinforcement-learning-paycheck-optimization-for-multivariate-financial-goals-melda-alaluf-et-al-2024>(17/27 | 75/138) Reinforcement Learning Paycheck Optimization for Multivariate Financial Goals (Melda Alaluf et al., 2024)</a></li><li><a href=#1827--76138-enhancing-classification-performance-via-reinforcement-learning-for-feature-selection-younes-ghazagh-jahed-et-al-2024>(18/27 | 76/138) Enhancing Classification Performance via Reinforcement Learning for Feature Selection (Younes Ghazagh Jahed et al., 2024)</a></li><li><a href=#1927--77138-adaptive-hyperparameter-optimization-for-continual-learning-scenarios-rudy-semola-et-al-2024>(19/27 | 77/138) Adaptive Hyperparameter Optimization for Continual Learning Scenarios (Rudy Semola et al., 2024)</a></li><li><a href=#2027--78138-semres-ddpm-residual-network-based-diffusion-modelling-applied-to-imbalanced-data-ming-zheng-et-al-2024>(20/27 | 78/138) SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to Imbalanced Data (Ming Zheng et al., 2024)</a></li><li><a href=#2127--79138-towards-efficient-replay-in-federated-incremental-learning-yichen-li-et-al-2024>(21/27 | 79/138) Towards Efficient Replay in Federated Incremental Learning (Yichen Li et al., 2024)</a></li><li><a href=#2227--80138-tlasdi-thermodynamics-informed-latent-space-dynamics-identification-jun-sur-richard-park-et-al-2024>(22/27 | 80/138) tLaSDI: Thermodynamics-informed latent space dynamics identification (Jun Sur Richard Park et al., 2024)</a></li><li><a href=#2327--81138-extending-activation-steering-to-broad-skills-and-multiple-behaviours-teun-van-der-weij-et-al-2024>(23/27 | 81/138) Extending Activation Steering to Broad Skills and Multiple Behaviours (Teun van der Weij et al., 2024)</a></li><li><a href=#2427--82138-multi-conditioned-graph-diffusion-for-neural-architecture-search-rohan-asthana-et-al-2024>(24/27 | 82/138) Multi-conditioned Graph Diffusion for Neural Architecture Search (Rohan Asthana et al., 2024)</a></li><li><a href=#2527--83138-are-classification-robustness-and-explanation-robustness-really-strongly-correlated-an-analysis-through-input-loss-landscape-tiejin-chen-et-al-2024>(25/27 | 83/138) Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape (Tiejin Chen et al., 2024)</a></li><li><a href=#2627--84138-optimistic-safety-for-linearly-constrained-online-convex-optimization-spencer-hutchinson-et-al-2024>(26/27 | 84/138) Optimistic Safety for Linearly-Constrained Online Convex Optimization (Spencer Hutchinson et al., 2024)</a></li><li><a href=#2727--85138-membership-testing-in-markov-equivalence-classes-via-independence-query-oracles-jiaqi-zhang-et-al-2024>(27/27 | 85/138) Membership Testing in Markov Equivalence Classes via Independence Query Oracles (Jiaqi Zhang et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--86138-hdreason-algorithm-hardware-codesign-for-hyperdimensional-knowledge-graph-reasoning-hanning-chen-et-al-2024>(1/1 | 86/138) HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge Graph Reasoning (Hanning Chen et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--87138-interest-aware-joint-caching-computing-and-communication-optimization-for-mobile-vr-delivery-in-mec-networks-baojie-fu-et-al-2024>(1/1 | 87/138) Interest-Aware Joint Caching, Computing, and Communication Optimization for Mobile VR Delivery in MEC Networks (Baojie Fu et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--88138-cached-model-as-a-resource-provisioning-large-language-model-agents-for-edge-intelligence-in-space-air-ground-integrated-networks-minrui-xu-et-al-2024>(1/1 | 88/138) Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks (Minrui Xu et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--89138-large-generative-model-assisted-3d-semantic-communication-feibo-jiang-et-al-2024>(1/6 | 89/138) Large Generative Model Assisted 3D Semantic Communication (Feibo Jiang et al., 2024)</a></li><li><a href=#26--90138-deep-reinforcement-learning-enhanced-rate-splitting-multiple-access-for-interference-mitigation-osman-nuri-irkicatal-et-al-2024>(2/6 | 90/138) Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation (Osman Nuri Irkicatal et al., 2024)</a></li><li><a href=#36--91138-electromagnetic-hybrid-beamforming-for-holographic-communications-ran-ji-et-al-2024>(3/6 | 91/138) Electromagnetic Hybrid Beamforming for Holographic Communications (Ran Ji et al., 2024)</a></li><li><a href=#46--92138-stacked-intelligent-metasurface-enabled-leo-satellite-communications-relying-on-statistical-csi-shining-lin-et-al-2024>(4/6 | 92/138) Stacked Intelligent Metasurface Enabled LEO Satellite Communications Relying on Statistical CSI (Shining Lin et al., 2024)</a></li><li><a href=#56--93138-derivation-of-mutual-information-and-linear-minimum-mean-square-error-for-viterbi-decoding-of-convolutional-codes-using-the-innovations-method-masato-tajima-2024>(5/6 | 93/138) Derivation of Mutual Information and Linear Minimum Mean-Square Error for Viterbi Decoding of Convolutional Codes Using the Innovations Method (Masato Tajima, 2024)</a></li><li><a href=#66--94138-channel-estimation-for-stacked-intelligent-metasurface-assisted-wireless-networks-xianghao-yao-et-al-2024>(6/6 | 94/138) Channel Estimation for Stacked Intelligent Metasurface-Assisted Wireless Networks (Xianghao Yao et al., 2024)</a></li></ul></li><li><a href=#csro-8>cs.RO (8)</a><ul><li><a href=#18--95138-image-guided-autonomous-guidewire-navigation-in-robot-assisted-endovascular-interventions-using-reinforcement-learning-wentao-liu-et-al-2024>(1/8 | 95/138) Image-Guided Autonomous Guidewire Navigation in Robot-Assisted Endovascular Interventions using Reinforcement Learning (Wentao Liu et al., 2024)</a></li><li><a href=#28--96138-imu-as-an-input-vs-a-measurement-of-the-state-in-inertial-aided-state-estimation-keenan-burnett-et-al-2024>(2/8 | 96/138) IMU as an Input vs. a Measurement of the State in Inertial-Aided State Estimation (Keenan Burnett et al., 2024)</a></li><li><a href=#38--97138-c3d-cascade-control-with-change-point-detection-and-deep-koopman-learning-for-autonomous-surface-vehicles-jianwen-li-et-al-2024>(3/8 | 97/138) C3D: Cascade Control with Change Point Detection and Deep Koopman Learning for Autonomous Surface Vehicles (Jianwen Li et al., 2024)</a></li><li><a href=#48--98138-cease-collision-evaluation-based-active-sense-system-for-collaborative-robotic-arms-xian-huang-et-al-2024>(4/8 | 98/138) CEASE: Collision-Evaluation-based Active Sense System for Collaborative Robotic Arms (Xian Huang et al., 2024)</a></li><li><a href=#58--99138-scaling-team-coordination-on-graphs-with-reinforcement-learning-manshi-limbu-et-al-2024>(5/8 | 99/138) Scaling Team Coordination on Graphs with Reinforcement Learning (Manshi Limbu et al., 2024)</a></li><li><a href=#68--100138-physics-informed-neural-motion-planning-on-constraint-manifolds-ruiqi-ni-et-al-2024>(6/8 | 100/138) Physics-informed Neural Motion Planning on Constraint Manifolds (Ruiqi Ni et al., 2024)</a></li><li><a href=#78--101138-matrix-multi-agent-trajectory-generation-with-diverse-contexts-zhuo-xu-et-al-2024>(7/8 | 101/138) MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts (Zhuo Xu et al., 2024)</a></li><li><a href=#88--102138-multi-robot-communication-aware-cooperative-belief-space-planning-with-inconsistent-beliefs-an-action-consistent-approach-tanmoy-kundu-et-al-2024>(8/8 | 102/138) Multi-Robot Communication-Aware Cooperative Belief Space Planning with Inconsistent Beliefs: An Action-Consistent Approach (Tanmoy Kundu et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--103138-legion-harnessing-pre-trained-language-models-for-github-topic-recommendations-with-distribution-balance-loss-yen-trang-dang-et-al-2024>(1/3 | 103/138) LEGION: Harnessing Pre-trained Language Models for GitHub Topic Recommendations with Distribution-Balance Loss (Yen-Trang Dang et al., 2024)</a></li><li><a href=#23--104138-a-tool-for-automated-reasoning-about-traces-based-on-configurable-formal-semantics-ferhat-erata-et-al-2024>(2/3 | 104/138) A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics (Ferhat Erata et al., 2024)</a></li><li><a href=#33--105138-a-novel-refactoring-and-semantic-aware-abstract-syntax-tree-differencing-tool-and-a-benchmark-for-evaluating-the-accuracy-of-diff-tools-pouria-alikhanifard-et-al-2024>(3/3 | 105/138) A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools (Pouria Alikhanifard et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--106138-a-preliminary-exploration-of-youtubers-use-of-generative-ai-in-content-creation-yao-lyu-et-al-2024>(1/5 | 106/138) A Preliminary Exploration of YouTubers&rsquo; Use of Generative-AI in Content Creation (Yao Lyu et al., 2024)</a></li><li><a href=#25--107138-towards-optimizing-human-centric-objectives-in-ai-assisted-decision-making-with-offline-reinforcement-learning-zana-buçinca-et-al-2024>(2/5 | 107/138) Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning (Zana Buçinca et al., 2024)</a></li><li><a href=#35--108138-leva-using-large-language-models-to-enhance-visual-analytics-yuheng-zhao-et-al-2024>(3/5 | 108/138) LEVA: Using Large Language Models to Enhance Visual Analytics (Yuheng Zhao et al., 2024)</a></li><li><a href=#45--109138-what-motivates-people-to-trust-ai-systems-nanna-inie-2024>(4/5 | 109/138) What Motivates People to Trust &lsquo;AI&rsquo; Systems? (Nanna Inie, 2024)</a></li><li><a href=#55--110138-content-moderation-justice-and-fairness-on-social-media-comparisons-across-different-contexts-and-platforms-jie-cai-et-al-2024>(5/5 | 110/138) Content Moderation Justice and Fairness on Social Media: Comparisons Across Different Contexts and Platforms (Jie Cai et al., 2024)</a></li></ul></li><li><a href=#statme-2>stat.ME (2)</a><ul><li><a href=#12--111138-online-identification-of-stochastic-continuous-time-wiener-models-using-sampled-data-mohamed-abdalmoaty-et-al-2024>(1/2 | 111/138) Online Identification of Stochastic Continuous-Time Wiener Models Using Sampled Data (Mohamed Abdalmoaty et al., 2024)</a></li><li><a href=#22--112138-model-free-local-recalibration-of-neural-networks-r-torres-et-al-2024>(2/2 | 112/138) Model-Free Local Recalibration of Neural Networks (R. Torres et al., 2024)</a></li></ul></li><li><a href=#eesssy-9>eess.SY (9)</a><ul><li><a href=#19--113138-measuring-robustness-in-cyber-physical-systems-under-sensor-attacks-jian-xiang-et-al-2024>(1/9 | 113/138) Measuring Robustness in Cyber-Physical Systems under Sensor Attacks (Jian Xiang et al., 2024)</a></li><li><a href=#29--114138-bounding-stochastic-safety-leveraging-freedmans-inequality-with-discrete-time-control-barrier-functions-ryan-k-cosner-et-al-2024>(2/9 | 114/138) Bounding Stochastic Safety: Leveraging Freedman&rsquo;s Inequality with Discrete-Time Control Barrier Functions (Ryan K. Cosner et al., 2024)</a></li><li><a href=#39--115138-efficient-fault-detection-and-categorization-in-electrical-distribution-systems-using-hessian-locally-linear-embedding-on-measurement-data-victor-sam-moses-babu-k-et-al-2024>(3/9 | 115/138) Efficient Fault Detection and Categorization in Electrical Distribution Systems Using Hessian Locally Linear Embedding on Measurement Data (Victor Sam Moses Babu K. et al., 2024)</a></li><li><a href=#49--116138-beacon-a-bayesian-evolutionary-approach-for-counterexample-generation-of-control-systems-joshua-yancosek-et-al-2024>(4/9 | 116/138) BEACON: A Bayesian Evolutionary Approach for Counterexample Generation of Control Systems (Joshua Yancosek et al., 2024)</a></li><li><a href=#59--117138-safe-merging-in-mixed-traffic-with-confidence-heeseung-bang-et-al-2024>(5/9 | 117/138) Safe Merging in Mixed Traffic with Confidence (Heeseung Bang et al., 2024)</a></li><li><a href=#69--118138-sample-optimal-zero-violation-safety-for-continuous-control-ritabrata-ray-et-al-2024>(6/9 | 118/138) Sample-Optimal Zero-Violation Safety For Continuous Control (Ritabrata Ray et al., 2024)</a></li><li><a href=#79--119138-frequency-domain-auto-tuning-of-structured-lpv-controllers-for-high-precision-motion-control-yorick-broens-et-al-2024>(7/9 | 119/138) Frequency Domain Auto-tuning of Structured LPV Controllers for High-Precision Motion Control (Yorick Broens et al., 2024)</a></li><li><a href=#89--120138-fault-classification-in-electrical-distribution-systems-using-grassmann-manifold-victor-sam-moses-babu-k-et-al-2024>(8/9 | 120/138) Fault Classification in Electrical Distribution Systems using Grassmann Manifold (Victor Sam Moses Babu K. et al., 2024)</a></li><li><a href=#99--121138-characterizing-flow-complexity-in-transportation-networks-using-graph-homology-shashank-a-deshpande-et-al-2024>(9/9 | 121/138) Characterizing Flow Complexity in Transportation Networks using Graph Homology (Shashank A Deshpande et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--122138-near-minimax-optimal-distributional-temporal-difference-algorithms-and-the-freedman-inequality-in-hilbert-spaces-yang-peng-et-al-2024>(1/1 | 122/138) Near Minimax-Optimal Distributional Temporal Difference Algorithms and The Freedman Inequality in Hilbert Spaces (Yang Peng et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--123138-quantum-hpc-framework-with-multi-gpu-enabled-hybrid-quantum-classical-workflow-applications-in-quantum-simulations-kuan-cheng-chen-et-al-2024>(1/2 | 123/138) Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical Workflow: Applications in Quantum Simulations (Kuan-Cheng Chen et al., 2024)</a></li><li><a href=#22--124138-investigation-into-the-potential-of-parallel-quantum-annealing-for-simultaneous-optimization-of-multiple-problems-a-comprehensive-study-arit-kumar-bishwas-et-al-2024>(2/2 | 124/138) Investigation into the Potential of Parallel Quantum Annealing for Simultaneous Optimization of Multiple Problems: A Comprehensive Study (Arit Kumar Bishwas et al., 2024)</a></li></ul></li><li><a href=#eessas-1>eess.AS (1)</a><ul><li><a href=#11--125138-asynchronous-microphone-array-calibration-using-hybrid-tdoa-information-chengjie-zhang-et-al-2024>(1/1 | 125/138) Asynchronous Microphone Array Calibration using Hybrid TDOA Information (Chengjie Zhang et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#12--126138-deciphering-crypto-twitter-inwon-kang-et-al-2024>(1/2 | 126/138) Deciphering Crypto Twitter (Inwon Kang et al., 2024)</a></li><li><a href=#22--127138-research-progress-on-intelligent-optimization-techniques-for-energy-efficient-design-of-ship-hull-forms-shuwei-zhu-et-al-2024>(2/2 | 127/138) Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms (Shuwei Zhu et al., 2024)</a></li></ul></li><li><a href=#q-bioto-1>q-bio.TO (1)</a><ul><li><a href=#11--128138-an-in-silico-approach-to-meniscus-tissue-regeneration-modeling-numerical-simulation-and-experimental-analysis-elise-grosjean-et-al-2024>(1/1 | 128/138) An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis (Elise Grosjean et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--129138-deepvm-integrating-spot-and-on-demand-vms-for-cost-efficient-deep-learning-clusters-in-the-cloud-yoochan-kim-et-al-2024>(1/1 | 129/138) DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud (Yoochan Kim et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--130138-a-feasibility-analysis-at-signal-free-intersections-filippos-n-tzortzoglou-et-al-2024>(1/1 | 130/138) A Feasibility Analysis at Signal-Free Intersections (Filippos N. Tzortzoglou et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--131138-adanovo-adaptive-emphde-novo-peptide-sequencing-with-conditional-mutual-information-jun-xia-et-al-2024>(1/1 | 131/138) AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional Mutual Information (Jun Xia et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--132138-invariant-properties-of-linear-iterative-distributed-averaging-algorithms-and-application-to-error-detection-christoforos-n-hadjicostis-et-al-2024>(1/1 | 132/138) Invariant Properties of Linear-Iterative Distributed Averaging Algorithms and Application to Error Detection (Christoforos N. Hadjicostis et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--133138-mathematics-of-multi-agent-learning-systems-at-the-interface-of-game-theory-and-artificial-intelligence-long-wang-et-al-2024>(1/1 | 133/138) Mathematics of multi-agent learning systems at the interface of game theory and artificial intelligence (Long Wang et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--134138-generative-probabilistic-forecasting-with-applications-in-market-operations-xinyi-wang-et-al-2024>(1/1 | 134/138) Generative Probabilistic Forecasting with Applications in Market Operations (Xinyi Wang et al., 2024)</a></li></ul></li><li><a href=#csir-1>cs.IR (1)</a><ul><li><a href=#11--135138-hierarchical-query-classification-in-e-commerce-search-bing-he-et-al-2024>(1/1 | 135/138) Hierarchical Query Classification in E-commerce Search (Bing He et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--136138-hamiltonicity-path-cover-and-independence-number-an-fpt-perspective-fedor-v-fomin-et-al-2024>(1/2 | 136/138) Hamiltonicity, Path Cover, and Independence Number: An FPT Perspective (Fedor V. Fomin et al., 2024)</a></li><li><a href=#22--137138-approximate-bipartite-b-matching-using-multiplicative-auction-bhargav-samineni-et-al-2024>(2/2 | 137/138) Approximate Bipartite $b$-Matching using Multiplicative Auction (Bhargav Samineni et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--138138-a-performance-analysis-of-basin-hopping-compared-to-established-metaheuristics-for-global-optimization-marco-baioletti-et-al-2024>(1/1 | 138/138) A Performance Analysis of Basin Hopping Compared to Established Metaheuristics for Global Optimization (Marco Baioletti et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>