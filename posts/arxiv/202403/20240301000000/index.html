<!doctype html><html><head><title>arXiv @ 2024.03.01</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.01"><meta property="og:description" content="Primary Categories cs.AI (12) cs.AR (5) cs.CC (1) cs.CE (1) cs.CL (64) cs.CR (9) cs.CV (58) cs.CY (1) cs.DC (2) cs.DM (1) cs.DS (6) cs.ET (1) cs.GR (1) cs.GT (1) cs.HC (3) cs.IR (4) cs.IT (2) cs.LG (53) cs.LO (1) cs.MM (3) cs.NE (1) cs.NI (2) cs.PL (1) cs.RO (20) cs.SD (2) cs.SE (2) cs.SI (1) eess.IV (7) eess.SP (3) eess.SY (3) math.AG (1) math.NA (2) math.OC (1) physics.ao-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240301000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-01T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.01"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240301000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Mar 1, 2024</p></div><div class=title><h1>arXiv @ 2024.03.01</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csai-12>cs.AI (12)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csar-5>cs.AR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cscl-64>cs.CL (64)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cscv-58>cs.CV (58)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csds-6>cs.DS (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cset-1>cs.ET (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cslg-53>cs.LG (53)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csmm-3>cs.MM (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csro-20>cs.RO (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#mathag-1>math.AG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#physicsapp-ph-1>physics.app-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#q-finrm-1>q-fin.RM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#q-fintr-1>q-fin.TR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/#statme-1>stat.ME (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Automatic Evaluation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>BLEU</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>2</td><td>15</td><td>12</td><td>4</td><td>2</td></tr><tr><td>Black Box</td><td>1</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td>1</td><td>4</td><td>1</td><td>1</td></tr><tr><td>Convolution</td><td></td><td></td><td>6</td><td>4</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>9</td><td>3</td><td>1</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Counterfactual Reasoning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Dialogue System</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>6</td><td>5</td><td></td></tr><tr><td>Direct Preference Optimization</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Distributional Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Fake News Detection</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td></td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>2</td><td>16</td><td>11</td><td>6</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>GPT</td><td></td><td>7</td><td>1</td><td>2</td><td></td></tr><tr><td>GPT-3</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-4</td><td></td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>GPT-4 turbo</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Gemini</td><td></td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Graph</td><td>1</td><td>2</td><td>2</td><td>8</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>2</td><td></td><td>3</td><td>2</td></tr><tr><td>Grounding</td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>4</td><td>7</td><td>3</td><td>4</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>7</td><td>6</td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>Language Generation</td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>15</td><td>67</td><td>1</td><td>11</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mistral</td><td></td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Model Quantization</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Morphological Analysis</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td>6</td><td>9</td><td>3</td><td>6</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>6</td><td></td><td></td></tr><tr><td>Open-Domain Dialogue</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td>1</td><td>2</td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Reasoning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>1</td><td>12</td><td>6</td><td>3</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Quantization</td><td></td><td>2</td><td>1</td><td>3</td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>10</td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>6</td><td>11</td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>2</td><td></td><td>6</td><td>5</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>3</td><td>3</td><td>1</td></tr><tr><td>Retrieval Augmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>4</td><td></td><td>3</td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td>1</td><td>4</td><td>6</td><td>1</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td>3</td><td>2</td><td>9</td></tr><tr><td>Simulator</td><td>1</td><td></td><td>3</td><td>2</td><td>9</td></tr><tr><td>Slot Filling</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>1</td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>5</td><td>4</td><td></td></tr><tr><td>T5</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Textual Entailment</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>1</td><td>2</td><td>1</td></tr><tr><td>Transformer</td><td>1</td><td></td><td>4</td><td>7</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>2</td><td>5</td><td></td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>4</td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td></td><td>5</td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>4</td><td>3</td><td>1</td><td>2</td></tr><tr><td>falcon</td><td></td><td>1</td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-64>cs.CL (64)</h2><h3 id=164--1286-unsupervised-information-refinement-training-of-large-language-models-for-retrieval-augmented-generation-shicheng-xu-et-al-2024>(1/64 | 1/286) Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation (Shicheng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, Jie Zhou. (2024)<br><strong>Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation</strong><br><button class=copy-to-clipboard title="Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 120<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Unsupervised Learning, Zero-shot, Code Generation, Question Answering, Slot Filling, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18150v1.pdf filename=2402.18150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> enhances <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> by incorporating additional information from <b>retrieval.</b> <b>However,</b> <b>studies</b> have shown that <b>LLMs</b> still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of <b>LLMs</b> does not clearly make <b>LLMs</b> learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of <b>LLMs</b> in <b>RAG</b> as ``Information Refiner&rsquo;&rsquo;, which means that regardless of correctness, completeness, or usefulness of retrieved texts, <b>LLMs</b> can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes <b>LLMs</b> for <b>RAG</b> in an <b>unsupervised</b> manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on <b>zero-shot</b> prediction of 11 datasets in diverse tasks including <b>Question</b> <b>Answering,</b> <b>Slot-Filling,</b> <b>Language</b> Modeling, Dialogue, and <b>Code</b> <b>Generation</b> show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39% relative points. InFO-RAG also shows advantages in <b>in-context</b> <b>learning</b> and robustness of <b>RAG.</b></p></p class="citation"></blockquote><h3 id=264--2286-cogbench-a-large-language-model-walks-into-a-psychology-lab-julian-coda-forno-et-al-2024>(2/64 | 2/286) CogBench: a large language model walks into a psychology lab (Julian Coda-Forno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz. (2024)<br><strong>CogBench: a large language model walks into a psychology lab</strong><br><button class=copy-to-clipboard title="CogBench: a large language model walks into a psychology lab" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Probabilistic Reasoning, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18225v1.pdf filename=2402.18225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most <b>benchmarks.</b> This paper introduces CogBench, a <b>benchmark</b> that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping <b>LLMs&rsquo;</b> behavior. We apply CogBench to 35 <b>LLMs,</b> yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among <b>fine-tuned</b> versions of specific <b>LLMs.</b> Our study highlights the crucial role of model size and <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that <b>fine-tuning</b> on code does not necessarily enhance <b>LLMs&rsquo;</b> behavior. Finally, we explore the effects of <b>prompt-engineering</b> techniques. We discover that <b>chain-of-thought</b> <b>prompting</b> improves <b>probabilistic</b> <b>reasoning,</b> while take-a-step-back <b>prompting</b> fosters model-based behaviors.</p></p class="citation"></blockquote><h3 id=364--3286-few-shot-fairness-unveiling-llms-potential-for-fairness-aware-classification-garima-chhikara-et-al-2024>(3/64 | 3/286) Few-Shot Fairness: Unveiling LLM&rsquo;s Potential for Fairness-Aware Classification (Garima Chhikara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, Abhijnan Chakraborty. (2024)<br><strong>Few-Shot Fairness: Unveiling LLM&rsquo;s Potential for Fairness-Aware Classification</strong><br><button class=copy-to-clipboard title="Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fairness, Few-shot, Fine-tuning, Retrieval-Augmented Generation, GPT, GPT-4, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18502v1.pdf filename=2402.18502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Employing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> in various downstream applications such as classification is crucial, especially for smaller companies lacking the expertise and resources required for <b>fine-tuning</b> a model. <b>Fairness</b> in <b>LLMs</b> helps ensure inclusivity, equal representation based on factors such as race, gender and promotes responsible AI deployment. As the use of <b>LLMs</b> has become increasingly prevalent, it is essential to assess whether <b>LLMs</b> can generate fair outcomes when subjected to considerations of <b>fairness.</b> In this study, we introduce a framework outlining <b>fairness</b> regulations aligned with various <b>fairness</b> definitions, with each definition being modulated by varying degrees of abstraction. We explore the configuration for <b>in-context</b> <b>learning</b> and the procedure for selecting <b>in-context</b> <b>demonstrations</b> using <b>RAG,</b> while incorporating <b>fairness</b> rules into the process. Experiments conducted with different <b>LLMs</b> indicate that <b>GPT-4</b> delivers superior results in terms of both accuracy and <b>fairness</b> compared to other models. This work is one of the early attempts to achieve <b>fairness</b> in prediction tasks by utilizing <b>LLMs</b> through <b>in-context</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=464--4286-learning-to-generate-instruction-tuning-datasets-for-zero-shot-task-adaptation-nihal-v-nayak-et-al-2024>(4/64 | 4/286) Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation (Nihal V. Nayak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach. (2024)<br><strong>Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation</strong><br><button class=copy-to-clipboard title="Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Self-supervised Learning, Supervised Learning, Zero-shot, Mistral, Natural Language Inference, Question Answering, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18334v1.pdf filename=2402.18334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for <b>instruction</b> <b>tuning.</b> Our goal is to enable <b>zero-shot</b> task adaptation of <b>large</b> <b>language</b> <b>models</b> on users&rsquo; specialized, private data. We train Bonito on a new <b>large-scale</b> <b>dataset</b> <b>with</b> 1.65M examples created by remixing existing <b>instruction</b> <b>tuning</b> datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the <b>instruction</b> <b>and</b> the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types &ndash; yes-no <b>question</b> <b>answering,</b> extractive <b>question</b> <b>answering,</b> and <b>natural</b> <b>language</b> <b>inference</b> &ndash; and adapt language models. We show that Bonito significantly improves the average performance of pretrained and <b>instruction</b> <b>tuned</b> models over the de facto self <b>supervised</b> baseline. For example, adapting <b>Mistral-Instruct-v2</b> and <b>instruction</b> <b>tuned</b> variants of <b>Mistral</b> and Llama2 with Bonito improves the strong <b>zero-shot</b> performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of <b>instruction</b> <b>tuning</b> and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic <b>instruction</b> <b>tuning</b> datasets is an effective way to adapt language models to new domains. The model, dataset, and code are available at <a href=https://github.com/BatsResearch/bonito>https://github.com/BatsResearch/bonito</a>.</p></p class="citation"></blockquote><h3 id=564--5286-is-crowdsourcing-breaking-your-bank-cost-effective-fine-tuning-of-pre-trained-language-models-with-proximal-policy-optimization-shuo-yang-et-al-2024>(5/64 | 5/286) Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization (Shuo Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Yang, Gjergji Kasneci. (2024)<br><strong>Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization</strong><br><button class=copy-to-clipboard title="Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Self-supervised Learning, ChatGPT, BLEU, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18284v1.pdf filename=2402.18284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wide usage of <b>ChatGPT</b> has highlighted the potential of <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback.</b> However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a <b>self-supervised</b> text ranking approach for applying Proximal-Policy-Optimization to <b>fine-tune</b> language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding <b>BLEU,</b> GLEU, and METEOR scores. Furthermore, our manual evaluation shows that our ranking results exhibit a remarkably high consistency with that of humans. This research significantly reduces training costs of proximal policy-guided models and demonstrates the potential for self-correction of language models.</p></p class="citation"></blockquote><h3 id=664--6286-towards-generalist-prompting-for-large-language-models-by-mental-models-haoxiang-guan-et-al-2024>(6/64 | 6/286) Towards Generalist Prompting for Large Language Models by Mental Models (Haoxiang Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxiang Guan, Jiyan He, Shuxin Zheng, En-Hong Chen, Weiming Zhang, Nenghai Yu. (2024)<br><strong>Towards Generalist Prompting for Large Language Models by Mental Models</strong><br><button class=copy-to-clipboard title="Towards Generalist Prompting for Large Language Models by Mental Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Knowledge Distillation, Zero-shot, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18252v1.pdf filename=2402.18252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed <b>prompting</b> methods are still needed. These methods either rely on task-specific <b>few-shot</b> examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist <b>prompting,</b> which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of <b>prompts</b> tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative <b>prompting</b> method that is simple-designed yet effectively fulfills the criteria of generalist <b>prompting.</b> MeMo <b>distills</b> the cores of various <b>prompting</b> methods into individual mental models and allows <b>LLMs</b> to autonomously select the most suitable mental models for the problem, achieving or being near to the state-of-the-art results on diverse tasks such as STEM, logical <b>reasoning,</b> and <b>commonsense</b> <b>reasoning</b> in <b>zero-shot</b> settings. We hope that the insights presented herein will stimulate further exploration of generalist <b>prompting</b> methods for <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=764--7286-evaluating-quantized-large-language-models-shiyao-li-et-al-2024>(7/64 | 7/286) Evaluating Quantized Large Language Models (Shiyao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang. (2024)<br><strong>Evaluating Quantized Large Language Models</strong><br><button class=copy-to-clipboard title="Evaluating Quantized Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Quantization, Quantization, Recommendation, Mistral, falcon, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18158v1.pdf filename=2402.18158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-training <b>quantization</b> (PTQ) has emerged as a promising technique to reduce the cost of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in <b>LLMs.</b> To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of <b>quantized</b> <b>LLMs</b> is essential to guide the selection of <b>quantization</b> methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, <b>Falcon,</b> Bloomz, <b>Mistral,</b> ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) <b>quantization</b> methods to demonstrate their applicability. Based on the extensive experiments, we systematically <b>summarize</b> the effect of <b>quantization,</b> provide <b>recommendations</b> to apply <b>quantization</b> techniques, and point out future directions.</p></p class="citation"></blockquote><h3 id=864--8286-hire-a-linguist-learning-endangered-languages-with-in-context-linguistic-descriptions-kexun-zhang-et-al-2024>(8/64 | 8/286) Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions (Kexun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang, Lei Li. (2024)<br><strong>Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions</strong><br><button class=copy-to-clipboard title="Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Low-Resource, GPT, GPT-4, BLEU, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18025v1.pdf filename=2402.18025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> process and translate endangered languages? Many languages lack a <b>large</b> <b>corpus</b> <b>to</b> train a decent <b>LLM;</b> therefore existing <b>LLMs</b> rarely perform well in unseen, endangered languages. On the contrary, we observe that 2000 endangered languages, though without a <b>large</b> <b>corpus,</b> <b>have</b> a grammar book or a dictionary. We propose LINGOLLM, a training-free approach to enable an <b>LLM</b> to process unseen languages that hardly occur in its pre-training. Our key insight is to demonstrate linguistic knowledge of an unseen language in an <b>LLM&rsquo;s</b> <b>prompt,</b> including a dictionary, a grammar book, and morphologically analyzed input text. We implement LINGOLLM on top of two models, <b>GPT-4</b> and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or <b>low-resource</b> languages. Our results show that LINGOLLM elevates translation capability from <b>GPT-4&rsquo;s</b> 0 to 10.5 <b>BLEU</b> for 10 language directions. Our findings demonstrate the tremendous value of linguistic knowledge in the age of <b>LLMs</b> for endangered languages. Our data, code, and model generations can be found at <a href=https://github.com/LLiLab/llm4endangeredlang>https://github.com/LLiLab/llm4endangeredlang</a>.</p></p class="citation"></blockquote><h3 id=964--9286-meta-task-prompting-elicits-embedding-from-large-language-models-yibin-lei-et-al-2024>(9/64 | 9/286) Meta-Task Prompting Elicits Embedding from Large Language Models (Yibin Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, Andrew Yates. (2024)<br><strong>Meta-Task Prompting Elicits Embedding from Large Language Models</strong><br><button class=copy-to-clipboard title="Meta-Task Prompting Elicits Embedding from Large Language Models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Unsupervised Learning, Sentence Embedding, Large Language Model, Large Language Model, Prompt, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18458v1.pdf filename=2402.18458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce a new <b>unsupervised</b> embedding method, Meta-Task <b>Prompting</b> with Explicit One-Word Limitation (MetaEOL), for generating high-quality <b>sentence</b> <b>embeddings</b> from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> without the need for model <b>fine-tuning</b> or task-specific engineering. Leveraging meta-task <b>prompting,</b> MetaEOL guides <b>LLMs</b> to produce embeddings through a series of carefully designed <b>prompts</b> that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) <b>benchmarks</b> and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new <b>scaling</b> <b>law</b> for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse <b>sentence-centric</b> <b>scenarios.</b></p></p class="citation"></blockquote><h3 id=1064--10286-decomposed-prompting-unveiling-multilingual-linguistic-structure-knowledge-in-english-centric-large-language-models-ercong-nie-et-al-2024>(10/64 | 10/286) Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models (Ercong Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ercong Nie, Shuzhou Yuan, Bolei Ma, Helmut Schmid, Michael Färber, Frauke Kreuter, Hinrich Schütze. (2024)<br><strong>Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models</strong><br><button class=copy-to-clipboard title="Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, GPT, GPT-3, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18397v1.pdf filename=2402.18397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the predominance of English in their training data, English-centric <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-3</b> and <b>LLaMA</b> display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed <b>prompting</b> approach to probe the linguistic structure understanding of these <b>LLMs</b> in sequence labeling tasks. Diverging from the single text-to-text <b>prompt,</b> our method generates for each token of the input sentence an individual <b>prompt</b> which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual <b>LLMs.</b> Our findings show that decomposed <b>prompting</b> surpasses the iterative <b>prompting</b> baseline in efficacy and efficiency under zero- and <b>few-shot</b> settings. Further analysis reveals the influence of evaluation methods and the use of instructions in <b>prompts.</b> Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric <b>LLMs,</b> contributing to the understanding of their multilingual linguistic knowledge.</p></p class="citation"></blockquote><h3 id=1164--11286-benchmarking-large-language-models-on-answering-and-explaining-challenging-medical-questions-hanjie-chen-et-al-2024>(11/64 | 11/286) Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions (Hanjie Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze. (2024)<br><strong>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</strong><br><button class=copy-to-clipboard title="Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18060v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18060v2.pdf filename=2402.18060v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have demonstrated impressive performance in answering medical <b>questions,</b> <b>such</b> as passing scores on medical licensing examinations. However, medical board exam <b>questions</b> <b>or</b> general clinical <b>questions</b> <b>do</b> not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the <b>reasoning</b> of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of <b>questions</b> <b>based</b> on challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 style clinical <b>questions.</b> <b>Both</b> datasets are structured as multiple-choice <b>question-answering</b> <b>tasks,</b> where each <b>question</b> <b>is</b> accompanied by an expert-written explanation. We evaluate four <b>LLMs</b> on the two datasets using various <b>prompts.</b> Experiments demonstrate that our datasets are harder than previous <b>benchmarks.</b> The inconsistency between automatic and human evaluations of model-generated explanations highlights the need to develop new metrics to support future research on explainable medical <b>QA.</b></p></p class="citation"></blockquote><h3 id=1264--12286-fofo-a-benchmark-to-evaluate-llms-format-following-capability-congying-xia-et-al-2024>(12/64 | 12/286) FOFO: A Benchmark to Evaluate LLMs&rsquo; Format-Following Capability (Congying Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, Caiming Xiong. (2024)<br><strong>FOFO: A Benchmark to Evaluate LLMs&rsquo; Format-Following Capability</strong><br><button class=copy-to-clipboard title="FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-4, Gemini, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18667v1.pdf filename=2402.18667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents FoFo, a pioneering <b>benchmark</b> for evaluating <b>large</b> <b>language</b> <b>models&rsquo;</b> <b>(LLMs)</b> ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite <b>LLMs&rsquo;</b> advancements, existing <b>benchmarks</b> fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., <b>Llama</b> 2, WizardLM) and closed-source (e.g., <b>GPT-4,</b> PALM2, <b>Gemini)</b> <b>LLMs</b> highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; <b>LLMs&rsquo;</b> format-following performance is independent of their content generation quality; and <b>LLMs&rsquo;</b> format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo&rsquo;s role in guiding the selection of domain-specific AI agents. FoFo is released here at <a href=https://github.com/SalesforceAIResearch/FoFo>https://github.com/SalesforceAIResearch/FoFo</a>.</p></p class="citation"></blockquote><h3 id=1364--13286-the-first-place-solution-of-wsdm-cup-2024-leveraging-large-language-models-for-conversational-multi-doc-qa-yiming-li-et-al-2024>(13/64 | 13/286) The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA (Yiming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Li, Zhao Zhang. (2024)<br><strong>The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA</strong><br><button class=copy-to-clipboard title="The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Natural Language Understanding, Question Answering, Question Answering, Large Language Model, Large Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18385v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18385v1.pdf filename=2402.18385v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational multi-doc <b>question</b> <b>answering</b> aims to answer specific <b>questions</b> <b>based</b> on the retrieved documents as well as the contextual conversations. In this paper, we introduce our winning approach for the &ldquo;Conversational Multi-Doc <b>QA&rdquo;</b> challenge in WSDM Cup 2024, which exploits the superior <b>natural</b> <b>language</b> <b>understanding</b> and generation capability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We first adapt <b>LLMs</b> to the task, then devise a hybrid training strategy to make the most of in-domain unlabeled data. Moreover, an advanced <b>text</b> <b>embedding</b> model is adopted to filter out potentially irrelevant documents and several approaches are designed and compared for the model ensemble. Equipped with all these techniques, our solution finally ranked 1st place in WSDM Cup 2024, surpassing its rivals to a <b>large</b> <b>extent.</b> <b>The</b> source codes have been released at <a href=https://github.com/zhangzhao219/WSDM-Cup-2024>https://github.com/zhangzhao219/WSDM-Cup-2024</a>.</p></p class="citation"></blockquote><h3 id=1464--14286-verifiner-verification-augmented-ner-via-knowledge-grounded-reasoning-with-large-language-models-seoyeon-kim-et-al-2024>(14/64 | 14/286) VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models (Seoyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee. (2024)<br><strong>VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Low-Resource, Out-of-domain, Named Entity Recognition, Named Entity Recognition, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18374v1.pdf filename=2402.18374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent approaches in domain-specific <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER),</b> such as biomedical <b>NER,</b> have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing <b>NER</b> methods using knowledge and revises them into more faithful predictions. Our framework leverages the <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models</b> to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on <b>out-of-domain</b> and <b>low-resource</b> settings show the usefulness of VerifiNER on real-world applications.</p></p class="citation"></blockquote><h3 id=1564--15286-how-to-think-step-by-step-a-mechanistic-understanding-of-chain-of-thought-reasoning-subhabrata-dutta-et-al-2024>(15/64 | 15/286) How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning (Subhabrata Dutta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty. (2024)<br><strong>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</strong><br><button class=copy-to-clipboard title="How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: LLaMA, Reasoning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18312v1.pdf filename=2402.18312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite superior <b>reasoning</b> prowess demonstrated by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with Chain-of-Thought (CoT) <b>prompting,</b> a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within <b>LLMs</b> that manifest CoT <b>reasoning</b> from a mechanistic point of view. From an analysis of <b>LLaMA-2</b> 7B applied to multistep <b>reasoning</b> over fictional ontologies, we demonstrate that <b>LLMs</b> deploy multiple parallel pathways of answer generation for step-by-step <b>reasoning.</b> These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a striking functional rift in the middle layers of the <b>LLM.</b> Token representations in the initial half remain strongly biased towards the pretraining prior, with the <b>in-context</b> taking over abruptly in the later half. This internal phase shift manifests in different functional components: attention heads that write the answer token predominantly appear in the later half, attention heads that move information along ontological relationships appear exclusively in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT <b>reasoning</b> in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1664--16286-reslora-identity-residual-mapping-in-low-rank-adaption-shuhua-shi-et-al-2024>(16/64 | 16/286) ResLoRA: Identity Residual Mapping in Low-Rank Adaption (Shuhua Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang. (2024)<br><strong>ResLoRA: Identity Residual Mapping in Low-Rank Adaption</strong><br><button class=copy-to-clipboard title="ResLoRA: Identity Residual Mapping in Low-Rank Adaption" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Natural Language Generation, Text2image, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18039v1.pdf filename=2402.18039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As one of the most popular parameter-efficient <b>fine-tuning</b> (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to <b>fine-tune</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on <b>NLG,</b> NLU, and <b>text-to-image</b> tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at <a href=https://github.com/microsoft/LMOps/tree/main/reslora>https://github.com/microsoft/LMOps/tree/main/reslora</a> .</p></p class="citation"></blockquote><h3 id=1764--17286-gradient-free-adaptive-global-pruning-for-pre-trained-language-models-guangji-bai-et-al-2024>(17/64 | 17/286) Gradient-Free Adaptive Global Pruning for Pre-trained Language Models (Guangji Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, Liang Zhao. (2024)<br><strong>Gradient-Free Adaptive Global Pruning for Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="Gradient-Free Adaptive Global Pruning for Pre-trained Language Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Pruning, GPT, LLaMA, Large Language Model, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17946v1.pdf filename=2402.17946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The transformative impact of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>LLaMA</b> and <b>GPT</b> on natural language processing is countered by their prohibitive computational demands. <b>Pruning</b> has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global <b>pruning</b> is impractical for <b>LLMs</b> due to scalability issues, while local <b>pruning,</b> despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose Adaptive Global <b>Pruning</b> (AdaGP), a novel framework that redefines the global <b>pruning</b> process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. AdaGP&rsquo;s approach, which conceptualizes <b>LLMs</b> as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on <b>LLMs</b> but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=1864--18286-miko-multimodal-intention-knowledge-distillation-from-large-language-models-for-social-media-commonsense-discovery-feihong-lu-et-al-2024>(18/64 | 18/286) MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery (Feihong Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feihong Lu, Weiqi Wang, Yangyifei Luo, Ziqin Zhu, Qingyun Sun, Baixuan Xu, Haochen Shi, Shiqi Gao, Qian Li, Yangqiu Song, Jianxin Li. (2024)<br><strong>MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery</strong><br><button class=copy-to-clipboard title="MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18169v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18169v2.pdf filename=2402.18169v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media has become a ubiquitous tool for connecting with others, staying updated with news, expressing opinions, and finding entertainment. However, understanding the intention behind social media posts remains challenging due to the implicitness of intentions in social media posts, the need for cross-modality understanding of both text and images, and the presence of noisy information such as hashtags, misspelled words, and complicated abbreviations. To address these challenges, we present MIKO, a <b>Multimodal</b> Intention Kowledge <b>DistillatiOn</b> framework that collaboratively leverages a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> and a <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Model</b> (MLLM) to uncover users&rsquo; intentions. Specifically, we use an MLLM to interpret the image and an <b>LLM</b> to extract key information from the text and finally instruct the <b>LLM</b> again to generate intentions. By applying MIKO to publicly available social media datasets, we construct an intention <b>knowledge</b> <b>base</b> featuring 1,372K intentions rooted in 137,287 posts. We conduct a two-stage annotation to verify the quality of the generated <b>knowledge</b> <b>and</b> <b>benchmark</b> the performance of widely used <b>LLMs</b> for intention generation. We further apply MIKO to a sarcasm detection dataset and <b>distill</b> a student model to demonstrate the downstream benefits of applying intention knowledge.</p></p class="citation"></blockquote><h3 id=1964--19286-leveraging-diverse-modeling-contexts-with-collaborating-learning-for-neural-machine-translation-yusheng-liao-et-al-2024>(19/64 | 19/286) Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation (Yusheng Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusheng Liao, Yanfeng Wang, Yu Wang. (2024)<br><strong>Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation</strong><br><button class=copy-to-clipboard title="Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Contrastive Learning, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18428v1.pdf filename=2402.18428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive (AR) and Non-autoregressive (NAR) models are two types of generative models for <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT).</b> AR models predict tokens in a word-by-word manner and can effectively capture the distribution of real translations. NAR models predict tokens by extracting bidirectional contextual information which can improve the inference speed but they suffer from performance degradation. Previous works utilized AR models to enhance NAR models by reducing the training data&rsquo;s complexity or incorporating the global information into AR models by virtue of NAR models. However, those investigated methods only take advantage of the contextual information of a single type of model while neglecting the diversity in the contextual information that can be provided by different types of models. In this paper, we propose a novel generic collaborative learning method, DCMCL, where AR and NAR models are treated as collaborators instead of teachers and students. To hierarchically leverage the bilateral contextual information, token-level mutual learning and sequence-level <b>contrastive</b> <b>learning</b> are adopted between AR and NAR models. Extensive experiments on four widely used <b>benchmarks</b> show that the proposed DCMCL method can simultaneously improve both AR and NAR models with up to 1.38 and 2.98 <b>BLEU</b> scores respectively, and can also outperform the current best-unified model with up to 0.97 <b>BLEU</b> scores for both AR and NAR decoding.</p></p class="citation"></blockquote><h3 id=2064--20286-challenges-in-pre-training-graph-neural-networks-for-context-based-fake-news-detection-an-evaluation-of-current-strategies-and-resource-limitations-gregor-donabauer-et-al-2024>(20/64 | 20/286) Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations (Gregor Donabauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregor Donabauer, Udo Kruschwitz. (2024)<br><strong>Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations</strong><br><button class=copy-to-clipboard title="Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transfer Learning, Fake News Detection, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18179v1.pdf filename=2402.18179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-training of neural networks has recently revolutionized the field of Natural Language Processing (NLP) and has before demonstrated its effectiveness in computer vision. At the same time, advances around the detection of <b>fake</b> <b>news</b> <b>were</b> mainly driven by the context-based paradigm, where different types of signals (e.g. from social media) form <b>graph-like</b> <b>structures</b> <b>that</b> hold contextual information apart from the news article to classify. We propose to merge these two developments by applying pre-training of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> in the domain of context-based <b>fake</b> <b>news</b> <b>detection.</b> Our experiments provide an evaluation of different pre-training strategies for <b>graph-based</b> <b>misinformation</b> <b>detection</b> and demonstrate that <b>transfer</b> <b>learning</b> does currently not lead to significant improvements over training a model from scratch in the domain. We argue that a major current issue is the lack of suitable large-scale resources that can be used for pre-training.</p></p class="citation"></blockquote><h3 id=2164--21286-cause-and-effect-can-large-language-models-truly-understand-causality-swagata-ashwani-et-al-2024>(21/64 | 21/286) Cause and Effect: Can Large Language Models Truly Understand Causality? (Swagata Ashwani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swagata Ashwani, Kshiteesh Hegde, Nishith Reddy Mannuru, Mayank Jindal, Dushyant Singh Sengar, Krishna Chaitanya Rao Kathala, Dishant Banga, Vinija Jain, Aman Chadha. (2024)<br><strong>Cause and Effect: Can Large Language Models Truly Understand Causality?</strong><br><button class=copy-to-clipboard title="Cause and Effect: Can Large Language Models Truly Understand Causality?" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Counter-factual, Counterfactual Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18139v1.pdf filename=2402.18139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of <b>Large</b> <b>Language</b> <b>Models(LLMs),</b> it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal <b>reasoning,</b> yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware <b>Reasoning</b> Enhancement with <b>Counterfactual</b> <b>Analysis(CARE</b> CA) framework to enhance causal <b>reasoning</b> and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and <b>counterfactual</b> <b>statements,</b> as well as implicit causal detection through <b>LLMs.</b> Our framework goes one step further with a layer of <b>counterfactual</b> <b>explanations</b> to accentuate <b>LLMs</b> understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal <b>reasoning</b> tasks such as causal discovery, causal identification and <b>counterfactual</b> <b>reasoning.</b> The <b>counterfactual</b> <b>sentences</b> add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of <b>benchmark</b> datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.</p></p class="citation"></blockquote><h3 id=2264--22286-can-gpt-improve-the-state-of-prior-authorization-via-guideline-based-automated-question-answering-shubham-vatsal-et-al-2024>(22/64 | 22/286) Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering? (Shubham Vatsal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Vatsal, Ayush Singh, Shabnam Tafreshi. (2024)<br><strong>Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?</strong><br><button class=copy-to-clipboard title="Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering?" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, Language Generation, Natural Language Generation, Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18419v1.pdf filename=2402.18419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Health insurance companies have a defined process called prior authorization (PA) which is a health plan cost-control process that requires doctors and other healthcare professionals to get clearance in advance from a health plan before performing a particular procedure on a patient in order to be eligible for payment coverage. For health insurance companies, approving PA requests for patients in the medical domain is a time-consuming and challenging task. One of those key challenges is validating if a request matches up to certain criteria such as age, gender, etc. In this work, we evaluate whether <b>GPT</b> can validate numerous key factors, in turn helping health plans reach a decision drastically faster. We frame it as a <b>question</b> <b>answering</b> task, <b>prompting</b> <b>GPT</b> to answer a <b>question</b> <b>from</b> patient electronic health record. We experiment with different conventional <b>prompting</b> techniques as well as introduce our own novel <b>prompting</b> technique. Moreover, we report qualitative assessment by humans on the <b>natural</b> <b>language</b> <b>generation</b> outputs from our approach. Results show that our method achieves superior performance with the mean weighted F1 score of 0.61 as compared to its standard counterparts.</p></p class="citation"></blockquote><h3 id=2364--23286-small-but-funny-a-feedback-driven-approach-to-humor-distillation-sahithya-ravi-et-al-2024>(23/64 | 23/286) Small But Funny: A Feedback-Driven Approach to Humor Distillation (Sahithya Ravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Aditya Sagar, Ahmed Aly, Vered Shwartz, Arash Einolghozati. (2024)<br><strong>Small But Funny: A Feedback-Driven Approach to Humor Distillation</strong><br><button class=copy-to-clipboard title="Small But Funny: A Feedback-Driven Approach to Humor Distillation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Language Generation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18113v1.pdf filename=2402.18113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has brought to light promising <b>language</b> <b>generation</b> capabilities, particularly in performing tasks like complex <b>reasoning</b> and creative writing. Consequently, <b>distillation</b> through imitation of teacher responses has emerged as a popular technique to transfer knowledge from <b>LLMs</b> to more accessible, Small <b>Language</b> <b>Models</b> (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate <b>language</b> <b>comprehension</b> and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the <b>LLM</b> - as a &ldquo;teacher&rdquo; generating data, as well as a &ldquo;critic&rdquo; evaluating the student&rsquo;s performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex <b>language</b> <b>abilities</b> via <b>distillation.</b></p></p class="citation"></blockquote><h3 id=2464--24286-on-the-use-of-silver-standard-data-for-zero-shot-classification-tasks-in-information-extraction-jianwei-wang-et-al-2024>(24/64 | 24/286) On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction (Jianwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianwei Wang, Tianyin Wang, Ziqian Zeng. (2024)<br><strong>On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction</strong><br><button class=copy-to-clipboard title="On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Supervised Learning, Zero-shot, Information Retrieval, Textual Entailment<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18061v1.pdf filename=2402.18061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The superior performance of <b>supervised</b> classification methods in the <b>information</b> <b>extraction</b> (IE) area heavily relies on a large amount of gold standard data. Recent <b>zero-shot</b> classification methods converted the task to other NLP tasks (e.g., <b>textual</b> <b>entailment)</b> and used off-the-shelf models of these NLP tasks to directly perform inference on the test data without using a large amount of IE annotation data. A potentially valuable by-product of these methods is the large-scale silver standard data, i.e., pseudo-labeled data by the off-the-shelf models of other NLP tasks. However, there is no further investigation into the use of these data. In this paper, we propose a new framework, Clean-LaVe, which aims to utilize silver standard data to enhance the <b>zero-shot</b> performance. Clean-LaVe includes four phases: (1) Obtaining silver data; (2) Identifying relatively clean data from silver data; (3) <b>Finetuning</b> the off-the-shelf model using clean data; (4) Inference on the test data. The experimental results show that Clean-LaVe can outperform the baseline by 5% and 6% on TACRED and Wiki80 dataset in the <b>zero-shot</b> relation classification task, and by 3%-7% on Smile (Korean and Polish) in the <b>zero-shot</b> cross-lingual relation classification task, and by 8% on ACE05-E+ in the <b>zero-shot</b> event argument classification task. The code is share in <a href=https://github.com/wjw136/Clean_LaVe.git>https://github.com/wjw136/Clean_LaVe.git</a>.</p></p class="citation"></blockquote><h3 id=2564--25286-exploring-multi-document-information-consolidation-for-scientific-sentiment-summarization-miao-li-et-al-2024>(25/64 | 25/286) Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization (Miao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miao Li, Jey Han Lau, Eduard Hovy. (2024)<br><strong>Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization</strong><br><button class=copy-to-clipboard title="Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Language Generation, Natural Language Generation, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18005v1.pdf filename=2402.18005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>natural</b> <b>language</b> <b>generation</b> systems with <b>LLMs</b> exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment <b>summarization</b> more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as <b>prompts</b> for <b>LLMs</b> to generate meta-reviews in extensive experiments.</p></p class="citation"></blockquote><h3 id=2664--26286-focus-on-your-question-interpreting-and-mitigating-toxic-cot-problems-in-commonsense-reasoning-jiachun-li-et-al-2024>(26/64 | 26/286) Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning (Jiachun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao. (2024)<br><strong>Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning</strong><br><button class=copy-to-clipboard title="Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18344v1.pdf filename=2402.18344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> exhibit high-level <b>commonsense</b> <b>reasoning</b> abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the <b>LLM</b> during CoT <b>reasoning.</b> Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple <b>commonsense</b> <b>reasoning</b> <b>benchmarks,</b> we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model&rsquo;s overall <b>commonsense</b> <b>reasoning</b> performance (increased by 5.5%).</p></p class="citation"></blockquote><h3 id=2764--27286-clustering-and-ranking-diversity-preserved-instruction-selection-through-expert-aligned-quality-estimation-yuan-ge-et-al-2024>(27/64 | 27/286) Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation (Yuan Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Hao Yang, Tong Xiao. (2024)<br><strong>Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation</strong><br><button class=copy-to-clipboard title="Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Alpaca, GPT, GPT-4, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18191v1.pdf filename=2402.18191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With contributions from the open-source community, a vast amount of <b>instruction</b> <b>tuning</b> (IT) data has emerged. Given the significant resource allocation required by training and evaluating models, it is advantageous to have an efficient method for selecting high-quality IT data. However, existing methods for <b>instruction</b> <b>data</b> selection have limitations such as relying on fragile external APIs, being affected by biases in <b>GPT</b> models, or reducing the diversity of the selected <b>instruction</b> <b>dataset.</b> In this paper, we propose an industrial-friendly, expert-aligned and diversity-preserved <b>instruction</b> <b>data</b> selection method: <b>Clustering</b> and Ranking (CaR). CaR consists of two steps. The first step involves ranking <b>instruction</b> <b>pairs</b> using a scoring model that is well aligned with expert preferences (achieving an accuracy of 84.25%). The second step involves preserving dataset diversity through a <b>clustering</b> process.In our experiment, CaR selected a subset containing only 1.96% of <b>Alpaca&rsquo;s</b> IT data, yet the underlying AlpaCaR model trained on this subset outperforms <b>Alpaca</b> by an average of 32.1% in <b>GPT-4</b> evaluations. Furthermore, our method utilizes small models (355M parameters) and requires only 11.2% of the monetary cost compared to existing methods, making it easily deployable in industrial scenarios.</p></p class="citation"></blockquote><h3 id=2864--28286-newsqs-multi-source-question-generation-for-the-inquiring-mind-alyssa-hwang-et-al-2024>(28/64 | 28/286) NewsQs: Multi-Source Question Generation for the Inquiring Mind (Alyssa Hwang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alyssa Hwang, Kalpit Dixit, Miguel Ballesteros, Yassine Benajiba, Vittorio Castelli, Markus Dreyer, Mohit Bansal, Kathleen McKeown. (2024)<br><strong>NewsQs: Multi-Source Question Generation for the Inquiring Mind</strong><br><button class=copy-to-clipboard title="NewsQs: Multi-Source Question Generation for the Inquiring Mind" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, T5, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18479v1.pdf filename=2402.18479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document <b>summarization</b> dataset with questions automatically generated by a <b>T5-Large</b> model <b>fine-tuned</b> on FAQ-style news articles from the News On the Web corpus. We show that <b>fine-tuning</b> a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document <b>summarization.</b></p></p class="citation"></blockquote><h3 id=2964--29286-characterizing-truthfulness-in-large-language-model-generations-with-local-intrinsic-dimension-fan-yin-et-al-2024>(29/64 | 29/286) Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension (Fan Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Yin, Jayanth Srinivasa, Kai-Wei Chang. (2024)<br><strong>Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension</strong><br><button class=copy-to-clipboard title="Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18048v1.pdf filename=2402.18048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study how to characterize and predict the truthfulness of texts generated from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which serves as a crucial step in building trust between humans and <b>LLMs.</b> Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with <b>LLMs.</b> In this paper, we suggest investigating internal activations and quantifying <b>LLM&rsquo;s</b> truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four <b>question</b> <b>answering</b> <b>(QA)</b> datasets, we demonstrate the effectiveness ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally, we study intrinsic dimensions in <b>LLMs</b> and their relations with model layers, autoregressive language modeling, and the training of <b>LLMs,</b> revealing that intrinsic dimensions can be a powerful approach to understanding <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3064--30286-a-survey-on-recent-advances-in-llm-based-multi-turn-dialogue-systems-zihao-yi-et-al-2024>(30/64 | 30/286) A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems (Zihao Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, Ying Shen. (2024)<br><strong>A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</strong><br><button class=copy-to-clipboard title="A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Dialogue System, Open-Domain Dialogue, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18013v1.pdf filename=2402.18013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This survey provides a comprehensive review of research on multi-turn <b>dialogue</b> <b>systems,</b> with a particular focus on multi-turn <b>dialogue</b> <b>systems</b> based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> This paper aims to (a) give a summary of existing <b>LLMs</b> and approaches for adapting <b>LLMs</b> to downstream tasks; (b) elaborate recent advances in multi-turn <b>dialogue</b> <b>systems,</b> covering both <b>LLM-based</b> <b>open-domain</b> <b>dialogue</b> <b>(ODD)</b> and task-oriented <b>dialogue</b> <b>(TOD)</b> systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of <b>LLMs</b> and the increasing demands on multi-turn <b>dialogue</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=3164--31286-collaborative-decoding-of-critical-tokens-for-boosting-factuality-of-large-language-models-lifeng-jin-et-al-2024>(31/64 | 31/286) Collaborative decoding of critical tokens for boosting factuality of large language models (Lifeng Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lifeng Jin, Baolin Peng, Linfeng Song, Haitao Mi, Ye Tian, Dong Yu. (2024)<br><strong>Collaborative decoding of critical tokens for boosting factuality of large language models</strong><br><button class=copy-to-clipboard title="Collaborative decoding of critical tokens for boosting factuality of large language models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17982v1.pdf filename=2402.17982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The most common training pipeline for <b>large</b> <b>language</b> <b>models</b> includes pretraining, <b>finetuning</b> and aligning phases, with their respective resulting models, such as the pretrained model and the <b>finetuned</b> model. <b>Finetuned</b> and aligned models show improved abilities of <b>instruction</b> <b>following</b> and safe generation, however their abilities to stay factual about the world are impacted by the <b>finetuning</b> process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.</p></p class="citation"></blockquote><h3 id=3264--32286-fine-tuned-machine-translation-metrics-struggle-in-unseen-domains-vilém-zouhar-et-al-2024>(32/64 | 32/286) Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains (Vilém Zouhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vilém Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, Brian Thompson. (2024)<br><strong>Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains</strong><br><button class=copy-to-clipboard title="Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18747v1.pdf filename=2402.18747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether <b>machine</b> <b>translation</b> <b>(MT)</b> metrics which are <b>fine-tuned</b> on human-generated <b>MT</b> quality judgements are robust to domain shifts between training and inference. We find that <b>fine-tuned</b> metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not <b>fine-tuned</b> on <b>MT</b> quality judgments.</p></p class="citation"></blockquote><h3 id=3364--33286-learning-to-compress-prompt-in-natural-language-formats-yu-neng-chuang-et-al-2024>(33/64 | 33/286) Learning to Compress Prompt in Natural Language Formats (Yu-Neng Chuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. (2024)<br><strong>Learning to Compress Prompt in Natural Language Formats</strong><br><button class=copy-to-clipboard title="Learning to Compress Prompt in Natural Language Formats" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18700v1.pdf filename=2402.18700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying <b>LLMs</b> with precise and informative context helps users process <b>large-scale</b> <b>datasets</b> <b>more</b> effectively and cost-efficiently. Existing works rely on compressing long <b>prompt</b> contexts into soft <b>prompts.</b> However, soft <b>prompt</b> compression encounters limitations in transferability across different <b>LLMs,</b> especially API-based <b>LLMs.</b> To this end, this work aims to compress lengthy <b>prompts</b> in the form of natural language with <b>LLM</b> transferability. This poses two challenges: (i) Natural Language (NL) <b>prompts</b> are incompatible with back-propagation, and (ii) NL <b>prompts</b> lack flexibility in imposing length constraints. In this work, we propose a Natural Language <b>Prompt</b> Encapsulation (Nano-Capsulator) framework compressing original <b>prompts</b> into NL formatted Capsule <b>Prompt</b> while maintaining the <b>prompt</b> utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule <b>Prompt</b> can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse <b>LLMs</b> and different datasets.</p></p class="citation"></blockquote><h3 id=3464--34286-simple-linear-attention-language-models-balance-the-recall-throughput-tradeoff-simran-arora-et-al-2024>(34/64 | 34/286) Simple linear attention language models balance the recall-throughput tradeoff (Simran Arora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher Ré. (2024)<br><strong>Simple linear attention language models balance the recall-throughput tradeoff</strong><br><button class=copy-to-clipboard title="Simple linear attention language models balance the recall-throughput tradeoff" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Language Generation, In-context Learning, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18668v1.pdf filename=2402.18668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown that attention-based <b>language</b> <b>models</b> excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache&rsquo;s aggressive memory consumption. In this work, we explore whether we can improve <b>language</b> <b>model</b> efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model&rsquo;s state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train <b>language</b> <b>models</b> up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in <b>perplexity</b> and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on <b>language</b> <b>generation</b> than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: <a href=https://github.com/HazyResearch/based>https://github.com/HazyResearch/based</a>.</p></p class="citation"></blockquote><h3 id=3564--35286-beyond-natural-language-llms-leveraging-alternative-formats-for-enhanced-reasoning-and-communication-weize-chen-et-al-2024>(35/64 | 35/286) Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication (Weize Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication</strong><br><button class=copy-to-clipboard title="Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18439v1.pdf filename=2402.18439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Yet, besides NL, <b>LLMs</b> have seen various non-NL formats during pre-training, such as code and logical expression. NL&rsquo;s status as the optimal format for <b>LLMs,</b> particularly in single-LLM <b>reasoning</b> and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing <b>LLMs</b> to autonomously select the most suitable format before <b>reasoning</b> or communicating leads to a 3.3 to 5.7% improvement in <b>reasoning</b> efficiency for different <b>LLMs,</b> and up to a 72.7% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that <b>LLMs</b> can devise a format from limited task instructions and that the devised format is effectively transferable across different <b>LLMs.</b> Intriguingly, the structured communication format decided by <b>LLMs</b> exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication. Our code is released at \url{https://github.com/thunlp/AutoForm}.</p></p class="citation"></blockquote><h3 id=3664--36286-emotion-classification-in-low-and-moderate-resource-languages-shabnam-tafreshi-et-al-2024>(36/64 | 36/286) Emotion Classification in Low and Moderate Resource Languages (Shabnam Tafreshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shabnam Tafreshi, Shubham Vatsal, Mona Diab. (2024)<br><strong>Emotion Classification in Low and Moderate Resource Languages</strong><br><button class=copy-to-clipboard title="Emotion Classification in Low and Moderate Resource Languages" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: High-Resource, Low-Resource, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18424v1.pdf filename=2402.18424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for <b>low-resource</b> and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and <b>transfer</b> <b>the</b> learning to low and moderate resource languages. We compare and contrast two approaches of <b>transfer</b> <b>learning</b> from a <b>high-resource</b> language to a low or moderate-resource language. One approach projects the annotation from a <b>high-resource</b> language to low and moderate-resource language in parallel corpora and the other one uses direct <b>transfer</b> <b>from</b> <b>high-resource</b> language to the other languages. We show the efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano, Odia, and Azerbaijani. Our results indicate that our approaches outperform random baselines and <b>transfer</b> <b>emotions</b> across languages successfully. For all languages, the direct cross-lingual <b>transfer</b> <b>of</b> emotion yields better results. We also create annotated emotion-labeled resources for four languages: Farsi, Azerbaijani, Ilocano and Odia.</p></p class="citation"></blockquote><h3 id=3764--37286-rethinking-the-bounds-of-llm-reasoning-are-multi-agent-discussions-the-key-qineng-wang-et-al-2024>(37/64 | 37/286) Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key? (Qineng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song. (2024)<br><strong>Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?</strong><br><button class=copy-to-clipboard title="Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18272v1.pdf filename=2402.18272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent progress in <b>LLMs</b> discussion suggests that multi-agent discussion improves the <b>reasoning</b> abilities of <b>LLMs.</b> In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent <b>LLM</b> with strong <b>prompts</b> can achieve almost the same performance as the best existing discussion approach on a wide range of <b>reasoning</b> tasks and backbone <b>LLMs.</b> We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the <b>prompt.</b> Further study reveals the common interaction mechanisms of <b>LLMs</b> during the discussion.</p></p class="citation"></blockquote><h3 id=3864--38286-llm-task-interference-an-initial-study-on-the-impact-of-task-switch-in-conversational-history-akash-gupta-et-al-2024>(38/64 | 38/286) LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History (Akash Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, Mario Fritz. (2024)<br><strong>LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History</strong><br><button class=copy-to-clipboard title="LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18216v1.pdf filename=2402.18216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the recent emergence of powerful instruction-tuned <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When <b>prompted</b> by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a task-switch. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational <b>LLMs</b> caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular <b>LLMs</b> reveal that many of the task-switches can lead to significant performance degradation.</p></p class="citation"></blockquote><h3 id=3964--39286-cutting-off-the-head-ends-the-conflict-a-mechanism-for-interpreting-and-mitigating-knowledge-conflicts-in-language-models-zhuoran-jin-et-al-2024>(39/64 | 39/286) Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models (Zhuoran Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao. (2024)<br><strong>Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models</strong><br><button class=copy-to-clipboard title="Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Pruning, Question Answering, Retrieval Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18154v1.pdf filename=2402.18154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>retrieval</b> <b>augmentation</b> and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Moreover, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by the insights, we propose a novel method called <b>Pruning</b> Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by <b>pruning</b> conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory ($\uparrow$ 44.0%) or external context ($\uparrow$ 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain <b>QA</b> tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method.</p></p class="citation"></blockquote><h3 id=4064--40286-learning-intrinsic-dimension-via-information-bottleneck-for-explainable-aspect-based-sentiment-analysis-zhenxiao-cheng-et-al-2024>(40/64 | 40/286) Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis (Zhenxiao Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenxiao Cheng, Jie Zhou, Wen Wu, Qin Chen, Liang He. (2024)<br><strong>Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Aspect-based Sentiment Analysis, Sentiment Analysis, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18145v1.pdf filename=2402.18145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient-based explanation methods are increasingly used to interpret neural models in natural language processing (NLP) due to their high fidelity. Such methods determine <b>word-level</b> <b>importance</b> using dimension-level gradient values through a norm function, often presuming equal significance for all gradient dimensions. However, in the context of <b>Aspect-based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA), our preliminary research suggests that only specific dimensions are pertinent. To address this, we propose the Information Bottleneck-based Gradient (\texttt{IBG}) explanation framework for ABSA. This framework leverages an information bottleneck to refine <b>word</b> <b>embeddings</b> into a concise intrinsic dimension, maintaining essential features and omitting unrelated information. Comprehensive tests show that our \texttt{IBG} approach considerably improves both the models&rsquo; performance and interpretability by identifying <b>sentiment-aware</b> <b>features.</b></p></p class="citation"></blockquote><h3 id=4164--41286-exploring-multilingual-human-value-concepts-in-large-language-models-is-value-alignment-consistent-transferable-and-controllable-across-languages-shaoyang-xu-et-al-2024>(41/64 | 41/286) Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages? (Shaoyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong. (2024)<br><strong>Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?</strong><br><button class=copy-to-clipboard title="Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18120v1.pdf filename=2402.18120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior research in representation engineering has revealed that <b>LLMs</b> encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in <b>LLMs.</b> Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 <b>LLM</b> series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in <b>LLMs.</b> Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and <b>low-resource</b> languages, all in terms of human value concepts. Additionally, we validate the feasibility of cross-lingual control over value alignment capabilities of <b>LLMs,</b> leveraging the dominant language as a source language. Drawing from our findings on multilingual value alignment, we prudently provide suggestions on the composition of multilingual data for <b>LLMs</b> pre-training: including a limited number of dominant languages for cross-lingual alignment transfer while avoiding their excessive prevalence, and keeping a balanced distribution of non-dominant languages. We aspire that our findings would contribute to enhancing the safety and utility of multilingual AI.</p></p class="citation"></blockquote><h3 id=4264--42286-datasets-for-large-language-models-a-comprehensive-survey-yang-liu-et-al-2024>(42/64 | 42/286) Datasets for Large Language Models: A Comprehensive Survey (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin. (2024)<br><strong>Datasets for Large Language Models: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Datasets for Large Language Models: A Comprehensive Survey" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18041v1.pdf filename=2402.18041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper embarks on an exploration into the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> datasets, which play a crucial role in the remarkable advancements of <b>LLMs.</b> The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of <b>LLMs.</b> Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of <b>LLM</b> datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of <b>LLM</b> datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction <b>Fine-tuning</b> Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of <b>LLM</b> text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: <a href=https://github.com/lmmlzn/Awesome-LLMs-Datasets>https://github.com/lmmlzn/Awesome-LLMs-Datasets</a>.</p></p class="citation"></blockquote><h3 id=4364--43286-multilingual-speech-models-for-automatic-speech-recognition-exhibit-gender-performance-gaps-giuseppe-attanasio-et-al-2024>(43/64 | 43/286) Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps (Giuseppe Attanasio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, Dirk Hovy. (2024)<br><strong>Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps</strong><br><button class=copy-to-clipboard title="Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17954v1.pdf filename=2402.17954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current voice recognition approaches use multi-task, multilingual models for <b>speech</b> <b>tasks</b> like <b>Automatic</b> <b>Speech</b> <b>Recognition</b> <b>(ASR)</b> to make them applicable to many languages without substantial changes. However, broad language coverage can still mask performance gaps within languages, for example, across genders. We systematically evaluate multilingual <b>ASR</b> systems on gendered performance gaps. Using two popular models on three datasets in 19 languages across seven language families, we find clear gender disparities. However, the advantaged group varies between languages. While there are no significant differences across groups in phonetic variables (pitch, speaking rate, etc.), probing the model&rsquo;s internal states reveals a negative correlation between probe performance and the gendered performance gap. I.e., the easier to distinguish speaker gender in a language, the more the models favor female speakers. Our results show that group disparities remain unsolved despite great progress on multi-tasking and multilinguality. We provide first valuable insights for evaluating gender gaps in multilingual <b>ASR</b> systems. We release all code and artifacts at <a href=https://github.com/g8a9/multilingual-asr-gender-gap>https://github.com/g8a9/multilingual-asr-gender-gap</a>.</p></p class="citation"></blockquote><h3 id=4464--44286-hierarchical-multimodal-pre-training-for-visually-rich-webpage-understanding-hongshen-xu-et-al-2024>(44/64 | 44/286) Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding (Hongshen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu, Kai Yu. (2024)<br><strong>Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding</strong><br><button class=copy-to-clipboard title="Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Information Retrieval, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18262v1.pdf filename=2402.18262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and <b>information</b> <b>extraction</b> across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human <b>information</b> <b>retrieval,</b> the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a <b>multimodal</b> pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at <a href=https://github.com/X-LANCE/weblm>https://github.com/X-LANCE/weblm</a>.</p></p class="citation"></blockquote><h3 id=4564--45286-m3-vrd-multimodal-multi-task-multi-teacher-visually-rich-form-document-understanding-yihao-ding-et-al-2024>(45/64 | 45/286) M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding (Yihao Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo Garza, Josiah Poon, Luca Cagliero. (2024)<br><strong>M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding</strong><br><button class=copy-to-clipboard title="M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 26<br>Keywords: Knowledge Distillation, Knowledge Distillation, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17983v1.pdf filename=2402.17983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a groundbreaking <b>multimodal,</b> multi-task, multi-teacher joint-grained <b>knowledge</b> <b>distillation</b> model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher <b>knowledge</b> <b>distillation</b> transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.</p></p class="citation"></blockquote><h3 id=4664--46286-towards-better-understanding-of-contrastive-sentence-representation-learning-a-unified-paradigm-for-gradient-mingxin-li-et-al-2024>(46/64 | 46/286) Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient (Mingxin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxin Li, Richong Zhang, Zhijie Nie. (2024)<br><strong>Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient</strong><br><button class=copy-to-clipboard title="Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 25<br>Keywords: Representation Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18281v1.pdf filename=2402.18281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence <b>Representation</b> <b>Learning</b> (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive <b>Self-Supervised</b> <b>Learning</b> (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL.</p></p class="citation"></blockquote><h3 id=4764--47286-how-much-annotation-is-needed-to-compare-summarization-models-chantal-shaib-et-al-2024>(47/64 | 47/286) How Much Annotation is Needed to Compare Summarization Models? (Chantal Shaib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chantal Shaib, Joe Barrow, Alexa F. Siu, Byron C. Wallace, Ani Nenkova. (2024)<br><strong>How Much Annotation is Needed to Compare Summarization Models?</strong><br><button class=copy-to-clipboard title="How Much Annotation is Needed to Compare Summarization Models?" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Sample Size, Text Generation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18756v1.pdf filename=2402.18756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern instruction-tuned models have become highly capable in <b>text</b> <b>generation</b> tasks such as <b>summarization,</b> and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing <b>summarization</b> model when applied to a new domain or purpose. In this work, we empirically investigate the test <b>sample</b> <b>size</b> necessary to select a preferred model in the context of news <b>summarization.</b> Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream <b>summarization</b> tasks. We find that, while automatic metrics are stable at smaller <b>sample</b> <b>sizes,</b> only some automatic metrics are able to moderately predict model win rates according to human preference.</p></p class="citation"></blockquote><h3 id=4864--48286-retrieval-based-full-length-wikipedia-generation-for-emergent-events-jiebin-zhang-et-al-2024>(48/64 | 48/286) Retrieval-based Full-length Wikipedia Generation for Emergent Events (Jiebin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, Sujian Li. (2024)<br><strong>Retrieval-based Full-length Wikipedia Generation for Emergent Events</strong><br><button class=copy-to-clipboard title="Retrieval-based Full-length Wikipedia Generation for Emergent Events" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18264v1.pdf filename=2402.18264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s fast-paced world, the growing demand to quickly generate comprehensive and accurate Wikipedia documents for emerging events is both crucial and challenging. However, previous efforts in Wikipedia generation have often fallen short of meeting real-world requirements. Some approaches focus solely on generating segments of a complete Wikipedia document, while others overlook the importance of faithfulness in generation or fail to consider the influence of the pre-training corpus. In this paper, we simulate a real-world scenario where structured full-length Wikipedia documents are generated for emergent events using input retrieved from web sources. To ensure that <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are not trained on corpora related to recently occurred events, we select events that have taken place recently and introduce a new <b>benchmark</b> Wiki-GenBen, which consists of 309 events paired with their corresponding retrieved web pages for generating evidence. Additionally, we design a comprehensive set of systematic evaluation metrics and baseline methods, to evaluate the capability of <b>LLMs</b> in generating factual full-length Wikipedia documents. The data and code are open-sourced at WikiGenBench.</p></p class="citation"></blockquote><h3 id=4964--49286-saving-the-legacy-of-hero-ibash-evaluating-four-language-models-for-aminoacian-yunze-xiao-et-al-2024>(49/64 | 49/286) Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian (Yunze Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunze Xiao, Yiyang Pan. (2024)<br><strong>Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian</strong><br><button class=copy-to-clipboard title="Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Low-Resource, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18121v1.pdf filename=2402.18121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study assesses four cutting-edge language models in the underexplored Aminoacian language. Through evaluation, it scrutinizes their adaptability, effectiveness, and limitations in <b>text</b> <b>generation,</b> semantic coherence, and contextual understanding. Uncovering insights into these models&rsquo; performance in a <b>low-resourced</b> language, this research pioneers pathways to bridge linguistic gaps. By offering <b>benchmarks</b> and understanding challenges, it lays groundwork for future advancements in natural language processing, aiming to elevate the applicability of language models in similar linguistic landscapes, marking a significant step toward inclusivity and progress in language technology.</p></p class="citation"></blockquote><h3 id=5064--50286-editing-factual-knowledge-and-explanatory-ability-of-medical-large-language-models-derong-xu-et-al-2024>(50/64 | 50/286) Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models (Derong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu, Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen. (2024)<br><strong>Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models</strong><br><button class=copy-to-clipboard title="Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18099v1.pdf filename=2402.18099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model editing aims to precisely modify the behaviours of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in <b>LLMs.</b> As a result, it can boost the application of <b>LLMs</b> in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of <b>LLMs.</b> These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two <b>benchmark</b> datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical <b>LLMs</b> demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited.</p></p class="citation"></blockquote><h3 id=5164--51286-large-language-models-and-games-a-survey-and-roadmap-roberto-gallotta-et-al-2024>(51/64 | 51/286) Large Language Models and Games: A Survey and Roadmap (Roberto Gallotta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis. (2024)<br><strong>Large Language Models and Games: A Survey and Roadmap</strong><br><button class=copy-to-clipboard title="Large Language Models and Games: A Survey and Roadmap" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18659v1.pdf filename=2402.18659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen an explosive increase in research on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> and accompanying public engagement on the topic. While starting as a niche area within natural language processing, <b>LLMs</b> have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of <b>LLMs</b> in and for games, and identifies the different roles <b>LLMs</b> can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of <b>LLMs</b> in games and we reconcile the potential and limitations of <b>LLMs</b> within the games domain. As the first comprehensive survey and roadmap at the intersection of <b>LLMs</b> and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.</p></p class="citation"></blockquote><h3 id=5264--52286-learning-or-self-aligning-rethinking-instruction-fine-tuning-mengjie-ren-et-al-2024>(52/64 | 52/286) Learning or Self-aligning? Rethinking Instruction Fine-tuning (Mengjie Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengjie Ren, Boxi Cao, Hongyu Lin, Liu Cao, Xianpei Han, Ke Zeng, Guanglu Wan, Xunliang Cai, Le Sun. (2024)<br><strong>Learning or Self-aligning? Rethinking Instruction Fine-tuning</strong><br><button class=copy-to-clipboard title="Learning or Self-aligning? Rethinking Instruction Fine-tuning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18243v1.pdf filename=2402.18243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction Fine-tuning~(IFT) is a critical phase in building large language models~(LLMs). Previous works mainly focus on the IFT&rsquo;s role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.</p></p class="citation"></blockquote><h3 id=5364--53286-dansk-and-dacy-260-domain-generalization-of-danish-named-entity-recognition-kenneth-enevoldsen-et-al-2024>(53/64 | 53/286) DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition (Kenneth Enevoldsen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kenneth Enevoldsen, Emil Trenckner Jessen, Rebekah Baglini. (2024)<br><strong>DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition</strong><br><button class=copy-to-clipboard title="DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18209v1.pdf filename=2402.18209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Named</b> <b>entity</b> <b>recognition</b> is one of the cornerstones of Danish NLP, essential for language technology applications within both industry and research. However, Danish <b>NER</b> is inhibited by a lack of available datasets. As a consequence, no current models are capable of fine-grained <b>named</b> <b>entity</b> <b>recognition,</b> nor have they been evaluated for potential generalizability issues across datasets and domains. To alleviate these limitations, this paper introduces: 1) DANSK: a <b>named</b> <b>entity</b> <b>dataset</b> providing for high-granularity tagging as well as within-domain evaluation of models across a diverse set of domains; 2) DaCy 2.6.0 that includes three generalizable models with fine-grained annotation; and 3) an evaluation of current state-of-the-art models&rsquo; ability to generalize across domains. The evaluation of existing and new models revealed notable performance discrepancies across domains, which should be addressed within the field. Shortcomings of the annotation quality of the dataset and its impact on model training and evaluation are also discussed. Despite these limitations, we advocate for the use of the new dataset DANSK alongside further work on the generalizability within Danish <b>NER.</b></p></p class="citation"></blockquote><h3 id=5464--54286-meganno-a-human-llm-collaborative-annotation-system-hannah-kim-et-al-2024>(54/64 | 54/286) MEGAnno+: A Human-LLM Collaborative Annotation System (Hannah Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, Dan Zhang. (2024)<br><strong>MEGAnno+: A Human-LLM Collaborative Annotation System</strong><br><button class=copy-to-clipboard title="MEGAnno+: A Human-LLM Collaborative Annotation System" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18050v1.pdf filename=2402.18050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can label data faster and cheaper than humans for various NLP tasks. Despite their prowess, <b>LLMs</b> may fall short in understanding of complex, sociocultural, or domain-specific context, potentially leading to incorrect annotations. Therefore, we advocate a collaborative approach where humans and <b>LLMs</b> work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective <b>LLM</b> agent and annotation management, convenient and robust <b>LLM</b> annotation, and exploratory verification of <b>LLM</b> labels by humans.</p></p class="citation"></blockquote><h3 id=5564--55286-multi-fact-assessing-multilingual-llms-multi-regional-knowledge-using-factscore-sheikh-shafayat-et-al-2024>(55/64 | 55/286) Multi-FAct: Assessing Multilingual LLMs&rsquo; Multi-Regional Knowledge using FActScore (Sheikh Shafayat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh. (2024)<br><strong>Multi-FAct: Assessing Multilingual LLMs&rsquo; Multi-Regional Knowledge using FActScore</strong><br><button class=copy-to-clipboard title="Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18045v1.pdf filename=2402.18045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are prone to factuality hallucination, generating text that contradicts established knowledge. While extensive research has addressed this in English, little is known about multilingual <b>LLMs.</b> This paper systematically evaluates multilingual <b>LLMs&rsquo;</b> factual accuracy across languages and geographic regions. We introduce a novel pipeline for multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for diverse languages. Our analysis across nine languages reveals that English consistently outperforms others in factual accuracy and quantity of generated facts. Furthermore, multilingual models demonstrate a bias towards factual information from Western continents. These findings highlight the need for improved multilingual factuality assessment and underscore geographical biases in <b>LLMs&rsquo;</b> fact generation.</p></p class="citation"></blockquote><h3 id=5664--56286-hop-to-the-next-tasks-and-domains-for-continual-learning-in-nlp-umberto-michieli-et-al-2024>(56/64 | 56/286) HOP to the Next Tasks and Domains for Continual Learning in NLP (Umberto Michieli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Umberto Michieli, Mete Ozay. (2024)<br><strong>HOP to the Next Tasks and Domains for Continual Learning in NLP</strong><br><button class=copy-to-clipboard title="HOP to the Next Tasks and Domains for Continual Learning in NLP" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18449v1.pdf filename=2402.18449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>Learning</b> (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 <b>benchmarks</b> and 2 CL setups demonstrates the effectiveness of our HOP.</p></p class="citation"></blockquote><h3 id=5764--57286-a-birgat-model-for-multi-intent-spoken-language-understanding-with-hierarchical-semantic-frames-hongshen-xu-et-al-2024>(57/64 | 57/286) A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames (Hongshen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongshen Xu, Ruisheng Cao, Su Zhu, Sheng Jiang, Hanchong Zhang, Lu Chen, Kai Yu. (2024)<br><strong>A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames</strong><br><button class=copy-to-clipboard title="A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Graph, Dialogue System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18258v1.pdf filename=2402.18258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle <b>dialogue</b> <b>System,</b> called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational <b>graph</b> attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.</p></p class="citation"></blockquote><h3 id=5864--58286-rora-robust-free-text-rationale-evaluation-zhengping-jiang-et-al-2024>(58/64 | 58/286) RORA: Robust Free-Text Rationale Evaluation (Zhengping Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengping Jiang, Yining Lu, Hanjie Chen, Daniel Khashabi, Benjamin Van Durme, Anqi Liu. (2024)<br><strong>RORA: Robust Free-Text Rationale Evaluation</strong><br><button class=copy-to-clipboard title="RORA: Robust Free-Text Rationale Evaluation" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18678v1.pdf filename=2402.18678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and <b>reasoning</b> gaps behind a model&rsquo;s decision-making. However, due to the diversity of potential <b>reasoning</b> paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.</p></p class="citation"></blockquote><h3 id=5964--59286-tokenization-is-more-than-compression-craig-w-schmidt-et-al-2024>(59/64 | 59/286) Tokenization Is More Than Compression (Craig W. Schmidt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter, Chris Tanner. (2024)<br><strong>Tokenization Is More Than Compression</strong><br><button class=copy-to-clipboard title="Tokenization Is More Than Compression" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18376v1.pdf filename=2402.18376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Tokenization</b> is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing <b>tokenization</b> approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document&rsquo;s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective <b>tokenization.</b> To examine which other factors play a role, we evaluate design decisions across all three phases of <b>tokenization:</b> pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying <b>tokenization,</b> ranging in size from 350M to 2.4B parameters, all of which are made publicly available.</p></p class="citation"></blockquote><h3 id=6064--60286-improving-open-ended-text-generation-via-adaptive-decoding-wenhong-zhu-et-al-2024>(60/64 | 60/286) Improving Open-Ended Text Generation via Adaptive Decoding (Wenhong Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhong Zhu, Hongkun Hao, Zhiwei He, Yiming Ai, Rui Wang. (2024)<br><strong>Improving Open-Ended Text Generation via Adaptive Decoding</strong><br><button class=copy-to-clipboard title="Improving Open-Ended Text Generation via Adaptive Decoding" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18223v1.pdf filename=2402.18223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current language models decode <b>text</b> <b>token</b> by token according to probabilistic distribution, and determining the appropriate candidates for the next token is crucial to ensure generation quality. This study introduces adaptive decoding, a mechanism that empowers the language models to ascertain a sensible candidate set during the generation process dynamically. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. The rationality of including a token in the candidate set is assessed by leveraging the increment of confidence, enabling the model to determine the most suitable candidate set adaptively. The experimental results reveal that our method achieves higher MAUVE and diversity in story generation tasks and maintains certain coherence, underscoring its superiority over existing algorithms. The code is available at <a href=https://github.com/zwhong714/adaptive_decoding>https://github.com/zwhong714/adaptive_decoding</a>.</p></p class="citation"></blockquote><h3 id=6164--61286-contextualizing-generated-citation-texts-biswadip-mandal-et-al-2024>(61/64 | 61/286) Contextualizing Generated Citation Texts (Biswadip Mandal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biswadip Mandal, Xiangci Li, Jessica Ouyang. (2024)<br><strong>Contextualizing Generated Citation Texts</strong><br><button class=copy-to-clipboard title="Contextualizing Generated Citation Texts" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18054v1.pdf filename=2402.18054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstractive citation <b>text</b> <b>generation</b> is usually framed as an infilling task, where a sequence-to-sequence model is trained to generate a citation given a reference paper and the context window around the target; the generated citation should be a brief discussion of the reference paper as it relates to the citing context. However, examining a recent LED-based citation generation system, we find that many of the generated citations are generic summaries of the reference papers main contribution, ignoring the citation contexts focus on a different topic. To address this problem, we propose a simple modification to the citation <b>text</b> <b>generation</b> task: the generation target is not only the citation itself, but the entire context window, including the target citation. This approach can be easily applied to any abstractive citation generation system, and our experimental results show that training in this way is preferred by human readers and allows the generation model to make use of contextual clues about what topic to discuss and what stance to take.</p></p class="citation"></blockquote><h3 id=6264--62286-an-iterative-associative-memory-model-for-empathetic-response-generation-zhou-yang-et-al-2024>(62/64 | 62/286) An Iterative Associative Memory Model for Empathetic Response Generation (Zhou Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Yang, Zhaochun Ren, Yufeng Wang, Chao Chen, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao. (2024)<br><strong>An Iterative Associative Memory Model for Empathetic Response Generation</strong><br><button class=copy-to-clipboard title="An Iterative Associative Memory Model for Empathetic Response Generation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17959v1.pdf filename=2402.17959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Meanwhile, variant experiments on <b>LLMs</b> also demonstrate that attending to associated words improves empathetic comprehension and expression.</p></p class="citation"></blockquote><h3 id=6364--63286-a-survey-on-neural-question-generation-methods-applications-and-prospects-shasha-guo-et-al-2024>(63/64 | 63/286) A Survey on Neural Question Generation: Methods, Applications, and Prospects (Shasha Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shasha Guo, Lizi Liao, Cuiping Li, Tat-Seng Chua. (2024)<br><strong>A Survey on Neural Question Generation: Methods, Applications, and Prospects</strong><br><button class=copy-to-clipboard title="A Survey on Neural Question Generation: Methods, Applications, and Prospects" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18267v1.pdf filename=2402.18267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG&rsquo;s background, encompassing the task&rsquo;s problem formulation, prevalent <b>benchmark</b> datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets and codes, systematically organized on Github, providing an extensive reference for those delving into NQG.</p></p class="citation"></blockquote><h3 id=6464--64286-assessing-the-efficacy-of-grammar-error-correction-a-human-evaluation-approach-in-the-japanese-context-qiao-wang-et-al-2024>(64/64 | 64/286) Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context (Qiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Wang, Zheng Yuan. (2024)<br><strong>Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context</strong><br><button class=copy-to-clipboard title="Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18101v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18101v2.pdf filename=2402.18101v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we evaluated the performance of the state-of-the-art sequence tagging grammar error detection and correction model (SeqTagger) using Japanese university students&rsquo; writing samples. With an automatic annotation toolkit, ERRANT, we first evaluated SeqTagger&rsquo;s performance on error correction with human expert correction as the <b>benchmark.</b> Then a human-annotated approach was adopted to evaluate Seqtagger&rsquo;s performance in error detection using a subset of the writing dataset. Results indicated a precision of 63.66% and a recall of 20.19% for error correction in the full dataset. For the subset, after manual exclusion of irrelevant errors such as semantic and mechanical ones, the model shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for error detection, indicating the model&rsquo;s high accuracy but also its conservativeness. Thematic analysis on errors undetected by the model revealed that determiners and articles, especially the latter, were predominant. Specifically, in terms of context-independent errors, the model occasionally overlooked basic ones and faced challenges with overly erroneous or complex structures. Meanwhile, context-dependent errors, notably those related to tense and noun number, as well as those possibly influenced by the students&rsquo; first language (L1), remained particularly challenging.</p></p class="citation"></blockquote><h2 id=cscv-58>cs.CV (58)</h2><h3 id=158--65286-objective-and-interpretable-breast-cosmesis-evaluation-with-attention-guided-denoising-diffusion-anomaly-detection-model-sangjoon-park-et-al-2024>(1/58 | 65/286) Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model (Sangjoon Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin Chung, Ik Jae Lee, Hwa Kyung Byun. (2024)<br><strong>Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model</strong><br><button class=copy-to-clipboard title="Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 100<br>Keywords: Diffusion Model, Vision Transformer, Anomaly Detection, Knowledge Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning, Unsupervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18362v1.pdf filename=2402.18362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As advancements in the field of breast cancer treatment continue to progress, the assessment of post-surgical cosmetic outcomes has gained increasing significance due to its substantial impact on patients&rsquo; quality of life. However, evaluating breast cosmesis presents challenges due to the inherently subjective nature of expert labeling. In this study, we present a novel automated approach, Attention-Guided Denoising <b>Diffusion</b> <b>Anomaly</b> <b>Detection</b> (AG-DDAD), designed to assess breast cosmesis following surgery, addressing the limitations of conventional <b>supervised</b> <b>learning</b> and existing <b>anomaly</b> <b>detection</b> models. Our approach leverages the attention mechanism of the <b>distillation</b> with no label (DINO) <b>self-supervised</b> <b>Vision</b> <b>Transformer</b> (ViT) in combination with a <b>diffusion</b> <b>model</b> to achieve high-quality image reconstruction and precise transformation of discriminative regions. By training the <b>diffusion</b> <b>model</b> on unlabeled data predominantly with normal cosmesis, we adopt an <b>unsupervised</b> <b>anomaly</b> <b>detection</b> perspective to automatically score the cosmesis. Real-world data experiments demonstrate the effectiveness of our method, providing visually appealing representations and quantifiable scores for cosmesis evaluation. Compared to commonly used rule-based programs, our fully automated approach eliminates the need for manual annotations and offers objective evaluation. Moreover, our <b>anomaly</b> <b>detection</b> model exhibits state-of-the-art performance, surpassing existing models in accuracy. Going beyond the scope of breast cosmesis, our research represents a significant advancement in <b>unsupervised</b> <b>anomaly</b> <b>detection</b> within the medical domain, thereby paving the way for future investigations.</p></p class="citation"></blockquote><h3 id=258--66286-self-supervised-learning-in-electron-microscopy-towards-a-foundation-model-for-advanced-image-analysis-bashir-kazimi-et-al-2024>(2/58 | 66/286) Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis (Bashir Kazimi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bashir Kazimi, Karina Ruzaeva, Stefan Sandfeld. (2024)<br><strong>Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cond-mat-mtrl-sci, cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Self-supervised Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18286v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18286v1.pdf filename=2402.18286v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we explore the potential of <b>self-supervised</b> <b>learning</b> from unlabeled electron microscopy datasets, taking a step toward building a <b>foundation</b> <b>model</b> in this field. We show how <b>self-supervised</b> <b>pretraining</b> facilitates efficient <b>fine-tuning</b> for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that <b>fine-tuned</b> models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of <b>self-supervised</b> <b>pretraining</b> across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that <b>self-supervised</b> <b>pretraining</b> serves as a powerful catalyst, being especially advantageous when limited annotated data are available and efficient scaling of computational cost are important.</p></p class="citation"></blockquote><h3 id=358--67286-grounding-language-models-for-visual-entity-recognition-zilin-xiao-et-al-2024>(3/58 | 67/286) Grounding Language Models for Visual Entity Recognition (Zilin Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, Vicente Ordonez. (2024)<br><strong>Grounding Language Models for Visual Entity Recognition</strong><br><button class=copy-to-clipboard title="Grounding Language Models for Visual Entity Recognition" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Multi-modal, Out-of-domain, Grounding, Language Generation, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18695v1.pdf filename=2402.18695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Model</b> by employing retrieval augmented constrained generation. It mitigates low performance on <b>out-of-domain</b> entities while excelling in queries that require visually-situated <b>reasoning.</b> Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides <b>language</b> <b>generation</b> by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki <b>benchmark.</b> Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial double-digit margin.</p></p class="citation"></blockquote><h3 id=458--68286-openmedlab-an-open-source-platform-for-multi-modality-foundation-models-in-medicine-xiaosong-wang-et-al-2024>(4/58 | 68/286) OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine (Xiaosong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaosong Wang, Xiaofan Zhang, Guotai Wang, Junjun He, Zhongyu Li, Wentao Zhu, Yi Guo, Qi Dou, Xiaoxiao Li, Dequan Wang, Liang Hong, Qicheng Lao, Tong Ruan, Yukun Zhou, Yixue Li, Jie Zhao, Kang Li, Xin Sun, Lifeng Zhu, Shaoting Zhang. (2024)<br><strong>OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine</strong><br><button class=copy-to-clipboard title="OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Multi-modal, Transfer Learning, Gemini, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18028v1.pdf filename=2402.18028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and <b>Gemini,</b> has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such <b>foundation</b> <b>models</b> (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of <b>transfer</b> <b>learning</b> and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained <b>foundation</b> <b>models</b> were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality <b>foundation</b> <b>models.</b> It encapsulates not only solutions of pioneering attempts in <b>prompting</b> and <b>fine-tuning</b> large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific <b>foundation</b> <b>models</b> with large-scale <b>multi-modal</b> medical data. Importantly, it opens access to a group of pre-trained <b>foundation</b> <b>models</b> for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of <b>benchmarks</b> for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via <a href=https://github.com/openmedlab>https://github.com/openmedlab</a>.</p></p class="citation"></blockquote><h3 id=558--69286-all-in-a-single-image-large-multimodal-models-are-in-image-learners-lei-wang-et-al-2024>(5/58 | 69/286) All in a Single Image: Large Multimodal Models are In-Image Learners (Lei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy Ka-Wei Lee, Ee-Peng Lim. (2024)<br><strong>All in a Single Image: Large Multimodal Models are In-Image Learners</strong><br><button class=copy-to-clipboard title="All in a Single Image: Large Multimodal Models are In-Image Learners" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, GPT, Reasoning, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17971v1.pdf filename=2402.17971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new <b>in-context</b> <b>learning</b> <b>(ICL)</b> mechanism called In-Image Learning (I$^2$L) that combines demonstration examples, visual cues, and instructions into a single image to enhance the capabilities of <b>GPT-4V.</b> Unlike previous approaches that rely on converting images to text or incorporating visual input into language models, I$^2$L consolidates all information into one image and primarily leverages image processing, understanding, and <b>reasoning</b> abilities. This has several advantages: it avoids inaccurate textual descriptions of complex images, provides flexibility in positioning demonstration examples, reduces the input burden, and avoids exceeding input limits by eliminating the need for multiple images and lengthy text. To further combine the strengths of different <b>ICL</b> methods, we introduce an automatic strategy to select the appropriate <b>ICL</b> method for a data example in a given task. We conducted experiments on MathVista and Hallusionbench to test the effectiveness of I$^2$L in complex <b>multimodal</b> <b>reasoning</b> tasks and mitigating language hallucination and visual illusion. Additionally, we explored the impact of image resolution, the number of demonstration examples, and their positions on the effectiveness of I$^2$L. Our code is publicly available at <a href=https://github.com/AGI-Edgerunners/IIL>https://github.com/AGI-Edgerunners/IIL</a>.</p></p class="citation"></blockquote><h3 id=658--70286-sunshine-to-rainstorm-cross-weather-knowledge-distillation-for-robust-3d-object-detection-xun-huang-et-al-2024>(6/58 | 70/286) Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection (Xun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xun Huang, Hai Wu, Xin Li, Xiaoliang Fan, Chenglu Wen, Cheng Wang. (2024)<br><strong>Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection</strong><br><button class=copy-to-clipboard title="Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18493v1.pdf filename=2402.18493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LiDAR-based 3D <b>object</b> <b>detection</b> models have traditionally struggled under rainy conditions due to the degraded and noisy scanning signals. Previous research has attempted to address this by simulating the noise from rain to improve the robustness of detection models. However, significant disparities exist between simulated and actual rain-impacted data points. In this work, we propose a novel rain <b>simulation</b> method, termed DRET, that unifies Dynamics and Rainy Environment Theory to provide a cost-effective means of expanding the available realistic rain data for 3D detection training. Furthermore, we present a Sunny-to-Rainy <b>Knowledge</b> <b>Distillation</b> (SRKD) approach to enhance 3D detection under rainy conditions. Extensive experiments on the WaymoOpenDataset large-scale dataset show that, when combined with the state-of-the-art DSVT model and other classical 3D detectors, our proposed framework demonstrates significant detection accuracy improvements, without losing efficiency. Remarkably, our framework also improves detection capabilities under sunny conditions, therefore offering a robust solution for 3D detection regardless of whether the weather is rainy or sunny</p></p class="citation"></blockquote><h3 id=758--71286-image2flow-a-hybrid-image-and-graph-convolutional-neural-network-for-rapid-patient-specific-pulmonary-artery-segmentation-and-cfd-flow-field-calculation-from-3d-cardiac-mri-data-tina-yao-et-al-2024>(7/58 | 71/286) Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data (Tina Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tina Yao, Endrit Pajaziti, Michael Quail, Silvia Schievano, Jennifer A Steeden, Vivek Muthurangu. (2024)<br><strong>Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data</strong><br><button class=copy-to-clipboard title="Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18236v1.pdf filename=2402.18236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational fluid dynamics (CFD) can be used for evaluation of hemodynamics. However, its routine use is limited by labor-intensive manual segmentation, CFD mesh creation, and time-consuming <b>simulation.</b> This study aims to train a deep learning model to both generate patient-specific volume-meshes of the pulmonary artery from 3D cardiac MRI data and directly estimate CFD flow fields. This study used 135 3D cardiac MRIs from both a public and private dataset. The pulmonary arteries in the MRIs were manually segmented and converted into volume-meshes. CFD <b>simulations</b> were performed on ground truth meshes and interpolated onto point-point correspondent meshes to create the ground truth dataset. The dataset was split 85/10/15 for training, validation and testing. Image2Flow, a hybrid image and <b>graph</b> <b>convolutional</b> <b>neural</b> <b>network,</b> was trained to transform a pulmonary artery template to patient-specific anatomy and CFD values. Image2Flow was evaluated in terms of segmentation and accuracy of CFD predicted was assessed using node-wise comparisons. Centerline comparisons of Image2Flow and CFD <b>simulations</b> performed using machine learning segmentation were also performed. Image2Flow achieved excellent segmentation accuracy with a median Dice score of 0.9 (IQR: 0.86-0.92). The median node-wise normalized absolute error for pressure and velocity magnitude was 11.98% (IQR: 9.44-17.90%) and 8.06% (IQR: 7.54-10.41), respectively. Centerline analysis showed no significant difference between the Image2Flow and conventional CFD simulated on machine learning-generated volume-meshes. This proof-of-concept study has shown it is possible to simultaneously perform patient specific volume-mesh based segmentation and pressure and flow field estimation. Image2Flow completes segmentation and CFD in ~205ms, which ~7000 times faster than manual methods, making it more feasible in a clinical environment.</p></p class="citation"></blockquote><h3 id=858--72286-prompt-driven-dynamic-object-centric-learning-for-single-domain-generalization-deng-li-et-al-2024>(8/58 | 72/286) Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization (Deng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deng Li, Aming Wu, Yaowei Wang, Yahong Han. (2024)<br><strong>Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization</strong><br><button class=copy-to-clipboard title="Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Graph Attention Networks, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18447v1.pdf filename=2402.18447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies significantly in cross-domain scenarios. In this paper, we propose a dynamic <b>object-centric</b> <b>perception</b> network based on <b>prompt</b> <b>learning,</b> aiming to adapt to the variations in image complexity. Specifically, we propose an <b>object-centric</b> <b>gating</b> module based on <b>prompt</b> <b>learning</b> to focus attention on the <b>object-centric</b> <b>features</b> guided by the various scene <b>prompts.</b> <b>Then,</b> with the <b>object-centric</b> <b>gating</b> masks, the dynamic selective module dynamically selects highly correlated feature regions in both spatial and channel dimensions enabling the model to adaptively perceive <b>object-centric</b> <b>relevant</b> features, thereby enhancing the generalization capability. Extensive experiments were conducted on single-domain generalization tasks in image classification and <b>object</b> <b>detection.</b> The experimental results demonstrate that our approach outperforms state-of-the-art methods, which validates the effectiveness and generally of our proposed method.</p></p class="citation"></blockquote><h3 id=958--73286-ef-quantface-streamlined-face-recognition-with-small-data-and-low-bit-precision-william-gazali-et-al-2024>(9/58 | 73/286) Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision (William Gazali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William Gazali, Jocelyn Michelle Kho, Joshua Santoso, Williem. (2024)<br><strong>Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision</strong><br><button class=copy-to-clipboard title="Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Face Recognition, Fine-tuning, Model Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18163v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18163v1.pdf filename=2402.18163v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>model</b> <b>quantization</b> for <b>face</b> <b>recognition</b> has gained prominence. Traditionally, compressing <b>models</b> <b>involved</b> vast datasets like the 5.8 million-image MS1M dataset as well as extensive training times, raising the question of whether such data enormity is essential. This paper addresses this by introducing an efficiency-driven approach, <b>fine-tuning</b> the <b>model</b> <b>with</b> just up to 14,000 images, 440 times smaller than MS1M. We demonstrate that effective <b>quantization</b> is achievable with a smaller dataset, presenting a new paradigm. Moreover, we incorporate an evaluation-based metric loss and achieve an outstanding 96.15% accuracy on the IJB-C dataset, establishing a new state-of-the-art compressed <b>model</b> <b>training</b> for <b>face</b> <b>recognition.</b> The subsequent analysis delves into potential applications, emphasizing the transformative power of this approach. This paper advances <b>model</b> <b>quantization</b> by highlighting the efficiency and optimal results with small data and training time.</p></p class="citation"></blockquote><h3 id=1058--74286-understanding-the-role-of-pathways-in-a-deep-neural-network-lei-lyu-et-al-2024>(10/58 | 74/286) Understanding the Role of Pathways in a Deep Neural Network (Lei Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Lyu, Chen Pang, Jihua Wang. (2024)<br><strong>Understanding the Role of Pathways in a Deep Neural Network</strong><br><button class=copy-to-clipboard title="Understanding the Role of Pathways in a Deep Neural Network" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-NE, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18132v1.pdf filename=2402.18132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus-response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding <b>adversarial</b> <b>attacks,</b> object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.</p></p class="citation"></blockquote><h3 id=1158--75286-synartifact-classifying-and-alleviating-artifacts-in-synthetic-images-via-vision-language-model-bin-cao-et-al-2024>(11/58 | 75/286) SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model (Bin Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Cao, Jianhao Yuan, Yexin Liu, Jian Li, Shuyang Sun, Jing Liu, Bo Zhao. (2024)<br><strong>SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model</strong><br><button class=copy-to-clipboard title="SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18068v1.pdf filename=2402.18068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving area of image synthesis, a serious challenge is the presence of complex artifacts that compromise perceptual realism of synthetic images. To alleviate artifacts and improve quality of synthetic images, we <b>fine-tune</b> <b>Vision-Language</b> Model (VLM) as artifact classifier to automatically identify and classify a wide range of artifacts and provide supervision for further optimizing generative models. Specifically, we develop a comprehensive artifact taxonomy and construct a dataset of synthetic images with artifact annotations for <b>fine-tuning</b> VLM, named SynArtifact-1K. The <b>fine-tuned</b> VLM exhibits superior ability of identifying artifacts and outperforms the baseline by 25.66%. To our knowledge, this is the first time such end-to-end artifact classification task and solution have been proposed. Finally, we leverage the output of VLM as feedback to refine the generative model for alleviating artifacts. Visualization results and user study demonstrate that the quality of images synthesized by the refined <b>diffusion</b> <b>model</b> has been obviously improved.</p></p class="citation"></blockquote><h3 id=1258--76286-rapid-hyperspectral-photothermal-mid-infrared-spectroscopic-imaging-from-sparse-data-for-gynecologic-cancer-tissue-subtyping-reza-reihanisaransari-et-al-2024>(12/58 | 76/286) Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping (Reza Reihanisaransari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Reihanisaransari, Chalapathi Charan Gajjela, Xinyu Wu, Ragib Ishrak, Sara Corvigno, Yanping Zhong, Jinsong Liu, Anil K. Sood, David Mayerich, Sebastian Berisha, Rohith Reddy. (2024)<br><strong>Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping</strong><br><button class=copy-to-clipboard title="Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, q-bio-BM, q-bio-QM, q-bio-TO<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Morphological Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17960v1.pdf filename=2402.17960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ovarian cancer detection has traditionally relied on a multi-step process that includes biopsy, tissue staining, and <b>morphological</b> <b>analysis</b> by experienced pathologists. While widely practiced, this conventional approach suffers from several drawbacks: it is qualitative, time-intensive, and heavily dependent on the quality of staining. Mid-infrared (MIR) hyperspectral photothermal imaging is a label-free, biochemically quantitative technology that, when combined with machine learning algorithms, can eliminate the need for staining and provide quantitative results comparable to traditional histology. However, this technology is slow. This work presents a novel approach to MIR photothermal imaging that enhances its speed by an order of magnitude. Our method significantly accelerates data collection by capturing a combination of high-resolution and interleaved, lower-resolution infrared band images and applying computational techniques for data interpolation. We effectively minimize data collection requirements by leveraging sparse data acquisition and employing curvelet-based reconstruction algorithms. This method enables the reconstruction of high-quality, high-resolution images from undersampled datasets and achieving a 10X improvement in data acquisition time. We assessed the performance of our sparse imaging methodology using a variety of quantitative metrics, including mean squared error (MSE), structural similarity index (SSIM), and tissue subtype classification accuracies, employing both random forest and <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> models, accompanied by ROC curves. Our statistically robust analysis, based on data from 100 ovarian cancer patient samples and over 65 million data points, demonstrates the method&rsquo;s capability to produce superior image quality and accurately distinguish between different gynecological tissue types with segmentation accuracy exceeding 95%.</p></p class="citation"></blockquote><h3 id=1358--77286-polos-multimodal-metric-learning-from-human-feedback-for-image-captioning-yuiga-wada-et-al-2024>(13/58 | 77/286) Polos: Multimodal Metric Learning from Human Feedback for Image Captioning (Yuiga Wada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura. (2024)<br><strong>Polos: Multimodal Metric Learning from Human Feedback for Image Captioning</strong><br><button class=copy-to-clipboard title="Polos: Multimodal Metric Learning from Human Feedback for Image Captioning" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Automatic Evaluation, Contrastive Learning, Multi-modal, Multi-modal, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18091v1.pdf filename=2402.18091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Establishing an <b>automatic</b> <b>evaluation</b> metric that closely aligns with human judgments is essential for effectively developing image captioning models. Recent data-driven metrics have demonstrated a stronger correlation with human judgments than classic metrics such as CIDEr; however they lack sufficient capabilities to handle hallucinations and generalize across diverse images and texts partially because they compute scalar similarities merely using embeddings learned from tasks unrelated to image captioning evaluation. In this study, we propose Polos, a <b>supervised</b> <b>automatic</b> <b>evaluation</b> metric for image captioning models. Polos computes scores from <b>multimodal</b> inputs, using a parallel feature extraction mechanism that leverages embeddings trained through large-scale <b>contrastive</b> <b>learning.</b> To train Polos, we introduce <b>Multimodal</b> Metric Learning from Human Feedback (M$^2$LHF), a framework for developing metrics based on human feedback. We constructed the Polaris dataset, which comprises 131K human judgments from 550 evaluators, which is approximately ten times larger than standard datasets. Our approach achieved state-of-the-art performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and the Polaris dataset, thereby demonstrating its effectiveness and robustness.</p></p class="citation"></blockquote><h3 id=1458--78286-downstream-task-guided-masking-learning-in-masked-autoencoders-using-multi-level-optimization-han-guo-et-al-2024>(14/58 | 78/286) Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization (Han Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie. (2024)<br><strong>Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization</strong><br><button class=copy-to-clipboard title="Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 35<br>Keywords: Autoencoder, Representation Learning, Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18128v1.pdf filename=2402.18128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked <b>Autoencoder</b> (MAE) is a notable method for <b>self-supervised</b> <b>pretraining</b> in visual <b>representation</b> <b>learning.</b> It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal <b>representations</b> <b>for</b> these tasks. In response, we introduce the Multi-level Optimized Mask <b>Autoencoder</b> (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE&rsquo;s significant advancements in visual <b>representation</b> <b>learning.</b> Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency. Our code is available at: <a href=https://github.com/Alexiland/MLOMAE>https://github.com/Alexiland/MLOMAE</a></p></p class="citation"></blockquote><h3 id=1558--79286-breaking-the-black-box-confidence-guided-model-inversion-attack-for-distribution-shift-xinhao-liu-et-al-2024>(15/58 | 79/286) Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift (Xinhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhao Liu, Yingzhao Jiang, Zetao Lin. (2024)<br><strong>Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift</strong><br><button class=copy-to-clipboard title="Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Black Box, Distribution Shift, Distribution Shift, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18027v1.pdf filename=2402.18027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model inversion attacks (MIAs) seek to infer the private training data of a target classifier by generating synthetic images that reflect the characteristics of the target class through querying the model. However, prior studies have relied on full access to the target model, which is not practical in real-world scenarios. Additionally, existing <b>black-box</b> <b>MIAs</b> assume that the image prior and target model follow the same <b>distribution.</b> <b>However,</b> when confronted with diverse data <b>distribution</b> <b>settings,</b> these methods may result in suboptimal performance in conducting attacks. To address these limitations, this paper proposes a \textbf{C}onfidence-\textbf{G}uided \textbf{M}odel \textbf{I}nversion attack method called CG-MI, which utilizes the latent space of a pre-trained publicly available <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> as prior information and gradient-free optimizer, enabling high-resolution MIAs across different data <b>distributions</b> <b>in</b> a <b>black-box</b> <b>setting.</b> Our experiments demonstrate that our method significantly \textbf{outperforms the SOTA <b>black-box</b> <b>MIA</b> by more than 49% for Celeba and 58% for Facescrub in different <b>distribution</b> <b>settings}.</b> Furthermore, our method exhibits the ability to generate high-quality images \textbf{comparable to those produced by white-box attacks}. Our method provides a practical and effective solution for <b>black-box</b> <b>model</b> inversion attacks.</p></p class="citation"></blockquote><h3 id=1658--80286-echotrack-auditory-referring-multi-object-tracking-for-autonomous-driving-jiacheng-lin-et-al-2024>(16/58 | 80/286) EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving (Jiacheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang. (2024)<br><strong>EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving</strong><br><button class=copy-to-clipboard title="EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV, eess-AS, eess-IV<br>Keyword Score: 33<br>Keywords: Vision Transformer, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18302v1.pdf filename=2402.18302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream <b>vision</b> <b>transformers.</b> The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT <b>benchmarks,</b> including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established <b>benchmarks</b> demonstrate the effectiveness of the proposed EchoTrack model and its components. The source code and datasets will be made publicly available at <a href=https://github.com/lab206/EchoTrack>https://github.com/lab206/EchoTrack</a>.</p></p class="citation"></blockquote><h3 id=1758--81286-coarse-to-fine-latent-diffusion-for-pose-guided-person-image-synthesis-yanzuo-lu-et-al-2024>(17/58 | 81/286) Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis (Yanzuo Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanzuo Lu, Manlin Zhang, Andy J Ma, Xiaohua Xie, Jian-Huang Lai. (2024)<br><strong>Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</strong><br><button class=copy-to-clipboard title="Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18078v1.pdf filename=2402.18078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>model</b> is a promising approach to image generation and has been employed for Pose-Guided Person Image Synthesis (PGPIS) with competitive performance. While existing methods simply align the person appearance to the target pose, they are prone to overfitting due to the lack of a high-level semantic understanding on the source person image. In this paper, we propose a novel Coarse-to-Fine Latent <b>Diffusion</b> <b>(CFLD)</b> method for PGPIS. In the absence of image-caption pairs and textual <b>prompts,</b> we develop a novel training paradigm purely based on images to control the generation process of the pre-trained <b>text-to-image</b> <b>diffusion</b> <b>model.</b> A perception-refined decoder is designed to progressively refine a set of learnable queries and extract semantic understanding of person images as a coarse-grained <b>prompt.</b> This allows for the decoupling of fine-grained appearance and pose information controls at different stages, and thus circumventing the potential overfitting problem. To generate more realistic texture details, a hybrid-granularity attention module is proposed to encode multi-scale fine-grained appearance features as bias terms to augment the coarse-grained <b>prompt.</b> Both quantitative and qualitative experimental results on the DeepFashion <b>benchmark</b> demonstrate the superiority of our method over the state of the arts for PGPIS. Code is available at <a href=https://github.com/YanzuoLu/CFLD>https://github.com/YanzuoLu/CFLD</a>.</p></p class="citation"></blockquote><h3 id=1858--82286-unsupervised-cross-domain-image-retrieval-via-prototypical-optimal-transport-bin-li-et-al-2024>(18/58 | 82/286) Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport (Bin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Li, Ye Shi, Qian Yu, Jingya Wang. (2024)<br><strong>Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport</strong><br><button class=copy-to-clipboard title="Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 31<br>Keywords: Benchmarking, Clustering, Contrastive Learning, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18411v1.pdf filename=2402.18411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> cross-domain image retrieval (UCIR) aims to retrieve images sharing the same category across diverse domains without relying on labeled data. Prior approaches have typically decomposed the UCIR problem into two distinct tasks: intra-domain <b>representation</b> <b>learning</b> and cross-domain feature alignment. However, these segregated strategies overlook the potential synergies between these tasks. This paper introduces ProtoOT, a novel Optimal Transport formulation explicitly tailored for UCIR, which integrates intra-domain feature <b>representation</b> <b>learning</b> and cross-domain alignment into a unified framework. ProtoOT leverages the strengths of the K-means <b>clustering</b> method to effectively manage distribution imbalances inherent in UCIR. By utilizing K-means for generating initial prototypes and approximating class marginal distributions, we modify the constraints in Optimal Transport accordingly, significantly enhancing its performance in UCIR scenarios. Furthermore, we incorporate <b>contrastive</b> <b>learning</b> into the ProtoOT framework to further improve <b>representation</b> <b>learning.</b> This encourages local semantic consistency among features with similar semantics, while also explicitly enforcing separation between features and unmatched prototypes, thereby enhancing global discriminativeness. ProtoOT surpasses existing state-of-the-art methods by a notable margin across <b>benchmark</b> datasets. Notably, on DomainNet, ProtoOT achieves an average P@200 enhancement of 24.44%, and on Office-Home, it demonstrates a P@15 improvement of 12.12%. Code is available at <a href=https://github.com/HCVLAB/ProtoOT>https://github.com/HCVLAB/ProtoOT</a>.</p></p class="citation"></blockquote><h3 id=1958--83286-finediffusion-scaling-up-diffusion-models-for-fine-grained-image-generation-with-10000-classes-ziying-pan-et-al-2024>(19/58 | 83/286) FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes (Ziying Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai. (2024)<br><strong>FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes</strong><br><button class=copy-to-clipboard title="FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18331v1.pdf filename=2402.18331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The class-conditional image generation based on <b>diffusion</b> <b>models</b> is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to <b>fine-tune</b> large pre-trained <b>diffusion</b> <b>models</b> scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only <b>fine-tuning</b> tiered class embedder, bias terms, and normalization layers&rsquo; parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full <b>fine-tuning,</b> FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient <b>fine-tuning</b> methods. The code and more generated results are available at our project website: <a href=https://finediffusion.github.io/>https://finediffusion.github.io/</a>.</p></p class="citation"></blockquote><h3 id=2058--84286-oil-spill-drone-a-dataset-of-drone-captured-segmented-rgb-images-for-oil-spill-detection-in-port-environments-t-de-kerf-et-al-2024>(20/58 | 84/286) Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments (T. De Kerf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>T. De Kerf, S. Sels, S. Samsonova, S. Vanlanduit. (2024)<br><strong>Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments</strong><br><button class=copy-to-clipboard title="Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18202v1.pdf filename=2402.18202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The high incidence of oil spills in port areas poses a serious threat to the environment, <b>prompting</b> the need for efficient detection mechanisms. Utilizing automated drones for this purpose can significantly improve the speed and accuracy of oil spill detection. Such advancements not only expedite cleanup operations, reducing environmental harm but also enhance polluter accountability, potentially deterring future incidents. Currently, there&rsquo;s a scarcity of datasets employing RGB images for oil spill detection in maritime settings. This paper presents a unique, annotated dataset aimed at addressing this gap, leveraging a neural network for analysis on both desktop and edge computing platforms. The dataset, captured via drone, comprises 1268 images categorized into oil, water, and other, with a <b>convolutional</b> <b>neural</b> <b>network</b> trained using an Unet model architecture achieving an F1 score of 0.71 for oil detection. This underscores the dataset&rsquo;s practicality for real-world applications, offering crucial resources for environmental conservation in port environments.</p></p class="citation"></blockquote><h3 id=2158--85286-3dsflabelling-boosting-3d-scene-flow-estimation-by-pseudo-auto-labelling-chaokang-jiang-et-al-2024>(21/58 | 85/286) 3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling (Chaokang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaokang Jiang, Guangming Wang, Jiuming Liu, Hesheng Wang, Zhuang Ma, Zhenqiang Liu, Zhujin Liang, Yi Shan, Dalong Du. (2024)<br><strong>3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling</strong><br><button class=copy-to-clipboard title="3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18146v1.pdf filename=2402.18146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning 3D scene flow from LiDAR point clouds presents significant difficulties, including poor generalization from synthetic datasets to real scenes, scarcity of real-world 3D labels, and poor performance on real sparse LiDAR point clouds. We present a novel approach from the perspective of auto-labelling, aiming to generate a large number of 3D scene flow pseudo labels for real-world LiDAR point clouds. Specifically, we employ the assumption of rigid body motion to simulate potential object-level rigid movements in autonomous driving scenarios. By updating different motion attributes for multiple anchor boxes, the rigid motion decomposition is obtained for the whole scene. Furthermore, we developed a novel 3D scene flow <b>data</b> <b>augmentation</b> method for global and local motion. By perfectly synthesizing target point clouds based on augmented motion parameters, we easily obtain lots of 3D scene flow labels in point clouds highly consistent with real scenarios. On multiple real-world datasets including LiDAR KITTI, nuScenes, and Argoverse, our method outperforms all previous <b>supervised</b> and <b>unsupervised</b> methods without requiring manual labelling. Impressively, our method achieves a tenfold reduction in EPE3D metric on the LiDAR KITTI dataset, reducing it from $0.190m$ to a mere $0.008m$ error.</p></p class="citation"></blockquote><h3 id=2258--86286-generalizable-two-branch-framework-for-image-class-incremental-learning-chao-wu-et-al-2024>(22/58 | 86/286) Generalizable Two-Branch Framework for Image Class-Incremental Learning (Chao Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Wu, Xiaobin Chang, Ruixuan Wang. (2024)<br><strong>Generalizable Two-Branch Framework for Image Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Generalizable Two-Branch Framework for Image Class-Incremental Learning" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Continual Learning, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18086v1.pdf filename=2402.18086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks often severely forget previously learned knowledge when learning new knowledge. Various <b>continual</b> <b>learning</b> (CL) methods have been proposed to handle such a catastrophic forgetting issue from different perspectives and achieved substantial improvements.In this paper, a novel two-branch <b>continual</b> <b>learning</b> framework is proposed to further enhance most existing CL methods. Specifically, the main branch can be any existing CL model and the newly introduced side branch is a lightweight <b>convolutional</b> <b>network.</b> The output of each main branch block is modulated by the output of the corresponding side branch block. Such a simple two-branch model can then be easily implemented and learned with the vanilla optimization setting without whistles and bells.Extensive experiments with various settings on multiple image datasets show that the proposed framework yields consistent improvements over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2358--87286-tamm-triadapter-multi-modal-learning-for-3d-shape-understanding-zhihao-zhang-et-al-2024>(23/58 | 87/286) TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding (Zhihao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Zhang, Shengcao Cao, Yu-Xiong Wang. (2024)<br><strong>TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding</strong><br><button class=copy-to-clipboard title="TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Multi-modal, Representation Learning, Zero-shot, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18490v1.pdf filename=2402.18490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The limited scale of current 3D shape datasets hinders the advancements in 3D shape understanding, and motivates <b>multi-modal</b> learning approaches which transfer learned knowledge from data-abundant 2D image and language modalities to 3D shapes. However, even though the image and language <b>representations</b> <b>have</b> been aligned by cross-modal models like CLIP, we find that the image modality fails to contribute as much as the language in existing <b>multi-modal</b> 3D <b>representation</b> <b>learning</b> methods. This is attributed to the domain shift in the 2D images and the distinct focus of each modality. To more effectively leverage both modalities in the pre-training, we introduce TriAdapter <b>Multi-Modal</b> Learning (TAMM) &ndash; a novel two-stage learning approach based on three synergetic adapters. First, our CLIP Image Adapter mitigates the domain gap between 3D-rendered images and natural images, by adapting the visual <b>representations</b> <b>of</b> CLIP for synthetic <b>image-text</b> pairs. Subsequently, our Dual Adapters decouple the 3D shape <b>representation</b> <b>space</b> into two complementary sub-spaces: one focusing on visual attributes and the other for semantic understanding, which ensure a more comprehensive and effective <b>multi-modal</b> pre-training. Extensive experiments demonstrate that TAMM consistently enhances 3D <b>representations</b> <b>for</b> a wide range of 3D encoder architectures, pre-training datasets, and downstream tasks. Notably, we boost the <b>zero-shot</b> classification accuracy on Objaverse-LVIS from 46.8 to 50.7, and improve the 5-way 10-shot linear probing classification accuracy on ModelNet40 from 96.1 to 99.0. Project page: \url{https://alanzhangcs.github.io/tamm-page}.</p></p class="citation"></blockquote><h3 id=2458--88286-a-modular-system-for-enhanced-robustness-of-multimedia-understanding-networks-via-deep-parametric-estimation-francesco-barbato-et-al-2024>(24/58 | 88/286) A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation (Francesco Barbato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Barbato, Umberto Michieli, Mehmet Kerim Yucel, Pietro Zanuttigh, Mete Ozay. (2024)<br><strong>A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation</strong><br><button class=copy-to-clipboard title="A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Data Augmentation, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18402v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18402v2.pdf filename=2402.18402v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multimedia understanding tasks, corrupted samples pose a critical challenge, because when fed to machine learning models they lead to performance degradation. In the past, three groups of approaches have been proposed to handle noisy <b>data:</b> <b>i)</b> enhancer and denoiser modules to improve the quality of the noisy <b>data,</b> <b>ii)</b> <b>data</b> <b>augmentation</b> approaches, and iii) <b>domain</b> <b>adaptation</b> strategies. All the aforementioned approaches come with drawbacks that limit their applicability; the first has high computational costs and requires pairs of clean-corrupted <b>data</b> <b>for</b> training, while the others only allow deployment of the same task/network they were trained on (\ie, when upstream and downstream task/network are the same). In this paper, we propose SyMPIE to solve these shortcomings. To this end, we design a small, modular, and efficient (just 2GFLOPs to process a Full HD image) system to enhance input <b>data</b> <b>for</b> robust downstream multimedia understanding with minimal computational cost. Our SyMPIE is pre-trained on an upstream task/network that should not match the downstream ones and does not need paired clean-corrupted samples. Our key insight is that most input corruptions found in real-world tasks can be modeled through global operations on color channels of images or spatial filters with small kernels. We validate our approach on multiple datasets and tasks, such as image classification (on ImageNetC, ImageNetC-Bar, VizWiz, and a newly proposed mixed corruption <b>benchmark</b> named ImageNetC-mixed) and semantic segmentation (on Cityscapes, ACDC, and DarkZurich) with consistent improvements of about 5% relative accuracy gain across the board. The code of our approach and the new ImageNetC-mixed <b>benchmark</b> will be made available upon publication.</p></p class="citation"></blockquote><h3 id=2558--89286-prcl-probabilistic-representation-contrastive-learning-for-semi-supervised-semantic-segmentation-haoyu-xie-et-al-2024>(25/58 | 89/286) PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation (Haoyu Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu, Baigui Sun. (2024)<br><strong>PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18117v1.pdf filename=2402.18117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through <b>contrastive</b> <b>learning.</b> However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the <b>unsupervised</b> training process. To address this issue, we propose a robust <b>contrastive-based</b> <b>S4</b> framework, termed the Probabilistic Representation <b>Contrastive</b> <b>Learning</b> (PRCL) framework to enhance the robustness of the <b>unsupervised</b> training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in <b>contrastive</b> <b>learning.</b> Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations with the same class, it is robust from the instant noise in representations and bears the intra-class variance of representations. In addition, we generate Virtual Negatives (VNs) based on GDP to involve the <b>contrastive</b> <b>learning</b> process. Extensive experiments on two public <b>benchmarks</b> demonstrate the superiority of our PRCL framework.</p></p class="citation"></blockquote><h3 id=2658--90286-enhancing-tracking-robustness-with-auxiliary-adversarial-defense-networks-zhewei-wu-et-al-2024>(26/58 | 90/286) Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks (Zhewei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou. (2024)<br><strong>Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks</strong><br><button class=copy-to-clipboard title="Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Adversarial Learning, Benchmarking, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17976v1.pdf filename=2402.17976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. These attack methods have garnered considerable attention from researchers in recent years. However, there is still a lack of research on designing <b>adversarial</b> <b>defense</b> methods specifically for visual object tracking. To address these issues, we propose an effective additional pre-processing network called DuaLossDef that eliminates <b>adversarial</b> <b>perturbations</b> during the tracking process. DuaLossDef is deployed ahead of the search branche or template branche of the tracker to apply defensive transformations to the input images. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without requiring any parameter adjustments. We train DuaLossDef using <b>adversarial</b> <b>training,</b> specifically employing Dua-Loss to generate <b>adversarial</b> <b>samples</b> that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 <b>benchmarks</b> demonstrate that DuaLossDef maintains excellent defense robustness against <b>adversarial</b> <b>attack</b> methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to other trackers, it exhibits reliable transferability. Finally, DuaLossDef achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead. We will make our code publicly available soon.</p></p class="citation"></blockquote><h3 id=2758--91286-spatial-coherence-loss-for-salient-and-camouflaged-object-detection-and-beyond-ziyun-yang-et-al-2024>(27/58 | 91/286) Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond (Ziyun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyun Yang, Kevin Choy, Sina Farsiu. (2024)<br><strong>Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond</strong><br><button class=copy-to-clipboard title="Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18698v1.pdf filename=2402.18698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generic <b>object</b> <b>detection</b> is a category-independent task that relies on accurate modeling of objectness. Most relevant <b>CNN-based</b> models of objectness utilize loss functions (e.g., binary cross entropy) that focus on the single-response, i.e., the loss response of a single pixel. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions (i.e., hard regions) before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that uses the mutual response between adjacent pixels to suppress or emphasize the single-response of pixels. We demonstrate that the proposed SCLoss can gradually learn the hard regions by detecting and emphasizing their boundaries. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged <b>object</b> <b>detection</b> (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in the SOTA outcomes for different applications. Finally, as a demonstrative example of the potential uses for other related tasks, we show an application of SCLoss for semantic segmentation.</p></p class="citation"></blockquote><h3 id=2858--92286-selection-of-appropriate-multispectral-camera-exposure-settings-and-radiometric-calibration-methods-for-applications-in-phenotyping-and-precision-agriculture-vaishali-swaminathan-et-al-2024>(28/58 | 92/286) Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture (Vaishali Swaminathan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaishali Swaminathan, J. Alex Thomasson, Robert G. Hardin, Nithya Rajan. (2024)<br><strong>Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture</strong><br><button class=copy-to-clipboard title="Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18553v1.pdf filename=2402.18553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiometric accuracy of data is crucial in quantitative precision agriculture, to produce reliable and repeatable data for modeling and decision making. The effect of exposure time and gain settings on the radiometric accuracy of multispectral images was not explored enough. The goal of this study was to determine if having a fixed exposure (FE) time during image acquisition improved radiometric accuracy of images, compared to the default auto-exposure (AE) settings. This involved quantifying the errors from auto-exposure and determining ideal exposure values within which radiometric mean absolute percentage error (MAPE) were minimal (&lt; 5%). The results showed that FE orthomosaic was closer to ground-truth (higher R2 and lower MAPE) than AE orthomosaic. An ideal exposure range was determined for capturing canopy and soil objects, without loss of information from under-exposure or saturation from over-exposure. A <b>simulation</b> of errors from AE showed that MAPE &lt; 5% for the blue, green, red, and NIR bands and &lt; 7% for the red edge band for exposure settings within the determined ideal ranges and increased exponentially beyond the ideal exposure upper limit. Further, prediction of total plant nitrogen uptake (g/plant) using vegetation indices (VIs) from two different growing seasons were closer to the ground truth (mostly, R2 > 0.40, and MAPE = 12 to 14%, p &lt; 0.05) when FE was used, compared to the prediction from AE images (mostly, R2 &lt; 0.13, MAPE = 15 to 18%, p >= 0.05).</p></p class="citation"></blockquote><h3 id=2958--93286-gradient-reweighting-towards-imbalanced-class-incremental-learning-jiangpeng-he-et-al-2024>(29/58 | 93/286) Gradient Reweighting: Towards Imbalanced Class-Incremental Learning (Jiangpeng He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiangpeng He, Fengqing Zhu. (2024)<br><strong>Gradient Reweighting: Towards Imbalanced Class-Incremental Learning</strong><br><button class=copy-to-clipboard title="Gradient Reweighting: Towards Imbalanced Class-Incremental Learning" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18528v1.pdf filename=2402.18528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class-Incremental Learning (CIL) trains a model to continually recognize new classes from non-stationary data while retaining learned <b>knowledge.</b> <b>A</b> major challenge of CIL arises when applying to real-world data characterized by non-uniform distribution, which introduces a dual imbalance problem involving (i) disparities between stored exemplars of old tasks and new class data (inter-phase imbalance), and (ii) severe class imbalances within each individual task (intra-phase imbalance). We show that this dual imbalance issue causes skewed gradient updates with biased weights in FC layers, thus inducing over/under-fitting and catastrophic forgetting in CIL. Our method addresses it by reweighting the gradients towards balanced optimization and unbiased classifier learning. Additionally, we observe imbalanced forgetting where paradoxically the instance-rich classes suffer higher performance degradation during CIL due to a larger amount of training data becoming unavailable in subsequent learning phases. To tackle this, we further introduce a distribution-aware <b>knowledge</b> <b>distillation</b> loss to mitigate forgetting by aligning output logits proportionally with the distribution of lost training data. We validate our method on CIFAR-100, ImageNetSubset, and Food101 across various evaluation protocols and demonstrate consistent improvements compared to existing works, showing great potential to apply CIL in real-world scenarios with enhanced robustness and effectiveness.</p></p class="citation"></blockquote><h3 id=3058--94286-separate-and-conquer-decoupling-co-occurrence-via-decomposition-and-representation-for-weakly-supervised-semantic-segmentation-zhiwei-yang-et-al-2024>(30/58 | 94/286) Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation (Zhiwei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Yang, Kexue Fu, Minghong Duan, Linhao Qu, Shuo Wang, Zhijian Song. (2024)<br><strong>Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18467v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18467v2.pdf filename=2402.18467v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attributed to the frequent coupling of co-occurring objects and the limited supervision from image-level labels, the challenging co-occurrence problem is widely present and leads to false activation of objects in weakly <b>supervised</b> semantic segmentation (WSSS). In this work, we devise a &lsquo;Separate and Conquer&rsquo; scheme SeCo to tackle this issue from dimensions of image space and feature space. In the image space, we propose to &lsquo;separate&rsquo; the co-occurring objects with image decomposition by subdividing images into patches. Importantly, we assign each patch a category tag from Class Activation Maps (CAMs), which spatially helps remove the co-context bias and guide the subsequent representation. In the feature space, we propose to &lsquo;conquer&rsquo; the false activation by enhancing semantic representation with multi-granularity knowledge contrast. To this end, a dual-teacher-single-student architecture is designed and tag-guided contrast is conducted to guarantee the correctness of knowledge and further facilitate the discrepancy among co-occurring objects. We streamline the multi-staged WSSS pipeline end-to-end and tackle co-occurrence without external supervision. Extensive experiments are conducted, validating the efficiency of our method tackling co-occurrence and the superiority over previous single-staged and even multi-staged competitors on PASCAL VOC and MS COCO. Code will be available at <a href=https://github.com/zwyang6/SeCo.git>https://github.com/zwyang6/SeCo.git</a>.</p></p class="citation"></blockquote><h3 id=3158--95286-attention-propagation-network-for-egocentric-heatmap-to-3d-pose-lifting-taeho-kang-et-al-2024>(31/58 | 95/286) Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting (Taeho Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taeho Kang, Youngki Lee. (2024)<br><strong>Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting</strong><br><button class=copy-to-clipboard title="Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-Attention, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18330v1.pdf filename=2402.18330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present EgoTAP, a heatmap-to-3D pose lifting method for highly accurate stereo egocentric 3D pose estimation. Severe self-occlusion and out-of-view limbs in egocentric camera views make accurate pose estimation a challenging problem. To address the challenge, prior methods employ joint heatmaps-probabilistic 2D representations of the body pose, but heatmap-to-3D pose conversion still remains an inaccurate process. We propose a novel heatmap-to-3D lifting method composed of the Grid ViT Encoder and the Propagation Network. The Grid ViT Encoder <b>summarizes</b> joint heatmaps into effective feature embedding using <b>self-attention.</b> Then, the Propagation Network estimates the 3D pose by utilizing skeletal information to better estimate the position of obscure joints. Our method significantly outperforms the previous state-of-the-art qualitatively and quantitatively demonstrated by a 23.9% reduction of error in an MPJPE metric. Our source code is available in GitHub.</p></p class="citation"></blockquote><h3 id=3258--96286-location-guided-head-pose-estimation-for-fisheye-image-bing-li-et-al-2024>(32/58 | 96/286) Location-guided Head Pose Estimation for Fisheye Image (Bing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, Dah-Jye Lee. (2024)<br><strong>Location-guided Head Pose Estimation for Fisheye Image</strong><br><button class=copy-to-clipboard title="Location-guided Head Pose Estimation for Fisheye Image" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18320v1.pdf filename=2402.18320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end <b>convolutional</b> <b>neural</b> <b>network</b> to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \textcolor{blue}{a} fisheye-\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experiments results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.</p></p class="citation"></blockquote><h3 id=3358--97286-grid-based-continuous-normal-representation-for-anomaly-detection-joo-chan-lee-et-al-2024>(33/58 | 97/286) Grid-Based Continuous Normal Representation for Anomaly Detection (Joo Chan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joo Chan Lee, Taejune Kim, Eunbyung Park, Simon S. Woo, Jong Hwan Ko. (2024)<br><strong>Grid-Based Continuous Normal Representation for Anomaly Detection</strong><br><button class=copy-to-clipboard title="Grid-Based Continuous Normal Representation for Anomaly Detection" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18293v1.pdf filename=2402.18293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There have been significant advancements in <b>anomaly</b> <b>detection</b> in an <b>unsupervised</b> manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing the input and the directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose GRAD, a novel <b>anomaly</b> <b>detection</b> method for representing normal features within a &ldquo;continuous&rdquo; feature space, enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for <b>anomaly</b> <b>detection,</b> representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that GRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, GRAD effectively handles diverse classes in a single model thanks to the high-granularity global representation. In an evaluation using the MVTec AD dataset, GRAD significantly outperforms the previous state-of-the-art method by reducing 65.0% of the error for multi-class unified <b>anomaly</b> <b>detection.</b> The project page is available at <a href=https://tae-mo.github.io/grad/>https://tae-mo.github.io/grad/</a>.</p></p class="citation"></blockquote><h3 id=3458--98286-fsl-model-can-score-higher-as-it-is-yunwei-bai-et-al-2024>(34/58 | 98/286) FSL Model can Score Higher as It Is (Yunwei Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunwei Bai, Ying Kiat Tan, Tsuhan Chen. (2024)<br><strong>FSL Model can Score Higher as It Is</strong><br><button class=copy-to-clipboard title="FSL Model can Score Higher as It Is" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18292v1.pdf filename=2402.18292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. <b>Few-shot-learning</b> <b>(FSL)</b> classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with <b>few-shot</b> <b>samples.</b> Our proposed method first captures the style or shape of the test image, and then identifies a suitable trained class sample. It then transfers the style or shape of the test image to the train-class images for generation of more test-class samples, before performing classification based on a set of generated samples instead of just one sample. Our method has potential in empowering a trained FSL model to score higher during the testing phase without any extra training nor dataset. According to our experiments, by augmenting the support set with just 1 additional generated sample, we can achieve around 2% improvement for trained FSL models on datasets consisting of either animal faces or traffic signs. By augmenting both the support set and the queries, we can achieve even more performance improvement. Our Github Repository is publicly available.</p></p class="citation"></blockquote><h3 id=3558--99286-zero-shot-aerial-object-detection-with-visual-description-regularization-zhengqing-zang-et-al-2024>(35/58 | 99/286) Zero-Shot Aerial Object Detection with Visual Description Regularization (Zhengqing Zang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengqing Zang, Chenyu Lin, Chenwei Tang, Tao Wang, Jiancheng Lv. (2024)<br><strong>Zero-Shot Aerial Object Detection with Visual Description Regularization</strong><br><button class=copy-to-clipboard title="Zero-Shot Aerial Object Detection with Visual Description Regularization" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18233v1.pdf filename=2402.18233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>object</b> <b>detection</b> models are mainly trained on large-scale labeled datasets. However, annotating data for novel aerial <b>object</b> <b>classes</b> is expensive since it is time-consuming and may require expert knowledge. Thus, it is desirable to study label-efficient <b>object</b> <b>detection</b> methods on aerial images. In this work, we propose a <b>zero-shot</b> method for aerial <b>object</b> <b>detection</b> named visual Description Regularization, or DescReg. Concretely, we identify the weak semantic-visual correlation of the aerial <b>objects</b> <b>and</b> aim to address the challenge with prior descriptions of their visual appearance. Instead of directly encoding the descriptions into class embedding space which suffers from the representation gap problem, we propose to infuse the prior inter-class visual similarity conveyed in the descriptions into the embedding learning. The infusion process is accomplished with a newly designed similarity-aware triplet loss which incorporates structured regularization on the representation space. We conduct extensive experiments with three challenging aerial <b>object</b> <b>detection</b> datasets, including DIOR, xView, and DOTA. The results demonstrate that DescReg significantly outperforms the state-of-the-art ZSD methods with complex projection designs and generative frameworks, e.g., DescReg outperforms best reported ZSD method on DIOR by 4.5 mAP on unseen classes and 8.1 in HM. We further show the generalizability of DescReg by integrating it into generative ZSD methods as well as varying the detection architecture.</p></p class="citation"></blockquote><h3 id=3658--100286-balancing-act-distribution-guided-debiasing-in-diffusion-models-rishubh-parihar-et-al-2024>(36/58 | 100/286) Balancing Act: Distribution-Guided Debiasing in Diffusion Models (Rishubh Parihar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishubh Parihar, Abhijnya Bhat, Saswat Mallick, Abhipsa Basu, Jogendra Nath Kundu, R. Venkatesh Babu. (2024)<br><strong>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</strong><br><button class=copy-to-clipboard title="Balancing Act: Distribution-Guided Debiasing in Diffusion Models" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18206v1.pdf filename=2402.18206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>Models</b> (DMs) have emerged as powerful generative models with unprecedented image generation capability. These models are widely used for <b>data</b> <b>augmentation</b> and creative applications. However, DMs reflect the biases present in the training datasets. This is especially concerning in the context of faces, where the DM prefers one demographic subgroup vs others (eg. female vs male). In this work, we present a method for debiasing DMs without relying on additional <b>data</b> <b>or</b> model retraining. Specifically, we propose Distribution Guidance, which enforces the generated images to follow the prescribed attribute distribution. To realize this, we build on the key insight that the latent features of denoising UNet hold rich demographic semantics, and the same can be leveraged to guide debiased generation. We train Attribute Distribution Predictor (ADP) - a small mlp that maps the latent features to the distribution of attributes. ADP is trained with pseudo labels generated from existing attribute classifiers. The proposed Distribution Guidance with ADP enables us to do fair generation. Our method reduces bias across single/multiple attributes and outperforms the baseline by a significant margin for unconditional and text-conditional <b>diffusion</b> <b>models.</b> Further, we present a downstream task of training a fair attribute classifier by rebalancing the training set with our generated data.</p></p class="citation"></blockquote><h3 id=3758--101286-ntop-nerf-powered-large-scale-dataset-generation-for-2d-and-3d-human-pose-estimation-in-top-view-fisheye-images-jingrui-yu-et-al-2024>(37/58 | 101/286) NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images (Jingrui Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingrui Yu, Dipankar Nandi, Roman Seidel, Gangolf Hirtz. (2024)<br><strong>NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images</strong><br><button class=copy-to-clipboard title="NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18196v1.pdf filename=2402.18196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human pose estimation (HPE) in the top-view using fisheye cameras presents a promising and innovative application domain. However, the availability of datasets capturing this viewpoint is extremely limited, especially those with high-quality 2D and 3D keypoint annotations. Addressing this gap, we leverage the capabilities of Neural Radiance Fields (NeRF) technique to establish a comprehensive pipeline for generating human pose datasets from existing 2D and 3D datasets, specifically tailored for the top-view fisheye perspective. Through this pipeline, we create a novel dataset NToP570K (NeRF-powered Top-view human Pose dataset for fisheye cameras with over 570 thousand images), and conduct an extensive evaluation of its efficacy in enhancing neural networks for 2D and 3D top-view human pose estimation. A pretrained ViTPose-B model achieves an improvement in AP of 33.3 % on our validation set for 2D HPE after <b>finetuning</b> on our training set. A similarly <b>finetuned</b> HybrIK-Transformer model gains 53.7 mm reduction in PA-MPJPE for 3D HPE on the validation set.</p></p class="citation"></blockquote><h3 id=3858--102286-cfdnet-a-generalizable-foggy-stereo-matching-network-with-contrastive-feature-distillation-zihua-liu-et-al-2024>(38/58 | 102/286) CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation (Zihua Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihua Liu, Yizhou Li, Masatoshi Okutomi. (2024)<br><strong>CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation</strong><br><button class=copy-to-clipboard title="CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18181v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18181v2.pdf filename=2402.18181v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on <b>contrastive</b> <b>feature</b> <b>distillation</b> (CFD). This strategy combines feature <b>distillation</b> from merged clean-fog features with <b>contrastive</b> <b>learning,</b> ensuring balanced dependence on fog depth hints and clean matching features. This framework helps to enhance model generalization across both clean and foggy environments. Comprehensive experiments on synthetic and real-world datasets affirm the superior strength and adaptability of our method.</p></p class="citation"></blockquote><h3 id=3958--103286-self-supervised-spatially-variant-psf-estimation-for-aberration-aware-depth-from-defocus-zhuofeng-wu-et-al-2024>(39/58 | 103/286) Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus (Zhuofeng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuofeng Wu, Yusuke Monno, Masatoshi Okutomi. (2024)<br><strong>Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus</strong><br><button class=copy-to-clipboard title="Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18175v1.pdf filename=2402.18175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the task of aberration-aware depth-from-defocus (DfD), which takes account of spatially variant point spread functions (PSFs) of a real camera. To effectively obtain the spatially variant PSFs of a real camera without requiring any ground-truth PSFs, we propose a novel <b>self-supervised</b> <b>learning</b> method that leverages the pair of real sharp and blurred images, which can be easily captured by changing the aperture setting of the camera. In our PSF estimation, we assume rotationally symmetric PSFs and introduce the polar coordinate system to more accurately learn the PSF estimation network. We also handle the focus breathing phenomenon that occurs in real DfD situations. Experimental results on synthetic and real data demonstrate the effectiveness of our method regarding both the PSF estimation and the depth estimation.</p></p class="citation"></blockquote><h3 id=4058--104286-from-generalization-to-precision-exploring-sam-for-tool-segmentation-in-surgical-environments-kanyifeechukwu-j-oguine-et-al-2024>(40/58 | 104/286) From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments (Kanyifeechukwu J. Oguine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanyifeechukwu J. Oguine, Roger D. Soberanis-Mukul, Nathan Drenkow, Mathias Unberath. (2024)<br><strong>From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments</strong><br><button class=copy-to-clipboard title="From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17972v1.pdf filename=2402.17972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Accurate tool segmentation is essential in computer-aided procedures. However, this task conveys challenges due to artifacts&rsquo; presence and the limited training data in medical scenarios. Methods that generalize to unseen data represent an interesting venue, where <b>zero-shot</b> segmentation presents an option to account for data limitation. Initial exploratory works with the Segment Anything Model (SAM) show that bounding-box-based <b>prompting</b> presents notable zero-short generalization. However, point-based <b>prompting</b> leads to a degraded performance that further deteriorates under image corruption. We argue that SAM drastically over-segment images with high corruption levels, resulting in degraded performance when only a single segmentation mask is considered, while the combination of the masks overlapping the object of interest generates an accurate prediction. Method: We use SAM to generate the over-segmented prediction of endoscopic frames. Then, we employ the ground-truth tool mask to analyze the results of SAM when the best single mask is selected as prediction and when all the individual masks overlapping the object of interest are combined to obtain the final predicted mask. We analyze the Endovis18 and Endovis17 instrument segmentation datasets using synthetic corruptions of various strengths and an In-House dataset featuring counterfactually created real-world corruptions. Results: Combining the over-segmented masks contributes to improvements in the IoU. Furthermore, selecting the best single segmentation presents a competitive IoU score for clean images. Conclusions: Combined SAM predictions present improved results and robustness up to a certain corruption level. However, appropriate <b>prompting</b> strategies are fundamental for implementing these models in the medical domain.</p></p class="citation"></blockquote><h3 id=4158--105286-vision-language-model-based-caption-evaluation-method-leveraging-visual-context-extraction-koki-maeda-et-al-2024>(41/58 | 105/286) Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction (Koki Maeda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki. (2024)<br><strong>Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction</strong><br><button class=copy-to-clipboard title="Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17969v1.pdf filename=2402.17969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fail short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE$^2$, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.</p></p class="citation"></blockquote><h3 id=4258--106286-unimode-unified-monocular-3d-object-detection-zhuoling-li-et-al-2024>(42/58 | 106/286) UniMODE: Unified Monocular 3D Object Detection (Zhuoling Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao. (2024)<br><strong>UniMODE: Unified Monocular 3D Object Detection</strong><br><button class=copy-to-clipboard title="UniMODE: Unified Monocular 3D Object Detection" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Object Detection, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18573v1.pdf filename=2402.18573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Realizing unified monocular 3D <b>object</b> <b>detection,</b> including both indoor and outdoor scenes, holds great importance in applications like robot navigation. However, involving various scenarios of data to train models poses challenges due to their significantly different characteristics, e.g., diverse <b>geometry</b> properties and heterogeneous domain distributions. To address these challenges, we build a detector based on the bird&rsquo;s-eye-view (BEV) detection paradigm, where the explicit feature projection is beneficial to addressing the <b>geometry</b> learning ambiguity when employing multiple scenarios of data to train detectors. Then, we split the classical BEV detection architecture into two stages and propose an uneven BEV grid design to handle the convergence instability caused by the aforementioned challenges. Moreover, we develop a sparse BEV feature projection strategy to reduce computational cost and a unified domain alignment method to handle heterogeneous domains. Combining these techniques, a unified detector UniMODE is derived, which surpasses the previous state-of-the-art on the challenging Omni3D dataset (a large-scale dataset including both indoor and outdoor scenes) by 4.9% AP_3D, revealing the first successful generalization of a BEV detector to unified 3D <b>object</b> <b>detection.</b></p></p class="citation"></blockquote><h3 id=4358--107286-defect-detection-in-tire-x-ray-images-conventional-methods-meet-deep-structures-andrei-cozma-et-al-2024>(43/58 | 107/286) Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures (Andrei Cozma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Cozma, Landon Harris, Hairong Qi, Ping Ji, Wenpeng Guo, Song Yuan. (2024)<br><strong>Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures</strong><br><button class=copy-to-clipboard title="Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-7; I-4-9; I-4-0, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18527v1.pdf filename=2402.18527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance the performance of defect detection systems. By meticulously integrating combinations of these features with a Random Forest (RF) classifier and comparing them against advanced models like YOLOv8, the research not only <b>benchmarks</b> the performance of traditional features in defect detection but also explores the synergy between classical and modern approaches. The experimental results demonstrate that these traditional features, when <b>fine-tuned</b> and combined with machine learning models, can significantly improve the accuracy and reliability of tire defect detection, aiming to set a new standard in automated quality assurance in tire manufacturing.</p></p class="citation"></blockquote><h3 id=4458--108286-learning-invariant-inter-pixel-correlations-for-superpixel-generation-sen-xu-et-al-2024>(44/58 | 108/286) Learning Invariant Inter-pixel Correlations for Superpixel Generation (Sen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sen Xu, Shikui Wei, Tao Ruan, Lixin Liao. (2024)<br><strong>Learning Invariant Inter-pixel Correlations for Superpixel Generation</strong><br><button class=copy-to-clipboard title="Learning Invariant Inter-pixel Correlations for Superpixel Generation" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18201v1.pdf filename=2402.18201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep superpixel algorithms have made remarkable strides by substituting hand-crafted features with learnable ones. Nevertheless, we observe that existing deep superpixel methods, serving as mid-level representation operations, remain sensitive to the statistical properties (e.g., color distribution, high-level semantics) embedded within the training dataset. Consequently, learnable features exhibit constrained discriminative capability, resulting in unsatisfactory pixel grouping performance, particularly in untrainable application scenarios. To address this issue, we propose the Content Disentangle Superpixel (CDS) algorithm to selectively separate the invariant inter-pixel correlations and statistical properties, i.e., style noise. Specifically, We first construct auxiliary modalities that are homologous to the original RGB image but have substantial stylistic variations. Then, driven by <b>mutual</b> <b>information,</b> we propose the local-grid correlation alignment across modalities to reduce the distribution discrepancy of adaptively selected features and learn invariant inter-pixel correlations. Afterwards, we perform global-style <b>mutual</b> <b>information</b> minimization to enforce the separation of invariant content and train data styles. The experimental results on four <b>benchmark</b> datasets demonstrate the superiority of our approach to existing state-of-the-art methods, regarding boundary adherence, generalization, and efficiency. Code and pre-trained model are available at <a href=https://github.com/rookiie/CDSpixel>https://github.com/rookiie/CDSpixel</a>.</p></p class="citation"></blockquote><h3 id=4558--109286-univs-unified-and-universal-video-segmentation-with-prompts-as-queries-minghan-li-et-al-2024>(45/58 | 109/286) UniVS: Unified and Universal Video Segmentation with Prompts as Queries (Minghan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghan Li, Shuai Li, Xindong Zhang, Lei Zhang. (2024)<br><strong>UniVS: Unified and Universal Video Segmentation with Prompts as Queries</strong><br><button class=copy-to-clipboard title="UniVS: Unified and Universal Video Segmentation with Prompts as Queries" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18115v1.pdf filename=2402.18115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent advances in unified image segmentation (IS), developing a unified video segmentation (VS) model remains a challenge. This is mainly because generic category-specified VS tasks need to detect all objects and track them across consecutive frames, while <b>prompt-guided</b> VS tasks require re-identifying the target with visual/text <b>prompts</b> throughout the entire video, making it hard to handle the different tasks with the same architecture. We make an attempt to address these issues and present a novel unified VS architecture, namely UniVS, by using <b>prompts</b> as queries. UniVS averages the <b>prompt</b> features of the target from previous frames as its initial query to explicitly decode masks, and introduces a target-wise <b>prompt</b> cross-attention layer in the mask decoder to integrate <b>prompt</b> features in the memory pool. By taking the predicted masks of entities from previous frames as their visual <b>prompts,</b> UniVS converts different VS tasks into <b>prompt-guided</b> target segmentation, eliminating the heuristic inter-frame matching process. Our framework not only unifies the different VS tasks but also naturally achieves universal training and testing, ensuring robust performance across different scenarios. UniVS shows a commendable balance between performance and universality on 10 challenging VS <b>benchmarks,</b> covering video instance, semantic, panoptic, object, and referring segmentation tasks. Code can be found at \url{https://github.com/MinghanLi/UniVS}.</p></p class="citation"></blockquote><h3 id=4658--110286-representing-3d-sparse-map-points-and-lines-for-camera-relocalization-bach-thuan-bui-et-al-2024>(46/58 | 110/286) Representing 3D sparse map points and lines for camera relocalization (Bach-Thuan Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bach-Thuan Bui, Huy-Hoang Bui, Dinh-Tuan Tran, Joo-Ho Lee. (2024)<br><strong>Representing 3D sparse map points and lines for camera relocalization</strong><br><button class=copy-to-clipboard title="Representing 3D sparse map points and lines for camera relocalization" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18011v1.pdf filename=2402.18011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in visual localization and mapping have demonstrated considerable success in integrating point and line features. However, expanding the localization framework to include additional mapping components frequently results in increased demand for memory and computational resources dedicated to matching tasks. In this study, we show how a lightweight neural network can learn to represent both 3D point and line features, and exhibit leading pose accuracy by harnessing the power of multiple learned mappings. Specifically, we utilize a single <b>transformer</b> block to encode line features, effectively transforming them into distinctive point-like descriptors. Subsequently, we treat these point and line descriptor sets as distinct yet interconnected feature sets. Through the integration of self- and cross-attention within several <b>graph</b> layers, our method effectively refines each feature before regressing 3D maps using two simple MLPs. In comprehensive experiments, our indoor localization findings surpass those of Hloc and Limap across both point-based and line-assisted configurations. Moreover, in outdoor scenarios, our method secures a significant lead, marking the most considerable enhancement over state-of-the-art learning-based methodologies. The source code and demo videos of this work are publicly available at: <a href=https://thpjp.github.io/pl2map/>https://thpjp.github.io/pl2map/</a></p></p class="citation"></blockquote><h3 id=4758--111286-trends-applications-and-challenges-in-human-attention-modelling-giuseppe-cartella-et-al-2024>(47/58 | 111/286) Trends, Applications, and Challenges in Human Attention Modelling (Giuseppe Cartella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Cartella, Marcella Cornia, Vittorio Cuculo, Alessandro D&rsquo;Amelio, Dario Zanca, Giuseppe Boccignone, Rita Cucchiara. (2024)<br><strong>Trends, Applications, and Challenges in Human Attention Modelling</strong><br><button class=copy-to-clipboard title="Trends, Applications, and Challenges in Human Attention Modelling" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18673v1.pdf filename=2402.18673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, <b>vision-and-language</b> applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges. For a comprehensive overview on the ongoing research refer to our dedicated repository available at <a href=https://github.com/aimagelab/awesome-human-visual-attention>https://github.com/aimagelab/awesome-human-visual-attention</a>.</p></p class="citation"></blockquote><h3 id=4858--112286-detection-of-micromobility-vehicles-in-urban-traffic-videos-khalil-sabri-et-al-2024>(48/58 | 112/286) Detection of Micromobility Vehicles in Urban Traffic Videos (Khalil Sabri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khalil Sabri, Célia Djilali, Guillaume-Alexandre Bilodeau, Nicolas Saunier, Wassim Bouachir. (2024)<br><strong>Detection of Micromobility Vehicles in Urban Traffic Videos</strong><br><button class=copy-to-clipboard title="Detection of Micromobility Vehicles in Urban Traffic Videos" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18503v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18503v1.pdf filename=2402.18503v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban traffic environments present unique challenges for <b>object</b> <b>detection,</b> particularly with the increasing presence of micromobility vehicles like e-scooters and bikes. To address this <b>object</b> <b>detection</b> problem, this work introduces an adapted detection model that combines the accuracy and speed of single-frame <b>object</b> <b>detection</b> with the richer features offered by video <b>object</b> <b>detection</b> frameworks. This is done by applying aggregated feature maps from consecutive frames processed through motion flow to the YOLOX architecture. This fusion brings a temporal perspective to YOLOX detection abilities, allowing for a better understanding of urban mobility patterns and substantially improving detection reliability. Tested on a custom dataset curated for urban micromobility scenarios, our model showcases substantial improvement over existing state-of-the-art methods, demonstrating the need to consider spatio-temporal information for detecting such small and thin <b>objects.</b> <b>Our</b> approach enhances detection in challenging conditions, including occlusions, ensuring temporal consistency, and effectively mitigating motion blur.</p></p class="citation"></blockquote><h3 id=4958--113286-ibd-alleviating-hallucinations-in-large-vision-language-models-via-image-biased-decoding-lanyun-zhu-et-al-2024>(49/58 | 113/286) IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding (Lanyun Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, Jun Liu. (2024)<br><strong>IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding</strong><br><button class=copy-to-clipboard title="IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18476v1.pdf filename=2402.18476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite achieving rapid developments and with widespread applications, Large <b>Vision-Language</b> Models (LVLMs) confront a serious challenge of being prone to generating hallucinations. An over-reliance on linguistic priors has been identified as a key factor leading to these hallucinations. In this paper, we propose to alleviate this problem by introducing a novel image-biased decoding (IBD) technique. Our method derives the next-token probability distribution by contrasting predictions from a conventional LVLM with those of an image-biased LVLM, thereby amplifying the correct information highly correlated with image content while mitigating the hallucinatory errors caused by excessive dependence on text. We further conduct a comprehensive statistical analysis to validate the reliability of our method, and design an adaptive adjustment strategy to achieve robust and flexible handling under varying conditions. Experimental results across multiple evaluation metrics verify that our method, despite not requiring additional training data and only with a minimal increase in model parameters, can significantly reduce hallucinations in LVLMs and enhance the truthfulness of the generated response.</p></p class="citation"></blockquote><h3 id=5058--114286-latentswap-an-efficient-latent-code-mapping-framework-for-face-swapping-changho-choi-et-al-2024>(50/58 | 114/286) LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping (Changho Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changho Choi, Minho Kim, Junhyeok Lee, Hyoung-Kyu Song, Younggeun Kim, Seungryong Kim. (2024)<br><strong>LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping</strong><br><button class=copy-to-clipboard title="LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18351v1.pdf filename=2402.18351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose LatentSwap, a simple face swapping framework generating a face swap latent code of a given generator. Utilizing randomly sampled latent codes, our framework is light and does not require datasets besides employing the pre-trained models, with the training procedure also being fast and straightforward. The loss objective consists of only three terms, and can effectively control the face swap results between source and target images. By attaching a pre-trained <b>GAN</b> inversion model independent to the model and using the StyleGAN2 generator, our model produces photorealistic and high-resolution images comparable to other competitive face swap models. We show that our framework is applicable to other generators such as StyleNeRF, paving a way to 3D-aware face swapping and is also compatible with other downstream StyleGAN2 generator tasks. The source code and models can be found at \url{https://github.com/usingcolor/LatentSwap}.</p></p class="citation"></blockquote><h3 id=5158--115286-ean-mapnet-efficient-vectorized-hd-map-construction-with-anchor-neighborhoods-huiyuan-xiong-et-al-2024>(51/58 | 115/286) EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods (Huiyuan Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huiyuan Xiong, Jun Shen, Taohong Zhu, Yuelong Pan. (2024)<br><strong>EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods</strong><br><button class=copy-to-clipboard title="EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18278v1.pdf filename=2402.18278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-definition (HD) map is crucial for autonomous driving systems. Most existing works design map elements detection heads based on the DETR decoder. However, the initial queries lack integration with the physical location feature of map elements, and vanilla <b>self-attention</b> entails high computational complexity. Therefore, we propose EAN-MapNet for Efficiently constructing HD map using Anchor Neighborhoods. Firstly, we design query units based on the physical location feature of anchor neighborhoods. Non-neighborhood central anchors effectively assist the neighborhood central anchors in fitting to the target points, significantly improving the prediction accuracy. Then, we introduce grouped local <b>self-attention</b> (GL-SA), which innovatively utilizes local queries as the medium for feature interaction, thereby substantially reducing the computational complexity of <b>self-attention</b> while facilitating ample feature interaction among queries. On nuScenes dataset, EAN-MapNet achieves a state-of-the-art performance with 63.0 mAP after training for 24 epochs. Furthermore, it considerably reduces memory consumption by 8198M compared to the baseline.</p></p class="citation"></blockquote><h3 id=5258--116286-misalignment-robust-frequency-distribution-loss-for-image-transformation-zhangkai-ni-et-al-2024>(52/58 | 116/286) Misalignment-Robust Frequency Distribution Loss for Image Transformation (Zhangkai Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangkai Ni, Juncheng Wu, Zian Wang, Wenhan Yang, Hanli Wang, Lin Ma. (2024)<br><strong>Misalignment-Robust Frequency Distribution Loss for Image Transformation</strong><br><button class=copy-to-clipboard title="Misalignment-Robust Frequency Distribution Loss for Image Transformation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18192v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18192v1.pdf filename=2402.18192v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims to address a common challenge in deep learning-based image transformation methods, such as image enhancement and super-resolution, which heavily rely on precisely aligned paired datasets with pixel-level alignments. However, creating precisely aligned paired images presents significant challenges and hinders the advancement of methods trained on such data. To overcome this challenge, this paper introduces a novel and simple Frequency Distribution Loss (FDL) for computing distribution distance within the frequency domain. Specifically, we transform image features into the frequency domain using Discrete Fourier Transformation (DFT). Subsequently, frequency components (amplitude and phase) are processed separately to form the FDL loss function. Our method is empirically proven effective as a training constraint due to the thoughtful utilization of global information in the frequency domain. Extensive experimental evaluations, focusing on image enhancement and super-resolution tasks, demonstrate that FDL outperforms existing misalignment-robust loss functions. Furthermore, we explore the potential of our FDL for image <b>style</b> <b>transfer</b> that relies solely on completely misaligned data. Our code is available at: <a href=https://github.com/eezkni/FDL>https://github.com/eezkni/FDL</a></p></p class="citation"></blockquote><h3 id=5358--117286-out-of-distribution-detection-using-neural-activation-prior-weilin-wan-et-al-2024>(53/58 | 117/286) Out-of-Distribution Detection using Neural Activation Prior (Weilin Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weilin Wan, Weizhong Zhang, Cheng Jin. (2024)<br><strong>Out-of-Distribution Detection using Neural Activation Prior</strong><br><button class=copy-to-clipboard title="Out-of-Distribution Detection using Neural Activation Prior" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18162v1.pdf filename=2402.18162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Out-of-distribution</b> detection is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios.In this paper, we propose a simple but effective Neural Activation Prior (NAP) for <b>out-of-distribution</b> detection (OOD). Our neural activation prior is based on a key observation that, for a channel before the global pooling layer of a fully trained neural network, the probability of a few of its neurons being activated with a larger response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is each channel in a model fully trained on ID dataset would play a role in detecting a certain pattern in the samples within the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Thus, a new scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. This approach is plug-and-play and does not lead to any performance degradation on in-distribution data classification and requires no extra training or statistics from training or external datasets. Notice that previous methods primarily rely on post-global-pooling features of the neural networks, while the within-channel distribution information we leverage would be discarded by the global pooling operator. Consequently, our method is orthogonal to existing approaches and can be effectively combined with them in various applications. Experimental results show that our method achieves the state-of-the-art performance on CIFAR-10, CIFAR-100 and ImageNet datasets, which demonstrates the power of the proposed prior.</p></p class="citation"></blockquote><h3 id=5458--118286-occtransformer-improving-bevformer-for-3d-camera-only-occupancy-prediction-jian-liu-et-al-2024>(54/58 | 118/286) OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction (Jian Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Liu, Sipeng Zhang, Chuixin Kong, Wenyuan Zhang, Yuhang Wu, Yikang Ding, Borun Xu, Ruibo Ming, Donglai Wei, Xianming Liu. (2024)<br><strong>OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction</strong><br><button class=copy-to-clipboard title="OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18140v1.pdf filename=2402.18140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report presents our solution, &ldquo;occTransformer&rdquo; for the 3D occupancy prediction track in the autonomous driving challenge at CVPR 2023. Our method builds upon the strong baseline BEVFormer and improves its performance through several simple yet effective techniques. Firstly, we employed <b>data</b> <b>augmentation</b> to increase the diversity of the training <b>data</b> <b>and</b> improve the model&rsquo;s generalization ability. Secondly, we used a strong image backbone to extract more informative features from the input <b>data.</b> <b>Thirdly,</b> we incorporated a 3D unet head to better capture the spatial information of the scene. Fourthly, we added more loss functions to better optimize the model. Additionally, we used an ensemble approach with the occ model BevDet and SurroundOcc to further improve the performance. Most importantly, we integrated 3D detection model StreamPETR to enhance the model&rsquo;s ability to detect objects in the scene. Using these methods, our solution achieved 49.23 miou on the 3D occupancy prediction track in the autonomous driving challenge.</p></p class="citation"></blockquote><h3 id=5558--119286-context-aware-talking-face-video-generation-meidai-xuanyuan-et-al-2024>(55/58 | 119/286) Context-aware Talking Face Video Generation (Meidai Xuanyuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meidai Xuanyuan, Yuwang Wang, Honglei Guo, Qionghai Dai. (2024)<br><strong>Context-aware Talking Face Video Generation</strong><br><button class=copy-to-clipboard title="Context-aware Talking Face Video Generation" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18092v1.pdf filename=2402.18092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider a novel and practical case for talking face video generation. Specifically, we focus on the scenarios involving multi-people interactions, where the talking context, such as audience or surroundings, is present. In these situations, the video generation should take the context into consideration in order to generate video content naturally aligned with driving audios and spatially coherent to the context. To achieve this, we provide a two-stage and cross-modal controllable video generation pipeline, taking facial landmarks as an explicit and compact control signal to bridge the driving audio, talking context and generated videos. Inside this pipeline, we devise a 3D video <b>diffusion</b> <b>model,</b> allowing for efficient contort of both spatial conditions (landmarks and context video), as well as audio condition for temporally coherent generation. The experimental results verify the advantage of the proposed method over other baselines in terms of audio-video synchronization, video fidelity and frame consistency.</p></p class="citation"></blockquote><h3 id=5658--120286-sftformer-a-spatial-frequency-temporal-correlation-decoupling-transformer-for-radar-echo-extrapolation-liangyu-xu-et-al-2024>(56/58 | 120/286) SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation (Liangyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Fanglong Yao, Xian Sun, Kun Fu. (2024)<br><strong>SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation</strong><br><button class=copy-to-clipboard title="SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18044v1.pdf filename=2402.18044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extrapolating future weather radar echoes from past observations is a complex task vital for precipitation nowcasting. The spatial morphology and temporal evolution of radar echoes exhibit a certain degree of correlation, yet they also possess independent characteristics. {Existing methods learn unified spatial and temporal representations in a highly coupled feature space, emphasizing the correlation between spatial and temporal features but neglecting the explicit modeling of their independent characteristics, which may result in mutual interference between them.} To effectively model the spatiotemporal dynamics of radar echoes, we propose a Spatial-Frequency-Temporal correlation-decoupling <b>Transformer</b> (SFTformer). The model leverages stacked multiple SFT-Blocks to not only mine the correlation of the spatiotemporal dynamics of echo cells but also avoid the mutual interference between the temporal modeling and the spatial morphology refinement by decoupling them. Furthermore, inspired by the practice that weather forecast experts effectively review historical echo evolution to make accurate predictions, SFTfomer incorporates a joint training paradigm for historical echo sequence reconstruction and future echo sequence prediction. Experimental results on the HKO-7 dataset and ChinaNorth-2021 dataset demonstrate the superior performance of SFTfomer in short(1h), mid(2h), and long-term(3h) precipitation nowcasting.</p></p class="citation"></blockquote><h3 id=5758--121286-multimodal-learning-to-improve-cardiac-late-mechanical-activation-detection-from-cine-mr-images-jiarui-xing-et-al-2024>(57/58 | 121/286) Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images (Jiarui Xing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiarui Xing, Nian Wu, Kenneth Bilchick, Frederick Epstein, Miaomiao Zhang. (2024)<br><strong>Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images</strong><br><button class=copy-to-clipboard title="Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18507v1.pdf filename=2402.18507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a <b>multimodal</b> deep learning framework that utilizes advanced image techniques to improve the performance of clinical analysis heavily dependent on routinely acquired standard images. More specifically, we develop a joint learning network that for the first time leverages the accuracy and reproducibility of myocardial strains obtained from Displacement Encoding with Stimulated Echo (DENSE) to guide the analysis of cine cardiac magnetic resonance (CMR) imaging in late mechanical activation (LMA) detection. An image registration network is utilized to acquire the knowledge of cardiac motions, an important feature estimator of strain values, from standard cine CMRs. Our framework consists of two major components: (i) a DENSE-supervised strain network leveraging latent motion features learned from a registration network to predict myocardial strains; and (ii) a LMA network taking advantage of the predicted strain for effective LMA detection. Experimental results show that our proposed work substantially improves the performance of strain analysis and LMA detection from cine CMR images, aligning more closely with the achievements of DENSE.</p></p class="citation"></blockquote><h3 id=5858--122286-attentive-illumination-decomposition-model-for-multi-illuminant-white-balancing-dongyoung-kim-et-al-2024>(58/58 | 122/286) Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing (Dongyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongyoung Kim, Jinwoo Kim, Junsang Yu, Seon Joo Kim. (2024)<br><strong>Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing</strong><br><button class=copy-to-clipboard title="Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18277v1.pdf filename=2402.18277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>White balance (WB) algorithms in many commercial cameras assume single and uniform illumination, leading to undesirable results when multiple lighting sources with different chromaticities exist in the scene. Prior research on multi-illuminant WB typically predicts illumination at the pixel level without fully grasping the scene&rsquo;s actual lighting conditions, including the number and color of light sources. This often results in unnatural outcomes lacking in overall consistency. To handle this problem, we present a deep white balancing model that leverages the slot attention, where each slot is in charge of representing individual illuminants. This design enables the model to generate chromaticities and weight maps for individual illuminants, which are then fused to compose the final illumination map. Furthermore, we propose the centroid-matching loss, which regulates the activation of each slot based on the color range, thereby enhancing the model to separate illumination more effectively. Our method achieves the state-of-the-art performance on both single- and multi-illuminant WB <b>benchmarks,</b> and also offers additional information such as the number of illuminants in the scene and their chromaticity. This capability allows for illumination editing, an application not feasible with prior methods.</p></p class="citation"></blockquote><h2 id=cslg-53>cs.LG (53)</h2><h3 id=153--123286-arithmetic-control-of-llms-for-diverse-user-preferences-directional-preference-alignment-with-multi-objective-rewards-haoxiang-wang-et-al-2024>(1/53 | 123/286) Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards (Haoxiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang. (2024)<br><strong>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards</strong><br><button class=copy-to-clipboard title="Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 90<br>Keywords: Direct Preference Optimization, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18571v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18571v2.pdf filename=2402.18571v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fine-grained control over <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> remains a significant challenge, hindering their adaptability to diverse user needs. While <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> shows promise in aligning <b>LLMs,</b> its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward <b>RLHF,</b> DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then <b>fine-tuning</b> the <b>LLM</b> with a preference-conditioned variant of Rejection Sampling <b>Finetuning</b> (RSF), an <b>RLHF</b> method adopted by <b>Llama</b> 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward <b>RLHF,</b> DPA offers users intuitive control over <b>LLM</b> generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on <b>Mistral-7B.</b> Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO).</p></p class="citation"></blockquote><h3 id=253--124286-keeping-llms-aligned-after-fine-tuning-the-crucial-role-of-prompt-templates-kaifeng-lyu-et-al-2024>(2/53 | 124/286) Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates (Kaifeng Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora. (2024)<br><strong>Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates</strong><br><button class=copy-to-clipboard title="Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, LLaMA, Mistral, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18540v1.pdf filename=2402.18540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Public <b>LLMs</b> such as the <b>Llama</b> 2-Chat have driven huge activity in <b>LLM</b> research. These models underwent alignment training and were considered safe. Recently Qi et al. (2023) reported that even benign <b>fine-tuning</b> (e.g., on seemingly safe datasets) can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. Through extensive experiments on several chat models (Meta&rsquo;s <b>Llama</b> 2-Chat, <b>Mistral</b> AI&rsquo;s <b>Mistral</b> 7B Instruct v0.2, and OpenAI&rsquo;s <b>GPT-3.5</b> Turbo), this paper uncovers that the <b>prompt</b> templates used during <b>fine-tuning</b> and inference play a crucial role in preserving safety alignment, and proposes the &ldquo;Pure Tuning, Safe Testing&rdquo; (PTST) principle &ndash; <b>fine-tune</b> models without a safety <b>prompt,</b> but include it at test time. <b>Fine-tuning</b> experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors, and even almost eliminates them in some cases.</p></p class="citation"></blockquote><h3 id=353--125286-rnns-are-not-transformers-yet-the-key-bottleneck-on-in-context-retrieval-kaiyue-wen-et-al-2024>(3/53 | 125/286) RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval (Kaiyue Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiyue Wen, Xingyu Dang, Kaifeng Lyu. (2024)<br><strong>RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval</strong><br><button class=copy-to-clipboard title="RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 83<br>Keywords: Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Recurrent Neural Network, Recurrent Neural Network, Transformer, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18510v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18510v2.pdf filename=2402.18510v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the gap in representation powers of <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> and <b>Transformers</b> in the context of solving algorithmic problems. We focus on understanding whether <b>RNNs,</b> known for their memory efficiency in handling long sequences, can match the performance of <b>Transformers,</b> particularly when enhanced with Chain-of-Thought (CoT) <b>prompting.</b> Our theoretical analysis reveals that CoT improves <b>RNNs</b> but is insufficient to close the gap with <b>Transformers.</b> A key bottleneck lies in the inability of <b>RNNs</b> to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a <b>graph</b> is a tree, we prove that <b>RNNs</b> are not expressive enough to solve the tasks while <b>Transformers</b> can solve them with ease. Conversely, we prove that adopting techniques to enhance the <b>in-context</b> <b>retrieval</b> <b>capability</b> <b>of</b> <b>RNNs,</b> including <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> and adding a single <b>Transformer</b> layer, can elevate <b>RNNs</b> to be capable of solving all polynomial-time solvable problems with CoT, hence closing the representation gap with <b>Transformers.</b></p></p class="citation"></blockquote><h3 id=453--126286-orchid-flexible-and-data-dependent-convolution-for-sequence-modeling-mahdi-karami-et-al-2024>(4/53 | 126/286) Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling (Mahdi Karami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Karami, Ali Ghodsi. (2024)<br><strong>Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling</strong><br><button class=copy-to-clipboard title="Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Vision Transformer, Graph Attention Networks, Convolution, BERT, Transformer, In-context Learning, In-context Learning, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18508v1.pdf filename=2402.18508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of deep learning, the quest for models that balance expressivity with computational efficiency has never been more critical. This paper introduces Orchid, a novel architecture that reimagines sequence modeling by incorporating a new data-dependent <b>convolution</b> mechanism. Orchid is designed to address the inherent limitations of traditional attention mechanisms, particularly their quadratic complexity, without compromising the ability to capture long-range dependencies and <b>in-context</b> <b>learning.</b> At the core of Orchid lies the data-dependent <b>convolution</b> layer, which dynamically adjusts its kernel conditioned on input data using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in the adaptive <b>convolution</b> operation. The dynamic nature of data-dependent <b>convolution</b> kernel, coupled with <b>gating</b> operations, grants Orchid high expressivity while maintaining efficiency and quasilinear scalability for long sequences. We rigorously evaluate Orchid across multiple domains, including language modeling and image classification, to showcase its performance and generality. Our experiments demonstrate that Orchid architecture not only outperforms traditional attention-based architectures such as <b>BERT</b> and <b>Vision</b> <b>Transformers</b> with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.</p></p class="citation"></blockquote><h3 id=553--127286-graph-regularized-encoder-training-for-extreme-classification-anshul-mittal-et-al-2024>(5/53 | 127/286) Graph Regularized Encoder Training for Extreme Classification (Anshul Mittal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anshul Mittal, Shikhar Mohan, Deepak Saini, Suchith C. Prabhu, Jain jiao, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma. (2024)<br><strong>Graph Regularized Encoder Training for Extreme Classification</strong><br><button class=copy-to-clipboard title="Graph Regularized Encoder Training for Extreme Classification" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Convolution, Convolutional Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18434v1.pdf filename=2402.18434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep extreme classification (XC) aims to train an encoder architecture and an accompanying classifier architecture to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, <b>recommendation</b> and tagging routinely encounter tail labels for which the amount of training data is exceedingly small. <b>Graph</b> <b>convolutional</b> <b>networks</b> <b>(GCN)</b> present a convenient but computationally expensive way to leverage task metadata and enhance model accuracies in these settings. This paper formally establishes that in several use cases, the steep computational cost of <b>GCNs</b> is entirely avoidable by replacing <b>GCNs</b> with non-GCN architectures. The paper notices that in these settings, it is much more effective to use <b>graph</b> <b>data</b> <b>to</b> regularize encoder training than to implement a <b>GCN.</b> Based on these insights, an alternative paradigm RAMEN is presented to utilize <b>graph</b> <b>metadata</b> <b>in</b> XC settings that offers significant performance boosts with zero increase in inference computational costs. RAMEN scales to datasets with up to 1M labels and offers prediction accuracy up to 15% higher on <b>benchmark</b> datasets than state of the art methods, including those that use <b>graph</b> <b>metadata</b> <b>to</b> train <b>GCNs.</b> RAMEN also offers 10% higher accuracy over the best baseline on a proprietary <b>recommendation</b> dataset sourced from click logs of a popular search engine. Code for RAMEN will be released publicly.</p></p class="citation"></blockquote><h3 id=653--128286-generalizability-under-sensor-failure-tokenization--transformers-enable-more-robust-latent-spaces-geeling-chau-et-al-2024>(6/53 | 128/286) Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces (Geeling Chau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong Yue, Sabera Talukder. (2024)<br><strong>Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces</strong><br><button class=copy-to-clipboard title="Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Discrete Time, Discrete Time, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18546v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18546v2.pdf filename=2402.18546v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major goal in neuroscience is to discover neural data representations that generalize. This goal is challenged by variability along recording sessions (e.g. environment), subjects (e.g. varying neural structures), and sensors (e.g. sensor noise), among others. Recent work has begun to address generalization across sessions and subjects, but few study robustness to sensor failure which is highly prevalent in neuroscience experiments. In order to address these generalizability dimensions we first collect our own electroencephalography dataset with numerous sessions, subjects, and sensors, then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM (Talukder et al., 2024). EEGNet is a widely used <b>convolutional</b> <b>neural</b> <b>network,</b> while TOTEM is a <b>discrete</b> <b>time</b> series tokenizer and <b>transformer</b> model. We find that TOTEM outperforms or matches EEGNet across all generalizability cases. Finally through analysis of TOTEM&rsquo;s latent codebook we observe that <b>tokenization</b> enables generalization</p></p class="citation"></blockquote><h3 id=753--129286-lemo-nade-multi-parameter-neural-architecture-discovery-with-llms-md-hafizur-rahman-et-al-2024>(7/53 | 129/286) LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs (Md Hafizur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Hafizur Rahman, Prabuddha Chakraborty. (2024)<br><strong>LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs</strong><br><button class=copy-to-clipboard title="LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, GPT-4 turbo, Gemini, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18443v1.pdf filename=2402.18443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building efficient neural network architectures can be a time-consuming task requiring extensive expert knowledge. This task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing, model size, inferencing speed, and CO2 emissions. In this article, we introduce a novel framework designed to automatically discover new neural network architectures based on user-defined parameters, an expert system, and an <b>LLM</b> trained on a large amount of open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be used by non-AI experts, does not require a predetermined neural architecture search space, and considers a large set of edge device-specific parameters. We implement and validate this proposed neural architecture discovery framework using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using <b>GPT-4</b> <b>Turbo</b> and <b>Gemini</b> as the <b>LLM</b> component. We observe that the proposed framework can rapidly (within hours) discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user.</p></p class="citation"></blockquote><h3 id=853--130286-why-attention-graphs-are-all-we-need-pioneering-hierarchical-classification-of-hematologic-cell-populations-with-leukograph-fatemeh-nassajian-mojarrad-et-al-2024>(8/53 | 130/286) Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph (Fatemeh Nassajian Mojarrad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Nassajian Mojarrad, Lorenzo Bini, Thomas Matthes, Stéphane Marchand-Maillet. (2024)<br><strong>Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph</strong><br><button class=copy-to-clipboard title="Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-CB<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18610v1.pdf filename=2402.18610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges. This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing <b>graph</b> <b>attention</b> <b>networks</b> <b>(GATs)</b> to navigate hierarchical classification (HC) complexities. Notably, LeukoGraph stands as a pioneering effort, marking the application of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> for hierarchical inference on <b>graphs,</b> <b>accommodating</b> <b>up</b> to one million nodes and millions of edges, all derived from flow cytometry data. LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets. The technique is more general than this example. A hallmark achievement of LeukoGraph is its F-score of 98%, significantly outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph&rsquo;s prowess extends beyond theoretical innovation, showcasing remarkable precision in predicting both flat and hierarchical cell types across flow cytometry datasets from 30 distinct patients. This precision is further underscored by LeukoGraph&rsquo;s ability to maintain a correct label ratio, despite the inherent challenges posed by hierarchical classifications.</p></p class="citation"></blockquote><h3 id=953--131286-hierarchical-multi-relational-graph-representation-learning-for-large-scale-prediction-of-drug-drug-interactions-mengying-jiang-et-al-2024>(9/53 | 131/286) Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions (Mengying Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengying Jiang, Guizhong Liu, Yuanchao Su, Weiqiang Jin, Biao Zhao. (2024)<br><strong>Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions</strong><br><button class=copy-to-clipboard title="Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 41<br>Keywords: Graph Convolutional Network, Graph, Clustering, Convolution, Convolutional Neural Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18127v1.pdf filename=2402.18127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions. To address this issue, this paper introduces a hierarchical multi-relational <b>graph</b> <b>representation</b> <b>learning</b> (HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous <b>graphs,</b> <b>where</b> <b>nodes</b> represent drugs and edges denote clear and various associations. The relational <b>graph</b> <b>convolutional</b> <b>network</b> (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous <b>graphs.</b> <b>Additionally,</b> <b>a</b> multi-view differentiable spectral <b>clustering</b> (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs. Within the MVDSC, we utilize multiple DP features to construct <b>graphs,</b> <b>where</b> <b>nodes</b> represent DPs and edges denote different implicit correlations. Subsequently, multiple DP <b>representations</b> <b>are</b> generated through <b>graph</b> <b>cutting,</b> <b>each</b> emphasizing distinct implicit correlations. The <b>graph-cutting</b> <b>strategy</b> <b>enables</b> our HMGRL to identify strongly connected communities of <b>graphs,</b> <b>thereby</b> <b>reducing</b> the fusion of irrelevant features. By combining every <b>representation</b> <b>view</b> of a DP, we create high-level DP <b>representations</b> <b>for</b> predicting DDIs. Two genuine datasets spanning three distinct tasks are adopted to gauge the efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL surpasses several leading-edge methods in performance.</p></p class="citation"></blockquote><h3 id=1053--132286-diffusion-based-neural-network-weights-generation-bedionita-soro-et-al-2024>(10/53 | 132/286) Diffusion-based Neural Network Weights Generation (Bedionita Soro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bedionita Soro, Bruno Andreis, Hayeon Lee, Song Chong, Frank Hutter, Sung Ju Hwang. (2024)<br><strong>Diffusion-based Neural Network Weights Generation</strong><br><button class=copy-to-clipboard title="Diffusion-based Neural Network Weights Generation" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Autoencoder, Transfer Learning, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18153v1.pdf filename=2402.18153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> is a topic of significant interest in recent deep learning research because it enables faster convergence and improved performance on new tasks. While the performance of <b>transfer</b> <b>learning</b> depends on the similarity of the source data to the target data, it is costly to train a model on a large number of datasets. Therefore, pretrained models are generally blindly selected with the hope that they will achieve good performance on the given task. To tackle such suboptimality of the pretrained models, we propose an efficient and adaptive <b>transfer</b> <b>learning</b> scheme through dataset-conditioned pretrained weights sampling. Specifically, we use a latent <b>diffusion</b> <b>model</b> with a <b>variational</b> <b>autoencoder</b> that can reconstruct the neural network weights, to learn the distribution of a set of pretrained weights conditioned on each dataset for <b>transfer</b> <b>learning</b> on unseen datasets. By learning the distribution of a neural network on a variety pretrained models, our approach enables adaptive sampling weights for unseen datasets achieving faster convergence and reaching competitive performance.</p></p class="citation"></blockquote><h3 id=1153--133286-flattenquant-breaking-through-the-inference-compute-bound-for-large-language-models-with-per-tensor-quantization-yi-zhang-et-al-2024>(11/53 | 133/286) FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization (Yi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin Pan. (2024)<br><strong>FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization</strong><br><button class=copy-to-clipboard title="FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17985v1.pdf filename=2402.17985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the <b>large</b> <b>GPU</b> <b>memory</b> consumption of <b>LLMs</b> restrict their deployment performance. Recently, there have been some efficient attempts to <b>quantize</b> <b>LLMs,</b> yet inference with <b>large</b> <b>batch</b> <b>size</b> or long sequence still has the issue of being compute-bound. Fine-grained <b>quantization</b> methods have showcased their proficiency in achieving low-bit <b>quantization</b> for <b>LLMs,</b> while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with <b>large</b> <b>batch</b> <b>size</b> or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the <b>large</b> <b>channels</b> <b>in</b> the tensor, to achieve low bit per-tensor <b>quantization</b> with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% of the linear layer calculation in <b>LLMs,</b> with the remaining layers using 8 bits. The 4-bit matrix multiplication introduced in the FlattenQuant method can effectively address the compute-bound caused by <b>large</b> <b>matrix</b> <b>calculation.</b> Our work achieves up to 2$\times$ speedup and 2.3$\times$ memory reduction for <b>LLMs</b> with negligible loss in accuracy.</p></p class="citation"></blockquote><h3 id=1253--134286-imagine-initialize-and-explore-an-effective-exploration-method-in-multi-agent-reinforcement-learning-zeyang-liu-et-al-2024>(12/53 | 134/286) Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning (Zeyang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen, Xuguang Lan. (2024)<br><strong>Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 40<br>Keywords: Diffusion Model, Reinforcement Learning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17978v1.pdf filename=2402.17978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective exploration is crucial to discovering optimal strategies for multi-agent <b>reinforcement</b> <b>learning</b> (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a <b>transformer</b> model to imagine how the agents reach a critical state that can influence each other&rsquo;s transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imagination as a sequence modeling problem, where the states, observations, <b>prompts,</b> actions, and rewards are predicted autoregressively. The <b>prompt</b> consists of timestep-to-go, return-to-go, influence value, and one-shot demonstration, specifying the desired state and trajectory as well as guiding the action generation. By initializing agents at the critical states, IIE significantly increases the likelihood of discovering potentially important under-explored regions. Despite its simplicity, empirical results demonstrate that our method outperforms multi-agent exploration baselines on the StarCraft Multi-Agent Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved performance in the sparse-reward SMAC tasks and produces more effective curricula over the initialized states than other generative methods, such as CVAE-GAN and <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1353--135286-pre-training-differentially-private-models-with-limited-public-data-zhiqi-bu-et-al-2024>(13/53 | 135/286) Pre-training Differentially Private Models with Limited Public Data (Zhiqi Bu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqi Bu, Xinwei Zhang, Mingyi Hong, Sheng Zha, George Karypis. (2024)<br><strong>Pre-training Differentially Private Models with Limited Public Data</strong><br><button class=copy-to-clipboard title="Pre-training Differentially Private Models with Limited Public Data" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Foundation Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18752v1.pdf filename=2402.18752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The superior performance of large <b>foundation</b> <b>models</b> relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While <b>differential</b> <b>privacy</b> (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model <b>fine-tuning</b> stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process. In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers&rsquo; performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, using only 10% of public data, our strategy can achieve DP accuracy of 41.5% on ImageNet-21k (with $\epsilon=8$), as well as non-DP accuracy of 55.7% and and 60.0% on downstream tasks Places365 and iNaturalist-2021, respectively, on par with state-of-the-art standard pre-training and substantially outperforming existing DP pre-trained models.</p></p class="citation"></blockquote><h3 id=1453--136286-efficiently-computable-safety-bounds-for-gaussian-processes-in-active-learning-jörn-tebbe-et-al-2024>(14/53 | 136/286) Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning (Jörn Tebbe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jörn Tebbe, Christoph Zimmer, Ansgar Steland, Markus Lange-Hegermann, Fabian Mies. (2024)<br><strong>Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning</strong><br><button class=copy-to-clipboard title="Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18260v1.pdf filename=2402.18260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> of physical systems must commonly respect practical safety constraints, which restricts the exploration of the design space. Gaussian Processes (GPs) and their calibrated uncertainty estimations are widely used for this purpose. In many technical applications the design space is explored via continuous trajectories, along which the safety needs to be assessed. This is particularly challenging for strict safety requirements in GP methods, as it employs computationally expensive Monte-Carlo sampling of high quantiles. We address these challenges by providing provable safety bounds based on the adaptively sampled median of the supremum of the posterior GP. Our method significantly reduces the number of samples required for estimating high safety probabilities, resulting in faster evaluation without sacrificing accuracy and exploration speed. The effectiveness of our safe <b>active</b> <b>learning</b> approach is demonstrated through extensive <b>simulations</b> and validated using a real-world engine example.</p></p class="citation"></blockquote><h3 id=1553--137286-forml-a-riemannian-hessian-free-method-for-meta-learning-with-orthogonality-constraint-hadi-tabealhojeh-et-al-2024>(15/53 | 137/286) FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint (Hadi Tabealhojeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi Tabealhojeh, Soumava Kumar Roy, Peyman Adibi, Hossein Karshenas. (2024)<br><strong>FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint</strong><br><button class=copy-to-clipboard title="FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18605v1.pdf filename=2402.18605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Meta-learning</b> <b>problem</b> is usually formulated as a bi-level optimization in which the task-specific and the <b>meta-parameters</b> <b>are</b> updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and <b>meta-parameters</b> <b>are</b> located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based <b>meta-learning</b> <b>methods.</b> Our experimental results across various <b>few-shot</b> <b>learning</b> datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods, especially MAML, its Euclidean counterpart.</p></p class="citation"></blockquote><h3 id=1653--138286-multi-objective-differentiable-neural-architecture-search-rhea-sanjay-sukthanker-et-al-2024>(16/53 | 138/286) Multi-objective Differentiable Neural Architecture Search (Rhea Sanjay Sukthanker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter. (2024)<br><strong>Multi-objective Differentiable Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Multi-objective Differentiable Neural Architecture Search" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Zero-shot, Transformer, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18213v1.pdf filename=2402.18213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling <b>zero-shot</b> transferability to new devices. Extensive experiments with up to 19 hardware devices and 3 objectives showcase the effectiveness and scalability of our method. Finally, we show that, without additional costs, our method outperforms existing MOO NAS methods across qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k and a <b>Transformer</b> space on <b>machine</b> <b>translation.</b></p></p class="citation"></blockquote><h3 id=1753--139286-provable-risk-sensitive-distributional-reinforcement-learning-with-general-function-approximation-yu-chen-et-al-2024>(17/53 | 139/286) Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation (Yu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Chen, Xiangcheng Zhang, Siwei Wang, Longbo Huang. (2024)<br><strong>Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation</strong><br><button class=copy-to-clipboard title="Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Distributional Reinforcement Learning, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18159v1.pdf filename=2402.18159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>reinforcement</b> <b>learning</b> (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive <b>Distributional</b> <b>Reinforcement</b> <b>Learning</b> (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in <b>distributional</b> <b>RL</b> <b>with</b> augmented <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP), we derive the first $\widetilde{\mathcal{O}}(\sqrt{K})$ dependency of the regret upper bound for RSRL with static LRM, marking a pioneering contribution towards statistically efficient algorithms in this domain.</p></p class="citation"></blockquote><h3 id=1853--140286-on-the-inductive-biases-of-demographic-parity-based-fair-learning-algorithms-haoyu-lei-et-al-2024>(18/53 | 140/286) On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms (Haoyu Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Lei, Amin Gohari, Farzan Farnia. (2024)<br><strong>On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms</strong><br><button class=copy-to-clipboard title="On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 30<br>Keywords: Fairness, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18129v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18129v1.pdf filename=2402.18129v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fair <b>supervised</b> <b>learning</b> algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model&rsquo;s <b>fairness</b> in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA-DRO) method improving robustness against the marginal distribution of the sensitive attribute. Finally, we present several numerical results on the application of DP-based learning methods to standard centralized and distributed learning problems. The empirical findings support our theoretical results on the inductive biases in DP-based fair learning algorithms and the debiasing effects of the proposed SA-DRO method.</p></p class="citation"></blockquote><h3 id=1953--141286-communication-efficient-confederated-learning-an-event-triggered-saga-approach-bin-wang-et-al-2024>(19/53 | 141/286) Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach (Bin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Wang, Jun Fang, Hongbin Li, Yonina C. Eldar. (2024)<br><strong>Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach</strong><br><button class=copy-to-clipboard title="Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18018v1.pdf filename=2402.18018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a machine learning paradigm that targets model training without gathering the local data dispersed over various data sources. Standard FL, which employs a single server, can only support a limited number of users, leading to degraded learning capability. In this work, we consider a multi-server FL framework, referred to as \emph{Confederated Learning} (CFL), in order to accommodate a larger number of users. A CFL system is composed of multiple networked edge servers, with each server connected to an individual set of users. Decentralized collaboration among servers is leveraged to harness all users&rsquo; data for model training. Due to the potentially massive number of users involved, it is crucial to reduce the communication overhead of the CFL system. We propose a stochastic gradient method for distributed learning in the CFL framework. The proposed method incorporates a conditionally-triggered user selection (CTUS) mechanism as the central component to effectively reduce communication overhead. Relying on a delicately designed triggering condition, the CTUS mechanism allows each server to select only a small number of users to upload their gradients, without significantly jeopardizing the convergence performance of the algorithm. Our theoretical analysis reveals that the proposed algorithm enjoys a linear convergence rate. <b>Simulation</b> results show that it achieves substantial improvement over state-of-the-art algorithms in terms of communication efficiency.</p></p class="citation"></blockquote><h3 id=2053--142286-conformer-embedding-continuous-attention-in-vision-transformer-for-weather-forecasting-hira-saleem-et-al-2024>(20/53 | 142/286) Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting (Hira Saleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hira Saleem, Flora Salim, Cormac Purcell. (2024)<br><strong>Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting</strong><br><button class=copy-to-clipboard title="Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17966v1.pdf filename=2402.17966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operational weather forecasting system relies on computationally expensive physics-based models. Although <b>Transformers-based</b> models have shown remarkable potential in weather forecasting, <b>Transformers</b> are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous <b>Vision</b> <b>Transformer</b> for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the <b>transformer</b> architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while only being trained at lower resolution data.</p></p class="citation"></blockquote><h3 id=2153--143286-mmsr-symbolic-regression-is-a-multimodal-task-yanjie-li-et-al-2024>(21/53 | 143/286) MMSR: Symbolic Regression is a Multimodal Task (Yanjie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan Hao, Su Wei, Yusong Deng. (2024)<br><strong>MMSR: Symbolic Regression is a Multimodal Task</strong><br><button class=copy-to-clipboard title="MMSR: Symbolic Regression is a Multimodal Task" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18603v1.pdf filename=2402.18603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and <b>reinforcement</b> <b>learning</b> algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR. The SR problem is solved as a pure <b>multimodal</b> problem, and <b>contrastive</b> <b>learning</b> is also introduced in the training process for modal alignment to facilitate later modal feature fusion. It is worth noting that in order to better promote the modal feature fusion, we adopt the strategy of training <b>contrastive</b> <b>learning</b> loss and other losses at the same time, which only needs one-step training, instead of training <b>contrastive</b> <b>learning</b> loss first and then training other losses. Because our experiments prove training together can make the feature extraction module and feature fusion module running-in better. Experimental results show that compared with multiple large-scale pre-training baselines, MMSR achieves the most advanced results on multiple mainstream datasets including SRBench.</p></p class="citation"></blockquote><h3 id=2253--144286-deep-neural-network-models-trained-with-a-fixed-random-classifier-transfer-better-across-domains-hafiz-tiomoko-ali-et-al-2024>(22/53 | 144/286) Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains (Hafiz Tiomoko Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hafiz Tiomoko Ali, Umberto Michieli, Ji Joong Moon, Daehyun Kim, Mete Ozay. (2024)<br><strong>Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains</strong><br><button class=copy-to-clipboard title="Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-NE, cs.LG<br>Keyword Score: 25<br>Keywords: Geometry, Out-of-domain, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18614v1.pdf filename=2402.18614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF <b>geometry</b> is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve <b>transfer</b> <b>performance,</b> particularly on <b>out-of-domain</b> datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explicitly whiten covariance of activations throughout training (up to 19%). Our findings suggest that DNNs trained with fixed ETF classifiers offer a powerful mechanism for improving <b>transfer</b> <b>learning</b> across domains.</p></p class="citation"></blockquote><h3 id=2353--145286-exploring-privacy-and-fairness-risks-in-sharing-diffusion-models-an-adversarial-perspective-xinjian-luo-et-al-2024>(23/53 | 145/286) Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective (Xinjian Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng Chin Ooi. (2024)<br><strong>Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective</strong><br><button class=copy-to-clipboard title="Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Diffusion Model, Black Box, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18607v1.pdf filename=2402.18607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained <b>diffusion</b> <b>models</b> across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined. In this paper, we take an adversarial perspective to investigate the potential privacy and <b>fairness</b> risks associated with the sharing of <b>diffusion</b> <b>models.</b> Specifically, we investigate the circumstances in which one party (the sharer) trains a <b>diffusion</b> <b>model</b> using private data and provides another party (the receiver) <b>black-box</b> <b>access</b> to the pre-trained model for downstream tasks. We demonstrate that the sharer can execute <b>fairness</b> poisoning attacks to undermine the receiver&rsquo;s downstream models by manipulating the training data distribution of the <b>diffusion</b> <b>model.</b> Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer&rsquo;s dataset. Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of <b>diffusion</b> <b>models,</b> which highlights the critical importance of robust data auditing and privacy protection protocols in pertinent applications.</p></p class="citation"></blockquote><h3 id=2453--146286-autoencoder-based-general-purpose-representation-learning-for-customer-embedding-jan-henrik-bertrand-et-al-2024>(24/53 | 146/286) Autoencoder-based General Purpose Representation Learning for Customer Embedding (Jan Henrik Bertrand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Henrik Bertrand, Jacopo Pio Gargano, Laurent Mombaerts, Jonathan Taws. (2024)<br><strong>Autoencoder-based General Purpose Representation Learning for Customer Embedding</strong><br><button class=copy-to-clipboard title="Autoencoder-based General Purpose Representation Learning for Customer Embedding" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T-02, cs-AI, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Autoencoder, Reconstruction Loss, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18164v1.pdf filename=2402.18164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, exploiting the domain-specific underlying structure of data and its generative factors for <b>representation</b> <b>learning</b> has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an <b>autoencoder-based</b> framework for building general purpose embeddings, we assess the performance of different <b>autoencoder</b> architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of <b>reconstruction</b> <b>loss</b> for multi-layer contractive <b>autoencoders</b> (CAE) by calculating the Jacobian of the entire encoder leading to a 15% improvement in <b>reconstruction</b> <b>quality</b> when compared to a stacked CAE.</p></p class="citation"></blockquote><h3 id=2553--147286-classes-are-not-equal-an-empirical-study-on-image-recognition-fairness-jiequan-cui-et-al-2024>(25/53 | 147/286) Classes Are Not Equal: An Empirical Study on Image Recognition Fairness (Jiequan Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang. (2024)<br><strong>Classes Are Not Equal: An Empirical Study on Image Recognition Fairness</strong><br><button class=copy-to-clipboard title="Classes Are Not Equal: An Empirical Study on Image Recognition Fairness" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Data Augmentation, Fairness, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18133v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18133v1.pdf filename=2402.18133v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present an empirical study on image recognition <b>fairness,</b> i.e., extreme class accuracy disparity on balanced <b>data</b> <b>like</b> ImageNet. We experimentally demonstrate that classes are not equal and the <b>fairness</b> issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of <b>fairness</b> are identified. First, the unfairness lies in problematic <b>representation</b> <b>rather</b> than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic <b>representation</b> <b>during</b> optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further, we conclude that <b>data</b> <b>augmentation</b> and <b>representation</b> <b>learning</b> algorithms improve overall performance by promoting <b>fairness</b> to some degree in image classification.</p></p class="citation"></blockquote><h3 id=2653--148286-rog_pl-robust-open-set-graph-learning-via-region-based-prototype-learning-qin-zhang-et-al-2024>(26/53 | 148/286) ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning (Qin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qin Zhang, Xiaowei Li, Jiexin Lu, Liping Qiu, Shirui Pan, Xiaojun Chen, Junyang Chen. (2024)<br><strong>ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning</strong><br><button class=copy-to-clipboard title="ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Node Classification, Graph, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18495v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18495v2.pdf filename=2402.18495v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-set <b>graph</b> learning is a practical task that aims to classify the known class <b>nodes</b> <b>and</b> to identify unknown class samples as unknowns. Conventional <b>node</b> <b>classification</b> methods usually perform unsatisfactorily in open-set scenarios due to the complex data they encounter, such as <b>out-of-distribution</b> (OOD) data and in-distribution (IND) noise. OOD data are samples that do not belong to any known classes. They are outliers if they occur in training (OOD noise), and open-set samples if they occur in testing. IND noise are training samples which are assigned incorrect labels. The existence of IND noise and OOD noise is prevalent, which usually cause the ambiguity problem, including the intra-class variety problem and the inter-class confusion problem. Thus, to explore robust open-set learning methods is necessary and difficult, and it becomes even more difficult for non-IID <b>graph</b> data.To this end, we propose a unified framework named ROG$<em>{PL}$ to achieve robust open-set learning on complex noisy <b>graph</b> data, by introducing prototype learning. In specific, ROG$</em>{PL}$ consists of two modules, i.e., denoising via label propagation and open-set prototype learning via regions. The first module corrects noisy labels through similarity-based label propagation and removes low-confidence samples, to solve the intra-class variety problem caused by noise. The second module learns open-set prototypes for each known class via non-overlapped regions and remains both interior and border prototypes to remedy the inter-class confusion problem.The two modules are iteratively updated under the constraints of classification loss and prototype diversity loss. To the best of our knowledge, the proposed ROG$_{PL}$ is the first robust open-set <b>node</b> <b>classification</b> method for <b>graph</b> data with complex noise.</p></p class="citation"></blockquote><h3 id=2753--149286-no-token-left-behind-reliable-kv-cache-compression-via-importance-aware-mixed-precision-quantization-june-yong-yang-et-al-2024>(27/53 | 149/286) No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization (June Yong Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee. (2024)<br><strong>No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization</strong><br><button class=copy-to-clipboard title="No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18096v1.pdf filename=2402.18096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in <b>LLM</b> deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision <b>quantization</b> substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse <b>benchmarks</b> and <b>LLM</b> backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.</p></p class="citation"></blockquote><h3 id=2853--150286-diffusion-language-models-are-versatile-protein-learners-xinyou-wang-et-al-2024>(28/53 | 150/286) Diffusion Language Models Are Versatile Protein Learners (Xinyou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu. (2024)<br><strong>Diffusion Language Models Are Versatile Protein Learners</strong><br><button class=copy-to-clipboard title="Diffusion Language Models Are Versatile Protein Learners" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 20<br>Keywords: Fine-tuning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18567v1.pdf filename=2402.18567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative <b>self-supervised</b> discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be <b>fine-tuned</b> for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.</p></p class="citation"></blockquote><h3 id=2953--151286-automated-machine-learning-for-multi-label-classification-marcel-wever-2024>(29/53 | 151/286) Automated Machine Learning for Multi-Label Classification (Marcel Wever, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Wever. (2024)<br><strong>Automated Machine Learning for Multi-Label Classification</strong><br><button class=copy-to-clipboard title="Automated Machine Learning for Multi-Label Classification" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18198v1.pdf filename=2402.18198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated machine learning (AutoML) aims to select and configure machine learning algorithms and combine them into machine learning pipelines tailored to a dataset at hand. For <b>supervised</b> <b>learning</b> tasks, most notably binary and multinomial classification, aka single-label classification (SLC), such AutoML approaches have shown promising results. However, the task of multi-label classification (MLC), where data points are associated with a set of class labels instead of a single class label, has received much less attention so far. In the context of multi-label classification, the data-specific selection and configuration of multi-label classifiers are challenging even for experts in the field, as it is a high-dimensional optimization problem with multi-level hierarchical dependencies. While for SLC, the space of machine learning pipelines is already huge, the size of the MLC search space outnumbers the one of SLC by several orders. In the first part of this thesis, we devise a novel AutoML approach for single-label classification tasks optimizing pipelines of machine learning algorithms, consisting of two algorithms at most. This approach is then extended first to optimize pipelines of unlimited length and eventually configure the complex hierarchical structures of multi-label classification methods. Furthermore, we investigate how well AutoML approaches that form the state of the art for single-label classification tasks scale with the increased problem complexity of AutoML for multi-label classification. In the second part, we explore how methods for SLC and MLC could be configured more flexibly to achieve better generalization performance and how to increase the efficiency of execution-based AutoML systems.</p></p class="citation"></blockquote><h3 id=3053--152286-provably-efficient-partially-observable-risk-sensitive-reinforcement-learning-with-hindsight-observation-tonghe-zhang-et-al-2024>(30/53 | 152/286) Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation (Tonghe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tonghe Zhang, Yu Chen, Longbo Huang. (2024)<br><strong>Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation</strong><br><button class=copy-to-clipboard title="Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18149v1.pdf filename=2402.18149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work pioneers regret analysis of risk-sensitive <b>reinforcement</b> <b>learning</b> in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable <b>Markov</b> <b>Decision</b> <b>Process</b> (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interest to the theoretical study of <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=3153--153286-token-specific-watermarking-with-enhanced-detectability-and-semantic-coherence-for-large-language-models-mingjia-huo-et-al-2024>(31/53 | 153/286) Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models (Mingjia Huo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie. (2024)<br><strong>Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models</strong><br><button class=copy-to-clipboard title="Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18059v1.pdf filename=2402.18059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the <b>LLM</b> inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by <b>LLMs</b> while maintaining their semantic coherence. Our code is available at <a href=https://github.com/mignonjia/TS_watermark>https://github.com/mignonjia/TS_watermark</a> .</p></p class="citation"></blockquote><h3 id=3253--154286-data-augmentation-method-for-modeling-health-records-with-applications-to-clopidogrel-treatment-failure-detection-sunwoong-choi-et-al-2024>(32/53 | 154/286) Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection (Sunwoong Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunwoong Choi, Samuel Kim. (2024)<br><strong>Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection</strong><br><button class=copy-to-clipboard title="Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Data Augmentation, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18046v1.pdf filename=2402.18046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel <b>data</b> <b>augmentation</b> method to address the challenge of <b>data</b> <b>scarcity</b> in modeling longitudinal patterns in Electronic Health Records (EHR) of patients using natural language processing (NLP) algorithms. The proposed method generates augmented <b>data</b> <b>by</b> rearranging the orders of medical records within a visit where the order of elements are not obvious, if any. Applying the proposed method to the clopidogrel treatment failure detection task enabled up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without augmentation to 0.961 with augmentation) when it was used during the pre-training procedure. It was also shown that the augmentation helped to improve performance during <b>fine-tuning</b> procedures, especially when the amount of labeled training <b>data</b> <b>is</b> limited.</p></p class="citation"></blockquote><h3 id=3353--155286-out-of-domain-generalization-in-dynamical-systems-reconstruction-niclas-göring-et-al-2024>(33/53 | 155/286) Out-of-Domain Generalization in Dynamical Systems Reconstruction (Niclas Göring et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niclas Göring, Florian Hess, Manuel Brenner, Zahra Monfared, Daniel Durstewitz. (2024)<br><strong>Out-of-Domain Generalization in Dynamical Systems Reconstruction</strong><br><button class=copy-to-clipboard title="Out-of-Domain Generalization in Dynamical Systems Reconstruction" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-DS, nlin-CD<br>Keyword Score: 15<br>Keywords: Black Box, Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18377v1.pdf filename=2402.18377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how <b>out-of-domain</b> (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that <b>black-box</b> <b>DL</b> techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.</p></p class="citation"></blockquote><h3 id=3453--156286-diffusion-models-as-constrained-samplers-for-optimization-with-unknown-constraints-lingkai-kong-et-al-2024>(34/53 | 156/286) Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints (Lingkai Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortol, Haorui Wang, Dongxia Wu, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao Zhang. (2024)<br><strong>Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints</strong><br><button class=copy-to-clipboard title="Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Diffusion Model, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18012v1.pdf filename=2402.18012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using <b>diffusion</b> <b>models.</b> To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the <b>diffusion</b> <b>model.</b> To enhance sampling efficiency, we propose a two-stage framework that begins with a guided <b>diffusion</b> <b>process</b> for warm-up, followed by a Langevin dynamics stage for further correction. Theoretical analysis shows that the initial stage results in a distribution focused on feasible solutions, thereby providing a better initialization for the later stage. Comprehensive experiments on a synthetic dataset, six real-world <b>black-box</b> <b>optimization</b> datasets, and a multi-objective optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=3553--157286-gnss-positioning-using-cost-function-regulated-multilateration-and-graph-neural-networks-amir-jalalirad-et-al-2024>(35/53 | 157/286) GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks (Amir Jalalirad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Jalalirad, Davide Belli, Bence Major, Songwon Jee, Himanshu Shah, Will Morrison. (2024)<br><strong>GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks</strong><br><button class=copy-to-clipboard title="GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18630v1.pdf filename=2402.18630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In urban environments, where line-of-sight signals from GNSS satellites are frequently blocked by high-rise objects, GNSS receivers are subject to large errors in measuring satellite ranges. Heuristic methods are commonly used to estimate these errors and reduce the impact of noisy measurements on localization accuracy. In our work, we replace these error estimation heuristics with a deep learning model based on <b>Graph</b> <b>Neural</b> <b>Networks.</b> Additionally, by analyzing the cost function of the multilateration process, we derive an optimal method to utilize the estimated errors. Our approach guarantees that the multilateration converges to the receiver&rsquo;s location as the error estimation accuracy increases. We evaluate our solution on a real-world dataset containing more than 100k GNSS epochs, collected from multiple cities with diverse characteristics. The empirical results show improvements from 40% to 80% in the horizontal localization error against recent deep learning baselines as well as classical localization approaches.</p></p class="citation"></blockquote><h3 id=3653--158286-impact-of-network-topology-on-the-performance-of-decentralized-federated-learning-luigi-palmieri-et-al-2024>(36/53 | 158/286) Impact of network topology on the performance of Decentralized Federated Learning (Luigi Palmieri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luigi Palmieri, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti. (2024)<br><strong>Impact of network topology on the performance of Decentralized Federated Learning</strong><br><button class=copy-to-clipboard title="Impact of network topology on the performance of Decentralized Federated Learning" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18606v1.pdf filename=2402.18606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fully decentralized learning is gaining momentum for training AI models at the Internet&rsquo;s edge, addressing infrastructure challenges and privacy concerns. In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset. The local models are then shared and combined to form a global model capable of making accurate predictions on new data. Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network. Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods. These methods consider different vertex properties, including degree centrality, betweenness centrality, and <b>clustering</b> coefficient, along with whether nodes exhibit high or low values of these metrics. Our findings underscore the significance of global centrality metrics (degree, betweenness) in correlating with learning performance, while local <b>clustering</b> proves less predictive. We highlight the challenges in transferring knowledge from peripheral to central nodes, attributed to a dilution effect during model aggregation. Additionally, we observe that central nodes exert a pull effect, facilitating the spread of knowledge. In examining degree distribution, hubs in Barabasi-Albert networks positively impact learning for central nodes but exacerbate dilution when knowledge originates from peripheral nodes. Finally, we demonstrate the formidable challenge of knowledge circulation outside of segregated communities.</p></p class="citation"></blockquote><h3 id=3753--159286-priority-sampling-of-large-language-models-for-compilers-dejan-grubisic-et-al-2024>(37/53 | 159/286) Priority Sampling of Large Language Models for Compilers (Dejan Grubisic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather. (2024)<br><strong>Priority Sampling of Large Language Models for Compilers</strong><br><button class=copy-to-clipboard title="Priority Sampling of Large Language Models for Compilers" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs-PF, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18734v1.pdf filename=2402.18734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model&rsquo;s confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples.</p></p class="citation"></blockquote><h3 id=3853--160286-unveiling-privacy-memorization-and-input-curvature-links-deepak-ravikumar-et-al-2024>(38/53 | 160/286) Unveiling Privacy, Memorization, and Input Curvature Links (Deepak Ravikumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy. (2024)<br><strong>Unveiling Privacy, Memorization, and Input Curvature Links</strong><br><button class=copy-to-clipboard title="Unveiling Privacy, Memorization, and Input Curvature Links" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18726v1.pdf filename=2402.18726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between <b>differential</b> <b>privacy,</b> memorization, and input loss curvature. First, we derive an upper bound on memorization characterized by both <b>differential</b> <b>privacy</b> and input loss curvature. Second, we present a novel insight showing that input loss curvature is upper-bounded by the <b>differential</b> <b>privacy</b> parameter. Our theoretical findings are further empirically validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice.</p></p class="citation"></blockquote><h3 id=3953--161286-learning-associative-memories-with-gradient-descent-vivien-cabannes-et-al-2024>(39/53 | 161/286) Learning Associative Memories with Gradient Descent (Vivien Cabannes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vivien Cabannes, Berfin Simsek, Alberto Bietti. (2024)<br><strong>Learning Associative Memories with Gradient Descent</strong><br><button class=copy-to-clipboard title="Learning Associative Memories with Gradient Descent" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18724v1.pdf filename=2402.18724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.&rsquo;&rsquo; Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small <b>Transformer</b> models.</p></p class="citation"></blockquote><h3 id=4053--162286-implicit-bias-of-next-token-prediction-christos-thrampoulidis-2024>(40/53 | 162/286) Implicit Bias of Next-Token Prediction (Christos Thrampoulidis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christos Thrampoulidis. (2024)<br><strong>Implicit Bias of Next-Token Prediction</strong><br><button class=copy-to-clipboard title="Implicit Bias of Next-Token Prediction" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18551v1.pdf filename=2402.18551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-token prediction (NTP), the go-to training paradigm in training <b>large</b> <b>language</b> <b>models,</b> involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that the parameters of GD projected onto an appropriate data subspace converge to the unique solution of a system of linear equations, which requires the logits&rsquo; difference of in-support tokens to be equal to the log-ratio of their respective probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge and converge in the direction of the solution of a max-margin quadratic program, minimizing the Euclidean norm of parameters satisfying the \NTP-separability conditions. Akin to prior research on implicit bias of one-hot classification, our work opens exciting avenues for future research that can lead to better understanding optimization, generalization and robustness principles of models trained with NTP.</p></p class="citation"></blockquote><h3 id=4153--163286-evolving-machine-learning-workflows-through-interactive-automl-rafael-barbudo-et-al-2024>(41/53 | 163/286) Evolving machine learning workflows through interactive AutoML (Rafael Barbudo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rafael Barbudo, Aurora Ramírez, José Raúl Romero. (2024)<br><strong>Evolving machine learning workflows through interactive AutoML</strong><br><button class=copy-to-clipboard title="Evolving machine learning workflows through interactive AutoML" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T05, I-2-6; I-2-8, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18505v1.pdf filename=2402.18505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic workflow composition (AWC) is a relevant problem in automated machine learning (AutoML) that allows finding suitable sequences of preprocessing and prediction models together with their optimal hyperparameters. This problem can be solved using evolutionary algorithms and, in particular, grammar-guided genetic programming (G3P). Current G3P approaches to AWC define a fixed grammar that formally specifies how workflow elements can be combined and which algorithms can be included. In this paper we present \ourmethod, an interactive G3P algorithm that allows users to dynamically modify the grammar to prune the search space and focus on their regions of interest. Our proposal is the first to combine the advantages of a G3P method with ideas from interactive optimisation and <b>human-guided</b> <b>machine</b> learning, an area little explored in the context of AutoML. To evaluate our approach, we present an experimental study in which 20 participants interact with \ourmethod to evolve workflows according to their preferences. Our results confirm that the collaboration between \ourmethod and <b>humans</b> <b>allows</b> us to find high-performance workflows in terms of accuracy that require less tuning time than those found without <b>human</b> <b>intervention.</b></p></p class="citation"></blockquote><h3 id=4253--164286-dynamical-regimes-of-diffusion-models-giulio-biroli-et-al-2024>(42/53 | 164/286) Dynamical Regimes of Diffusion Models (Giulio Biroli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulio Biroli, Tony Bonnaire, Valentin de Bortoli, Marc Mézard. (2024)<br><strong>Dynamical Regimes of Diffusion Models</strong><br><button class=copy-to-clipboard title="Dynamical Regimes of Diffusion Models" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-stat-mech, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18491v1.pdf filename=2402.18491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using statistical physics methods, we study generative <b>diffusion</b> <b>models</b> in the regime where the dimension of space and the number of data are large, and the score function has been trained optimally. Our analysis reveals three distinct dynamical regimes during the backward generative <b>diffusion</b> <b>process.</b> The generative dynamics, starting from pure noise, encounters first a &lsquo;speciation&rsquo; transition where the gross structure of data is unraveled, through a mechanism similar to symmetry breaking in phase transitions. It is followed at later time by a &lsquo;collapse&rsquo; transition where the trajectories of the dynamics become attracted to one of the memorized data points, through a mechanism which is similar to the condensation in a glass phase. For any dataset, the speciation time can be found from a spectral analysis of the correlation matrix, and the collapse time can be found from the estimation of an &rsquo;excess entropy&rsquo; in the data. The dependence of the collapse time on the dimension and number of data provides a thorough characterization of the curse of dimensionality for <b>diffusion</b> <b>models.</b> Analytical solutions for simple models like high-dimensional Gaussian mixtures substantiate these findings and provide a theoretical framework, while extensions to more complex scenarios and numerical validations with real datasets confirm the theoretical predictions.</p></p class="citation"></blockquote><h3 id=4353--165286-unveiling-the-potential-of-robustness-in-evaluating-causal-inference-models-yiyan-huang-et-al-2024>(43/53 | 165/286) Unveiling the Potential of Robustness in Evaluating Causal Inference Models (Yiyan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyan Huang, Cheuk Hang Leung, Siyi Wang, Yijun Li, Qi Wu. (2024)<br><strong>Unveiling the Potential of Robustness in Evaluating Causal Inference Models</strong><br><button class=copy-to-clipboard title="Unveiling the Potential of Robustness in Evaluating Causal Inference Models" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, econ-EM, stat-ML<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18392v1.pdf filename=2402.18392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). The intersection of machine learning and causal inference has yielded various effective CATE estimators. However, deploying these estimators in practice is often hindered by the absence of <b>counterfactual</b> labels, making it challenging to select the desirable CATE estimator using conventional model selection procedures like cross-validation. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two inherent challenges. Firstly, they are required to determine the metric form and the underlying machine learning models for fitting nuisance parameters or plug-in learners. Secondly, they lack a specific focus on selecting a robust estimator. To address these challenges, this paper introduces a novel approach, the Distributionally Robust Metric (DRM), for CATE estimator selection. The proposed DRM not only eliminates the need to fit additional models but also excels at selecting a robust CATE estimator. Experimental studies demonstrate the efficacy of the DRM method, showcasing its consistent effectiveness in identifying superior estimators while mitigating the risk of selecting inferior ones.</p></p class="citation"></blockquote><h3 id=4453--166286-ice-search-a-language-model-driven-feature-selection-approach-tianze-et-al-2024>(44/53 | 166/286) ICE-SEARCH: A Language Model-Driven Feature Selection Approach (Tianze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianze, Yang, Tianyi, Yang, Shaoshan Liu, Fuyuan Lvu, Xue Liu. (2024)<br><strong>ICE-SEARCH: A Language Model-Driven Feature Selection Approach</strong><br><button class=copy-to-clipboard title="ICE-SEARCH: A Language Model-Driven Feature Selection Approach" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18609v1.pdf filename=2402.18609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study unveils the <b>In-Context</b> Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model&rsquo;s comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate the efficacy of ICE-SEARCH in medical FS but also underscore the versatility, efficiency, and scalability of integrating LMs in FS tasks. The study emphasizes the critical role of incorporating domain-specific insights, illustrating ICE-SEARCH&rsquo;s robustness, generalizability, and swift convergence. This opens avenues for further research into comprehensive and intricate FS landscapes, marking a significant stride in the application of artificial intelligence in medical predictive analytics.</p></p class="citation"></blockquote><h3 id=4553--167286-catastrophic-overfitting-a-potential-blessing-in-disguise-mengnan-zhao-et-al-2024>(45/53 | 167/286) Catastrophic Overfitting: A Potential Blessing in Disguise (Mengnan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin. (2024)<br><strong>Catastrophic Overfitting: A Potential Blessing in Disguise</strong><br><button class=copy-to-clipboard title="Catastrophic Overfitting: A Potential Blessing in Disguise" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18211v1.pdf filename=2402.18211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fast <b>Adversarial</b> <b>Training</b> (FAT) has gained increasing attention within the research community owing to its efficacy in improving <b>adversarial</b> <b>robustness.</b> Particularly noteworthy is the challenge posed by catastrophic overfitting (CO) in this field. Although existing FAT approaches have made strides in mitigating CO, the ascent of <b>adversarial</b> <b>robustness</b> occurs with a non-negligible decline in classification accuracy on clean samples. To tackle this issue, we initially employ the feature activation differences between clean and <b>adversarial</b> <b>examples</b> to analyze the underlying causes of CO. Intriguingly, our findings reveal that CO can be attributed to the feature coverage induced by a few specific pathways. By intentionally manipulating feature activation differences in these pathways with well-designed regularization terms, we can effectively mitigate and induce CO, providing further evidence for this observation. Notably, models trained stably with these terms exhibit superior performance compared to prior FAT work. On this basis, we harness CO to achieve `attack obfuscation&rsquo;, aiming to bolster model performance. Consequently, the models suffering from CO can attain optimal classification accuracy on both clean and <b>adversarial</b> <b>data</b> when adding random noise to inputs during evaluation. We also validate their robustness against transferred <b>adversarial</b> <b>examples</b> and the necessity of inducing CO to improve robustness. Hence, CO may not be a problem that has to be solved.</p></p class="citation"></blockquote><h3 id=4653--168286-decentralised-traffic-incident-detection-via-network-lasso-qiyuan-zhu-et-al-2024>(46/53 | 168/286) Decentralised Traffic Incident Detection via Network Lasso (Qiyuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan Zhu, A. K. Qin, Prabath Abeysekara, Hussein Dia, Hanna Grzybowska. (2024)<br><strong>Decentralised Traffic Incident Detection via Network Lasso</strong><br><button class=copy-to-clipboard title="Decentralised Traffic Incident Detection via Network Lasso" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18167v1.pdf filename=2402.18167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traffic incident detection plays a key role in intelligent transportation systems, which has gained great attention in transport engineering. In the past, traditional machine learning (ML) based detection methods achieved good performance under a centralised computing paradigm, where all data are transmitted to a central server for building ML models therein. Nowadays, deep neural networks based <b>federated</b> <b>learning</b> (FL) has become a mainstream detection approach to enable the model training in a decentralised manner while warranting local data governance. Such neural networks-centred techniques, however, have overshadowed the utility of well-established ML-based detection methods. In this work, we aim to explore the potential of potent conventional ML-based detection models in modern traffic scenarios featured by distributed data. We leverage an elegant but less explored distributed optimisation framework named Network Lasso, with guaranteed global convergence for convex problem formulations, integrate the potent convex ML model with it, and compare it with centralised learning, local learning, and <b>federated</b> <b>learning</b> methods atop a well-known traffic incident detection dataset. Experimental results show that the proposed network lasso-based approach provides a promising alternative to the FL-based approach in data-decentralised traffic scenarios, with a strong convergence guarantee while rekindling the significance of conventional ML-based detection methods.</p></p class="citation"></blockquote><h3 id=4753--169286-mixer-is-more-than-just-a-model-qingfeng-ji-et-al-2024>(47/53 | 169/286) Mixer is more than just a model (Qingfeng Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingfeng Ji, Yuxin Wang, Letong Sun. (2024)<br><strong>Mixer is more than just a model</strong><br><button class=copy-to-clipboard title="Mixer is more than just a model" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18007v1.pdf filename=2402.18007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data <b>information</b> <b>from</b> both channel and token perspectives, effectively acting as a fusion of channel and token <b>information.</b> <b>Indeed,</b> Mixer represents a paradigm for <b>information</b> <b>extraction</b> that amalgamates channel and token <b>information.</b> <b>The</b> essence of Mixer lies in its ability to blend <b>information</b> <b>from</b> diverse perspectives, epitomizing the true concept of &ldquo;mixing&rdquo; in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and frequency domains. Experimental results demonstrate that ASM-RH is particularly well-suited for audio data and yields promising outcomes across multiple classification tasks.</p></p class="citation"></blockquote><h3 id=4853--170286-imitation-regularized-optimal-transport-on-networks-provable-robustness-and-application-to-logistics-planning-koshi-oishi-et-al-2024>(48/53 | 170/286) Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning (Koshi Oishi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koshi Oishi, Yota Hashizume, Tomohiko Jimbo, Hirotaka Kaji, Kenji Kashima. (2024)<br><strong>Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning</strong><br><button class=copy-to-clipboard title="Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17967v1.pdf filename=2402.17967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network systems form the foundation of modern society, playing a critical role in various applications. However, these systems are at significant risk of being adversely affected by unforeseen circumstances, such as disasters. Considering this, there is a pressing need for research to enhance the robustness of network systems. Recently, in <b>reinforcement</b> <b>learning,</b> the relationship between acquiring robustness and regularizing entropy has been identified. Additionally, imitation learning is used within this framework to reflect experts&rsquo; behavior. However, there are no comprehensive studies on the use of a similar imitation framework for optimal transport on networks. Therefore, in this study, imitation-regularized optimal transport (I-OT) on networks was investigated. It encodes prior knowledge on the network by imitating a given prior distribution. The I-OT solution demonstrated robustness in terms of the cost defined on the network. Moreover, we applied the I-OT to a logistics planning problem using real data. We also examined the imitation and apriori risk information scenarios to demonstrate the usefulness and implications of the proposed method.</p></p class="citation"></blockquote><h3 id=4953--171286-multi-sensor-and-multi-temporal-high-throughput-phenotyping-for-monitoring-and-early-detection-of-water-limiting-stress-in-soybean-sarah-e-jones-et-al-2024>(49/53 | 171/286) Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean (Sarah E. Jones et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarah E. Jones, Timilehin Ayanlade, Benjamin Fallen, Talukder Z. Jubery, Arti Singh, Baskar Ganapathysubramanian, Soumik Sarkar, Asheesh K. Singh. (2024)<br><strong>Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean</strong><br><button class=copy-to-clipboard title="Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18751v1.pdf filename=2402.18751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events. Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production. This project combines <b>multi-modal</b> information to identify the most effective and efficient automated methods to investigate drought response. We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress. We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping. The red-edge and green bands were most effective to classify canopy wilting stress. The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated susceptible and tolerant soybean accessions prior to visual symptom development. We report pre-visual detection of soybean wilting using a combination of different vegetation indices. These results can contribute to early stress detection methodologies and rapid classification of drought responses in screening nurseries for breeding and production applications.</p></p class="citation"></blockquote><h3 id=5053--172286-quantifying-human-priors-over-social-and-navigation-networks-gecia-bravo-hermsdorff-2024>(50/53 | 172/286) Quantifying Human Priors over Social and Navigation Networks (Gecia Bravo-Hermsdorff, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gecia Bravo-Hermsdorff. (2024)<br><strong>Quantifying Human Priors over Social and Navigation Networks</strong><br><button class=copy-to-clipboard title="Quantifying Human Priors over Social and Navigation Networks" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG, physics-soc-ph, q-bio-NC, stat-ME<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18651v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18651v1.pdf filename=2402.18651v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human knowledge is largely implicit and relational &ndash; do we have a friend in common? can I walk from here to there? In this work, we leverage the combinatorial structure of <b>graphs</b> to quantify human priors over such relational data. Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of <b>graph</b> size. Other features are domain-specific, such as the propensity for triadic closure in social interactions. More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data.</p></p class="citation"></blockquote><h3 id=5153--173286-log-neural-controlled-differential-equations-the-lie-brackets-make-a-difference-benjamin-walker-et-al-2024>(51/53 | 173/286) Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference (Benjamin Walker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Walker, Andrew D. McLeod, Tiexin Qin, Yichuan Cheng, Haoliang Li, Terry Lyons. (2024)<br><strong>Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference</strong><br><button class=copy-to-clipboard title="Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18512v1.pdf filename=2402.18512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE&rsquo;s vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE&rsquo;s solution. On a range of multivariate time series classification <b>benchmarks,</b> Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurrent unit.</p></p class="citation"></blockquote><h3 id=5253--174286-signature-kernel-conditional-independence-tests-in-causal-discovery-for-stochastic-processes-georg-manten-et-al-2024>(52/53 | 174/286) Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes (Georg Manten et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus. (2024)<br><strong>Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes</strong><br><button class=copy-to-clipboard title="Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18477v1.pdf filename=2402.18477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via &ldquo;which variables enter the differential of which other variables&rdquo;. In this paper, we develop a kernel-based test of conditional independence (CI) on &ldquo;path-space&rdquo; &ndash; solutions to SDEs &ndash; by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed <b>graph.</b> Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.</p></p class="citation"></blockquote><h3 id=5353--175286-escaping-local-optima-in-global-placement-ke-xue-et-al-2024>(53/53 | 175/286) Escaping Local Optima in Global Placement (Ke Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ke Xue, Xi Lin, Yunqi Shi, Shixiong Kai, Siyuan Xu, Chao Qian. (2024)<br><strong>Escaping Local Optima in Global Placement</strong><br><button class=copy-to-clipboard title="Escaping Local Optima in Global Placement" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18311v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18311v1.pdf filename=2402.18311v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Placement is crucial in the physical design, as it greatly affects power, performance, and area metrics. Recent advancements in analytical methods, such as DREAMPlace, have demonstrated impressive performance in global placement. However, DREAMPlace has some limitations, e.g., may not guarantee legalizable placements under the same settings, leading to fragile and unpredictable results. This paper highlights the main issue as being stuck in local optima, and proposes a hybrid optimization framework to efficiently escape the local optima, by perturbing the placement result iteratively. The proposed framework achieves significant improvements compared to state-of-the-art methods on two popular <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=csai-12>cs.AI (12)</h2><h3 id=112--176286-large-language-models-as-evolution-strategies-robert-tjarko-lange-et-al-2024>(1/12 | 176/286) Large Language Models As Evolution Strategies (Robert Tjarko Lange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robert Tjarko Lange, Yingtao Tian, Yujin Tang. (2024)<br><strong>Large Language Models As Evolution Strategies</strong><br><button class=copy-to-clipboard title="Large Language Models As Evolution Strategies" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-NE, cs.AI<br>Keyword Score: 85<br>Keywords: Black Box, Fine-tuning, Zero-shot, Transformer, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18381v1.pdf filename=2402.18381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Transformer</b> <b>models</b> are capable of implementing a plethora of so-called <b>in-context</b> <b>learning</b> algorithms. These include gradient descent, classification, sequence completion, transformation, and improvement. In this work, we investigate whether <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which never explicitly encountered the task of <b>black-box</b> <b>optimization,</b> are in principle capable of implementing evolutionary optimization algorithms. While previous works have solely focused on language-based task specification, we move forward and focus on the <b>zero-shot</b> application of <b>LLMs</b> to <b>black-box</b> <b>optimization.</b> We introduce a novel <b>prompting</b> strategy, consisting of least-to-most sorting of discretized population members and querying the <b>LLM</b> to propose an improvement to the mean statistic, i.e. perform a type of <b>black-box</b> <b>recombination</b> operation. Empirically, we find that our setup allows the user to obtain an <b>LLM-based</b> evolution strategy, which we call <code>EvoLLM', that robustly outperforms baseline algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB functions as well as small neuroevolution tasks. Hence, &lt;b>LLMs&lt;/b> can act as </code>plug-in&rsquo; <b>in-context</b> <b>recombination</b> operators. We provide several comparative studies of the <b>LLM&rsquo;s</b> model size, <b>prompt</b> strategy, and context construction. Finally, we show that one can flexibly improve EvoLLM&rsquo;s performance by providing teacher algorithm information via instruction <b>fine-tuning</b> on previously collected teacher optimization trajectories.</p></p class="citation"></blockquote><h3 id=212--177286-from-summary-to-action-enhancing-large-language-models-for-complex-tasks-with-open-world-apis-yulong-liu-et-al-2024>(2/12 | 177/286) From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs (Yulong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li Zhang, Nanning Zheng, Hang Xu. (2024)<br><strong>From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs</strong><br><button class=copy-to-clipboard title="From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18157v1.pdf filename=2402.18157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The distinction between humans and animals lies in the unique ability of humans to use and create tools. Tools empower humans to overcome physiological limitations, fostering the creation of magnificent civilizations. Similarly, enabling foundational models like <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with the capacity to learn external tool usage may serve as a pivotal step toward realizing artificial general intelligence. Previous studies in this field have predominantly pursued two distinct approaches to augment the tool invocation capabilities of <b>LLMs.</b> The first approach emphasizes the construction of relevant datasets for model <b>fine-tuning.</b> The second approach, in contrast, aims to fully exploit the inherent <b>reasoning</b> abilities of <b>LLMs</b> through <b>in-context</b> <b>learning</b> strategies. In this work, we introduce a novel tool invocation pipeline designed to control massive real-world APIs. This pipeline mirrors the human task-solving process, addressing complicated real-life user queries. At each step, we guide <b>LLMs</b> to <b>summarize</b> the achieved results and determine the next course of action. We term this pipeline `from Summary to action&rsquo;, Sum2Act for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench <b>benchmark</b> show significant performance improvements, outperforming established methods like ReAct and DFSDT. This highlights Sum2Act&rsquo;s effectiveness in enhancing <b>LLMs</b> for complex real-world tasks.</p></p class="citation"></blockquote><h3 id=312--178286-do-large-language-models-mirror-cognitive-language-processing-yuqi-ren-et-al-2024>(3/12 | 178/286) Do Large Language Models Mirror Cognitive Language Processing? (Yuqi Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong. (2024)<br><strong>Do Large Language Models Mirror Cognitive Language Processing?</strong><br><button class=copy-to-clipboard title="Do Large Language Models Mirror Cognitive Language Processing?" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 50<br>Keywords: Chatbot, Massive Multitask Language Understanding (MMLU), Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18023v1.pdf filename=2402.18023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities in text comprehension and logical <b>reasoning,</b> achiving or even surpassing human-level performance in numerous cognition tasks. As <b>LLMs</b> are trained from massive textual outputs of human language cognition, it is natural to ask whether <b>LLMs</b> mirror cognitive language processing. Or to what extend <b>LLMs</b> resemble cognitive language processing? In this paper, we propose a novel method that bridge between <b>LLM</b> representations and human cognition signals to evaluate how effectively <b>LLMs</b> simulate cognitive language processing. We employ Representational Similarity Analysis (RSA) to mearsure the alignment between 16 mainstream <b>LLMs</b> and fMRI signals of the brain. We empirically investigate the impact of a variety of factors (e.g., model scaling, alignment training, instruction appending) on such <b>LLM-brain</b> alignment. Experimental results indicate that model scaling is positively correlated with <b>LLM-brain</b> similarity, and alignment training can significantly improve <b>LLM-brain</b> similarity. Additionally, the performance of a wide range of <b>LLM</b> evaluations (e.g., <b>MMLU,</b> <b>Chatbot</b> Arena) is highly correlated with the <b>LLM-brain</b> similarity.</p></p class="citation"></blockquote><h3 id=412--179286-a-cognitive-evaluation-benchmark-of-image-reasoning-and-description-for-large-vision-language-models-xiujie-song-et-al-2024>(4/12 | 179/286) A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models (Xiujie Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen. (2024)<br><strong>A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models</strong><br><button class=copy-to-clipboard title="A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CV, cs.AI<br>Keyword Score: 43<br>Keywords: Benchmarking, Question Answering, Reasoning, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18409v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18409v2.pdf filename=2402.18409v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the &ldquo;Cookie Theft&rdquo; task in human cognition test, we propose a novel evaluation <b>benchmark</b> to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight <b>reasoning</b> capabilities and consists of an image description task and a <b>visual</b> <b>question</b> <b>answering</b> task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.</p></p class="citation"></blockquote><h3 id=512--180286-data-interpreter-an-llm-agent-for-data-science-sirui-hong-et-al-2024>(5/12 | 180/286) Data Interpreter: An LLM Agent For Data Science (Sirui Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu Zheng. (2024)<br><strong>Data Interpreter: An LLM Agent For Data Science</strong><br><button class=copy-to-clipboard title="Data Interpreter: An LLM Agent For Data Science" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 33<br>Keywords: Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18679v1.pdf filename=2402.18679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise <b>reasoning.</b> In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical <b>graph</b> structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at <a href=https://github.com/geekan/MetaGPT>https://github.com/geekan/MetaGPT</a>.</p></p class="citation"></blockquote><h3 id=612--181286-language-models-represent-beliefs-of-self-and-others-wentao-zhu-et-al-2024>(6/12 | 181/286) Language Models Represent Beliefs of Self and Others (Wentao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Zhu, Zhining Zhang, Yizhou Wang. (2024)<br><strong>Language Models Represent Beliefs of Self and Others</strong><br><button class=copy-to-clipboard title="Language Models Represent Beliefs of Self and Others" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18496v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18496v2.pdf filename=2402.18496v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social <b>reasoning.</b> While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others&rsquo; beliefs. By manipulating these representations, we observe dramatic changes in the models&rsquo; ToM performance, underscoring their pivotal role in the social <b>reasoning</b> process. Additionally, our findings extend to diverse social <b>reasoning</b> tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.</p></p class="citation"></blockquote><h3 id=712--182286-random-silicon-sampling-simulating-human-sub-population-opinion-using-a-large-language-model-based-on-group-level-demographic-information-seungjong-sun-et-al-2024>(7/12 | 182/286) Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information (Seungjong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee, Bernard J. Jansen, Jang Hyun Kim. (2024)<br><strong>Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information</strong><br><button class=copy-to-clipboard title="Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-7, cs-AI, cs-CY, cs.AI<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18144v1.pdf filename=2402.18144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose &ldquo;random silicon sampling,&rdquo; a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group&rsquo;s opinion using only demographic distribution and elucidate the effect of social biases in language models on such <b>simulations.</b></p></p class="citation"></blockquote><h3 id=812--183286-commonsense-ontology-micropatterns-andrew-eells-et-al-2024>(8/12 | 183/286) Commonsense Ontology Micropatterns (Andrew Eells et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Eells, Brandon Dave, Pascal Hitzler, Cogan Shimizu. (2024)<br><strong>Commonsense Ontology Micropatterns</strong><br><button class=copy-to-clipboard title="Commonsense Ontology Micropatterns" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18715v1.pdf filename=2402.18715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes. However, a major bottleneck to <b>large-scale</b> <b>deployment</b> <b>of</b> MOMo is the (to-date) limited availability of ready-to-use ontology design patterns. At the same time, <b>Large</b> <b>Language</b> <b>Models</b> have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in <b>LLMs,</b> organized into a fully-annotated modular ontology design library ready for use with MOMo.</p></p class="citation"></blockquote><h3 id=912--184286-automated-discovery-of-integral-with-deep-learning-xiaoxin-yin-2024>(9/12 | 184/286) Automated Discovery of Integral with Deep Learning (Xiaoxin Yin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxin Yin. (2024)<br><strong>Automated Discovery of Integral with Deep Learning</strong><br><button class=copy-to-clipboard title="Automated Discovery of Integral with Deep Learning" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18040v1.pdf filename=2402.18040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in the realm of deep learning, particularly in the development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> have demonstrated AI&rsquo;s ability to tackle complex mathematical problems or solving programming challenges. However, the capability to solve well-defined problems based on extensive training data differs significantly from the nuanced process of making scientific discoveries. Trained on almost all human knowledge available, today&rsquo;s sophisticated <b>LLMs</b> basically learn to predict sequences of tokens. They generate mathematical derivations and write code in a similar way as writing an essay, and do not have the ability to pioneer scientific discoveries in the manner a human scientist would do. In this study we delve into the potential of using deep learning to rediscover a fundamental mathematical concept: integrals. By defining integrals as area under the curve, we illustrate how AI can deduce the integral of a given function, exemplified by inferring $\int_{0}^{x} t^2 dt = \frac{x^3}{3}$ and $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$. Our experiments show that deep learning models can approach the task of inferring integrals either through a sequence-to-sequence model, akin to language translation, or by uncovering the rudimentary principles of integration, such as $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$.</p></p class="citation"></blockquote><h3 id=1012--185286-sample-efficient-preference-based-reinforcement-learning-with-dynamics-aware-rewards-katherine-metcalf-et-al-2024>(10/12 | 185/286) Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards (Katherine Metcalf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katherine Metcalf, Miguel Sarabia, Natalie Mackraz, Barry-John Theobald. (2024)<br><strong>Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards</strong><br><button class=copy-to-clipboard title="Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17975v1.pdf filename=2402.17975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference-based <b>reinforcement</b> <b>learning</b> (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a <b>self-supervised</b> temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83% and 66% of ground truth reward policy performance versus only 38% and 21%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model. Repo: \texttt{https://github.com/apple/ml-reed}.</p></p class="citation"></blockquote><h3 id=1112--186286-gaia-categorical-foundations-of-generative-ai-sridhar-mahadevan-2024>(11/12 | 186/286) GAIA: Categorical Foundations of Generative AI (Sridhar Mahadevan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sridhar Mahadevan. (2024)<br><strong>GAIA: Categorical Foundations of Generative AI</strong><br><button class=copy-to-clipboard title="GAIA: Categorical Foundations of Generative AI" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18732v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18732v1.pdf filename=2402.18732v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose GAIA, a <b>generative</b> <b>AI</b> architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.</p></p class="citation"></blockquote><h3 id=1212--187286-a-relational-inductive-bias-for-dimensional-abstraction-in-neural-networks-declan-campbell-et-al-2024>(12/12 | 187/286) A Relational Inductive Bias for Dimensional Abstraction in Neural Networks (Declan Campbell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Declan Campbell, Jonathan D. Cohen. (2024)<br><strong>A Relational Inductive Bias for Dimensional Abstraction in Neural Networks</strong><br><button class=copy-to-clipboard title="A Relational Inductive Bias for Dimensional Abstraction in Neural Networks" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18426v1.pdf filename=2402.18426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The human cognitive system exhibits remarkable flexibility and generalization capabilities, partly due to its ability to form low-dimensional, compositional representations of the environment. In contrast, standard neural network architectures often struggle with abstract <b>reasoning</b> tasks, overfitting, and requiring extensive data for training. This paper investigates the impact of the relational bottleneck &ndash; a mechanism that focuses processing on relations among inputs &ndash; on the learning of factorized representations conducive to compositional coding and the attendant flexibility of processing. We demonstrate that such a bottleneck not only improves generalization and learning efficiency, but also aligns network performance with human-like behavioral biases. Networks trained with the relational bottleneck developed orthogonal representations of feature dimensions latent in the dataset, reflecting the factorized structure thought to underlie human cognitive flexibility. Moreover, the relational network mimics human biases towards regularity without pre-specified symbolic primitives, suggesting that the bottleneck fosters the emergence of abstract representations that confer flexibility akin to symbols.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--188286-making-them-ask-and-answer-jailbreaking-large-language-models-in-few-queries-via-disguise-and-reconstruction-tong-liu-et-al-2024>(1/9 | 188/286) Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction (Tong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen. (2024)<br><strong>Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction</strong><br><button class=copy-to-clipboard title="Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 75<br>Keywords: Black Box, Fine-tuning, GPT, GPT-4, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18104v1.pdf filename=2402.18104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated notable success across various tasks, but the trustworthiness of <b>LLMs</b> is still an open problem. One specific threat is the potential to generate toxic or harmful responses. Attackers can craft adversarial <b>prompts</b> that induce harmful responses from <b>LLMs.</b> In this work, we pioneer a theoretical foundation in <b>LLMs</b> security by identifying bias vulnerabilities within the safety <b>fine-tuning</b> and design a <b>black-box</b> <b>jailbreak</b> method named DRA (Disguise and Reconstruction Attack), which conceals harmful instructions through disguise and <b>prompts</b> the model to reconstruct the original harmful instruction within its completion. We evaluate DRA across various open-source and close-source models, showcasing state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA boasts a 90% attack success rate on <b>LLM</b> <b>chatbots</b> <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=29--189286-chatspamdetector-leveraging-large-language-models-for-effective-phishing-email-detection-takashi-koide-et-al-2024>(2/9 | 189/286) ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection (Takashi Koide et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takashi Koide, Naoki Fukushi, Hiroki Nakano, Daiki Chiba. (2024)<br><strong>ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection</strong><br><button class=copy-to-clipboard title="ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18093v1.pdf filename=2402.18093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in spam filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as spam, risking the possibility of missing important communications or mistakenly trusting phishing emails. This study introduces ChatSpamDetector, a system that uses <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to detect phishing emails. By converting email data into a <b>prompt</b> suitable for <b>LLM</b> analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed <b>reasoning</b> for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several <b>LLMs</b> and baseline systems. We confirmed that our system using <b>GPT-4</b> has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by <b>LLMs</b> enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.</p></p class="citation"></blockquote><h3 id=39--190286-vulmci--code-splicing-based-pixel-row-oversampling-for-more-continuous-vulnerability-image-generation-tao-peng-et-al-2024>(3/9 | 190/286) VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation (Tao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Peng, Ling Gui, Yi Sun. (2024)<br><strong>VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation</strong><br><button class=copy-to-clipboard title="VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18189v1.pdf filename=2402.18189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the rapid development of deep learning technology has brought new prospects to the field of vulnerability detection. Many vulnerability detection methods involve converting source code into images for detection, yet they often overlook the quality of the generated images. Due to the fact that vulnerability images lack clear and continuous contours, unlike images used in <b>object</b> <b>detection,</b> <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> tend to lose semantic information during the <b>convolution</b> and pooling processes. Therefore, this paper proposes a pixel row oversampling method based on code line concatenation to generate more continuous code features, addressing the issue of discontinuity in code image coloration.Building upon these contributions, we propose the vulnerability detection system VulMCI and conduct tests on the SARD and NVD datasets. Experimental results demonstrate that VulMCI outperforms seven state-of-the-art vulnerability detectors (namely Checkmarx, FlawFinder, RATS, VulDeePecker, SySeVR, VulCNN, and Devign). Compared to other image-based methods, VulMCI shows improvements in various metrics, including a 2.877% increase in True Positive Rate (TPR), a 5.446% increase in True Negative Rate (TNR), and a 5.91% increase in Accuracy (ACC). On the NVD real-world dataset, VulMCI achieves an average accuracy of 5.162%, confirming its value in practical vulnerability detection applications.</p></p class="citation"></blockquote><h3 id=49--191286-efficient-fault-detection-architectures-for-modular-exponentiation-targeting-cryptographic-applications-benchmarked-on-fpgas-saeed-aghapour-et-al-2024>(4/9 | 191/286) Efficient Fault Detection Architectures for Modular Exponentiation Targeting Cryptographic Applications Benchmarked on FPGAs (Saeed Aghapour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Aghapour, Kasra Ahmadi, Mehran Mozaffari Kermani, Reza Azarderakhsh. (2024)<br><strong>Efficient Fault Detection Architectures for Modular Exponentiation Targeting Cryptographic Applications Benchmarked on FPGAs</strong><br><button class=copy-to-clipboard title="Efficient Fault Detection Architectures for Modular Exponentiation Targeting Cryptographic Applications Benchmarked on FPGAs" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18033v1.pdf filename=2402.18033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whether <b>stemming</b> from malicious intent or natural occurrences, faults and errors can significantly undermine the reliability of any architecture. In response to this challenge, fault detection assumes a pivotal role in ensuring the secure deployment of cryptosystems. Even when a cryptosystem boasts mathematical security, its practical implementation may remain susceptible to exploitation through side-channel attacks. In this paper, we propose a lightweight fault detection architecture tailored for modular exponentiation, a building block of numerous cryptographic applications spanning from classical cryptography to post quantum cryptography. Based on our <b>simulation</b> and implementation results on ARM Cortex-A72 processor, and AMD/Xilinx Zynq Ultrascale+, and Artix-7 FPGAs, our approach achieves an error detection rate close to 100%, all while introducing a modest computational overhead of approximately 7% and area overhead of less than 1% compared to the unprotected architecture. To the best of our knowledge, such an approach <b>benchmarked</b> on ARM processor and FPGA has not been proposed and assessed to date.</p></p class="citation"></blockquote><h3 id=59--192286-a-new-era-in-llm-security-exploring-security-concerns-in-real-world-llm-based-systems-fangzhou-wu-et-al-2024>(5/9 | 192/286) A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems (Fangzhou Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao. (2024)<br><strong>A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems</strong><br><button class=copy-to-clipboard title="A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18649v1.pdf filename=2402.18649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> systems are inherently compositional, with individual <b>LLM</b> serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on <b>LLM</b> security often focus on individual <b>LLM,</b> but without examining the ecosystem through the lens of <b>LLM</b> systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of <b>LLM</b> systems, instead of focusing on the individual <b>LLMs.</b> To do so, we build on top of the information flow and formulate the security of <b>LLM</b> systems as constraints on the alignment of the information flow within <b>LLM</b> and between <b>LLM</b> and other objects. Based on this construction and the unique probabilistic nature of <b>LLM,</b> the attack surface of the <b>LLM</b> system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art <b>LLM</b> system, OpenAI <b>GPT4.</b> Our investigation exposes several security issues, not just within the <b>LLM</b> model itself but also in its integration with other components. We found that although the OpenAI <b>GPT4</b> has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user&rsquo;s chat history, all without the need to manipulate the user&rsquo;s input or gain direct access to OpenAI <b>GPT4.</b> Our demo is in the link: <a href=https://fzwark.github.io/LLM-System-Attack-Demo/>https://fzwark.github.io/LLM-System-Attack-Demo/</a></p></p class="citation"></blockquote><h3 id=69--193286-exploring-advanced-methodologies-in-security-evaluation-for-llms-jun-huang-et-al-2024>(6/9 | 193/286) Exploring Advanced Methodologies in Security Evaluation for LLMs (Jun Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Huang, Jiawei Zhang, Qi Wang, Weihong Han, Yanchun Zhang. (2024)<br><strong>Exploring Advanced Methodologies in Security Evaluation for LLMs</strong><br><button class=copy-to-clipboard title="Exploring Advanced Methodologies in Security Evaluation for LLMs" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17970v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17970v2.pdf filename=2402.17970v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> represent an advanced evolution of earlier, simpler language models. They boast enhanced abilities to handle complex language patterns and generate coherent text, images, audios, and videos. Furthermore, they can be <b>fine-tuned</b> for specific tasks. This versatility has led to the proliferation and extensive use of numerous commercialized <b>large</b> <b>models.</b> <b>However,</b> the rapid expansion of <b>LLMs</b> has raised security and ethical concerns within the academic community. This emphasizes the need for ongoing research into security evaluation during their development and deployment. Over the past few years, a substantial body of research has been dedicated to the security evaluation of <b>large-scale</b> <b>models.</b> <b>This</b> article an in-depth review of the most recent advancements in this field, providing a comprehensive analysis of commonly used evaluation metrics, advanced evaluation frameworks, and the routine evaluation processes for <b>LLMs.</b> Furthermore, we also discuss the future directions for advancing the security evaluation of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=79--194286-living-off-the-land-reverse-shell-detection-by-informed-data-augmentation-dmitrijs-trizna-et-al-2024>(7/9 | 194/286) Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation (Dmitrijs Trizna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Fabio Roli. (2024)<br><strong>Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation</strong><br><button class=copy-to-clipboard title="Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18329v1.pdf filename=2402.18329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an extensive ablation study to understand which models better handle our augmented dataset, also manipulated to mimic the presence of model-agnostic evasion and poisoning attacks. Our results suggest that augmentation is needed to maintain high-predictive capabilities, robustness to attack is achieved through specific hardening techniques like <b>adversarial</b> <b>training,</b> and it is possible to deploy near-real-time models with almost-zero false alarms.</p></p class="citation"></blockquote><h3 id=89--195286-performance-modeling-of-public-permissionless-blockchains-a-survey-molud-esmaili-et-al-2024>(8/9 | 195/286) Performance modeling of public permissionless blockchains: A survey (Molud Esmaili et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Molud Esmaili, Ken Christensen. (2024)<br><strong>Performance modeling of public permissionless blockchains: A survey</strong><br><button class=copy-to-clipboard title="Performance modeling of public permissionless blockchains: A survey" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18049v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18049v1.pdf filename=2402.18049v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Public permissionless blockchains facilitate peer-to-peer digital transactions, yet face performance challenges specifically minimizing transaction confirmation time to decrease energy and time consumption per transaction. Performance evaluation and prediction are crucial in achieving this objective, with performance modeling as a key solution despite the complexities involved in assessing these blockchains. This survey examines prior research concerning the performance modeling blockchain systems, specifically focusing on public permissionless blockchains. Initially, it provides foundational knowledge about these blockchains and the crucial performance parameters for their assessment. Additionally, the study delves into research on the performance modeling of public permissionless blockchains, predominantly considering these systems as bulk service queues. It also examines prior studies on workload and traffic modeling, characterization, and analysis within these blockchain networks. By analyzing existing research, our survey aims to provide insights and <b>recommendations</b> for researchers keen on enhancing the performance of public permissionless blockchains or devising novel mechanisms in this domain.</p></p class="citation"></blockquote><h3 id=99--196286-on-defeating-graph-analysis-of-anonymous-transactions-christoph-egger-et-al-2024>(9/9 | 196/286) On Defeating Graph Analysis of Anonymous Transactions (Christoph Egger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoph Egger, Russell W. F. Lai, Viktoria Ronge, Ivy K. Y. Woo, Hoover H. F. Yin. (2024)<br><strong>On Defeating Graph Analysis of Anonymous Transactions</strong><br><button class=copy-to-clipboard title="On Defeating Graph Analysis of Anonymous Transactions" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18755v1.pdf filename=2402.18755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a ring-signature-based anonymous cryptocurrency, signers of a transaction are hidden among a set of potential signers, called a ring, whose size is much smaller than the number of all users. The ring-membership relations specified by the sets of transactions thus induce bipartite transaction <b>graphs,</b> whose distribution is in turn induced by the ring sampler underlying the cryptocurrency. Since efficient <b>graph</b> analysis could be performed on transaction <b>graphs</b> to potentially deanonymise signers, it is crucial to understand the resistance of (the transaction <b>graphs</b> induced by) a ring sampler against <b>graph</b> analysis. Of particular interest is the class of partitioning ring samplers. Although previous works showed that they provide almost optimal local anonymity, their resistance against global, e.g. <b>graph-based,</b> attacks were unclear. In this work, we analyse transaction <b>graphs</b> induced by partitioning ring samplers. Specifically, we show (partly analytically and partly empirically) that, somewhat surprisingly, by setting the ring size to be at least logarithmic in the number of users, a <b>graph-analysing</b> adversary is no better than the one that performs random guessing in deanonymisation up to constant factor of 2.</p></p class="citation"></blockquote><h2 id=csro-20>cs.RO (20)</h2><h3 id=120--197286-cafknet-gnn-empowered-forward-kinematic-modeling-for-cable-driven-parallel-robots-zeqing-zhang-et-al-2024>(1/20 | 197/286) CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots (Zeqing Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeqing Zhang, Linhan Yang, Cong Sun, Weiwei Shang, Jia Pan. (2024)<br><strong>CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots</strong><br><button class=copy-to-clipboard title="CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18420v1.pdf filename=2402.18420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When deploying Cable-Driven Parallel Robots (CDPRs) in practice, one of the challenges is kinematic modeling. Unlike serial mechanisms, CDPRs have a simple inverse kinematics problem but a complex forward kinematics (FK) issue. Therefore, the development of accurate and efficient FK solvers has been a prominent research focus in CDPR applications. By observing the topology within CDPRs, in this letter, we propose a <b>graph-based</b> <b>representation</b> <b>to</b> model CDPRs and introduce CafkNet, a fast and general FK solver, leveraging <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN).</b> Extensive experiments are conducted on 3D and 2D CDPRs across various configurations, including under-constrained, fully-constrained, and over-constrained cases, in both <b>simulation</b> environments and real-world scenarios. The experimental results showcase that CafkNet can learn the internal topological information of CDPRs and accurately solve the FK problem as an FK solver. Furthermore, training the CafkNet model on partial configurations enables <b>zero-shot</b> generalization to other configurations. Lastly, CafkNet effectively bridges the sim2real gap by using both <b>simulation</b> data and part of real-world data. To the best of our knowledge, it is the first study that employs the <b>GNN</b> to solve the FK problem for CDPRs.</p></p class="citation"></blockquote><h3 id=220--198286-solving-multi-entity-robotic-problems-using-permutation-invariant-neural-networks-tianxu-an-et-al-2024>(2/20 | 198/286) Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks (Tianxu An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxu An, Joonho Lee, Marko Bjelonic, Flavio De Vincenti, Marco Hutter. (2024)<br><strong>Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks</strong><br><button class=copy-to-clipboard title="Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18345v1.pdf filename=2402.18345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Challenges in real-world robotic applications often stem from managing multiple, dynamically varying entities such as neighboring robots, manipulable objects, and navigation goals. Existing multi-agent control strategies face scalability limitations, struggling to handle arbitrary numbers of entities. Additionally, they often rely on engineered heuristics for assigning entities among agents. We propose a data driven approach to address these limitations by introducing a decentralized control system using neural network policies trained in <b>simulation.</b> Leveraging permutation invariant neural network architectures and model-free <b>reinforcement</b> <b>learning,</b> our approach allows control agents to autonomously determine the relative importance of different entities without being biased by ordering or limited by a fixed capacity. We validate our approach through both <b>simulations</b> and real-world experiments involving multiple wheeled-legged quadrupedal robots, demonstrating their collaborative control capabilities. We prove the effectiveness of our architectural choice through experiments with three exemplary multi-entity problems. Our analysis underscores the pivotal role of the end-to-end trained permutation invariant encoders in achieving scalability and improving the task performance in multi-object manipulation or multi-goal navigation problems. The adaptability of our policy is further evidenced by its ability to manage varying numbers of entities in a <b>zero-shot</b> manner, showcasing near-optimal autonomous task distribution and collision avoidance behaviors.</p></p class="citation"></blockquote><h3 id=320--199286-decisionnce-embodied-multimodal-representations-via-implicit-preference-learning-jianxiong-li-et-al-2024>(3/20 | 199/286) DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning (Jianxiong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan. (2024)<br><strong>DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning</strong><br><button class=copy-to-clipboard title="DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 31<br>Keywords: Contrastive Learning, Multi-modal, Multi-modal, Representation Learning, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18137v1.pdf filename=2402.18137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> pretraining has emerged as an effective strategy for the trinity of goals of <b>representation</b> <b>learning</b> in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual <b>representation;</b> <b>3)</b> capturing trajectory-level language <b>grounding.</b> Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into <b>representation</b> <b>learning</b> through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively tailored for decision-making tasks, providing an embodied <b>representation</b> <b>learning</b> framework that elegantly extracts both local and global task progression features, with temporal consistency enforced through implicit time <b>contrastive</b> <b>learning,</b> while ensuring trajectory-level instruction <b>grounding</b> via <b>multimodal</b> joint encoding. Evaluation on both simulated and real robots demonstrates that DecisionNCE effectively facilitates diverse downstream policy learning tasks, offering a versatile solution for unified <b>representation</b> <b>and</b> reward learning. Project Page: <a href=https://2toinf.github.io/DecisionNCE/>https://2toinf.github.io/DecisionNCE/</a></p></p class="citation"></blockquote><h3 id=420--200286-automated-testing-of-spatially-dependent-environmental-hypotheses-through-active-transfer-learning-nicholas-harrison-et-al-2024>(4/20 | 200/286) Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning (Nicholas Harrison et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Harrison, Nathan Wallace, Salah Sukkarieh. (2024)<br><strong>Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning</strong><br><button class=copy-to-clipboard title="Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Active Learning, Gaussian Process, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18064v1.pdf filename=2402.18064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines <b>transfer</b> <b>learning</b> and <b>active</b> <b>learning</b> through a Multi-Task <b>Gaussian</b> <b>Process</b> and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate multiple hypotheses correctly. Its effectiveness is also demonstrated on real datasets. The technique is able to identify and leverage hypotheses which show a medium or strong correlation to reduce prediction error by a factor of 1.5&ndash;6 within the first 5 samples, and poor hypotheses are quickly identified and rejected, having no adverse effect on planning after around 3 samples.</p></p class="citation"></blockquote><h3 id=520--201286-symmetry-aware-reinforcement-learning-for-robotic-assembly-under-partial-observability-with-a-soft-wrist-hai-nguyen-et-al-2024>(5/20 | 201/286) Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist (Hai Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hai Nguyen, Tadashi Kozuno, Cristian C. Beltran-Hernandez, Masashi Hamaya. (2024)<br><strong>Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist</strong><br><button class=copy-to-clipboard title="Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18002v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18002v1.pdf filename=2402.18002v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study tackles the representative yet challenging contact-rich peg-in-hole task of robotic assembly, using a soft wrist that can operate more safely and tolerate lower-frequency control signals than a rigid one. Previous studies often use a fully observable formulation, requiring external setups or estimators for the peg-to-hole pose. In contrast, we use a partially observable formulation and deep <b>reinforcement</b> <b>learning</b> from demonstrations to learn a memory-based agent that acts purely on haptic and proprioceptive signals. Moreover, previous works do not incorporate potential domain symmetry and thus must search for solutions in a bigger space. Instead, we propose to leverage the symmetry for sample efficiency by augmenting the training data and constructing auxiliary losses to force the agent to adhere to the symmetry. Results in <b>simulation</b> with five different symmetric peg shapes show that our proposed agent can be comparable to or even outperform a state-based agent. In particular, the sample efficiency also allows us to learn directly on the real robot within 3 hours.</p></p class="citation"></blockquote><h3 id=620--202286-extending-qgroundcontrol-for-automated-mission-planning-of-uavs-cristian-ramirez-atencia-et-al-2024>(6/20 | 202/286) Extending QGroundControl for Automated Mission Planning of UAVs (Cristian Ramirez-Atencia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian Ramirez-Atencia, David Camacho. (2024)<br><strong>Extending QGroundControl for Automated Mission Planning of UAVs</strong><br><button class=copy-to-clipboard title="Extending QGroundControl for Automated Mission Planning of UAVs" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18754v1.pdf filename=2402.18754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unmanned Aerial Vehicle (UAVs) have become very popular in the last decade due to some advantages such as strong terrain adaptation, low cost, zero casualties, and so on. One of the most interesting advances in this field is the automation of mission planning (task allocation) and real-time replanning, which are highly useful to increase the autonomy of the vehicle and reduce the operator workload. These automated mission planning and replanning systems require a Human Computer Interface (HCI) that facilitates the visualization and selection of plans that will be executed by the vehicles. In addition, most missions should be assessed before their real-life execution. This paper extends QGroundControl, an open-source <b>simulation</b> environment for flight control of multiple vehicles, by adding a mission designer that permits the operator to build complex missions with tasks and other scenario items; an interface for automated mission planning and replanning, which works as a test bed for different algorithms, and a Decision Support System (DSS) that helps the operator in the selection of the plan. In this work, a complete guide of these systems and some practical use cases are provided.</p></p class="citation"></blockquote><h3 id=720--203286-articulated-object-manipulation-with-coarse-to-fine-affordance-for-mitigating-the-effect-of-point-cloud-noise-suhan-ling-et-al-2024>(7/20 | 203/286) Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise (Suhan Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Suhan Ling, Yian Wang, Shiguang Wu, Yuzheng Zhuang, Tianyi Xu, Yu Li, Chang Liu, Hao Dong. (2024)<br><strong>Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise</strong><br><button class=copy-to-clipboard title="Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18699v1.pdf filename=2402.18699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects.Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challenge remains: while previous works use perfect point cloud generated in <b>simulation,</b> the models cannot directly apply to the noisy point cloud in the real-world.To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem.</p></p class="citation"></blockquote><h3 id=820--204286-robot-body-schema-learning-from-full-body-exteroproprioception-sensors-shuo-jiang-et-al-2024>(8/20 | 204/286) Robot Body Schema Learning from Full-body Extero/Proprioception Sensors (Shuo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Jiang, Jinkun Zhang, Lawson Wong. (2024)<br><strong>Robot Body Schema Learning from Full-body Extero/Proprioception Sensors</strong><br><button class=copy-to-clipboard title="Robot Body Schema Learning from Full-body Extero/Proprioception Sensors" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18675v1.pdf filename=2402.18675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a robot, its body structure is an a-prior knowledge when it is designed. However, when such information is not available, can a robot recognize it by itself? In this paper, we aim to grant a robot such ability to learn its body structure from exteroception and proprioception data collected from on-body sensors. By a novel machine learning method, the robot can learn a binary Heterogeneous Dependency Matrix from its sensor readings. We showed such matrix is equivalent to a Heterogeneous out-tree structure which can uniquely represent the robot body topology. We explored the properties of such matrix and the out-tree, and proposed a remedy to fix them when they are contaminated by partial observability or data noise. We ran our algorithm on 6 different robots with different body structures in <b>simulation</b> and 1 real robot. Our algorithm correctly recognized their body structures with only on-body sensor readings but no topology prior knowledge.</p></p class="citation"></blockquote><h3 id=920--205286-dual-imu-state-estimation-for-relative-localization-of-two-mobile-agents-wenqian-lai-et-al-2024>(9/20 | 205/286) Dual-IMU State Estimation for Relative Localization of Two Mobile Agents (Wenqian Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqian Lai, Ruonan Guo, Kejian J. Wu. (2024)<br><strong>Dual-IMU State Estimation for Relative Localization of Two Mobile Agents</strong><br><button class=copy-to-clipboard title="Dual-IMU State Estimation for Relative Localization of Two Mobile Agents" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18394v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18394v2.pdf filename=2402.18394v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the problem of relative localization of two mobile agents. Specifically, we consider the Dual-IMU system, where each agent is equipped with one IMU, and employs relative pose observations between them. Previous works, however, typically assumed known ego motion and ignored biases of the IMUs. Instead, we study the most general case of unknown biases for both IMUs. Besides the derivation of dynamic model equations of the proposed system, we focus on the observability analysis, for the observability under general motion and the unobservable directions arising from various special motions. Through numerical <b>simulations,</b> we validate our key observability findings and examine their impact on the estimation accuracy and consistency. Finally, the system is implemented to achieve effective relative localization of an HMD with respect to a vehicle moving in the real world.</p></p class="citation"></blockquote><h3 id=1020--206286-fixture-calibration-with-guaranteed-bounds-from-a-few-correspondence-free-surface-points-rasmus-laurvig-haugaard-et-al-2024>(10/20 | 206/286) Fixture calibration with guaranteed bounds from a few correspondence-free surface points (Rasmus Laurvig Haugaard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rasmus Laurvig Haugaard, Yitaek Kim, Thorbjørn Mosekjær Iversen. (2024)<br><strong>Fixture calibration with guaranteed bounds from a few correspondence-free surface points</strong><br><button class=copy-to-clipboard title="Fixture calibration with guaranteed bounds from a few correspondence-free surface points" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18123v1.pdf filename=2402.18123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Calibration of fixtures in robotic work cells is essential but also time consuming and error-prone, and poor calibration can easily lead to wasted debugging time in downstream tasks. Contact-based calibration methods let the user measure points on the fixture&rsquo;s surface with a tool tip attached to the robot&rsquo;s end effector. Most methods require the user to manually annotate correspondences on the CAD model, however, this is error-prone and a cumbersome user experience. We propose a correspondence-free alternative: The user simply measures a few points from the fixture&rsquo;s surface, and our method provides a tight superset of the poses which could explain the measured points. This naturally detects ambiguities related to symmetry and uninformative points and conveys this uncertainty to the user. Perhaps more importantly, it provides guaranteed bounds on the pose. The computation of such bounds is made tractable by the use of a hierarchical grid on SE(3). Our method is evaluated both in <b>simulation</b> and on a real collaborative robot, showing great potential for easier and less error-prone fixture calibration.</p></p class="citation"></blockquote><h3 id=1120--207286-online-time-optimal-trajectory-generation-for-two-quadrotors-with-multi-waypoints-constraints-fangguo-zhao-et-al-2024>(11/20 | 207/286) Online Time-Optimal Trajectory Generation for Two Quadrotors with Multi-Waypoints Constraints (Fangguo Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangguo Zhao, Jiahao Mei, Jin Zhou, Jiming Chen, Shuo Li. (2024)<br><strong>Online Time-Optimal Trajectory Generation for Two Quadrotors with Multi-Waypoints Constraints</strong><br><button class=copy-to-clipboard title="Online Time-Optimal Trajectory Generation for Two Quadrotors with Multi-Waypoints Constraints" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18021v1.pdf filename=2402.18021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The autonomous quadrotor&rsquo;s flying speed has kept increasing in the past 5 years, especially in the field of autonomous drone racing. However, the majority of the research mainly focuses on the aggressive flight of a single quadrotor. In this letter, we propose a novel method called Pairwise Model Predictive Control (PMPC) that can guide two quadrotors online to fly through the waypoints with minimum time without collisions. The flight task is first modeled as a nonlinear optimization problem and then an efficient two-step mass point velocity search method is used to provide initial values and references to improve the solving efficiency so that the method can run online with a frequency of 50 Hz and can handle dynamic waypoints. The <b>simulation</b> and real-world experiments validate the feasibility of the proposed method and in the real-world experiments, the two quadrotors can achieve a top speed of 8.1m/s in a 6-waypoint racing track in a compact flying arena of 6m<em>4m</em>2m.</p></p class="citation"></blockquote><h3 id=1220--208286-a-multimodal-handover-failure-detection-dataset-and-baselines-santosh-thoduka-et-al-2024>(12/20 | 208/286) A Multimodal Handover Failure Detection Dataset and Baselines (Santosh Thoduka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santosh Thoduka, Nico Hochgeschwender, Juergen Gall, Paul G. Plöger. (2024)<br><strong>A Multimodal Handover Failure Detection Dataset and Baselines</strong><br><button class=copy-to-clipboard title="A Multimodal Handover Failure Detection Dataset and Baselines" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 16<br>Keywords: Convolutional Neural Network, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18319v1.pdf filename=2402.18319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An object handover between a robot and a human is a coordinated action which is prone to failure for reasons such as miscommunication, incorrect actions and unexpected object properties. Existing works on handover failure detection and prevention focus on preventing failures due to object slip or external disturbances. However, there is a lack of datasets and evaluation methods that consider unpreventable failures caused by the human participant. To address this deficit, we present the <b>multimodal</b> Handover Failure Detection dataset, which consists of failures induced by the human participant, such as ignoring the robot or not releasing the object. We also present two baseline methods for handover failure detection: (i) a video classification method using 3D <b>CNNs</b> and (ii) a temporal action segmentation approach which jointly classifies the human action, robot action and overall outcome of the action. The results show that video is an important modality, but using force-torque data and gripper position help improve failure detection and action segmentation accuracy.</p></p class="citation"></blockquote><h3 id=1320--209286-unifying-f1tenth-autonomous-racing-survey-methods-and-benchmarks-benjamin-david-evans-et-al-2024>(13/20 | 209/286) Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks (Benjamin David Evans et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin David Evans, Raphael Trumpp, Marco Caccamo, Hendrik Willem Jordaan, Herman Arnold Engelbrecht. (2024)<br><strong>Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks</strong><br><button class=copy-to-clipboard title="Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18558v1.pdf filename=2402.18558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The F1TENTH autonomous racing platform, consisting of 1:10 scale RC cars, has evolved into a leading research platform. The many publications and real-world competitions span many domains, from classical path planning to novel learning-based algorithms. Consequently, the field is wide and disjointed, hindering direct comparison of methods and making it difficult to assess the state-of-the-art. Therefore, we aim to unify the field by surveying current approaches, describing common methods and providing <b>benchmark</b> results to facilitate clear comparison and establish a baseline for future work. We survey current work in F1TENTH racing in the classical and learning categories, explaining the different solution approaches. We describe particle filter localisation, trajectory optimisation and tracking, model predictive contouring control (MPCC), follow-the-gap and end-to-end <b>reinforcement</b> <b>learning.</b> We provide an open-source evaluation of <b>benchmark</b> methods and investigate overlooked factors of control frequency and localisation accuracy for classical methods and reward signal and training map for learning methods. The evaluation shows that the optimisation and tracking method achieves the fastest lap times, followed by the MPCC planner. Finally, our work identifies and outlines the relevant research aspects to help motivate future work in the F1TENTH domain.</p></p class="citation"></blockquote><h3 id=1420--210286-a-probabilistic-motion-model-for-skid-steer-wheeled-mobile-robot-navigation-on-off-road-terrains-ananya-trivedi-et-al-2024>(14/20 | 210/286) A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains (Ananya Trivedi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ananya Trivedi, Mark Zolotas, Adeeb Abbas, Sarvesh Prajapati, Salah Bazzi, Taskın Padır. (2024)<br><strong>A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains</strong><br><button class=copy-to-clipboard title="A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18065v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18065v2.pdf filename=2402.18065v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using <b>Gaussian</b> <b>Process</b> Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a <b>benchmark</b> real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.</p></p class="citation"></blockquote><h3 id=1520--211286-the-grasp-reset-mechanism-an-automated-apparatus-for-conducting-grasping-trials-kyle-dufrene-et-al-2024>(15/20 | 211/286) The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials (Kyle DuFrene et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle DuFrene, Keegan Nave, Joshua Campbell, Ravi Balasubramanian, Cindy Grimm. (2024)<br><strong>The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials</strong><br><button class=copy-to-clipboard title="The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18650v1.pdf filename=2402.18650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no <b>human</b> <b>intervention.</b> We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial.</p></p class="citation"></blockquote><h3 id=1620--212286-human-centric-aware-uav-trajectory-planning-in-search-and-rescue-missions-employing-multi-objective-reinforcement-learning-with-ahp-and-similarity-based-experience-replay-mahya-ramezani-et-al-2024>(16/20 | 212/286) Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay (Mahya Ramezani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahya Ramezani, Jose Luis Sanchez-Lopez. (2024)<br><strong>Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay</strong><br><button class=copy-to-clipboard title="Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-HC, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18487v1.pdf filename=2402.18487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue (SAR) missions presents a promising avenue for enhancing operational efficiency and effectiveness. However, the success of these missions is not solely dependent on the technical capabilities of the drones but also on their acceptance and interaction with humans on the ground. This paper explores the effect of human-centric factor in UAV trajectory planning for SAR missions. We introduce a novel approach based on the <b>reinforcement</b> <b>learning</b> augmented with Analytic Hierarchy Process and novel similarity-based experience replay to optimize UAV trajectories, balancing operational objectives with human comfort and safety considerations. Additionally, through a comprehensive survey, we investigate the impact of gender cues and anthropomorphism in UAV design on public acceptance and trust, revealing significant implications for drone interaction strategies in SAR. Our contributions include (1) a <b>reinforcement</b> <b>learning</b> framework for UAV trajectory planning that dynamically integrates multi-objective considerations, (2) an analysis of human perceptions towards gendered and anthropomorphized drones in SAR contexts, and (3) the application of similarity-based experience replay for enhanced learning efficiency in complex SAR scenarios. The findings offer valuable insights into designing UAV systems that are not only technically proficient but also aligned with human-centric values.</p></p class="citation"></blockquote><h3 id=1720--213286-whole-body-humanoid-robot-locomotion-with-human-reference-qiang-zhang-et-al-2024>(17/20 | 213/286) Whole-body Humanoid Robot Locomotion with Human Reference (Qiang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Zhang, Peter Cui, David Yan, Jingkai Sun, Yiqun Duan, Arthur Zhang, Renjing Xu. (2024)<br><strong>Whole-body Humanoid Robot Locomotion with Human Reference</strong><br><button class=copy-to-clipboard title="Whole-body Humanoid Robot Locomotion with Human Reference" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18294v1.pdf filename=2402.18294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, humanoid robots have made significant advances in their ability to perform complex tasks due to the deployment of <b>Reinforcement</b> <b>Learning</b> (RL), however, the inherent complexity of humanoid robots, including the difficulty of planning complex reward functions and training entire complex systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, &lsquo;&lsquo;Adam&rsquo;&rsquo;, whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.</p></p class="citation"></blockquote><h3 id=1820--214286-generative-ai-for-unmanned-vehicle-swarms-challenges-applications-and-opportunities-guangyuan-liu-et-al-2024>(18/20 | 214/286) Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities (Guangyuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyuan Liu, Nguyen Van Huynh, Hongyang Du, Dinh Thai Hoang, Dusit Niyato, Kun Zhu, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim. (2024)<br><strong>Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities</strong><br><button class=copy-to-clipboard title="Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18062v1.pdf filename=2402.18062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With recent advances in artificial intelligence (AI) and robotics, unmanned vehicle swarms have received great attention from both academia and industry due to their potential to provide services that are difficult and dangerous to perform by humans. However, learning and coordinating movements and actions for a large number of unmanned vehicles in complex and dynamic environments introduce significant challenges to conventional AI methods. <b>Generative</b> <b>AI</b> (GAI), with its capabilities in complex data feature extraction, transformation, and enhancement, offers great potential in solving these challenges of unmanned vehicle swarms. For that, this paper aims to provide a comprehensive survey on applications, challenges, and opportunities of GAI in unmanned vehicle swarms. Specifically, we first present an overview of unmanned vehicles and unmanned vehicle swarms as well as their use cases and existing issues. Then, an in-depth background of various GAI techniques together with their capabilities in enhancing unmanned vehicle swarms are provided. After that, we present a comprehensive review on the applications and challenges of GAI in unmanned vehicle swarms with various insights and discussions. Finally, we highlight open issues of GAI in unmanned vehicle swarms and discuss potential research directions.</p></p class="citation"></blockquote><h3 id=1920--215286-generation-of-skill-specific-maps-from-graph-world-models-for-robotic-systems-koen-de-vos-et-al-2024>(19/20 | 215/286) Generation of skill-specific maps from graph world models for robotic systems (Koen de Vos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koen de Vos, Gijs van den Brandt, Jordy Senden, Pieter Pauwels, Rene van de Molengraft, Elena Torta. (2024)<br><strong>Generation of skill-specific maps from graph world models for robotic systems</strong><br><button class=copy-to-clipboard title="Generation of skill-specific maps from graph world models for robotic systems" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18174v1.pdf filename=2402.18174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increase in the availability of Building Information Models (BIM) and (semi-) automatic tools to generate BIM from point clouds, we propose a world model architecture and algorithms to allow the use of the semantic and geometric knowledge encoded within these models to generate maps for robot localization and navigation. When heterogeneous robots are deployed within an environment, maps obtained from classical SLAM approaches might not be shared between all agents within a team of robots, e.g. due to a mismatch in sensor type, or a difference in physical robot dimensions. Our approach extracts the 3D <b>geometry</b> and semantic description of building elements (e.g. material, element type, color) from BIM, and represents this knowledge in a <b>graph.</b> Based on queries on the <b>graph</b> and knowledge of the skills of the robot, we can generate skill-specific maps that can be used during the execution of localization or navigation tasks. The approach is validated with data from complex build environments and integrated into existing navigation frameworks.</p></p class="citation"></blockquote><h3 id=2020--216286-ukf-based-sensor-fusion-for-joint-torque-sensorless-humanoid-robots-ines-sorrentino-et-al-2024>(20/20 | 216/286) UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots (Ines Sorrentino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ines Sorrentino, Giulio Romualdi, Daniele Pucci. (2024)<br><strong>UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots</strong><br><button class=copy-to-clipboard title="UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18380v1.pdf filename=2402.18380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers <b>multimodal</b> measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the underlying robot sensor suit. To validate the approach, we show how the proposed sensor fusion can be integrated into a twolevel torque control architecture aiming at task-space torquecontrol. The performances of the proposed approach are shown through extensive tests on the new humanoid robot ergoCub, currently being developed at Istituto Italiano di Tecnologia. We also compare our strategy with the existing state-of-theart approach based on the recursive Newton-Euler algorithm. Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.</p></p class="citation"></blockquote><h2 id=csmm-3>cs.MM (3)</h2><h3 id=13--217286-balanced-similarity-with-auxiliary-prompts-towards-alleviating-text-to-image-retrieval-bias-for-clip-in-zero-shot-learning-hanyao-wang-et-al-2024>(1/3 | 217/286) Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning (Hanyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanyao Wang, Yibing Zhan, Liu Liu, Liang Ding, Jun Yu. (2024)<br><strong>Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning</strong><br><button class=copy-to-clipboard title="Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 50<br>Keywords: Foundation Model, Zero-shot, Text2image, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18400v1.pdf filename=2402.18400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CLIP has the ability to align texts and images and is nearly the most frequently used <b>foundation</b> <b>model</b> in cross-modal <b>zero-shot</b> <b>learning.</b> However, our experimental findings reveal that CLIP suffers from a bias in <b>text-to-image</b> retrieval, resulting in a decrease in CLIP&rsquo;s <b>zero-shot</b> <b>learning</b> performance. We analytically discover that the bias partly arises from the imbalanced range of similarity scores obtained by CLIP. Accordingly, we propose a Balanced Similarity with Auxiliary <b>Prompts</b> (BSAP) to mitigate the <b>text-to-image</b> retrieval bias of CLIP. Specifically, our BSAP designs auxiliary <b>prompts</b> for CLIP to calculate multiple similarity scores for the retrieval images and then normalizes the scores between each image and the given query text as well as our auxiliary <b>prompts</b> to obtain balanced similarity scores. The balanced similarity score of the given query text is used for the final retrieval. In addition, we attempt to adopt a hybrid similarity that combines our BSAP with the original similarity of CLIP to obtain a more robust outcome. Extensive experiments on two typical <b>zero-shot</b> <b>learning</b> tasks,i.e., Referring Expression Comprehension (REC) and Referring Image Segmentation (RIS), are conducted to demonstrate the effectiveness of our BSAP. Specifically, when using the val dataset of RefCOCO in REC, BSAP increases CLIP&rsquo;s performance by 20.6%.</p></p class="citation"></blockquote><h3 id=23--218286-multimodal-interaction-modeling-via-self-supervised-multi-task-learning-for-review-helpfulness-prediction-honglin-gong-et-al-2024>(2/3 | 218/286) Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction (HongLin Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>HongLin Gong, Mengzhao Jia, Liqiang Jing. (2024)<br><strong>Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction</strong><br><button class=copy-to-clipboard title="Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18107v1.pdf filename=2402.18107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In line with the latest research, the task of identifying helpful reviews from a vast pool of user-generated textual and visual data has become a prominent area of study. Effective modal representations are expected to possess two key attributes: consistency and differentiation. Current methods designed for <b>Multimodal</b> Review Helpfulness Prediction (MRHP) face limitations in capturing distinctive information due to their reliance on uniform <b>multimodal</b> annotation. The process of adding varied <b>multimodal</b> annotations is not only time-consuming but also labor-intensive. To tackle these challenges, we propose an auto-generated scheme based on multi-task learning to generate pseudo labels. This approach allows us to simultaneously train for the global <b>multimodal</b> interaction task and the separate cross-modal interaction subtasks, enabling us to learn and leverage both consistency and differentiation effectively. Subsequently, experimental results validate the effectiveness of pseudo labels, and our approach surpasses previous textual and <b>multimodal</b> baseline models on two widely accessible <b>benchmark</b> datasets, providing a solution to the MRHP problem.</p></p class="citation"></blockquote><h3 id=33--219286-characterizing-multimedia-information-environment-through-multi-modal-clustering-of-youtube-videos-niloofar-yousefi-et-al-2024>(3/3 | 219/286) Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos (Niloofar Yousefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niloofar Yousefi, Mainuddin Shaik, Nitin Agarwal. (2024)<br><strong>Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos</strong><br><button class=copy-to-clipboard title="Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-MM, cs.MM<br>Keyword Score: 6<br>Keywords: Clustering, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18702v1.pdf filename=2402.18702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to investigate the comprehensive characterization of information content in multimedia (videos), particularly on YouTube. The research presents a multi-method framework for characterizing multimedia content by <b>clustering</b> signals from various modalities, such as audio, video, and text. With a focus on South China Sea videos as a case study, this approach aims to enhance our understanding of online content, especially on YouTube. The dataset includes 160 videos, and our findings offer insights into content themes and patterns within different modalities of a video based on clusters. Text modality analysis revealed topical themes related to geopolitical countries, strategies, and global security, while video and audio modality analysis identified distinct patterns of signals related to diverse sets of videos, including news analysis/reporting, educational content, and interviews. Furthermore, our findings uncover instances of content repurposing within video clusters, which were identified using the barcode technique and audio similarity assessments. These findings indicate potential content amplification techniques. In conclusion, this study uniquely enhances our current understanding of multimedia content information based on modality <b>clustering</b> techniques.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--220286-prospect-personalized-recommendation-on-large-language-model-based-agent-platform-jizhi-zhang-et-al-2024>(1/4 | 220/286) Prospect Personalized Recommendation on Large Language Model-based Agent Platform (Jizhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jizhi Zhang, Keqin Bao, Wenjie Wang, Yang Zhang, Wentao Shi, Wanhong Xu, Fuli Feng, Tat-Seng Chua. (2024)<br><strong>Prospect Personalized Recommendation on Large Language Model-based Agent Platform</strong><br><button class=copy-to-clipboard title="Prospect Personalized Recommendation on Large Language Model-based Agent Platform" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Recommendation, Recommender System, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18240v1.pdf filename=2402.18240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The new kind of Agent-oriented information system, exemplified by <b>GPTs,</b> urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)-based</b> Agents, such as interactivity. In this work, we envisage the prospect of the <b>recommender</b> <b>system</b> on <b>LLM-based</b> Agent platforms and introduce a novel <b>recommendation</b> paradigm called Rec4Agentverse, comprised of Agent Items and Agent <b>Recommender.</b> <b>Rec4Agentverse</b> emphasizes the collaboration between Agent Items and Agent <b>Recommender,</b> <b>thereby</b> promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent <b>Recommender,</b> <b>and</b> the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.</p></p class="citation"></blockquote><h3 id=24--221286-sequence-level-semantic-representation-fusion-for-recommender-systems-lanling-xu-et-al-2024>(2/4 | 221/286) Sequence-level Semantic Representation Fusion for Recommender Systems (Lanling Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lanling Xu, Zhen Tian, Bingqian Li, Junjie Zhang, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao. (2024)<br><strong>Sequence-level Semantic Representation Fusion for Recommender Systems</strong><br><button class=copy-to-clipboard title="Sequence-level Semantic Representation Fusion for Recommender Systems" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Convolution, Recommendation, Recommender System, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18166v1.pdf filename=2402.18166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of <b>recommender</b> <b>systems,</b> there is increasing side information that can be employed to improve the <b>recommendation</b> performance. Specially, we focus on the utilization of the associated \emph{textual data} of items (eg product title) and study how <b>text</b> <b>features</b> can be effectively fused with ID features in sequential <b>recommendation.</b> However, there exists distinct data characteristics for the two kinds of item features, making a direct fusion method (eg adding <b>text</b> <b>and</b> ID embeddings as item representation) become less effective. To address this issue, we propose a novel {\ul \emph{Te}}xt-I{\ul \emph{D}} semantic fusion approach for sequential {\ul \emph{Rec}}ommendation, namely \textbf{\our}. The core idea of our approach is to conduct a sequence-level semantic fusion approach by better integrating global contexts. The key strategy lies in that we transform the <b>text</b> <b>embeddings</b> and ID embeddings by Fourier Transform from \emph{time domain} to \emph{frequency domain}. In the frequency domain, the global sequential characteristics of the original sequences are inherently aggregated into the transformed representations, so that we can employ simple multiplicative operations to effectively fuse the two kinds of item features. Our fusion approach can be proved to have the same effects of contextual <b>convolution,</b> so as to achieving sequence-level semantic fusion. In order to further improve the fusion performance, we propose to enhance the discriminability of the <b>text</b> <b>embeddings</b> from the <b>text</b> <b>encoder,</b> by adaptively injecting positional information via a mixture-of-experts~(MoE) modulation method. Our implementation is available at this repository: \textcolor{magenta}{\url{https://github.com/RUCAIBox/TedRec}}.</p></p class="citation"></blockquote><h3 id=34--222286-a-categorization-of-complexity-classes-for-information-retrieval-and-synthesis-using-natural-logic-gregory-coppola-2024>(3/4 | 222/286) A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic (Gregory Coppola, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregory Coppola. (2024)<br><strong>A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic</strong><br><button class=copy-to-clipboard title="A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Information Retrieval, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18566v1.pdf filename=2402.18566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the emergent <b>reasoning</b> abilities of <b>large</b> <b>language</b> <b>models,</b> <b>information</b> <b>retrieval</b> is becoming more complex. Rather than just retrieve a document, modern <b>information</b> <b>retrieval</b> systems advertise that they can synthesize an answer based on potentially many different documents, conflicting data sources, and using <b>reasoning.</b> But, different kinds of questions have different answers, and different answers have different complexities. In this paper, we introduce a novel framework for analyzing the complexity of a question answer based on the natural deduction calculus as presented in Prawitz (1965). Our framework is novel both in that no one to our knowledge has used this logic as a basis for complexity classes, and also in that no other existing complexity classes to these have been delineated using any analogous methods either. We identify three decidable fragments in particular called the forward, query and planning fragments, and we compare this to what would be needed to do proofs for the complete first-order calculus, for which theorem-proving is long known to be undecidable.</p></p class="citation"></blockquote><h3 id=44--223286-corpus-steered-query-expansion-with-large-language-models-yibin-lei-et-al-2024>(4/4 | 223/286) Corpus-Steered Query Expansion with Large Language Models (Yibin Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yibin Lei, Yu Cao, Tianyi Zhou, Tao Shen, Andrew Yates. (2024)<br><strong>Corpus-Steered Query Expansion with Large Language Models</strong><br><button class=copy-to-clipboard title="Corpus-Steered Query Expansion with Large Language Models" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18031v1.pdf filename=2402.18031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies demonstrate that query expansions generated by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can considerably enhance <b>information</b> <b>retrieval</b> systems by generating hypothetical documents that answer the queries as expansions. However, challenges arise from misalignments between the expansions and the retrieval corpus, resulting in issues like hallucinations and outdated <b>information</b> <b>due</b> to the limited intrinsic knowledge of <b>LLMs.</b> Inspired by Pseudo Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to promote the incorporation of knowledge embedded within the corpus. CSQE utilizes the relevance assessing capability of <b>LLMs</b> to systematically identify pivotal sentences in the initially-retrieved documents. These corpus-originated texts are subsequently used to expand the query together with <b>LLM-knowledge</b> empowered expansions, improving the relevance prediction between the query and the target documents. Extensive experiments reveal that CSQE exhibits strong performance without necessitating any training, especially with queries for which <b>LLMs</b> lack knowledge.</p></p class="citation"></blockquote><h2 id=physicsapp-ph-1>physics.app-ph (1)</h2><h3 id=11--224286-physics-informed-machine-learning-for-seismic-response-prediction-of-nonlinear-steel-moment-resisting-frame-structures-r-bailey-bond-et-al-2024>(1/1 | 224/286) Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures (R. Bailey Bond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Bailey Bond, Pu Ren, Jerome F. Hajjar, Hao Sun. (2024)<br><strong>Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures</strong><br><button class=copy-to-clipboard title="Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.app-ph<br>Categories: cs-LG, physics-app-ph, physics.app-ph<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17992v1.pdf filename=2402.17992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing interest in utilizing machine learning (ML) methods for structural metamodeling due to the substantial computational cost of traditional numerical <b>simulations.</b> The existing data-driven strategies show potential limitations to the model robustness and interpretability as well as the dependency of rich data. To address these challenges, this paper presents a novel physics-informed machine learning (PiML) method, which incorporates scientific principles and physical laws into deep neural networks for modeling seismic responses of nonlinear structures. The basic concept is to constrain the solution space of the ML model within known physical bounds. This is made possible with three main features, namely, model order reduction, a <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> networks, and Newton&rsquo;s second law (e.g., the equation of motion). Model order reduction is essential for handling structural systems with inherent redundancy and enhancing model efficiency. The <b>LSTM</b> network captures temporal dependencies, enabling accurate prediction of time series responses. The equation of motion is manipulated to learn system nonlinearities and confines the solution space within physically interpretable results. These features enable model training with relatively sparse data and offer benefits in terms of accuracy, interpretability, and robustness. Furthermore, a dataset of seismically designed archetype ductile planar steel moment resistant frames under horizontal seismic loading, available in the DesignSafe-CI Database, is considered for evaluation of the proposed method. The resulting metamodel is capable of handling more complex data compared to existing physics-guided <b>LSTM</b> models and outperforms other non-physics data-driven neural networks.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--225286-implementing-online-reinforcement-learning-with-clustering-neural-networks-james-e-smith-2024>(1/1 | 225/286) Implementing Online Reinforcement Learning with Clustering Neural Networks (James E. Smith, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James E. Smith. (2024)<br><strong>Implementing Online Reinforcement Learning with Clustering Neural Networks</strong><br><button class=copy-to-clipboard title="Implementing Online Reinforcement Learning with Clustering Neural Networks" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: 68T07, I-2, cs-NE, cs.NE<br>Keyword Score: 43<br>Keywords: Clustering, Online Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18472v1.pdf filename=2402.18472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An agent employing <b>reinforcement</b> <b>learning</b> takes inputs (state variables) from an environment and performs actions that affect the environment in order to achieve some objective. Rewards (positive or negative) guide the agent toward improved future actions. This paper builds on prior <b>clustering</b> neural network research by constructing an agent with biologically plausible neo-Hebbian three-factor synaptic learning rules, with a reward signal as the third factor (in addition to pre- and post-synaptic spikes). The classic cart-pole problem (balancing an inverted pendulum) is used as a running example throughout the exposition. <b>Simulation</b> results demonstrate the efficacy of the approach, and the proposed method may eventually serve as a low-level component of a more general method.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--226286-hemagraph-breaking-barriers-in-hematologic-single-cell-classification-with-graph-attention-lorenzo-bini-et-al-2024>(1/1 | 226/286) HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention (Lorenzo Bini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Bini, Fatemeh Nassajian Mojarrad, Thomas Matthes, Stéphane Marchand-Maillet. (2024)<br><strong>HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention</strong><br><button class=copy-to-clipboard title="HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-CB, q-bio-QM, q-bio.QM<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18611v1.pdf filename=2402.18611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of hematologic cell populations classification, the intricate patterns within flow cytometry data necessitate advanced analytical tools. This paper presents &lsquo;HemaGraph&rsquo;, a novel framework based on <b>Graph</b> <b>Attention</b> <b>Networks</b> <b>(GATs)</b> for single-cell multi-class classification of hematological cells from flow cytometry data. Harnessing the power of <b>GATs,</b> our method captures subtle cell relationships, offering highly accurate patient profiling. Based on evaluation of data from 30 patients, HemaGraph demonstrates classification performance across five different cell classes, outperforming traditional methodologies and state-of-the-art methods. Moreover, the uniqueness of this framework lies in the training and testing phase of HemaGraph, where it has been applied for extremely large <b>graphs,</b> <b>containing</b> <b>up</b> to hundreds of thousands of nodes and two million edges, to detect low frequency cell populations (e.g. 0.01% for one population), with accuracies reaching 98%. Our findings underscore the potential of HemaGraph in improving hematoligic multi-class classification, paving the way for patient-personalized interventions. To the best of our knowledge, this is the first effort to use <b>GATs,</b> and <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> in general, to classify cell populations from single-cell flow cytometry data. We envision applying this method to single-cell data from larger cohort of patients and on other hematologic diseases.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--227286-ela-exploited-level-augmentation-for-offline-learning-in-zero-sum-games-shiqi-lei-et-al-2024>(1/1 | 227/286) ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games (Shiqi Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Lei, Kanghoon Lee, Linjing Li, Jinkyoo Park, Jiachen Li. (2024)<br><strong>ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games</strong><br><button class=copy-to-clipboard title="ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-GT, cs-LG, cs-MA, cs.GT<br>Keyword Score: 40<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18617v1.pdf filename=2402.18617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>learning</b> <b>has</b> become widely used due to its ability to derive effective policies from <b>offline</b> <b>datasets</b> <b>gathered</b> by expert demonstrators without interacting with the environment directly. Recent research has explored various ways to enhance <b>offline</b> <b>learning</b> <b>efficiency</b> by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset. However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent. In this study, we introduce a novel approach that uses <b>unsupervised</b> <b>learning</b> techniques to estimate the exploited level of each trajectory from the <b>offline</b> <b>dataset</b> <b>of</b> zero-sum games made by diverse demonstrators. Subsequently, we incorporate the estimated exploited level into the <b>offline</b> <b>learning</b> <b>to</b> maximize the influence of the dominant strategy. Our method enables interpretable exploited level estimation in multiple zero-sum games and effectively identifies dominant strategy data. Also, our exploited level augmented <b>offline</b> <b>learning</b> <b>significantly</b> enhances the original <b>offline</b> <b>learning</b> <b>algorithms</b> including imitation learning and <b>offline</b> <b>reinforcement</b> <b>learning</b> for zero-sum games.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--228286-take-it-leave-it-or-fix-it-measuring-productivity-and-trust-in-human-ai-collaboration-crystal-qian-et-al-2024>(1/3 | 228/286) Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration (Crystal Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Crystal Qian, James Wexler. (2024)<br><strong>Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration</strong><br><button class=copy-to-clipboard title="Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Generative AI, Recommendation, Bard, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18498v1.pdf filename=2402.18498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although recent developments in <b>generative</b> <b>AI</b> have greatly enhanced the capabilities of conversational agents such as Google&rsquo;s <b>Bard</b> or OpenAI&rsquo;s <b>ChatGPT,</b> it&rsquo;s unclear whether the usage of these agents aids users across various contexts. To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to <b>Bard.</b> Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended &ldquo;solve&rdquo; questions vs. definitive &ldquo;search&rdquo; questions), and measurement type (demonstrated vs. self-reported). Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on &ldquo;solve&rdquo;-type questions when using the AI. We discuss common behaviors, design <b>recommendations,</b> and impact considerations to improve collaborations with conversational AI.</p></p class="citation"></blockquote><h3 id=23--229286-hearhere-mitigating-echo-chambers-in-news-consumption-through-an-ai-based-web-system-youngseung-jeon-et-al-2024>(2/3 | 229/286) HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System (Youngseung Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngseung Jeon, Jaehoon Kim, Sohyun Park, Yunyong Ko, Seongeun Ryu, Sang-Wook Kim, Kyungsik Han. (2024)<br><strong>HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System</strong><br><button class=copy-to-clipboard title="HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 13<br>Keywords: Graph, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18222v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18222v2.pdf filename=2402.18222v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considerable efforts are currently underway to mitigate the negative impacts of echo chambers, such as increased susceptibility to <b>fake</b> <b>news</b> and resistance towards accepting scientific evidence. Prior research has presented the development of computer systems that support the consumption of news information from diverse political perspectives to mitigate the echo chamber effect. However, existing studies still lack the ability to effectively support the key processes of news information consumption and quantitatively identify a political stance towards the information. In this paper, we present HearHere, an AI-based web system designed to help users accommodate information and opinions from diverse perspectives. HearHere facilitates the key processes of news information consumption through two visualizations. Visualization 1 provides political news with quantitative political stance information, derived from our <b>graph-based</b> political classification model, and users can experience diverse perspectives (Hear). Visualization 2 allows users to express their opinions on specific political issues in a comment form and observe the position of their own opinions relative to pro-liberal and pro-conservative comments presented on a map interface (Here). Through a user study with 94 participants, we demonstrate the feasibility of HearHere in supporting the consumption of information from various perspectives. Our findings highlight the importance of providing political stance information and quantifying users&rsquo; political status as a means to mitigate political polarization. In addition, we propose design implications for system development, including the consideration of demographics such as political interest and providing users with initiatives.</p></p class="citation"></blockquote><h3 id=33--230286-dynamic-explanation-selection-towards-successful-user-decision-support-with-explainable-ai-yosuke-fukuchi-et-al-2024>(3/3 | 230/286) Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI (Yosuke Fukuchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yosuke Fukuchi, Seiji Yamada. (2024)<br><strong>Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI</strong><br><button class=copy-to-clipboard title="Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18016v1.pdf filename=2402.18016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of how to select explanations for XAI <b>(Explainable</b> <b>AI)-based</b> Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions. As the development of XAI made various explanations available, we believe that IDSSs can be greatly improved if they can strategically select explanations that guide users to better decisions. This paper proposes X-Selector, a method for dynamically selecting explanations. X-Selector aims to guide users to better decisions by predicting the impact of different combinations of explanations on user decisions. We compared X-Selector&rsquo;s performance with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support). The results suggest the potential of X-Selector to guide users to recommended decisions and improve the performance when AI accuracy is high and a challenge when it is low.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--231286-investigation-of-adapter-for-automatic-speech-recognition-in-noisy-environment-hao-shi-et-al-2024>(1/2 | 231/286) Investigation of Adapter for Automatic Speech Recognition in Noisy Environment (Hao Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Shi, Tatsuya Kawahara. (2024)<br><strong>Investigation of Adapter for Automatic Speech Recognition in Noisy Environment</strong><br><button class=copy-to-clipboard title="Investigation of Adapter for Automatic Speech Recognition in Noisy Environment" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Transfer Learning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18275v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18275v2.pdf filename=2402.18275v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting an <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> system to unseen noise environments is crucial. Integrating adapters into neural networks has emerged as a potent technique for <b>transfer</b> <b>learning.</b> This study thoroughly investigates adapter-based <b>ASR</b> adaptation in noisy environments. We conducted experiments using the CHiME&ndash;4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. The simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training is still useful for adapter training. Furthermore, integrating adapters into <b>speech</b> <b>enhancement-based</b> <b>ASR</b> systems yields substantial improvements.</p></p class="citation"></blockquote><h3 id=22--232286-convdtw-acs-audio-segmentation-for-track-type-detection-during-car-manufacturing-álvaro-lópez-chilet-et-al-2024>(2/2 | 232/286) ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing (Álvaro López-Chilet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Álvaro López-Chilet, Zhaoyi Liu, Jon Ander Gómez, Carlos Alvarez, Marivi Alonso Ortiz, Andres Orejuela Mesa, David Newton, Friedrich Wolf-Monheim, Sam Michiels, Danny Hughes. (2024)<br><strong>ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing</strong><br><button class=copy-to-clipboard title="ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18204v1.pdf filename=2402.18204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a method for Acoustic Constrained Segmentation (ACS) in audio recordings of vehicles driven through a production test track, delimiting the boundaries of surface types in the track. ACS is a variant of classical acoustic segmentation where the sequence of labels is known, contiguous and invariable, which is especially useful in this work as the test track has a standard configuration of surface types. The proposed ConvDTW-ACS method utilizes a <b>Convolutional</b> <b>Neural</b> <b>Network</b> for classifying overlapping image chunks extracted from the full audio spectrogram. Then, our custom Dynamic Time Warping algorithm aligns the sequence of predicted probabilities to the sequence of surface types in the track, from which timestamps of the surface type boundaries can be extracted. The method was evaluated on a real-world dataset collected from the Ford Manufacturing Plant in Valencia (Spain), achieving a mean error of 166 milliseconds when delimiting, within the audio, the boundaries of the surfaces in the track. The results demonstrate the effectiveness of the proposed method in accurately segmenting different surface types, which could enable the development of more specialized AI systems to improve the quality inspection process.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--233286-human-simulacra-a-step-toward-the-personification-of-large-language-models-qiuejie-xie-et-al-2024>(1/1 | 233/286) Human Simulacra: A Step toward the Personification of Large Language Models (Qiuejie Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuejie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Yuejie Zhang, Rui Feng, Shang Gao. (2024)<br><strong>Human Simulacra: A Step toward the Personification of Large Language Models</strong><br><button class=copy-to-clipboard title="Human Simulacra: A Step toward the Personification of Large Language Models" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18180v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18180v1.pdf filename=2402.18180v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging <b>LLMs</b> to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for <b>large</b> <b>language</b> <b>models</b> personification, including a strategy for constructing virtual characters&rsquo; life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human <b>simulations</b> from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--234286-understanding-random-forests-and-overfitting-a-visualization-and-simulation-study-lasai-barreñada-et-al-2024>(1/1 | 234/286) Understanding random forests and overfitting: a visualization and simulation study (Lasai Barreñada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lasai Barreñada, Paula Dhiman, Dirk Timmerman, Anne-Laure Boulesteix, Ben Van Calster. (2024)<br><strong>Understanding random forests and overfitting: a visualization and simulation study</strong><br><button class=copy-to-clipboard title="Understanding random forests and overfitting: a visualization and simulation study" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-CY, cs-LG, stat-ME, stat.ME<br>Keyword Score: 33<br>Keywords: Recommendation, Sample Size, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18612v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18612v1.pdf filename=2402.18612v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random forests have become popular for clinical risk prediction modelling. In a case study on predicting ovarian malignancy, we observed training c-statistics close to 1. Although this suggests overfitting, performance was competitive on test data. We aimed to understand the behaviour of random forests by (1) visualizing data space in three real world case studies and (2) a <b>simulation</b> study. For the case studies, risk estimates were visualised using heatmaps in a 2-dimensional subspace. The <b>simulation</b> study included 48 logistic data generating mechanisms (DGM), varying the predictor distribution, the number of predictors, the correlation between predictors, the true c-statistic and the strength of true predictors. For each DGM, 1000 training datasets of size 200 or 4000 were simulated and RF models trained with minimum node size 2 or 20 using ranger package, resulting in 192 scenarios in total. The visualizations suggested that the model learned spikes of probability around events in the training set. A cluster of events created a bigger peak, isolated events local peaks. In the <b>simulation</b> study, median training c-statistics were between 0.97 and 1 unless there were 4 or 16 binary predictors with minimum node size 20. Median test c-statistics were higher with higher events per variable, higher minimum node size, and binary predictors. Median training slopes were always above 1, and were not correlated with median test slopes across scenarios (correlation -0.11). Median test slopes were higher with higher true c-statistic, higher minimum node size, and higher <b>sample</b> <b>size.</b> Random forests learn local probability peaks that often yield near perfect training c-statistics without strongly affecting c-statistics on test data. When the aim is probability estimation, the <b>simulation</b> results go against the common <b>recommendation</b> to use fully grown trees in random forest models.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--235286-reinforcement-learning-and-graph-neural-networks-for-probabilistic-risk-assessment-joachim-grimstad-et-al-2024>(1/3 | 235/286) Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment (Joachim Grimstad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joachim Grimstad, Andrey Morozov. (2024)<br><strong>Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment</strong><br><button class=copy-to-clipboard title="Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18246v1.pdf filename=2402.18246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new approach to the solution of Probabilistic Risk Assessment (PRA) models using the combination of <b>Reinforcement</b> <b>Learning</b> (RL) and <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> The paper introduces and demonstrates the concept using one of the most popular PRA models - Fault Trees. This paper&rsquo;s original idea is to apply RL algorithms to solve a PRA model represented with a <b>graph</b> <b>model.</b> <b>Given</b> enough training data, or through RL, such an approach helps train generic PRA solvers that can optimize and partially substitute classical PRA solvers that are based on existing formal methods. Such an approach helps to solve the problem of the fast-growing complexity of PRA models of modern technical systems.</p></p class="citation"></blockquote><h3 id=23--236286-timer-based-coverage-control-for-mobile-sensors-federico-m-zegers-et-al-2024>(2/3 | 236/286) Timer-Based Coverage Control for Mobile Sensors (Federico M. Zegers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico M. Zegers, Sean Phillips, Gregory P. Hicks. (2024)<br><strong>Timer-Based Coverage Control for Mobile Sensors</strong><br><button class=copy-to-clipboard title="Timer-Based Coverage Control for Mobile Sensors" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18744v1.pdf filename=2402.18744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work studies the coverage control problem over a static, bounded, and convex workspace and develops a hybrid extension of the <b>continuous-time</b> <b>Lloyd</b> algorithm. Each agent in a multi-agent system (MAS) is equipped with a timer that generates intermittent sampling events, which may occur asynchronously between agents. At each sampling event, the corresponding agents update their controllers, which are otherwise held constant. These controllers are shown to drive the MAS into a neighborhood of the configurations corresponding to a centroidal Voronoi tessellation, that is, a local minimizer of the standard locational cost. The result is a distributed control strategy that leverages intermittent and asynchronous position measurements to disperse the agents within the workspace. The combination of <b>continuous-time</b> <b>dynamics</b> with intermittently updated control inputs is modeled as a hybrid system. The coverage control objective is posed as a set stabilization problem for hybrid systems, where an invariance based convergence analysis yields sufficient conditions that ensure all maximal solutions of the hybrid system asymptotically converge to a desired set. A brief <b>simulation</b> example is included to showcase the result.</p></p class="citation"></blockquote><h3 id=33--237286-exergetic-port-hamiltonian-systems-for-multibody-dynamics-markus-lohmayer-et-al-2024>(3/3 | 237/286) Exergetic Port-Hamiltonian Systems for Multibody Dynamics (Markus Lohmayer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Lohmayer, Giuseppe Capobianco, Sigrid Leyendecker. (2024)<br><strong>Exergetic Port-Hamiltonian Systems for Multibody Dynamics</strong><br><button class=copy-to-clipboard title="Exergetic Port-Hamiltonian Systems for Multibody Dynamics" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-MP, math-ph, physics-class-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18095v1.pdf filename=2402.18095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multibody dynamics <b>simulation</b> plays an important role in various fields, including mechanical engineering, robotics, and biomechanics. Setting up computational models however becomes increasingly challenging as systems grow in size and complexity. Especially the consistent combination of models across different physical domains usually demands a lot of attention. This motivates us to study formal languages for compositional modeling of multiphysical systems. This article shows how multibody systems, or more precisely assemblies of rigid bodies connected by lower kinematic pairs, fit into the framework of Exergetic Port-Hamiltonian Systems (EPHS). This approach is based on the hierarchical decomposition of systems into their ultimately primitive components, using a simple graphical syntax. Thereby, cognitive load can be reduced and communication is facilitated, even with non-experts. Moreover, the encapsulation and reuse of subsystems promotes efficient model development and management. In contrast to established modeling languages such as Modelica, the primitive components of EPHS are not defined by arbitrary equations. Instead, there are four kinds of components, each defined by a particular geometric structure with a clear physical interpretation. This higher-level approach could make the process of building and maintaining large-scale models simpler and also safer.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--238286-a-priori-uncertainty-quantification-of-reacting-turbulence-closure-models-using-bayesian-neural-networks-graham-pash-et-al-2024>(1/1 | 238/286) A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks (Graham Pash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Graham Pash, Malik Hassanaly, Shashank Yellapantula. (2024)<br><strong>A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks</strong><br><button class=copy-to-clipboard title="A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-data-an, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18729v1.pdf filename=2402.18729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy <b>simulation</b> (LES), vast amounts of data available from direct numerical <b>simulation</b> (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and <b>out-of-distribution</b> regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the incorporation of <b>out-of-distribution</b> information in a BNN. The efficacy of the model is demonstrated by a priori evaluation on a dataset consisting of a variety of flame conditions and fuels.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--239286-bluebell-an-alliance-of-relational-lifting-and-independence-for-probabilistic-reasoning-jialu-bao-et-al-2024>(1/1 | 239/286) Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning (Jialu Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialu Bao, Emanuele D&rsquo;Osualdo, Azadeh Farzan. (2024)<br><strong>Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning</strong><br><button class=copy-to-clipboard title="Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-PL, cs.LO<br>Keyword Score: 30<br>Keywords: Probabilistic Reasoning, Reasoning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18708v1.pdf filename=2402.18708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Bluebell, a program logic for <b>reasoning</b> about <b>probabilistic</b> <b>programs</b> where unary and relational styles of <b>reasoning</b> come together to create new <b>reasoning</b> tools. Unary-style <b>reasoning</b> is very expressive and is powered by foundational mechanisms to reason about <b>probabilistic</b> <b>behaviour</b> like independence and conditioning. The relational style of <b>reasoning,</b> on the other hand, naturally shines when the properties of interest compare the behaviour of similar programs (e.g. when proving <b>differential</b> <b>privacy)</b> managing to avoid having to characterize the output distributions of the individual programs. So far, the two styles of <b>reasoning</b> have largely remained separate in the many program logics designed for the deductive verification of <b>probabilistic</b> <b>programs.</b> In Bluebell, we unify these styles of <b>reasoning</b> through the introduction of a new modality called &ldquo;joint conditioning&rdquo; that can encode and illuminate the rich interaction between conditional independence and relational liftings; the two powerhouses from the two styles of <b>reasoning.</b></p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--240286-unveiling-novel-insights-into-kirchhoff-migration-for-effective-object-detection-using-experimental-fresnel-dataset-won-kwang-park-2024>(1/2 | 240/286) Unveiling novel insights into Kirchhoff migration for effective object detection using experimental Fresnel dataset (Won-Kwang Park, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Won-Kwang Park. (2024)<br><strong>Unveiling novel insights into Kirchhoff migration for effective object detection using experimental Fresnel dataset</strong><br><button class=copy-to-clipboard title="Unveiling novel insights into Kirchhoff migration for effective object detection using experimental Fresnel dataset" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 78A46, cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Object Detection, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18322v1.pdf filename=2402.18322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the applicability of Kirchhoff migration (KM) for a fast identification of unknown <b>objects</b> <b>in</b> a real-world limited-aperture inverse scattering problem. To demonstrate the theoretical basis for the applicability including unique determination of <b>objects,</b> <b>the</b> imaging function of the KM was formulated using a uniformly convergent infinite series of Bessel functions of integer order of the first kind based on the integral equation formula for the scattered field. Numerical <b>simulations</b> performed using the experimental Fresnel dataset are exhibited to achieve the theoretical results.</p></p class="citation"></blockquote><h3 id=22--241286-tensor-network-space-time-spectral-collocation-method-for-time-dependent-convection-diffusion-reaction-equations-dibyendu-adak-et-al-2024>(2/2 | 241/286) Tensor Network Space-Time Spectral Collocation Method for Time Dependent Convection-Diffusion-Reaction Equations (Dibyendu Adak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dibyendu Adak, Duc P. Truong, Gianmarco Manzini, Kim Ø. Rasmussen, Boian S. Alexandrov. (2024)<br><strong>Tensor Network Space-Time Spectral Collocation Method for Time Dependent Convection-Diffusion-Reaction Equations</strong><br><button class=copy-to-clipboard title="Tensor Network Space-Time Spectral Collocation Method for Time Dependent Convection-Diffusion-Reaction Equations" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 15A69, 35Q79, 65M70, cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18073v1.pdf filename=2402.18073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging tensor network techniques for solutions of Partial Differential Equations (PDEs), known for their ability to break the curse of dimensionality, deliver new mathematical methods for ultrafast numerical solutions of high-dimensional problems. Here, we introduce a Tensor Train (TT) Chebyshev spectral collocation method, in both space and time, for solution of the time dependent convection-diffusion-reaction (CDR) equation with inhomogeneous boundary conditions, in Cartesian <b>geometry.</b> Previous methods for numerical solution of time dependent PDEs often use finite difference for time, and a spectral scheme for the spatial dimensions, which leads to slow linear convergence. Spectral collocation space-time methods show exponential convergence, however, for realistic problems they need to solve large four-dimensional systems. We overcome this difficulty by using a TT approach as its complexity only grows linearly with the number of dimensions. We show that our TT space-time Chebyshev spectral collocation method converges exponentially, when the solution of the CDR is smooth, and demonstrate that it leads to very high compression of linear operators from terabytes to kilobytes in TT-format, and tens of thousands times speedup when compared to full grid space-time spectral method. These advantages allow us to obtain the solutions at much higher resolutions.</p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--242286-simple-but-effective-rethinking-the-ability-of-deep-learning-in-fnirs-to-exclude-abnormal-input-zhihao-cao-2024>(1/3 | 242/286) Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input (Zhihao Cao, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Cao. (2024)<br><strong>Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input</strong><br><button class=copy-to-clipboard title="Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18112v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18112v1.pdf filename=2402.18112v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for monitoring brain activity. To better understand the brain, researchers often use deep learning to address the classification challenges of fNIRS data. Our study shows that while current networks in fNIRS are highly accurate for predictions within their training distribution, they falter at identifying and excluding abnormal data which is <b>out-of-distribution,</b> affecting their reliability. We propose integrating metric learning and <b>supervised</b> methods into fNIRS research to improve networks capability in identifying and excluding <b>out-of-distribution</b> outliers. This method is simple yet effective. In our experiments, it significantly enhances the performance of various networks in fNIRS, particularly <b>transformer-based</b> one, which shows the great improvement in reliability. We will make our experiment data available on GitHub.</p></p class="citation"></blockquote><h3 id=23--243286-joint-activity-delay-detection-and-channel-estimation-for-asynchronous-massive-random-access-a-free-probability-theory-approach-xinyu-bian-et-al-2024>(2/3 | 243/286) Joint Activity-Delay Detection and Channel Estimation for Asynchronous Massive Random Access: A Free Probability Theory Approach (Xinyu Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Bian, Yuyi Mao, Jun Zhang. (2024)<br><strong>Joint Activity-Delay Detection and Channel Estimation for Asynchronous Massive Random Access: A Free Probability Theory Approach</strong><br><button class=copy-to-clipboard title="Joint Activity-Delay Detection and Channel Estimation for Asynchronous Massive Random Access: A Free Probability Theory Approach" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17996v1.pdf filename=2402.17996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grant-free random access (RA) has been recognized as a promising solution to support massive connectivity due to the removal of the uplink grant request procedures. While most endeavours assume perfect synchronization among users and the base station, this paper investigates asynchronous grant-free massive RA, and develop efficient algorithms for joint user activity detection, synchronization delay detection, and channel estimation. Considering the sparsity on user activity, we formulate a sparse signal recovery problem and propose to utilize the framework of orthogonal approximate message passing (OAMP) to deal with the non-independent and identically distributed (i.i.d.) Gaussian pilot matrices caused by the synchronization delays. In particular, an OAMP-based algorithm is developed to fully harness the common sparsity among received pilot signals from multiple base station antennas. To reduce the computational complexity, we further propose a free probability AMP (FPAMP)-based algorithm, which exploits the rectangular free cumulants to make the cost-effective AMP framework compatible to general pilot matrices. <b>Simulation</b> results demonstrate that the two proposed algorithms outperform various baselines, and the FPAMP-based algorithm reduces 40% of the computations while maintaining comparable detection/estimation accuracy with the OAMP-based algorithm.</p></p class="citation"></blockquote><h3 id=33--244286-distributed-intelligent-integrated-sensing-and-communications-the-6g-disac-approach-emilio-calvanese-strinati-et-al-2024>(3/3 | 244/286) Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach (Emilio Calvanese Strinati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emilio Calvanese Strinati, George C. Alexandropoulos, Madhusudan Giyyarpuram, Philippe Sehier, Sami Mekki, Vincenzo Sciancalepore, Maximilian Stark, Mohamed Sana, Benoit Denis, Maurizio Crozzoli, Navid Amani, Placido Mursia, Raffaele D Errico, Mauro Boldi, Francesca Costanzo, Francois Rivet, Henk Wymeerschx. (2024)<br><strong>Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach</strong><br><button class=copy-to-clipboard title="Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18271v1.pdf filename=2402.18271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the concept of Distributed Intelligent integrated Sensing and Communications (DISAC), which expands the capabilities of Integrated Sensing and Communications (ISAC) towards distributed architectures. Additionally, the DISAC framework integrates novel waveform design with new semantic and goal-oriented communication paradigms, enabling ISAC technologies to transition from traditional data fusion to the semantic composition of diverse sensed and shared information. This progress facilitates large-scale, energy-efficient support for high-precision spatial-temporal processing, optimizing ISAC resource utilization, and enabling effective <b>multi-modal</b> sensing performance. Addressing key challenges such as efficient data management and connect-compute resource utilization, 6G- DISAC stands to revolutionize applications in diverse sectors including transportation, healthcare, and industrial automation. Our study encapsulates the project vision, methodologies, and potential impact, marking a significant stride towards a more connected and intelligent world.</p></p class="citation"></blockquote><h2 id=q-fintr-1>q-fin.TR (1)</h2><h3 id=11--245286-a-multimodal-foundation-agent-for-financial-trading-tool-augmented-diversified-and-generalist-wentao-zhang-et-al-2024>(1/1 | 245/286) A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist (Wentao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An. (2024)<br><strong>A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist</strong><br><button class=copy-to-clipboard title="A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.TR<br>Categories: cs-AI, q-fin-TR, q-fin.TR<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18485v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18485v2.pdf filename=2402.18485v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Financial trading is a crucial component of the markets, informed by a <b>multimodal</b> information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and <b>reinforcement</b> <b>learning</b> are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of <b>multimodal</b> data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a <b>multimodal</b> foundational agent with tool augmentation for financial trading. FinAgent&rsquo;s market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent&rsquo;s ability to learn from historical data and improve decision-making processes. The agent&rsquo;s emphasis on <b>reasoning</b> for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced <b>multimodal</b> foundation agent designed for financial trading tasks.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--246286-a-higher-order-lens-for-social-systems-giulia-preti-et-al-2024>(1/1 | 246/286) A Higher-Order Lens for Social Systems (Giulia Preti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulia Preti, Adriano Fazzone, Giovanni Petri, Gianmarco De Francisci Morales. (2024)<br><strong>A Higher-Order Lens for Social Systems</strong><br><button class=copy-to-clipboard title="A Higher-Order Lens for Social Systems" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-data-an<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18470v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18470v2.pdf filename=2402.18470v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional <b>graphs.</b> This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM). These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor. We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles. To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics. First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily. Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between <b>simulations</b> and theoretical predictions can be explained by considering higher-order joint degree distributions. Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes. This work pioneers the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--247286-deep-confident-steps-to-new-pockets-strategies-for-docking-generalization-gabriele-corso-et-al-2024>(1/1 | 247/286) Deep Confident Steps to New Pockets: Strategies for Docking Generalization (Gabriele Corso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Corso, Arthur Deng, Benjamin Fry, Nicholas Polizzi, Regina Barzilay, Tommi Jaakkola. (2024)<br><strong>Deep Confident Steps to New Pockets: Strategies for Docking Generalization</strong><br><button class=copy-to-clipboard title="Deep Confident Steps to New Pockets: Strategies for Docking Generalization" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18396v1.pdf filename=2402.18396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing <b>benchmarks,</b> however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new <b>benchmark</b> based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the <b>scaling</b> <b>laws</b> of ML-based docking and show that, by <b>scaling</b> <b>data</b> and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across <b>benchmarks.</b> Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between <b>diffusion</b> <b>and</b> confidence models and exploits the multi-resolution generation process of <b>diffusion</b> <b>models.</b> We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.</p></p class="citation"></blockquote><h2 id=csar-5>cs.AR (5)</h2><h3 id=15--248286-accelerating-computer-architecture-simulation-through-machine-learning-wajid-ali-et-al-2024>(1/5 | 248/286) Accelerating Computer Architecture Simulation through Machine Learning (Wajid Ali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wajid Ali, Ayaz Akram. (2024)<br><strong>Accelerating Computer Architecture Simulation through Machine Learning</strong><br><button class=copy-to-clipboard title="Accelerating Computer Architecture Simulation through Machine Learning" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-LG, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18746v1.pdf filename=2402.18746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents our approach to accelerate computer architecture <b>simulation</b> by leveraging machine learning techniques. Traditional computer architecture <b>simulations</b> are time-consuming, making it challenging to explore different design choices efficiently. Our proposed model utilizes a combination of application features and micro-architectural features to predict the performance of an application. These features are derived from <b>simulations</b> of a small portion of the application. We demonstrate the effectiveness of our approach by building and evaluating a machine learning model that offers significant speedup in architectural exploration. This model demonstrates the ability to predict IPC values for the testing data with a root mean square error of less than 0.1.</p></p class="citation"></blockquote><h3 id=25--249286-pimsim-nn-an-isa-based-simulation-framework-for-processing-in-memory-accelerators-xinyu-wang-et-al-2024>(2/5 | 249/286) PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators (Xinyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Wang, Xiaotian Sun, Yinhe Han, Xiaoming Chen. (2024)<br><strong>PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators</strong><br><button class=copy-to-clipboard title="PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18089v1.pdf filename=2402.18089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing-in-memory (PIM) has shown extraordinary potential in accelerating neural networks. To evaluate the performance of PIM accelerators, we present an ISA-based <b>simulation</b> framework including a dedicated ISA targeting neural networks running on PIM architectures, a compiler, and a cycleaccurate configurable simulator. Compared with prior works, this work decouples software algorithms and hardware architectures through the proposed ISA, providing a more convenient way to evaluate the effectiveness of software/hardware optimizations. The simulator adopts an event-driven <b>simulation</b> approach and has better support for hardware parallelism. The framework is open-sourced at <a href=https://github.com/wangxy-2000/pimsim-nn>https://github.com/wangxy-2000/pimsim-nn</a>.</p></p class="citation"></blockquote><h3 id=35--250286-energy-aware-heterogeneous-federated-learning-via-approximate-systolic-dnn-accelerators-kilian-pfeiffer-et-al-2024>(3/5 | 250/286) Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators (Kilian Pfeiffer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kilian Pfeiffer, Konstantinos Balaskas, Kostas Siozios, Jörg Henkel. (2024)<br><strong>Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators</strong><br><button class=copy-to-clipboard title="Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18569v1.pdf filename=2402.18569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), devices that participate in the training usually have heterogeneous resources, i.e., energy availability. In current deployments of FL, devices that do not fulfill certain hardware requirements are often dropped from the collaborative training. However, dropping devices in FL can degrade training accuracy and introduce bias or unfairness. Several works have tacked this problem on an algorithmic level, e.g., by letting constrained devices train a subset of the server neural network (NN) model. However, it has been observed that these techniques are not effective w.r.t. accuracy. Importantly, they make simplistic assumptions about devices&rsquo; resources via indirect metrics such as multiply accumulate (MAC) operations or peak memory requirements. In this work, for the first time, we consider on-device accelerator design for FL with heterogeneous devices. We utilize compressed arithmetic formats and approximate computing, targeting to satisfy limited energy budgets. Using a hardware-aware energy model, we observe that, contrary to the state of the art&rsquo;s moderate energy reduction, our technique allows for lowering the energy requirements (by 4x) while maintaining higher accuracy.</p></p class="citation"></blockquote><h3 id=45--251286-pimsyn-synthesizing-processing-in-memory-cnn-accelerators-wanqian-li-et-al-2024>(4/5 | 251/286) PIMSYN: Synthesizing Processing-in-memory CNN Accelerators (Wanqian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanqian Li, Xiaotian Sun, Xinyu Wang, Lei Wang, Yinhe Han, Xiaoming Chen. (2024)<br><strong>PIMSYN: Synthesizing Processing-in-memory CNN Accelerators</strong><br><button class=copy-to-clipboard title="PIMSYN: Synthesizing Processing-in-memory CNN Accelerators" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18114v1.pdf filename=2402.18114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Processing-in-memory architectures have been regarded as a promising solution for <b>CNN</b> acceleration. Existing PIM accelerator designs rely heavily on the experience of experts and require significant manual design overhead. Manual design cannot effectively optimize and explore architecture implementations. In this work, we develop an automatic framework PIMSYN for synthesizing PIM-based <b>CNN</b> accelerators, which greatly facilitates architecture design and helps generate energyefficient accelerators. PIMSYN can automatically transform <b>CNN</b> applications into execution workflows and hardware construction of PIM accelerators. To systematically optimize the architecture, we embed an architectural exploration flow into the synthesis framework, providing a more comprehensive design space. Experiments demonstrate that PIMSYN improves the power efficiency by several times compared with existing works. PIMSYN can be obtained from <a href=https://github.com/lixixi-jook/PIMSYN-NN>https://github.com/lixixi-jook/PIMSYN-NN</a>.</p></p class="citation"></blockquote><h3 id=55--252286-a-hierarchical-dataflow-driven-heterogeneous-architecture-for-wireless-baseband-processing-limin-jiang-et-al-2024>(5/5 | 252/286) A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing (Limin Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Limin Jiang, Yi Shi, Haiqin Hu, Qingyu Deng, Siyi Xu, Yintao Liu, Feng Yuan, Si Wang, Yihao Shen, Fangfang Ye, Shan Cao, Zhiyuan Jiang. (2024)<br><strong>A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing</strong><br><button class=copy-to-clipboard title="A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR, eess-SP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18070v1.pdf filename=2402.18070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading. Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP. Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency. To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP. A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner. We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources. Experiment results demonstrate that our prototype achieves $2\times$ and $2.3\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP <b>benchmarks.</b> Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--253286-integrated-sensing-and-communication-meets-smart-propagation-engineering-opportunities-and-challenges-kaitao-meng-et-al-2024>(1/2 | 253/286) Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges (Kaitao Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaitao Meng, Christos Masouros, Kai-Kit Wong, Athina P. Petropulu, Lajos Hanzo. (2024)<br><strong>Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges</strong><br><button class=copy-to-clipboard title="Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18683v1.pdf filename=2402.18683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both smart propagation engineering as well as integrated sensing and communication (ISAC) constitute promising candidates for next-generation (NG) mobile networks. We provide a synergistic view of these technologies, and explore their mutual benefits. First, moving beyond just intelligent surfaces, we provide a holistic view of the engineering aspects of smart propagation environments. By delving into the fundamental characteristics of intelligent surfaces, fluid antennas, and unmanned aerial vehicles, we reveal that more efficient control of the pathloss and fading can be achieved, thus facilitating intrinsic integration and mutual assistance between sensing and communication functionalities. In turn, with the exploitation of the sensing capabilities of ISAC to orchestrate the efficient configuration of radio environments, both the computational effort and signaling overheads can be reduced. We present indicative <b>simulation</b> results, which verify that cooperative smart propagation environment design significantly enhances the ISAC performance. Finally, some promising directions are outlined for combining ISAC with smart propagation engineering.</p></p class="citation"></blockquote><h3 id=22--254286-precoding-for-multi-cell-isac-from-coordinated-beamforming-to-coordinated-multipoint-and-bi-static-sensing-nithin-babu-et-al-2024>(2/2 | 254/286) Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing (Nithin Babu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nithin Babu, Christos Masouros, Constantinos B. Papadias, Yonina C. Eldar. (2024)<br><strong>Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing</strong><br><button class=copy-to-clipboard title="Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18387v1.pdf filename=2402.18387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a framework for designing robust precoders for a multi-input single-output (MISO) system that performs integrated sensing and communication (ISAC) across multiple cells and users. We use Cramer-Rao-Bound (CRB) to measure the sensing performance and derive its expressions for two multi-cell scenarios, namely coordinated beamforming (CBF) and coordinated multi-point (CoMP). In the CBF scheme, a BS shares channel state information (CSI) and estimates target parameters using monostatic sensing. In contrast, a BS in the CoMP scheme shares the CSI and data, allowing bistatic sensing through inter-cell reflection. We consider both block-level (BL) and symbol-level (SL) precoding schemes for both the multi-cell scenarios that are robust to channel state estimation errors. The formulated optimization problems to minimize the CRB in estimating the parameters of a target and maximize the minimum communication signal-to-interference-plus-noise-ratio (SINR) while satisfying a given total transmit power budget are non-convex. We tackle the non-convexity using a combination of semidefinite relaxation (SDR) and alternating optimization (AO) techniques. <b>Simulations</b> suggest that neglecting the inter-cell reflection and communication links degrades the performance of an ISAC system. The CoMP scenario employing SL precoding performs the best, whereas the BL precoding applied in the CBF scenario produces relatively high estimation error for a given minimum SINR value.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=11--255286-a-non-intrusive-machine-learning-framework-for-debiasing-long-time-coarse-resolution-climate-simulations-and-quantifying-rare-events-statistics-benedikt-barthel-sorensen-et-al-2024>(1/1 | 255/286) A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics (Benedikt Barthel Sorensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Barthel Sorensen, Alexis Charalampopoulos, Shixuan Zhang, Bryce Harrop, Ruby Leung, Themistoklis Sapsis. (2024)<br><strong>A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics</strong><br><button class=copy-to-clipboard title="A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-LG, physics-ao-ph, physics.ao-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18484v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18484v1.pdf filename=2402.18484v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the rapidly changing climate, the frequency and severity of extreme weather is expected to increase over the coming decades. As fully-resolved climate <b>simulations</b> remain computationally intractable, policy makers must rely on coarse-models to quantify risk for extremes. However, coarse models suffer from inherent bias due to the ignored &ldquo;sub-grid&rdquo; scales. We propose a framework to non-intrusively debias coarse-resolution climate predictions using neural-network (NN) correction operators. Previous efforts have attempted to train such operators using loss functions that match statistics. However, this approach falls short with events that have longer return period than that of the training data, since the reference statistics have not converged. Here, the scope is to formulate a learning method that allows for correction of dynamics and quantification of extreme events with longer return period than the training data. The key obstacle is the chaotic nature of the underlying dynamics. To overcome this challenge, we introduce a dynamical systems approach where the correction operator is trained using reference data and a coarse model <b>simulation</b> nudged towards that reference. The method is demonstrated on debiasing an under-resolved quasi-geostrophic model and the Energy Exascale Earth System Model (E3SM). For the former, our method enables the quantification of events that have return period two orders longer than the training data. For the latter, when trained on 8 years of ERA5 data, our approach is able to correct the coarse E3SM output to closely reflect the 36-year ERA5 statistics for all prognostic variables and significantly reduce their spatial biases.</p></p class="citation"></blockquote><h2 id=cset-1>cs.ET (1)</h2><h3 id=11--256286-neuromorphic-event-driven-semantic-communication-in-microgrids-xiaoguang-diao-et-al-2024>(1/1 | 256/286) Neuromorphic Event-Driven Semantic Communication in Microgrids (Xiaoguang Diao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoguang Diao, Yubo Song, Subham Sahoo, Yuan Li. (2024)<br><strong>Neuromorphic Event-Driven Semantic Communication in Microgrids</strong><br><button class=copy-to-clipboard title="Neuromorphic Event-Driven Semantic Communication in Microgrids" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.ET<br>Categories: cs-AI, cs-ET, cs-NE, cs-SY, cs.ET, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18390v1.pdf filename=2402.18390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synergies between advanced communications, computing and artificial intelligence are unraveling new directions of coordinated operation and resiliency in microgrids. On one hand, coordination among sources is facilitated by distributed, privacy-minded processing at multiple locations, whereas on the other hand, it also creates exogenous data arrival paths for adversaries that can lead to cyber-physical attacks amongst other reliability issues in the communication layer. This long-standing problem necessitates new intrinsic ways of exchanging information between converters through power lines to optimize the system&rsquo;s control performance. Going beyond the existing power and data co-transfer technologies that are limited by efficiency and scalability concerns, this paper proposes neuromorphic learning to implant communicative features using spiking neural networks (SNNs) at each node, which is trained collaboratively in an online manner simply using the power exchanges between the nodes. As opposed to the conventional neuromorphic sensors that operate with spiking signals, we employ an event-driven selective process to collect sparse data for training of SNNs. Finally, its multi-fold effectiveness and reliable performance is validated under <b>simulation</b> conditions with different microgrid topologies and components to establish a new direction in the sense-actuate-compute cycle for power electronic dominated grids and microgrids.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--257286-utilization-of-reconfigurable-intelligent-surfaces-with-context-information-use-cases-łukasz-kułacz-2024>(1/2 | 257/286) Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases (Łukasz Kułacz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Łukasz Kułacz. (2024)<br><strong>Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases</strong><br><button class=copy-to-clipboard title="Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18224v1.pdf filename=2402.18224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In terms of complex radio environments especially in dense urban areas, a very interesting topic is considered - the utilization of reconfigurable intelligent surfaces. Basically, based on simple controls of the angle of reflection of the signal from the surface, it is possible to achieve different effects in a radio communication system. Maximizing or minimizing the received power at specific locations near the reflecting surface is the most important effect. Thanks to this, it is possible to: receive a signal in a place where it was not possible, detect spectrum occupancy in a place where the sensor could not make a correct detection, or minimize interference in a specific receiver. In this paper, all three concepts are presented, and, using a simple ray tracing <b>simulation,</b> the potential profit in each scenario is shown. In addition, a scenario was analyzed in which several of the aforementioned situations are combined.</p></p class="citation"></blockquote><h3 id=22--258286-hyperfednet-communication-efficient-personalized-federated-learning-via-hypernetwork-xingyun-chen-et-al-2024>(2/2 | 258/286) HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork (Xingyun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyun Chen, Yan Huang, Zhenzhen Xie, Junjie Pang. (2024)<br><strong>HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork</strong><br><button class=copy-to-clipboard title="HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18445v1.pdf filename=2402.18445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are still many challenges in <b>Federated</b> <b>Learning</b> (FL). First, during the model update process, the model parameters on the local user need to be sent to the server for aggregation. This involves the consumption of network bandwidth, especially when the number of users participating in FL is large. High communication costs may limit the application of FL in certain scenarios. Secondly, since users participating in FL usually have different data distributions, this heterogeneity of data may lead to poor model performance or even failure to converge. Third, privacy and security issues are also challenges that need to be addressed in FL. There is still a risk of information leakage during model aggregation. Malicious users may obtain sensitive information by analyzing communications during model updates or aggregation processes. To address these challenges, we propose HyperFedNet (HFN), an innovative approach that leverages hypernetwork. HFN introduces a paradigm shift in transmission aggregation within FL. Unlike traditional FL methods that transmit a large number of parameters from the main network, HFN reduces the communication burden and improves security by transmitting a compact set of hypernetwork parameters. After the parameters of the hypernetwork are deployed locally to the user, the local database features quantified by the embedding vector can be used as input, and parameters can be dynamically generated for the FL main network through user forward propagation. HFN efficiently reduces communication costs while improving accuracy. Extensive experimentation demonstrates that HFN outperforms traditional FL methods significantly. By seamlessly integrating this concept into the conventional FL algorithm, we achieve even more impressive results compared to the original approach.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--259286-lemur-log-parsing-with-entropy-sampling-and-chain-of-thought-merging-hongcheng-guo-et-al-2024>(1/2 | 259/286) Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging (Hongcheng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongcheng Guo, Wei Zhang, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, Bo Zhang. (2024)<br><strong>Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging</strong><br><button class=copy-to-clipboard title="Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18205v1.pdf filename=2402.18205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> <b>LLMs</b> exhibit exceptional semantic comprehension, deftly distinguishing between parameters and invariant tokens. We have conducted experiments on <b>large-scale</b> <b>public</b> <b>datasets.</b> Extensive evaluation demonstrates that Lemur achieves the state-of-the-art performance and impressive efficiency.</p></p class="citation"></blockquote><h3 id=22--260286-potentials-of-green-coding----findings-and-recommendations-for-industry-education-and-science----extended-paper-dennis-junger-et-al-2024>(2/2 | 260/286) Potentials of Green Coding &ndash; Findings and Recommendations for Industry, Education and Science &ndash; Extended Paper (Dennis Junger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Junger, Max Westing, Christopher P. Freitag, Achim Guldner, Konstantin Mittelbach, Kira Obergöker, Sebastian Weber, Stefan Naumann, Volker Wohlgemuth. (2024)<br><strong>Potentials of Green Coding &ndash; Findings and Recommendations for Industry, Education and Science &ndash; Extended Paper</strong><br><button class=copy-to-clipboard title="Potentials of Green Coding -- Findings and Recommendations for Industry, Education and Science -- Extended Paper" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18227v1.pdf filename=2402.18227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Progressing digitalization and increasing demand and use of software cause rises in energy- and resource consumption from information and communication technologies (ICT). This raises the issue of sustainability in ICT, which increasingly includes the sustainability of the software products themselves and the art of creating sustainable software. To this end, we conducted an analysis to gather and present existing literature on three research questions relating to the production of ecologically sustainable software (&ldquo;Green Coding&rdquo;) and to provide orientation for stakeholders approaching the subject. We compile the approaches to Green Coding and Green Software Engineering (GSE) that have been published since 2010. Furthermore, we considered ways to integrate the findings into existing industrial processes and higher education curricula to influence future development in an environmentally friendly way.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=17--261286-a-lightweight-low-light-image-enhancement-network-via-channel-prior-and-gamma-correction-shyang-en-weng-et-al-2024>(1/7 | 261/286) A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction (Shyang-En Weng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shyang-En Weng, Shaou-Gang Miaou, Ricky Christanto. (2024)<br><strong>A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction</strong><br><button class=copy-to-clipboard title="A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18147v1.pdf filename=2402.18147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human vision relies heavily on available ambient light to perceive objects. Low-light scenes pose two distinct challenges: information loss due to insufficient illumination and undesirable brightness shifts. Low-light image enhancement (LLIE) refers to image enhancement technology tailored to handle this scenario. We introduce CPGA-Net, an innovative LLIE network that combines dark/bright channel priors and gamma correction via deep learning and integrates features inspired by the Atmospheric Scattering Model and the Retinex Theory. This approach combines the use of traditional and deep learning methodologies, designed within a simple yet efficient architectural framework that focuses on essential feature extraction. The resulting CPGA-Net is a lightweight network with only 0.025 million parameters and 0.030 seconds for inference time, yet it achieves superior performance over existing LLIE methods on both objective and subjective evaluation criteria. Furthermore, we utilized <b>knowledge</b> <b>distillation</b> with explainable factors and proposed an efficient version that achieves 0.018 million parameters and 0.006 seconds for inference time. The proposed approaches inject new solution ideas into LLIE, providing practical applications in challenging low-light scenarios.</p></p class="citation"></blockquote><h3 id=27--262286-passive-snapshot-coded-aperture-dual-pixel-rgb-d-imaging-bhargav-ghanekar-et-al-2024>(2/7 | 262/286) Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging (Bhargav Ghanekar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhargav Ghanekar, Salman Siddique Khan, Vivek Boominathan, Pranav Sharma, Shreyas Singh, Kaushik Mitra, Ashok Veeraraghavan. (2024)<br><strong>Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging</strong><br><button class=copy-to-clipboard title="Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18102v1.pdf filename=2402.18102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Passive, compact, single-shot 3D sensing is useful in many application areas such as microscopy, medical imaging, surgical navigation, and autonomous driving where form factor, time, and power constraints can exist. Obtaining RGB-D scene information over a short imaging distance, in an ultra-compact form factor, and in a passive, snapshot manner is challenging. Dual-pixel (DP) sensors are a potential solution to achieve the same. DP sensors collect light rays from two different halves of the lens in two interleaved pixel arrays, thus capturing two slightly different views of the scene, like a stereo camera system. However, imaging with a DP sensor implies that the defocus blur size is directly proportional to the disparity seen between the views. This creates a trade-off between disparity estimation vs. deblurring accuracy. To improve this trade-off effect, we propose CADS (Coded Aperture Dual-Pixel Sensing), in which we use a coded aperture in the imaging lens along with a DP sensor. In our approach, we jointly learn an optimal coded pattern and the reconstruction algorithm in an end-to-end optimization setting. Our resulting CADS imaging system demonstrates improvement of $>$1.5dB PSNR in all-in-focus (AIF) estimates and 5-6% in depth estimation quality over naive DP sensing for a wide range of aperture settings. Furthermore, we build the proposed CADS prototypes for DSLR photography settings and in an endoscope and a dermoscope form factor. Our novel coded dual-pixel sensing approach demonstrates accurate RGB-D reconstruction results in <b>simulations</b> and real-world experiments in a passive, snapshot, and compact manner.</p></p class="citation"></blockquote><h3 id=37--263286-mambamir-an-arbitrary-masked-mamba-for-joint-medical-image-reconstruction-and-uncertainty-estimation-jiahao-huang-et-al-2024>(3/7 | 263/286) MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation (Jiahao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Huang, Liutao Yang, Fanwen Wang, Yinzhe Wu, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang. (2024)<br><strong>MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation</strong><br><button class=copy-to-clipboard title="MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 15<br>Keywords: Generative Adversarial Network, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18451v1.pdf filename=2402.18451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent Mamba model has shown remarkable adaptability for visual <b>representation</b> <b>learning,</b> including in medical imaging tasks. This study introduces MambaMIR, a Mamba-based model for medical image reconstruction, as well as its <b>Generative</b> <b>Adversarial</b> <b>Network-based</b> variant, MambaMIR-GAN. Our proposed MambaMIR inherits several advantages, such as linear complexity, global receptive fields, and dynamic weights, from the original Mamba model. The innovated arbitrary-mask mechanism effectively adapt Mamba to our image reconstruction task, providing randomness for subsequent Monte Carlo-based uncertainty estimation. Experiments conducted on various medical image reconstruction tasks, including fast MRI and SVCT, which cover anatomical regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR and MambaMIR-GAN achieve comparable or superior reconstruction results relative to state-of-the-art methods. Additionally, the estimated uncertainty maps offer further insights into the reliability of the reconstruction quality. The code is publicly available at <a href=https://github.com/ayanglab/MambaMIR>https://github.com/ayanglab/MambaMIR</a>.</p></p class="citation"></blockquote><h3 id=47--264286-nerv-an-enhanced-implicit-neural-video-representation-ahmed-ghorbel-et-al-2024>(4/7 | 264/286) NERV++: An Enhanced Implicit Neural Video Representation (Ahmed Ghorbel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Ghorbel, Wassim Hamidouche, Luce Morin. (2024)<br><strong>NERV++: An Enhanced Implicit Neural Video Representation</strong><br><button class=copy-to-clipboard title="NERV++: An Enhanced Implicit Neural Video Representation" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18305v1.pdf filename=2402.18305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural fields, also known as implicit neural representations (INRs), have shown a remarkable capability of representing, generating, and manipulating various data types, allowing for continuous data reconstruction at a low memory footprint. Though promising, INRs applied to video compression still need to improve their rate-distortion performance by a large margin, and require a huge number of parameters and long training iterations to capture high-frequency details, limiting their wider applicability. Resolving this problem remains a quite challenging task, which would make INRs more accessible in compression tasks. We take a step towards resolving these shortcomings by introducing neural representations for videos NeRV++, an enhanced implicit neural video representation, as more straightforward yet effective enhancement over the original NeRV decoder architecture, featuring separable conv2d residual blocks (SCRBs) that sandwiches the upsampling block (UB), and a bilinear interpolation skip layer for improved feature representation. NeRV++ allows videos to be directly represented as a function approximated by a neural network, and significantly enhance the representation capacity beyond current INR-based video codecs. We evaluate our method on UVG, MCL JVC, and Bunny datasets, achieving competitive results for video compression with INRs. This achievement narrows the gap to <b>autoencoder-based</b> video coding, marking a significant stride in INR-based video compression research.</p></p class="citation"></blockquote><h3 id=57--265286-boosting-neural-representations-for-videos-with-a-conditional-decoder-xinjie-zhang-et-al-2024>(5/7 | 265/286) Boosting Neural Representations for Videos with a Conditional Decoder (Xinjie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang, Hongwei Qin, Jun Zhang. (2024)<br><strong>Boosting Neural Representations for Videos with a Conditional Decoder</strong><br><button class=copy-to-clipboard title="Boosting Neural Representations for Videos with a Conditional Decoder" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Reconstruction Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18152v1.pdf filename=2402.18152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representations (INRs) have emerged as a promising approach for video storage and processing, showing remarkable versatility across various video tasks. However, existing methods often fail to fully leverage their representation capabilities, primarily due to inadequate alignment of intermediate features during target frame decoding. This paper introduces a universal boosting framework for current implicit video representation approaches. Specifically, we utilize a conditional decoder with a temporal-aware affine transform module, which uses the frame index as a prior condition to effectively align intermediate features with target frames. Besides, we introduce a sinusoidal NeRV-like block to generate diverse intermediate features and achieve a more balanced parameter distribution, thereby enhancing the model&rsquo;s capacity. With a high-frequency information-preserving <b>reconstruction</b> <b>loss,</b> our approach successfully boosts multiple baseline INRs in the <b>reconstruction</b> <b>quality</b> and convergence speed for video regression, and exhibits superior inpainting and interpolation results. Further, we integrate a consistent entropy minimization technique and develop video codecs based on these boosted INRs. Experiments on the UVG dataset confirm that our enhanced codecs significantly outperform baseline INRs and offer competitive rate-distortion performance compared to traditional and learning-based codecs.</p></p class="citation"></blockquote><h3 id=67--266286-improvement-of-audiovisual-quality-estimation-using-a-nonlinear-autoregressive-exogenous-neural-network-and-bitstream-parameters-koffi-kossi-et-al-2024>(6/7 | 266/286) Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters (Koffi Kossi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koffi Kossi, Stephane Coulombe, Christian Desrosiers, Ghyslain Gagnon. (2024)<br><strong>Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters</strong><br><button class=copy-to-clipboard title="Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-SD, eess-AS, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18056v1.pdf filename=2402.18056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing demand for audiovisual services, telecom service providers and application developers are compelled to ensure that their services provide the best possible user experience. Particularly, services such as videoconferencing are very sensitive to network conditions. Therefore, their performance should be monitored in real time in order to adjust parameters to any network perturbation. In this paper, we developed a parametric model for estimating the perceived audiovisual quality in videoconference services. Our model is developed with the nonlinear autoregressive exogenous (NARX) <b>recurrent</b> <b>neural</b> <b>network</b> and estimates the perceived quality in terms of mean opinion score (MOS). We validate our model using the publicly available INRS bitstream audiovisual quality dataset. This dataset contains bitstream parameters such as loss per frame, bit rate and video duration. We compare the proposed model against state-of-the-art methods based on machine learning and show our model to outperform these methods in terms of mean square error (MSE=0.150) and Pearson correlation coefficient (R=0.931)</p></p class="citation"></blockquote><h3 id=77--267286-prediction-of-recurrence-free-survival-of-head-and-neck-cancer-using-petct-radiomics-and-clinical-information-mona-furukawa-et-al-2024>(7/7 | 267/286) Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information (Mona Furukawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mona Furukawa, Daniel R. McGowan, Bartłomiej W. Papież. (2024)<br><strong>Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information</strong><br><button class=copy-to-clipboard title="Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18417v1.pdf filename=2402.18417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The 5-year survival rate of Head and Neck Cancer (HNC) has not improved over the past decade and one common cause of treatment failure is recurrence. In this paper, we built Cox proportional hazard (CoxPH) models that predict the recurrence free survival (RFS) of oropharyngeal HNC patients. Our models utilise both clinical information and <b>multimodal</b> radiomics features extracted from tumour regions in Computed Tomography (CT) and Positron Emission Tomography (PET). Furthermore, we were one of the first studies to explore the impact of segmentation accuracy on the predictive power of the extracted radiomics features, through under- and over-segmentation study. Our models were trained using the HEad and neCK TumOR (HECKTOR) challenge data, and the best performing model achieved a concordance index (C-index) of 0.74 for the model utilising clinical information and <b>multimodal</b> CT and PET radiomics features, which compares favourably with the model that only used clinical information (C-index of 0.67). Our under- and over-segmentation study confirms that segmentation accuracy affects radiomics extraction, however, it affects PET and CT differently.</p></p class="citation"></blockquote><h2 id=q-finrm-1>q-fin.RM (1)</h2><h3 id=11--268286-modeling-and-analysis-of-crypto-backed-over-collateralized-stable-derivatives-in-defi-zhenbang-feng-et-al-2024>(1/1 | 268/286) Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi (Zhenbang Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenbang Feng, Hardhik Mohanty, Bhaskar Krishnamachari. (2024)<br><strong>Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi</strong><br><button class=copy-to-clipboard title="Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.RM<br>Categories: cs-CR, q-fin-RM, q-fin.RM<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18119v1.pdf filename=2402.18119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In decentralized finance (DeFi), stablecoins like DAI are designed to offer a stable value amidst the fluctuating nature of cryptocurrencies. We examine the class of crypto-backed stable derivatives, with a focus on mechanisms for price stabilization, which is exemplified by the well-known stablecoin DAI from MakerDAO. For simplicity, we focus on a single-collateral setting. We introduce a belief parameter to the <b>simulation</b> model of DAI in a previous work (DAISIM), reflecting market sentiments about the value and stability of DAI, and show that it better matches the expected behavior when this parameter is set to a sufficiently high value. We also propose a simple mathematical model of DAI price to explain its stability and dependency on ETH price. Finally, we analyze possible risk factors associated with these stable derivatives to provide valuable insights for stakeholders in the DeFi ecosystem.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--269286-eccbo-an-inherently-safe-bayesian-optimization-with-embedded-constraint-control-for-real-time-optimization-dinesh-krishnamoorthy-2024>(1/1 | 269/286) ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization (Dinesh Krishnamoorthy, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dinesh Krishnamoorthy. (2024)<br><strong>ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization</strong><br><button class=copy-to-clipboard title="ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18415v1.pdf filename=2402.18415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a model-free real-time optimization (RTO) framework based on unconstrained Bayesian optimization with embedded constraint control. The main contribution lies in demonstrating how this approach simplifies the <b>black-box</b> <b>optimization</b> problem while ensuring &ldquo;always-feasible&rdquo; setpoints, addressing a critical challenge in real-time optimization with unknown cost and constraints. Noting that controlling the constraint does not require detailed process models, the key idea of this paper is to control the constraints to &ldquo;some&rdquo; setpoint using simple feedback controllers. Bayesian optimization then computes the optimum setpoint for the constraint controllers. By searching over the setpoints for the constraint controllers, as opposed to searching directly over the RTO degrees of freedom, this paper achieves an inherently safe and practical model-free RTO scheme. In particular, this paper shows that the proposed approach can achieve zero cumulative constraint violation without relying on assumptions about the <b>Gaussian</b> <b>process</b> model used in Bayesian optimization. The effectiveness of the proposed approach is demonstrated on a <b>benchmark</b> Williams-Otto reactor example.</p></p class="citation"></blockquote><h2 id=csds-6>cs.DS (6)</h2><h3 id=16--270286-tighter-bounds-for-local-differentially-private-core-decomposition-and-densest-subgraph-monika-henzinger-et-al-2024>(1/6 | 270/286) Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph (Monika Henzinger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Monika Henzinger, A. R. Sricharan, Leqi Zhu. (2024)<br><strong>Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph</strong><br><button class=copy-to-clipboard title="Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CR, cs-DS, cs.DS<br>Keyword Score: 18<br>Keywords: Graph, Black Box, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18020v1.pdf filename=2402.18020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computing the core decomposition of a <b>graph</b> is a fundamental problem that has recently been studied in the differentially private setting, motivated by practical applications in data mining. In particular, Dhulipala et al. [FOCS 2022] gave the first mechanism for approximate core decomposition in the challenging and practically relevant setting of local <b>differential</b> <b>privacy.</b> One of the main open problems left by their work is whether the accuracy, i.e., the approximation ratio and additive error, of their mechanism can be improved. We show the first lower bounds on the additive error of approximate and exact core decomposition mechanisms in the centralized and local model of <b>differential</b> <b>privacy,</b> respectively. We also give mechanisms for exact and approximate core decomposition in the local model, with almost matching additive error bounds. Our mechanisms are based on a <b>black-box</b> <b>application</b> of continual counting. They also yield improved mechanisms for the approximate densest subgraph problem in the local model.</p></p class="citation"></blockquote><h3 id=26--271286-dynamic-deterministic-constant-approximate-distance-oracles-with-nε-worst-case-update-time-bernhard-haeupler-et-al-2024>(2/6 | 271/286) Dynamic Deterministic Constant-Approximate Distance Oracles with $n^ε$ Worst-Case Update Time (Bernhard Haeupler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernhard Haeupler, Yaowei Long, Thatchaphol Saranurak. (2024)<br><strong>Dynamic Deterministic Constant-Approximate Distance Oracles with $n^ε$ Worst-Case Update Time</strong><br><button class=copy-to-clipboard title="Dynamic Deterministic Constant-Approximate Distance Oracles with $n^ε$ Worst-Case Update Time" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18541v1.pdf filename=2402.18541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new distance oracle in the fully dynamic setting: given a weighted undirected <b>graph</b> $G=(V,E)$ with $n$ vertices undergoing both edge insertions and deletions, and an arbitrary parameter $\epsilon$ where $1/\log^{c} n&lt;\epsilon&lt;1$ and $c>0$ is a small constant, we can deterministically maintain a data structure with $n^{\epsilon}$ worst-case update time that, given any pair of vertices $(u,v)$, returns a $2^{{\rm poly}(1/\epsilon)}$-approximate distance between $u$ and $v$ in ${\rm poly}(1/\epsilon)\log\log n$ query time. Our algorithm significantly advances the state-of-the-art in two aspects, both for fully dynamic algorithms and even decremental algorithms. First, no existing algorithm with worst-case update time guarantees a $o(n)$-approximation while also achieving an $n^{2-\Omega(1)}$ update and $n^{o(1)}$ query time, while our algorithm offers a constant $O_{\epsilon}(1)$-approximation with $n^{\epsilon}$ update time and $O_{\epsilon}(\log \log n)$ query time. Second, even if amortized update time is allowed, it is the first deterministic constant-approximation algorithm with $n^{1-\Omega(1)}$ update and query time. The best result in this direction is the recent deterministic distance oracle by Chuzhoy and Zhang [STOC 2023] which achieves an approximation of $(\log\log n)^{2^{O(1/\epsilon^{3})}}$ with amortized update time of $n^{\epsilon}$ and query time of $2^{{\rm poly}(1/\epsilon)}\log n\log\log n$. We obtain the result by dynamizing tools related to length-constrained expanders [Haeupler-R"acke-Ghaffari, STOC 2022; Haeupler-Hershkowitz-Tan, 2023; Haeupler-Huebotter-Ghaffari, 2022]. Our technique completely bypasses the 40-year-old Even-Shiloach tree, which has remained the most pervasive tool in the area but is inherently amortized.</p></p class="citation"></blockquote><h3 id=36--272286-polynomial-time-approximation-schemes-for-induced-subgraph-problems-on-fractionally-tree-independence-number-fragile-graphs-esther-galby-et-al-2024>(3/6 | 272/286) Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs (Esther Galby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esther Galby, Andrea Munaro, Shizhou Yang. (2024)<br><strong>Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs</strong><br><button class=copy-to-clipboard title="Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 05C85, 05C75, 05C62, 68W25, cs-CG, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18352v1.pdf filename=2402.18352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate a relaxation of the notion of fractional treewidth-fragility, namely fractional tree-independence-number-fragility. In particular, we obtain polynomial-time approximation schemes for meta-problems such as finding a maximum-weight sparse induced subgraph satisfying a given $\mathsf{CMSO}_2$ formula on fractionally tree-independence-number-fragile <b>graph</b> classes. Our approach unifies and extends several known polynomial-time approximation schemes on seemingly unrelated <b>graph</b> classes, such as classes of intersection <b>graphs</b> of fat objects in a fixed dimension or proper minor-closed classes. We also study the related notion of layered tree-independence number, a relaxation of layered treewidth, and its applications to exact subexponential-time algorithms.</p></p class="citation"></blockquote><h3 id=46--273286-online-edge-coloring-is-nearly-as-easy-as-offline-joakim-blikstad-et-al-2024>(4/6 | 273/286) Online Edge Coloring is (Nearly) as Easy as Offline (Joakim Blikstad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joakim Blikstad, Ola Svensson, Radu Vintan, David Wajc. (2024)<br><strong>Online Edge Coloring is (Nearly) as Easy as Offline</strong><br><button class=copy-to-clipboard title="Online Edge Coloring is (Nearly) as Easy as Offline" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18339v1.pdf filename=2402.18339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The classic theorem of Vizing (Diskret. Analiz.&lsquo;64) asserts that any <b>graph</b> of maximum degree $\Delta$ can be edge colored (offline) using no more than $\Delta+1$ colors (with $\Delta$ being a trivial lower bound). In the online setting, Bar-Noy, Motwani and Naor (IPL'92) conjectured that a $(1+o(1))\Delta$-edge-coloring can be computed online in $n$-vertex <b>graphs</b> of maximum degree $\Delta=\omega(\log n)$. Numerous algorithms made progress on this question, using a higher number of colors or assuming restricted arrival models, such as random-order edge arrivals or vertex arrivals (e.g., AGKM FOCS'03, BMM SODA'10, CPW FOCS'19, BGW SODA'21, KLSST STOC'22). In this work, we resolve this longstanding conjecture in the affirmative in the most general setting of adversarial edge arrivals. We further generalize this result to obtain online counterparts of the list edge coloring result of Kahn (J. Comb. Theory. A'96) and of the recent &ldquo;local&rdquo; edge coloring result of Christiansen (STOC'23).</p></p class="citation"></blockquote><h3 id=56--274286-output-sensitive-enumeration-of-potential-maximal-cliques-in-polynomial-space-caroline-brosse-et-al-2024>(5/6 | 274/286) Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space (Caroline Brosse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Caroline Brosse, Alessio Conte, Vincent Limouzy, Giulia Punzi, Davide Rucci. (2024)<br><strong>Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space</strong><br><button class=copy-to-clipboard title="Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18265v1.pdf filename=2402.18265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A set of vertices in a <b>graph</b> forms a potential maximal clique if there exists a minimal chordal completion in which it is a maximal clique. Potential maximal cliques were first introduced as a key tool to obtain an efficient, though exponential-time algorithm to compute the treewidth of a <b>graph.</b> As a byproduct, this allowed to compute the treewidth of various <b>graph</b> classes in polynomial time. In recent years, the concept of potential maximal cliques regained interest as it proved to be useful for a handful of <b>graph</b> algorithmic problems. In particular, it turned out to be a key tool to obtain a polynomial time algorithm for computing maximum weight independent sets in $P_5$-free and $P_6$-free <b>graphs</b> (Lokshtanov et al., SODA <code>14 and Grzeskik et al., SODA </code>19. In most of their applications, obtaining all the potential maximal cliques constitutes an algorithmic bottleneck, thus motivating the question of how to efficiently enumerate all the potential maximal cliques in a <b>graph</b> $G$. The state-of-the-art algorithm by Bouchitt'e & Todinca can enumerate potential maximal cliques in output-polynomial time by using exponential space, a significant limitation for the size of feasible instances. In this paper, we revisit this algorithm and design an enumeration algorithm that preserves an output-polynomial time complexity while only requiring polynomial space.</p></p class="citation"></blockquote><h3 id=66--275286-computing-minimal-absent-words-and-extended-bispecial-factors-with-cdawg-space-shunsuke-inenaga-et-al-2024>(6/6 | 275/286) Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space (Shunsuke Inenaga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunsuke Inenaga, Takuya Mieno, Hiroki Arimura, Mitsuru Funakoshi, Yuta Fujishige. (2024)<br><strong>Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space</strong><br><button class=copy-to-clipboard title="Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-FL, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18090v1.pdf filename=2402.18090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A string $w$ is said to be a minimal absent word (MAW) for a string $S$ if $w$ does not occur in $S$ and any proper substring of $w$ occurs in $S$. We focus on non-trivial MAWs which are of length at least 2. Finding such non-trivial MAWs for a given string is motivated for applications in bioinformatics and data compression. Fujishige et al. [TCS 2023] proposed a data structure of size $\Theta(n)$ that can output the set $\mathsf{MAW}(S)$ of all MAWs for a given string $S$ of length $n$ in $O(n + |\mathsf{MAW}(S)|)$ time, based on the directed acyclic word <b>graph</b> (DAWG). In this paper, we present a more space efficient data structure based on the compact DAWG (CDAWG), which can output $\mathsf{MAW}(S)$ in $O(|\mathsf{MAW}(S)|)$ time with $O(e)$ space, where $e$ denotes the minimum of the sizes of the CDAWGs for $S$ and for its reversal $S^R$. For any strings of length $n$, it holds that $e &lt; 2n$, and for highly repetitive strings $e$ can be sublinear (up to logarithmic) in $n$. We also show that MAWs and their generalization minimal rare words have close relationships with extended bispecial factors, via the CDAWG.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--276286-block-and-detail-scaffolding-sketch-to-image-generation-vishnu-sarukkai-et-al-2024>(1/1 | 276/286) Block and Detail: Scaffolding Sketch-to-Image Generation (Vishnu Sarukkai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishnu Sarukkai, Lu Yuan, Mia Tang, Maneesh Agrawala, Kayvon Fatahalian. (2024)<br><strong>Block and Detail: Scaffolding Sketch-to-Image Generation</strong><br><button class=copy-to-clipboard title="Block and Detail: Scaffolding Sketch-to-Image Generation" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs.GR<br>Keyword Score: 10<br>Keywords: ControlNet<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18116v1.pdf filename=2402.18116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel sketch-to-image tool that aligns with the iterative refinement process of artists. Our tool lets users sketch blocking strokes to coarsely represent the placement and form of objects and detail strokes to refine their shape and silhouettes. We develop a two-pass algorithm for generating high-fidelity images from such sketches at any point in the iterative process. In the first pass we use a <b>ControlNet</b> to generate an image that strictly follows all the strokes (blocking and detail) and in the second pass we add variation by renoising regions surrounding blocking strokes. We also present a dataset generation scheme that, when used to train a <b>ControlNet</b> architecture, allows regions that do not contain strokes to be interpreted as not-yet-specified regions rather than empty space. We show that this partial-sketch-aware <b>ControlNet</b> can generate coherent elements from partial sketches that only contain a small number of strokes. The high-fidelity images produced by our approach serve as scaffolds that can help the user adjust the shape and proportions of objects or add additional elements to the composition. We demonstrate the effectiveness of our approach with a variety of examples and evaluative comparisons.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--277286-constrained-decoding-for-code-language-models-via-efficient-left-and-right-quotienting-of-context-sensitive-grammars-daniel-melcer-et-al-2024>(1/1 | 277/286) Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars (Daniel Melcer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Melcer, Nathan Fulton, Sanjay Krishna Gouda, Haifeng Qian. (2024)<br><strong>Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars</strong><br><button class=copy-to-clipboard title="Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-LG, cs-PL, cs-SE, cs.PL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17988v1.pdf filename=2402.17988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing. To validate our theoretical contributions &ndash; and the practical effectiveness of certain design decisions &ndash; we evaluate our method on the particularly difficult case of FItM completion for Python 3. Our results demonstrate that constrained generation can significantly reduce the incidence of syntax errors in recommended code.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--278286-exploring-the-space-of-graphs-with-fixed-discrete-curvatures-michelle-roost-et-al-2024>(1/1 | 278/286) Exploring the space of graphs with fixed discrete curvatures (Michelle Roost et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michelle Roost, Karel Devriendt, Giulio Zucal, Jürgen Jost. (2024)<br><strong>Exploring the space of graphs with fixed discrete curvatures</strong><br><button class=copy-to-clipboard title="Exploring the space of graphs with fixed discrete curvatures" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: 05C30, 05C50, 05C82, 05C85, 68W50, cs-DM, math-CO, physics-soc-ph, physics.soc-ph<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18203v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18203v1.pdf filename=2402.18203v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discrete curvatures are quantities associated to the nodes and edges of a <b>graph</b> that reflect the local <b>geometry</b> around them. These curvatures have a rich mathematical theory and they have recently found success as a tool to analyze networks across a wide range of domains. In this work, we consider the problem of constructing <b>graphs</b> with a prescribed set of discrete edge curvatures, and explore the space of such <b>graphs.</b> We address this problem in two ways: first, we develop an evolutionary algorithm to sample <b>graphs</b> with discrete curvatures close to a given set. We use this algorithm to explore how other network statistics vary when constrained by the discrete curvatures in the network. Second, we solve the exact reconstruction problem for the specific case of Forman-Ricci curvature. By leveraging the theory of Markov bases, we obtain a finite set of rewiring moves that connects the space of all <b>graphs</b> with a fixed discrete curvature.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--279286-play-like-a-vertex-a-stackelberg-game-approach-for-streaming-graph-partitioning-zezhong-ding-et-al-2024>(1/2 | 279/286) Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning (Zezhong Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zezhong Ding, Yongan Xiang, Shangyou Wang, Xike Xie, S. Kevin Zhou. (2024)<br><strong>Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning</strong><br><button class=copy-to-clipboard title="Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: 97P30, H-2-4, cs-DB, cs-DC, cs-GT, cs.DC<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18304v1.pdf filename=2402.18304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of distributed systems tasked with managing and processing large-scale <b>graph-structured</b> data, optimizing <b>graph</b> partitioning stands as a pivotal challenge. The primary goal is to minimize communication overhead and runtime cost. However, alongside the computational complexity associated with optimal <b>graph</b> partitioning, a critical factor to consider is memory overhead. Real-world <b>graphs</b> often reach colossal sizes, making it impractical and economically unviable to load the entire <b>graph</b> into memory for partitioning. This is also a fundamental premise in distributed <b>graph</b> processing, where accommodating a <b>graph</b> with non-distributed systems is unattainable. Currently, existing streaming partitioning algorithms exhibit a skew-oblivious nature, yielding satisfactory partitioning results exclusively for specific <b>graph</b> types. In this paper, we propose a novel streaming partitioning algorithm, the Skewness-aware Vertex-cut Partitioner S5P, designed to leverage the skewness characteristics of real <b>graphs</b> for achieving high-quality partitioning. S5P offers high partitioning quality by segregating the <b>graph&rsquo;s</b> edge set into two subsets, head and tail sets. Following processing by a skewness-aware <b>clustering</b> algorithm, these two subsets subsequently undergo a Stackelberg <b>graph</b> game. Our extensive evaluations conducted on substantial real-world and synthetic <b>graphs</b> demonstrate that, in all instances, the partitioning quality of S5P surpasses that of existing streaming partitioning algorithms, operating within the same load balance constraints. For example, S5P can bring up to a 51% improvement in partitioning quality compared to the top partitioner among the baselines. Lastly, we showcase that the implementation of S5P results in up to an 81% reduction in communication cost and a 130% increase in runtime efficiency for distributed <b>graph</b> processing tasks on PowerGraph.</p></p class="citation"></blockquote><h3 id=22--280286-libfork-portable-continuation-stealing-with-stackless-coroutines-conor-john-williams-et-al-2024>(2/2 | 280/286) Libfork: portable continuation-stealing with stackless coroutines (Conor John Williams et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Conor John Williams, James Elliott. (2024)<br><strong>Libfork: portable continuation-stealing with stackless coroutines</strong><br><button class=copy-to-clipboard title="Libfork: portable continuation-stealing with stackless coroutines" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18480v1.pdf filename=2402.18480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages &ndash; where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of <b>benchmarks.</b> Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel&rsquo;s TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--281286-fractional-linear-matroid-matching-is-in-quasi-nc-rohit-gurjar-et-al-2024>(1/1 | 281/286) Fractional Linear Matroid Matching is in quasi-NC (Rohit Gurjar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Gurjar, Taihei Oki, Roshan Raj. (2024)<br><strong>Fractional Linear Matroid Matching is in quasi-NC</strong><br><button class=copy-to-clipboard title="Fractional Linear Matroid Matching is in quasi-NC" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DM, cs-DS, cs.CC<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18276v1.pdf filename=2402.18276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The matching and linear matroid intersection problems are solvable in quasi-NC, meaning that there exist deterministic algorithms that run in polylogarithmic time and use quasi-polynomially many parallel processors. However, such a parallel algorithm is unknown for linear matroid matching, which generalizes both of these problems. In this work, we propose a quasi-NC algorithm for fractional linear matroid matching, which is a relaxation of linear matroid matching and commonly generalizes fractional matching and linear matroid intersection. Our algorithm builds upon the connection of fractional matroid matching to non-commutative Edmonds&rsquo; problem recently revealed by Oki and Soma~(2023). As a corollary, we also solve <b>black-box</b> <b>non-commutative</b> Edmonds&rsquo; problem with rank-two skew-symmetric coefficients.</p></p class="citation"></blockquote><h2 id=mathag-1>math.AG (1)</h2><h3 id=11--282286-counting-points-with-riemann-roch-formulas-jorge-martín-morales-2024>(1/1 | 282/286) Counting points with Riemann-Roch formulas (Jorge Martín-Morales, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge Martín-Morales. (2024)<br><strong>Counting points with Riemann-Roch formulas</strong><br><button class=copy-to-clipboard title="Counting points with Riemann-Roch formulas" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AG<br>Categories: 14B05, 32S45, 14H20, 14C40, 11P21, cs-CC, math-AG, math.AG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18193v1.pdf filename=2402.18193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We provide an algorithm for computing the number of integral points lying in certain triangles that do not have integral vertices. We use techniques from Algebraic <b>Geometry</b> such as the Riemann-Roch formula for weighted projective planes and resolution of singularities. We analyze the complexity of the method and show that the worst case is given by the Fibonacci sequence. At the end of the manuscript a concrete example is developed in detail where the interplay with other invariants of singularity theory is also treated.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--283286-qaoa-with-random-and-subgraph-driver-hamiltonians-anthony-wilkie-et-al-2024>(1/2 | 283/286) QAOA with random and subgraph driver Hamiltonians (Anthony Wilkie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anthony Wilkie, Igor Gaidai, James Ostrowski, Rebekah Herrman. (2024)<br><strong>QAOA with random and subgraph driver Hamiltonians</strong><br><button class=copy-to-clipboard title="QAOA with random and subgraph driver Hamiltonians" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-ET, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18412v1.pdf filename=2402.18412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quantum approximate optimization algorithm (QAOA) is a promising quantum algorithm that can be used to approximately solve combinatorial optimization problems. The usual QAOA ansatz consists of an alternating application of the cost and mixer Hamiltonians. In this work, we study how using Hamiltonians other than the usual cost Hamiltonian, dubbed custom driver Hamiltonians, can affect the performance of QAOA. We derive an expected value formula for QAOA with custom driver Hamiltonians at p = 1 and show numerically that some of these custom drivers can achieve higher approximation ratio than the original algorithm implementation. Out of all the <b>graphs</b> tested, 0.036% of the random custom drivers, 75.9% of the subgraph custom drivers, 95.1% of the triangle-removed custom drivers, and 93.9% of the maximal degree edge-removed custom drivers have a higher approximation ratio than the original QAOA implementation. This finding opens up the question of whether better driver Hamiltonians can be designed to further improve the performance of QAOA.</p></p class="citation"></blockquote><h3 id=22--284286-indirect-job-shop-coding-using-rank-application-to-qaoa-iqaoa-eric-bourreau-et-al-2024>(2/2 | 284/286) Indirect Job-Shop coding using rank: application to QAOA (IQAOA) (Eric Bourreau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric Bourreau, Gerard Fleury, Phlippe Lacomme. (2024)<br><strong>Indirect Job-Shop coding using rank: application to QAOA (IQAOA)</strong><br><button class=copy-to-clipboard title="Indirect Job-Shop coding using rank: application to QAOA (IQAOA)" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DM, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18280v1.pdf filename=2402.18280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Job-Shop Scheduling Problem (JSSP) stands as one of the most renowned challenges in scheduling. It is characterized as a disjunctive problem, wherein a solution is fully depicted through an oriented disjunctive <b>graph,</b> with earliest starting times computed using a longest path algorithm. The complexity of solving this problem arises in part from the requirement that disjunctive <b>graphs</b> representing solutions must be acyclic. Consequently, enumerating these <b>graphs</b> is feasible for small-scale instances only. A significant advancement in this field, credited to (Bierwith, 1995), is the introduction of the &lsquo;vector by repetition&rsquo; (commonly known as Bierwith&rsquo;s vector). Notably, this vector possesses the property that it can be mapped to an acyclic disjunctive <b>graph,</b> thereby enabling the mapping of a vector to a solution. This property has facilitated the development of highly efficient resolution schemes, as it allows the enumeration of solutions only i.e. acyclic disjunctive <b>graphs.</b> Our objective is to demonstrate how Bierwith&rsquo;s vector can be integrated into a Quantum Approximate Optimization Algorithm (QAOA) to tackle the job-shop problem using a novel quantum approach.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--285286-lower-bounds-for-leaf-rank-of-leaf-powers-svein-høgemo-2024>(1/1 | 285/286) Lower Bounds for Leaf Rank of Leaf Powers (Svein Høgemo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Svein Høgemo. (2024)<br><strong>Lower Bounds for Leaf Rank of Leaf Powers</strong><br><button class=copy-to-clipboard title="Lower Bounds for Leaf Rank of Leaf Powers" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: G-2-2, cs-DM, cs-DS, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18245v1.pdf filename=2402.18245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leaf powers and $k$-leaf powers have been studied for over 20 years, but there are still several aspects of this <b>graph</b> class that are poorly understood. One such aspect is the leaf rank of leaf powers, i.e. the smallest number $k$ such that a <b>graph</b> $G$ is a $k$-leaf power. Computing the leaf rank of leaf powers has proved a hard task, and furthermore, results about the asymptotic growth of the leaf rank as a function of the number of vertices in the <b>graph</b> have been few and far between. We present an infinite family of rooted directed path <b>graphs</b> that are leaf powers, and prove that they have leaf rank exponential in the number of vertices (utilizing a type of subtree model first presented by Rautenbach [Some remarks about leaf roots. Discrete mathematics, 2006]). This answers an open question by Brandst"adt et al. [Rooted directed path <b>graphs</b> are leaf powers. Discrete mathematics, 2010].</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--286286-ensemble-methodologyinnovations-in-credit-default-prediction-using-lightgbm-xgboost-and-localensemble-mengran-zhu-et-al-2024>(1/1 | 286/286) Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble (Mengran Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengran Zhu, Ye Zhang, Yulu Gong, Kaijuan Xing, Xu Yan, Jintong Song. (2024)<br><strong>Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble</strong><br><button class=copy-to-clipboard title="Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-AI, cs-CE, cs-LG, cs.CE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17979v1.pdf filename=2402.17979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of consumer lending, accurate credit default prediction stands as a critical element in risk mitigation and lending decision optimization. Extensive research has sought continuous improvement in existing models to enhance customer experiences and ensure the sound economic functioning of lending institutions. This study responds to the evolving landscape of credit default prediction, challenging conventional models and introducing innovative approaches. By building upon foundational research and recent innovations, our work aims to redefine the standards of accuracy in credit default prediction, setting a new <b>benchmark</b> for the industry. To overcome these challenges, we present an Ensemble Methods framework comprising LightGBM, XGBoost, and LocalEnsemble modules, each making unique contributions to amplify diversity and improve generalization. By utilizing distinct feature sets, our methodology directly tackles limitations identified in previous studies, with the overarching goal of establishing a novel standard for credit default prediction accuracy. Our experimental findings validate the effectiveness of the ensemble model on the dataset, signifying substantial contributions to the field. This innovative approach not only addresses existing obstacles but also sets a precedent for advancing the accuracy and robustness of credit default prediction models.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.29</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-64>cs.CL (64)</a><ul><li><a href=#164--1286-unsupervised-information-refinement-training-of-large-language-models-for-retrieval-augmented-generation-shicheng-xu-et-al-2024>(1/64 | 1/286) Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation (Shicheng Xu et al., 2024)</a></li><li><a href=#264--2286-cogbench-a-large-language-model-walks-into-a-psychology-lab-julian-coda-forno-et-al-2024>(2/64 | 2/286) CogBench: a large language model walks into a psychology lab (Julian Coda-Forno et al., 2024)</a></li><li><a href=#364--3286-few-shot-fairness-unveiling-llms-potential-for-fairness-aware-classification-garima-chhikara-et-al-2024>(3/64 | 3/286) Few-Shot Fairness: Unveiling LLM&rsquo;s Potential for Fairness-Aware Classification (Garima Chhikara et al., 2024)</a></li><li><a href=#464--4286-learning-to-generate-instruction-tuning-datasets-for-zero-shot-task-adaptation-nihal-v-nayak-et-al-2024>(4/64 | 4/286) Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation (Nihal V. Nayak et al., 2024)</a></li><li><a href=#564--5286-is-crowdsourcing-breaking-your-bank-cost-effective-fine-tuning-of-pre-trained-language-models-with-proximal-policy-optimization-shuo-yang-et-al-2024>(5/64 | 5/286) Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization (Shuo Yang et al., 2024)</a></li><li><a href=#664--6286-towards-generalist-prompting-for-large-language-models-by-mental-models-haoxiang-guan-et-al-2024>(6/64 | 6/286) Towards Generalist Prompting for Large Language Models by Mental Models (Haoxiang Guan et al., 2024)</a></li><li><a href=#764--7286-evaluating-quantized-large-language-models-shiyao-li-et-al-2024>(7/64 | 7/286) Evaluating Quantized Large Language Models (Shiyao Li et al., 2024)</a></li><li><a href=#864--8286-hire-a-linguist-learning-endangered-languages-with-in-context-linguistic-descriptions-kexun-zhang-et-al-2024>(8/64 | 8/286) Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions (Kexun Zhang et al., 2024)</a></li><li><a href=#964--9286-meta-task-prompting-elicits-embedding-from-large-language-models-yibin-lei-et-al-2024>(9/64 | 9/286) Meta-Task Prompting Elicits Embedding from Large Language Models (Yibin Lei et al., 2024)</a></li><li><a href=#1064--10286-decomposed-prompting-unveiling-multilingual-linguistic-structure-knowledge-in-english-centric-large-language-models-ercong-nie-et-al-2024>(10/64 | 10/286) Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models (Ercong Nie et al., 2024)</a></li><li><a href=#1164--11286-benchmarking-large-language-models-on-answering-and-explaining-challenging-medical-questions-hanjie-chen-et-al-2024>(11/64 | 11/286) Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions (Hanjie Chen et al., 2024)</a></li><li><a href=#1264--12286-fofo-a-benchmark-to-evaluate-llms-format-following-capability-congying-xia-et-al-2024>(12/64 | 12/286) FOFO: A Benchmark to Evaluate LLMs&rsquo; Format-Following Capability (Congying Xia et al., 2024)</a></li><li><a href=#1364--13286-the-first-place-solution-of-wsdm-cup-2024-leveraging-large-language-models-for-conversational-multi-doc-qa-yiming-li-et-al-2024>(13/64 | 13/286) The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA (Yiming Li et al., 2024)</a></li><li><a href=#1464--14286-verifiner-verification-augmented-ner-via-knowledge-grounded-reasoning-with-large-language-models-seoyeon-kim-et-al-2024>(14/64 | 14/286) VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models (Seoyeon Kim et al., 2024)</a></li><li><a href=#1564--15286-how-to-think-step-by-step-a-mechanistic-understanding-of-chain-of-thought-reasoning-subhabrata-dutta-et-al-2024>(15/64 | 15/286) How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning (Subhabrata Dutta et al., 2024)</a></li><li><a href=#1664--16286-reslora-identity-residual-mapping-in-low-rank-adaption-shuhua-shi-et-al-2024>(16/64 | 16/286) ResLoRA: Identity Residual Mapping in Low-Rank Adaption (Shuhua Shi et al., 2024)</a></li><li><a href=#1764--17286-gradient-free-adaptive-global-pruning-for-pre-trained-language-models-guangji-bai-et-al-2024>(17/64 | 17/286) Gradient-Free Adaptive Global Pruning for Pre-trained Language Models (Guangji Bai et al., 2024)</a></li><li><a href=#1864--18286-miko-multimodal-intention-knowledge-distillation-from-large-language-models-for-social-media-commonsense-discovery-feihong-lu-et-al-2024>(18/64 | 18/286) MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery (Feihong Lu et al., 2024)</a></li><li><a href=#1964--19286-leveraging-diverse-modeling-contexts-with-collaborating-learning-for-neural-machine-translation-yusheng-liao-et-al-2024>(19/64 | 19/286) Leveraging Diverse Modeling Contexts with Collaborating Learning for Neural Machine Translation (Yusheng Liao et al., 2024)</a></li><li><a href=#2064--20286-challenges-in-pre-training-graph-neural-networks-for-context-based-fake-news-detection-an-evaluation-of-current-strategies-and-resource-limitations-gregor-donabauer-et-al-2024>(20/64 | 20/286) Challenges in Pre-Training Graph Neural Networks for Context-Based Fake News Detection: An Evaluation of Current Strategies and Resource Limitations (Gregor Donabauer et al., 2024)</a></li><li><a href=#2164--21286-cause-and-effect-can-large-language-models-truly-understand-causality-swagata-ashwani-et-al-2024>(21/64 | 21/286) Cause and Effect: Can Large Language Models Truly Understand Causality? (Swagata Ashwani et al., 2024)</a></li><li><a href=#2264--22286-can-gpt-improve-the-state-of-prior-authorization-via-guideline-based-automated-question-answering-shubham-vatsal-et-al-2024>(22/64 | 22/286) Can GPT Improve the State of Prior Authorization via Guideline Based Automated Question Answering? (Shubham Vatsal et al., 2024)</a></li><li><a href=#2364--23286-small-but-funny-a-feedback-driven-approach-to-humor-distillation-sahithya-ravi-et-al-2024>(23/64 | 23/286) Small But Funny: A Feedback-Driven Approach to Humor Distillation (Sahithya Ravi et al., 2024)</a></li><li><a href=#2464--24286-on-the-use-of-silver-standard-data-for-zero-shot-classification-tasks-in-information-extraction-jianwei-wang-et-al-2024>(24/64 | 24/286) On the use of Silver Standard Data for Zero-shot Classification Tasks in Information Extraction (Jianwei Wang et al., 2024)</a></li><li><a href=#2564--25286-exploring-multi-document-information-consolidation-for-scientific-sentiment-summarization-miao-li-et-al-2024>(25/64 | 25/286) Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization (Miao Li et al., 2024)</a></li><li><a href=#2664--26286-focus-on-your-question-interpreting-and-mitigating-toxic-cot-problems-in-commonsense-reasoning-jiachun-li-et-al-2024>(26/64 | 26/286) Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning (Jiachun Li et al., 2024)</a></li><li><a href=#2764--27286-clustering-and-ranking-diversity-preserved-instruction-selection-through-expert-aligned-quality-estimation-yuan-ge-et-al-2024>(27/64 | 27/286) Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation (Yuan Ge et al., 2024)</a></li><li><a href=#2864--28286-newsqs-multi-source-question-generation-for-the-inquiring-mind-alyssa-hwang-et-al-2024>(28/64 | 28/286) NewsQs: Multi-Source Question Generation for the Inquiring Mind (Alyssa Hwang et al., 2024)</a></li><li><a href=#2964--29286-characterizing-truthfulness-in-large-language-model-generations-with-local-intrinsic-dimension-fan-yin-et-al-2024>(29/64 | 29/286) Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension (Fan Yin et al., 2024)</a></li><li><a href=#3064--30286-a-survey-on-recent-advances-in-llm-based-multi-turn-dialogue-systems-zihao-yi-et-al-2024>(30/64 | 30/286) A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems (Zihao Yi et al., 2024)</a></li><li><a href=#3164--31286-collaborative-decoding-of-critical-tokens-for-boosting-factuality-of-large-language-models-lifeng-jin-et-al-2024>(31/64 | 31/286) Collaborative decoding of critical tokens for boosting factuality of large language models (Lifeng Jin et al., 2024)</a></li><li><a href=#3264--32286-fine-tuned-machine-translation-metrics-struggle-in-unseen-domains-vilém-zouhar-et-al-2024>(32/64 | 32/286) Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains (Vilém Zouhar et al., 2024)</a></li><li><a href=#3364--33286-learning-to-compress-prompt-in-natural-language-formats-yu-neng-chuang-et-al-2024>(33/64 | 33/286) Learning to Compress Prompt in Natural Language Formats (Yu-Neng Chuang et al., 2024)</a></li><li><a href=#3464--34286-simple-linear-attention-language-models-balance-the-recall-throughput-tradeoff-simran-arora-et-al-2024>(34/64 | 34/286) Simple linear attention language models balance the recall-throughput tradeoff (Simran Arora et al., 2024)</a></li><li><a href=#3564--35286-beyond-natural-language-llms-leveraging-alternative-formats-for-enhanced-reasoning-and-communication-weize-chen-et-al-2024>(35/64 | 35/286) Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication (Weize Chen et al., 2024)</a></li><li><a href=#3664--36286-emotion-classification-in-low-and-moderate-resource-languages-shabnam-tafreshi-et-al-2024>(36/64 | 36/286) Emotion Classification in Low and Moderate Resource Languages (Shabnam Tafreshi et al., 2024)</a></li><li><a href=#3764--37286-rethinking-the-bounds-of-llm-reasoning-are-multi-agent-discussions-the-key-qineng-wang-et-al-2024>(37/64 | 37/286) Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key? (Qineng Wang et al., 2024)</a></li><li><a href=#3864--38286-llm-task-interference-an-initial-study-on-the-impact-of-task-switch-in-conversational-history-akash-gupta-et-al-2024>(38/64 | 38/286) LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History (Akash Gupta et al., 2024)</a></li><li><a href=#3964--39286-cutting-off-the-head-ends-the-conflict-a-mechanism-for-interpreting-and-mitigating-knowledge-conflicts-in-language-models-zhuoran-jin-et-al-2024>(39/64 | 39/286) Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models (Zhuoran Jin et al., 2024)</a></li><li><a href=#4064--40286-learning-intrinsic-dimension-via-information-bottleneck-for-explainable-aspect-based-sentiment-analysis-zhenxiao-cheng-et-al-2024>(40/64 | 40/286) Learning Intrinsic Dimension via Information Bottleneck for Explainable Aspect-based Sentiment Analysis (Zhenxiao Cheng et al., 2024)</a></li><li><a href=#4164--41286-exploring-multilingual-human-value-concepts-in-large-language-models-is-value-alignment-consistent-transferable-and-controllable-across-languages-shaoyang-xu-et-al-2024>(41/64 | 41/286) Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages? (Shaoyang Xu et al., 2024)</a></li><li><a href=#4264--42286-datasets-for-large-language-models-a-comprehensive-survey-yang-liu-et-al-2024>(42/64 | 42/286) Datasets for Large Language Models: A Comprehensive Survey (Yang Liu et al., 2024)</a></li><li><a href=#4364--43286-multilingual-speech-models-for-automatic-speech-recognition-exhibit-gender-performance-gaps-giuseppe-attanasio-et-al-2024>(43/64 | 43/286) Multilingual Speech Models for Automatic Speech Recognition Exhibit Gender Performance Gaps (Giuseppe Attanasio et al., 2024)</a></li><li><a href=#4464--44286-hierarchical-multimodal-pre-training-for-visually-rich-webpage-understanding-hongshen-xu-et-al-2024>(44/64 | 44/286) Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding (Hongshen Xu et al., 2024)</a></li><li><a href=#4564--45286-m3-vrd-multimodal-multi-task-multi-teacher-visually-rich-form-document-understanding-yihao-ding-et-al-2024>(45/64 | 45/286) M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding (Yihao Ding et al., 2024)</a></li><li><a href=#4664--46286-towards-better-understanding-of-contrastive-sentence-representation-learning-a-unified-paradigm-for-gradient-mingxin-li-et-al-2024>(46/64 | 46/286) Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient (Mingxin Li et al., 2024)</a></li><li><a href=#4764--47286-how-much-annotation-is-needed-to-compare-summarization-models-chantal-shaib-et-al-2024>(47/64 | 47/286) How Much Annotation is Needed to Compare Summarization Models? (Chantal Shaib et al., 2024)</a></li><li><a href=#4864--48286-retrieval-based-full-length-wikipedia-generation-for-emergent-events-jiebin-zhang-et-al-2024>(48/64 | 48/286) Retrieval-based Full-length Wikipedia Generation for Emergent Events (Jiebin Zhang et al., 2024)</a></li><li><a href=#4964--49286-saving-the-legacy-of-hero-ibash-evaluating-four-language-models-for-aminoacian-yunze-xiao-et-al-2024>(49/64 | 49/286) Saving the legacy of Hero Ibash: Evaluating Four Language Models for Aminoacian (Yunze Xiao et al., 2024)</a></li><li><a href=#5064--50286-editing-factual-knowledge-and-explanatory-ability-of-medical-large-language-models-derong-xu-et-al-2024>(50/64 | 50/286) Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models (Derong Xu et al., 2024)</a></li><li><a href=#5164--51286-large-language-models-and-games-a-survey-and-roadmap-roberto-gallotta-et-al-2024>(51/64 | 51/286) Large Language Models and Games: A Survey and Roadmap (Roberto Gallotta et al., 2024)</a></li><li><a href=#5264--52286-learning-or-self-aligning-rethinking-instruction-fine-tuning-mengjie-ren-et-al-2024>(52/64 | 52/286) Learning or Self-aligning? Rethinking Instruction Fine-tuning (Mengjie Ren et al., 2024)</a></li><li><a href=#5364--53286-dansk-and-dacy-260-domain-generalization-of-danish-named-entity-recognition-kenneth-enevoldsen-et-al-2024>(53/64 | 53/286) DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity Recognition (Kenneth Enevoldsen et al., 2024)</a></li><li><a href=#5464--54286-meganno-a-human-llm-collaborative-annotation-system-hannah-kim-et-al-2024>(54/64 | 54/286) MEGAnno+: A Human-LLM Collaborative Annotation System (Hannah Kim et al., 2024)</a></li><li><a href=#5564--55286-multi-fact-assessing-multilingual-llms-multi-regional-knowledge-using-factscore-sheikh-shafayat-et-al-2024>(55/64 | 55/286) Multi-FAct: Assessing Multilingual LLMs&rsquo; Multi-Regional Knowledge using FActScore (Sheikh Shafayat et al., 2024)</a></li><li><a href=#5664--56286-hop-to-the-next-tasks-and-domains-for-continual-learning-in-nlp-umberto-michieli-et-al-2024>(56/64 | 56/286) HOP to the Next Tasks and Domains for Continual Learning in NLP (Umberto Michieli et al., 2024)</a></li><li><a href=#5764--57286-a-birgat-model-for-multi-intent-spoken-language-understanding-with-hierarchical-semantic-frames-hongshen-xu-et-al-2024>(57/64 | 57/286) A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames (Hongshen Xu et al., 2024)</a></li><li><a href=#5864--58286-rora-robust-free-text-rationale-evaluation-zhengping-jiang-et-al-2024>(58/64 | 58/286) RORA: Robust Free-Text Rationale Evaluation (Zhengping Jiang et al., 2024)</a></li><li><a href=#5964--59286-tokenization-is-more-than-compression-craig-w-schmidt-et-al-2024>(59/64 | 59/286) Tokenization Is More Than Compression (Craig W. Schmidt et al., 2024)</a></li><li><a href=#6064--60286-improving-open-ended-text-generation-via-adaptive-decoding-wenhong-zhu-et-al-2024>(60/64 | 60/286) Improving Open-Ended Text Generation via Adaptive Decoding (Wenhong Zhu et al., 2024)</a></li><li><a href=#6164--61286-contextualizing-generated-citation-texts-biswadip-mandal-et-al-2024>(61/64 | 61/286) Contextualizing Generated Citation Texts (Biswadip Mandal et al., 2024)</a></li><li><a href=#6264--62286-an-iterative-associative-memory-model-for-empathetic-response-generation-zhou-yang-et-al-2024>(62/64 | 62/286) An Iterative Associative Memory Model for Empathetic Response Generation (Zhou Yang et al., 2024)</a></li><li><a href=#6364--63286-a-survey-on-neural-question-generation-methods-applications-and-prospects-shasha-guo-et-al-2024>(63/64 | 63/286) A Survey on Neural Question Generation: Methods, Applications, and Prospects (Shasha Guo et al., 2024)</a></li><li><a href=#6464--64286-assessing-the-efficacy-of-grammar-error-correction-a-human-evaluation-approach-in-the-japanese-context-qiao-wang-et-al-2024>(64/64 | 64/286) Assessing the Efficacy of Grammar Error Correction: A Human Evaluation Approach in the Japanese Context (Qiao Wang et al., 2024)</a></li></ul></li><li><a href=#cscv-58>cs.CV (58)</a><ul><li><a href=#158--65286-objective-and-interpretable-breast-cosmesis-evaluation-with-attention-guided-denoising-diffusion-anomaly-detection-model-sangjoon-park-et-al-2024>(1/58 | 65/286) Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model (Sangjoon Park et al., 2024)</a></li><li><a href=#258--66286-self-supervised-learning-in-electron-microscopy-towards-a-foundation-model-for-advanced-image-analysis-bashir-kazimi-et-al-2024>(2/58 | 66/286) Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis (Bashir Kazimi et al., 2024)</a></li><li><a href=#358--67286-grounding-language-models-for-visual-entity-recognition-zilin-xiao-et-al-2024>(3/58 | 67/286) Grounding Language Models for Visual Entity Recognition (Zilin Xiao et al., 2024)</a></li><li><a href=#458--68286-openmedlab-an-open-source-platform-for-multi-modality-foundation-models-in-medicine-xiaosong-wang-et-al-2024>(4/58 | 68/286) OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine (Xiaosong Wang et al., 2024)</a></li><li><a href=#558--69286-all-in-a-single-image-large-multimodal-models-are-in-image-learners-lei-wang-et-al-2024>(5/58 | 69/286) All in a Single Image: Large Multimodal Models are In-Image Learners (Lei Wang et al., 2024)</a></li><li><a href=#658--70286-sunshine-to-rainstorm-cross-weather-knowledge-distillation-for-robust-3d-object-detection-xun-huang-et-al-2024>(6/58 | 70/286) Sunshine to Rainstorm: Cross-Weather Knowledge Distillation for Robust 3D Object Detection (Xun Huang et al., 2024)</a></li><li><a href=#758--71286-image2flow-a-hybrid-image-and-graph-convolutional-neural-network-for-rapid-patient-specific-pulmonary-artery-segmentation-and-cfd-flow-field-calculation-from-3d-cardiac-mri-data-tina-yao-et-al-2024>(7/58 | 71/286) Image2Flow: A hybrid image and graph convolutional neural network for rapid patient-specific pulmonary artery segmentation and CFD flow field calculation from 3D cardiac MRI data (Tina Yao et al., 2024)</a></li><li><a href=#858--72286-prompt-driven-dynamic-object-centric-learning-for-single-domain-generalization-deng-li-et-al-2024>(8/58 | 72/286) Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization (Deng Li et al., 2024)</a></li><li><a href=#958--73286-ef-quantface-streamlined-face-recognition-with-small-data-and-low-bit-precision-william-gazali-et-al-2024>(9/58 | 73/286) Ef-QuantFace: Streamlined Face Recognition with Small Data and Low-Bit Precision (William Gazali et al., 2024)</a></li><li><a href=#1058--74286-understanding-the-role-of-pathways-in-a-deep-neural-network-lei-lyu-et-al-2024>(10/58 | 74/286) Understanding the Role of Pathways in a Deep Neural Network (Lei Lyu et al., 2024)</a></li><li><a href=#1158--75286-synartifact-classifying-and-alleviating-artifacts-in-synthetic-images-via-vision-language-model-bin-cao-et-al-2024>(11/58 | 75/286) SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model (Bin Cao et al., 2024)</a></li><li><a href=#1258--76286-rapid-hyperspectral-photothermal-mid-infrared-spectroscopic-imaging-from-sparse-data-for-gynecologic-cancer-tissue-subtyping-reza-reihanisaransari-et-al-2024>(12/58 | 76/286) Rapid hyperspectral photothermal mid-infrared spectroscopic imaging from sparse data for gynecologic cancer tissue subtyping (Reza Reihanisaransari et al., 2024)</a></li><li><a href=#1358--77286-polos-multimodal-metric-learning-from-human-feedback-for-image-captioning-yuiga-wada-et-al-2024>(13/58 | 77/286) Polos: Multimodal Metric Learning from Human Feedback for Image Captioning (Yuiga Wada et al., 2024)</a></li><li><a href=#1458--78286-downstream-task-guided-masking-learning-in-masked-autoencoders-using-multi-level-optimization-han-guo-et-al-2024>(14/58 | 78/286) Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization (Han Guo et al., 2024)</a></li><li><a href=#1558--79286-breaking-the-black-box-confidence-guided-model-inversion-attack-for-distribution-shift-xinhao-liu-et-al-2024>(15/58 | 79/286) Breaking the Black-Box: Confidence-Guided Model Inversion Attack for Distribution Shift (Xinhao Liu et al., 2024)</a></li><li><a href=#1658--80286-echotrack-auditory-referring-multi-object-tracking-for-autonomous-driving-jiacheng-lin-et-al-2024>(16/58 | 80/286) EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving (Jiacheng Lin et al., 2024)</a></li><li><a href=#1758--81286-coarse-to-fine-latent-diffusion-for-pose-guided-person-image-synthesis-yanzuo-lu-et-al-2024>(17/58 | 81/286) Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis (Yanzuo Lu et al., 2024)</a></li><li><a href=#1858--82286-unsupervised-cross-domain-image-retrieval-via-prototypical-optimal-transport-bin-li-et-al-2024>(18/58 | 82/286) Unsupervised Cross-Domain Image Retrieval via Prototypical Optimal Transport (Bin Li et al., 2024)</a></li><li><a href=#1958--83286-finediffusion-scaling-up-diffusion-models-for-fine-grained-image-generation-with-10000-classes-ziying-pan-et-al-2024>(19/58 | 83/286) FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10,000 Classes (Ziying Pan et al., 2024)</a></li><li><a href=#2058--84286-oil-spill-drone-a-dataset-of-drone-captured-segmented-rgb-images-for-oil-spill-detection-in-port-environments-t-de-kerf-et-al-2024>(20/58 | 84/286) Oil Spill Drone: A Dataset of Drone-Captured, Segmented RGB Images for Oil Spill Detection in Port Environments (T. De Kerf et al., 2024)</a></li><li><a href=#2158--85286-3dsflabelling-boosting-3d-scene-flow-estimation-by-pseudo-auto-labelling-chaokang-jiang-et-al-2024>(21/58 | 85/286) 3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling (Chaokang Jiang et al., 2024)</a></li><li><a href=#2258--86286-generalizable-two-branch-framework-for-image-class-incremental-learning-chao-wu-et-al-2024>(22/58 | 86/286) Generalizable Two-Branch Framework for Image Class-Incremental Learning (Chao Wu et al., 2024)</a></li><li><a href=#2358--87286-tamm-triadapter-multi-modal-learning-for-3d-shape-understanding-zhihao-zhang-et-al-2024>(23/58 | 87/286) TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding (Zhihao Zhang et al., 2024)</a></li><li><a href=#2458--88286-a-modular-system-for-enhanced-robustness-of-multimedia-understanding-networks-via-deep-parametric-estimation-francesco-barbato-et-al-2024>(24/58 | 88/286) A Modular System for Enhanced Robustness of Multimedia Understanding Networks via Deep Parametric Estimation (Francesco Barbato et al., 2024)</a></li><li><a href=#2558--89286-prcl-probabilistic-representation-contrastive-learning-for-semi-supervised-semantic-segmentation-haoyu-xie-et-al-2024>(25/58 | 89/286) PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation (Haoyu Xie et al., 2024)</a></li><li><a href=#2658--90286-enhancing-tracking-robustness-with-auxiliary-adversarial-defense-networks-zhewei-wu-et-al-2024>(26/58 | 90/286) Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks (Zhewei Wu et al., 2024)</a></li><li><a href=#2758--91286-spatial-coherence-loss-for-salient-and-camouflaged-object-detection-and-beyond-ziyun-yang-et-al-2024>(27/58 | 91/286) Spatial Coherence Loss for Salient and Camouflaged Object Detection and Beyond (Ziyun Yang et al., 2024)</a></li><li><a href=#2858--92286-selection-of-appropriate-multispectral-camera-exposure-settings-and-radiometric-calibration-methods-for-applications-in-phenotyping-and-precision-agriculture-vaishali-swaminathan-et-al-2024>(28/58 | 92/286) Selection of appropriate multispectral camera exposure settings and radiometric calibration methods for applications in phenotyping and precision agriculture (Vaishali Swaminathan et al., 2024)</a></li><li><a href=#2958--93286-gradient-reweighting-towards-imbalanced-class-incremental-learning-jiangpeng-he-et-al-2024>(29/58 | 93/286) Gradient Reweighting: Towards Imbalanced Class-Incremental Learning (Jiangpeng He et al., 2024)</a></li><li><a href=#3058--94286-separate-and-conquer-decoupling-co-occurrence-via-decomposition-and-representation-for-weakly-supervised-semantic-segmentation-zhiwei-yang-et-al-2024>(30/58 | 94/286) Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation (Zhiwei Yang et al., 2024)</a></li><li><a href=#3158--95286-attention-propagation-network-for-egocentric-heatmap-to-3d-pose-lifting-taeho-kang-et-al-2024>(31/58 | 95/286) Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting (Taeho Kang et al., 2024)</a></li><li><a href=#3258--96286-location-guided-head-pose-estimation-for-fisheye-image-bing-li-et-al-2024>(32/58 | 96/286) Location-guided Head Pose Estimation for Fisheye Image (Bing Li et al., 2024)</a></li><li><a href=#3358--97286-grid-based-continuous-normal-representation-for-anomaly-detection-joo-chan-lee-et-al-2024>(33/58 | 97/286) Grid-Based Continuous Normal Representation for Anomaly Detection (Joo Chan Lee et al., 2024)</a></li><li><a href=#3458--98286-fsl-model-can-score-higher-as-it-is-yunwei-bai-et-al-2024>(34/58 | 98/286) FSL Model can Score Higher as It Is (Yunwei Bai et al., 2024)</a></li><li><a href=#3558--99286-zero-shot-aerial-object-detection-with-visual-description-regularization-zhengqing-zang-et-al-2024>(35/58 | 99/286) Zero-Shot Aerial Object Detection with Visual Description Regularization (Zhengqing Zang et al., 2024)</a></li><li><a href=#3658--100286-balancing-act-distribution-guided-debiasing-in-diffusion-models-rishubh-parihar-et-al-2024>(36/58 | 100/286) Balancing Act: Distribution-Guided Debiasing in Diffusion Models (Rishubh Parihar et al., 2024)</a></li><li><a href=#3758--101286-ntop-nerf-powered-large-scale-dataset-generation-for-2d-and-3d-human-pose-estimation-in-top-view-fisheye-images-jingrui-yu-et-al-2024>(37/58 | 101/286) NToP: NeRF-Powered Large-scale Dataset Generation for 2D and 3D Human Pose Estimation in Top-View Fisheye Images (Jingrui Yu et al., 2024)</a></li><li><a href=#3858--102286-cfdnet-a-generalizable-foggy-stereo-matching-network-with-contrastive-feature-distillation-zihua-liu-et-al-2024>(38/58 | 102/286) CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation (Zihua Liu et al., 2024)</a></li><li><a href=#3958--103286-self-supervised-spatially-variant-psf-estimation-for-aberration-aware-depth-from-defocus-zhuofeng-wu-et-al-2024>(39/58 | 103/286) Self-Supervised Spatially Variant PSF Estimation for Aberration-Aware Depth-from-Defocus (Zhuofeng Wu et al., 2024)</a></li><li><a href=#4058--104286-from-generalization-to-precision-exploring-sam-for-tool-segmentation-in-surgical-environments-kanyifeechukwu-j-oguine-et-al-2024>(40/58 | 104/286) From Generalization to Precision: Exploring SAM for Tool Segmentation in Surgical Environments (Kanyifeechukwu J. Oguine et al., 2024)</a></li><li><a href=#4158--105286-vision-language-model-based-caption-evaluation-method-leveraging-visual-context-extraction-koki-maeda-et-al-2024>(41/58 | 105/286) Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction (Koki Maeda et al., 2024)</a></li><li><a href=#4258--106286-unimode-unified-monocular-3d-object-detection-zhuoling-li-et-al-2024>(42/58 | 106/286) UniMODE: Unified Monocular 3D Object Detection (Zhuoling Li et al., 2024)</a></li><li><a href=#4358--107286-defect-detection-in-tire-x-ray-images-conventional-methods-meet-deep-structures-andrei-cozma-et-al-2024>(43/58 | 107/286) Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures (Andrei Cozma et al., 2024)</a></li><li><a href=#4458--108286-learning-invariant-inter-pixel-correlations-for-superpixel-generation-sen-xu-et-al-2024>(44/58 | 108/286) Learning Invariant Inter-pixel Correlations for Superpixel Generation (Sen Xu et al., 2024)</a></li><li><a href=#4558--109286-univs-unified-and-universal-video-segmentation-with-prompts-as-queries-minghan-li-et-al-2024>(45/58 | 109/286) UniVS: Unified and Universal Video Segmentation with Prompts as Queries (Minghan Li et al., 2024)</a></li><li><a href=#4658--110286-representing-3d-sparse-map-points-and-lines-for-camera-relocalization-bach-thuan-bui-et-al-2024>(46/58 | 110/286) Representing 3D sparse map points and lines for camera relocalization (Bach-Thuan Bui et al., 2024)</a></li><li><a href=#4758--111286-trends-applications-and-challenges-in-human-attention-modelling-giuseppe-cartella-et-al-2024>(47/58 | 111/286) Trends, Applications, and Challenges in Human Attention Modelling (Giuseppe Cartella et al., 2024)</a></li><li><a href=#4858--112286-detection-of-micromobility-vehicles-in-urban-traffic-videos-khalil-sabri-et-al-2024>(48/58 | 112/286) Detection of Micromobility Vehicles in Urban Traffic Videos (Khalil Sabri et al., 2024)</a></li><li><a href=#4958--113286-ibd-alleviating-hallucinations-in-large-vision-language-models-via-image-biased-decoding-lanyun-zhu-et-al-2024>(49/58 | 113/286) IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding (Lanyun Zhu et al., 2024)</a></li><li><a href=#5058--114286-latentswap-an-efficient-latent-code-mapping-framework-for-face-swapping-changho-choi-et-al-2024>(50/58 | 114/286) LatentSwap: An Efficient Latent Code Mapping Framework for Face Swapping (Changho Choi et al., 2024)</a></li><li><a href=#5158--115286-ean-mapnet-efficient-vectorized-hd-map-construction-with-anchor-neighborhoods-huiyuan-xiong-et-al-2024>(51/58 | 115/286) EAN-MapNet: Efficient Vectorized HD Map Construction with Anchor Neighborhoods (Huiyuan Xiong et al., 2024)</a></li><li><a href=#5258--116286-misalignment-robust-frequency-distribution-loss-for-image-transformation-zhangkai-ni-et-al-2024>(52/58 | 116/286) Misalignment-Robust Frequency Distribution Loss for Image Transformation (Zhangkai Ni et al., 2024)</a></li><li><a href=#5358--117286-out-of-distribution-detection-using-neural-activation-prior-weilin-wan-et-al-2024>(53/58 | 117/286) Out-of-Distribution Detection using Neural Activation Prior (Weilin Wan et al., 2024)</a></li><li><a href=#5458--118286-occtransformer-improving-bevformer-for-3d-camera-only-occupancy-prediction-jian-liu-et-al-2024>(54/58 | 118/286) OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction (Jian Liu et al., 2024)</a></li><li><a href=#5558--119286-context-aware-talking-face-video-generation-meidai-xuanyuan-et-al-2024>(55/58 | 119/286) Context-aware Talking Face Video Generation (Meidai Xuanyuan et al., 2024)</a></li><li><a href=#5658--120286-sftformer-a-spatial-frequency-temporal-correlation-decoupling-transformer-for-radar-echo-extrapolation-liangyu-xu-et-al-2024>(56/58 | 120/286) SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation (Liangyu Xu et al., 2024)</a></li><li><a href=#5758--121286-multimodal-learning-to-improve-cardiac-late-mechanical-activation-detection-from-cine-mr-images-jiarui-xing-et-al-2024>(57/58 | 121/286) Multimodal Learning To Improve Cardiac Late Mechanical Activation Detection From Cine MR Images (Jiarui Xing et al., 2024)</a></li><li><a href=#5858--122286-attentive-illumination-decomposition-model-for-multi-illuminant-white-balancing-dongyoung-kim-et-al-2024>(58/58 | 122/286) Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing (Dongyoung Kim et al., 2024)</a></li></ul></li><li><a href=#cslg-53>cs.LG (53)</a><ul><li><a href=#153--123286-arithmetic-control-of-llms-for-diverse-user-preferences-directional-preference-alignment-with-multi-objective-rewards-haoxiang-wang-et-al-2024>(1/53 | 123/286) Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards (Haoxiang Wang et al., 2024)</a></li><li><a href=#253--124286-keeping-llms-aligned-after-fine-tuning-the-crucial-role-of-prompt-templates-kaifeng-lyu-et-al-2024>(2/53 | 124/286) Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates (Kaifeng Lyu et al., 2024)</a></li><li><a href=#353--125286-rnns-are-not-transformers-yet-the-key-bottleneck-on-in-context-retrieval-kaiyue-wen-et-al-2024>(3/53 | 125/286) RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval (Kaiyue Wen et al., 2024)</a></li><li><a href=#453--126286-orchid-flexible-and-data-dependent-convolution-for-sequence-modeling-mahdi-karami-et-al-2024>(4/53 | 126/286) Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling (Mahdi Karami et al., 2024)</a></li><li><a href=#553--127286-graph-regularized-encoder-training-for-extreme-classification-anshul-mittal-et-al-2024>(5/53 | 127/286) Graph Regularized Encoder Training for Extreme Classification (Anshul Mittal et al., 2024)</a></li><li><a href=#653--128286-generalizability-under-sensor-failure-tokenization--transformers-enable-more-robust-latent-spaces-geeling-chau-et-al-2024>(6/53 | 128/286) Generalizability Under Sensor Failure: Tokenization + Transformers Enable More Robust Latent Spaces (Geeling Chau et al., 2024)</a></li><li><a href=#753--129286-lemo-nade-multi-parameter-neural-architecture-discovery-with-llms-md-hafizur-rahman-et-al-2024>(7/53 | 129/286) LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs (Md Hafizur Rahman et al., 2024)</a></li><li><a href=#853--130286-why-attention-graphs-are-all-we-need-pioneering-hierarchical-classification-of-hematologic-cell-populations-with-leukograph-fatemeh-nassajian-mojarrad-et-al-2024>(8/53 | 130/286) Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph (Fatemeh Nassajian Mojarrad et al., 2024)</a></li><li><a href=#953--131286-hierarchical-multi-relational-graph-representation-learning-for-large-scale-prediction-of-drug-drug-interactions-mengying-jiang-et-al-2024>(9/53 | 131/286) Hierarchical Multi-Relational Graph Representation Learning for Large-Scale Prediction of Drug-Drug Interactions (Mengying Jiang et al., 2024)</a></li><li><a href=#1053--132286-diffusion-based-neural-network-weights-generation-bedionita-soro-et-al-2024>(10/53 | 132/286) Diffusion-based Neural Network Weights Generation (Bedionita Soro et al., 2024)</a></li><li><a href=#1153--133286-flattenquant-breaking-through-the-inference-compute-bound-for-large-language-models-with-per-tensor-quantization-yi-zhang-et-al-2024>(11/53 | 133/286) FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization (Yi Zhang et al., 2024)</a></li><li><a href=#1253--134286-imagine-initialize-and-explore-an-effective-exploration-method-in-multi-agent-reinforcement-learning-zeyang-liu-et-al-2024>(12/53 | 134/286) Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning (Zeyang Liu et al., 2024)</a></li><li><a href=#1353--135286-pre-training-differentially-private-models-with-limited-public-data-zhiqi-bu-et-al-2024>(13/53 | 135/286) Pre-training Differentially Private Models with Limited Public Data (Zhiqi Bu et al., 2024)</a></li><li><a href=#1453--136286-efficiently-computable-safety-bounds-for-gaussian-processes-in-active-learning-jörn-tebbe-et-al-2024>(14/53 | 136/286) Efficiently Computable Safety Bounds for Gaussian Processes in Active Learning (Jörn Tebbe et al., 2024)</a></li><li><a href=#1553--137286-forml-a-riemannian-hessian-free-method-for-meta-learning-with-orthogonality-constraint-hadi-tabealhojeh-et-al-2024>(15/53 | 137/286) FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint (Hadi Tabealhojeh et al., 2024)</a></li><li><a href=#1653--138286-multi-objective-differentiable-neural-architecture-search-rhea-sanjay-sukthanker-et-al-2024>(16/53 | 138/286) Multi-objective Differentiable Neural Architecture Search (Rhea Sanjay Sukthanker et al., 2024)</a></li><li><a href=#1753--139286-provable-risk-sensitive-distributional-reinforcement-learning-with-general-function-approximation-yu-chen-et-al-2024>(17/53 | 139/286) Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation (Yu Chen et al., 2024)</a></li><li><a href=#1853--140286-on-the-inductive-biases-of-demographic-parity-based-fair-learning-algorithms-haoyu-lei-et-al-2024>(18/53 | 140/286) On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms (Haoyu Lei et al., 2024)</a></li><li><a href=#1953--141286-communication-efficient-confederated-learning-an-event-triggered-saga-approach-bin-wang-et-al-2024>(19/53 | 141/286) Communication Efficient ConFederated Learning: An Event-Triggered SAGA Approach (Bin Wang et al., 2024)</a></li><li><a href=#2053--142286-conformer-embedding-continuous-attention-in-vision-transformer-for-weather-forecasting-hira-saleem-et-al-2024>(20/53 | 142/286) Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting (Hira Saleem et al., 2024)</a></li><li><a href=#2153--143286-mmsr-symbolic-regression-is-a-multimodal-task-yanjie-li-et-al-2024>(21/53 | 143/286) MMSR: Symbolic Regression is a Multimodal Task (Yanjie Li et al., 2024)</a></li><li><a href=#2253--144286-deep-neural-network-models-trained-with-a-fixed-random-classifier-transfer-better-across-domains-hafiz-tiomoko-ali-et-al-2024>(22/53 | 144/286) Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains (Hafiz Tiomoko Ali et al., 2024)</a></li><li><a href=#2353--145286-exploring-privacy-and-fairness-risks-in-sharing-diffusion-models-an-adversarial-perspective-xinjian-luo-et-al-2024>(23/53 | 145/286) Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective (Xinjian Luo et al., 2024)</a></li><li><a href=#2453--146286-autoencoder-based-general-purpose-representation-learning-for-customer-embedding-jan-henrik-bertrand-et-al-2024>(24/53 | 146/286) Autoencoder-based General Purpose Representation Learning for Customer Embedding (Jan Henrik Bertrand et al., 2024)</a></li><li><a href=#2553--147286-classes-are-not-equal-an-empirical-study-on-image-recognition-fairness-jiequan-cui-et-al-2024>(25/53 | 147/286) Classes Are Not Equal: An Empirical Study on Image Recognition Fairness (Jiequan Cui et al., 2024)</a></li><li><a href=#2653--148286-rog_pl-robust-open-set-graph-learning-via-region-based-prototype-learning-qin-zhang-et-al-2024>(26/53 | 148/286) ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype Learning (Qin Zhang et al., 2024)</a></li><li><a href=#2753--149286-no-token-left-behind-reliable-kv-cache-compression-via-importance-aware-mixed-precision-quantization-june-yong-yang-et-al-2024>(27/53 | 149/286) No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization (June Yong Yang et al., 2024)</a></li><li><a href=#2853--150286-diffusion-language-models-are-versatile-protein-learners-xinyou-wang-et-al-2024>(28/53 | 150/286) Diffusion Language Models Are Versatile Protein Learners (Xinyou Wang et al., 2024)</a></li><li><a href=#2953--151286-automated-machine-learning-for-multi-label-classification-marcel-wever-2024>(29/53 | 151/286) Automated Machine Learning for Multi-Label Classification (Marcel Wever, 2024)</a></li><li><a href=#3053--152286-provably-efficient-partially-observable-risk-sensitive-reinforcement-learning-with-hindsight-observation-tonghe-zhang-et-al-2024>(30/53 | 152/286) Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation (Tonghe Zhang et al., 2024)</a></li><li><a href=#3153--153286-token-specific-watermarking-with-enhanced-detectability-and-semantic-coherence-for-large-language-models-mingjia-huo-et-al-2024>(31/53 | 153/286) Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models (Mingjia Huo et al., 2024)</a></li><li><a href=#3253--154286-data-augmentation-method-for-modeling-health-records-with-applications-to-clopidogrel-treatment-failure-detection-sunwoong-choi-et-al-2024>(32/53 | 154/286) Data augmentation method for modeling health records with applications to clopidogrel treatment failure detection (Sunwoong Choi et al., 2024)</a></li><li><a href=#3353--155286-out-of-domain-generalization-in-dynamical-systems-reconstruction-niclas-göring-et-al-2024>(33/53 | 155/286) Out-of-Domain Generalization in Dynamical Systems Reconstruction (Niclas Göring et al., 2024)</a></li><li><a href=#3453--156286-diffusion-models-as-constrained-samplers-for-optimization-with-unknown-constraints-lingkai-kong-et-al-2024>(34/53 | 156/286) Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints (Lingkai Kong et al., 2024)</a></li><li><a href=#3553--157286-gnss-positioning-using-cost-function-regulated-multilateration-and-graph-neural-networks-amir-jalalirad-et-al-2024>(35/53 | 157/286) GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks (Amir Jalalirad et al., 2024)</a></li><li><a href=#3653--158286-impact-of-network-topology-on-the-performance-of-decentralized-federated-learning-luigi-palmieri-et-al-2024>(36/53 | 158/286) Impact of network topology on the performance of Decentralized Federated Learning (Luigi Palmieri et al., 2024)</a></li><li><a href=#3753--159286-priority-sampling-of-large-language-models-for-compilers-dejan-grubisic-et-al-2024>(37/53 | 159/286) Priority Sampling of Large Language Models for Compilers (Dejan Grubisic et al., 2024)</a></li><li><a href=#3853--160286-unveiling-privacy-memorization-and-input-curvature-links-deepak-ravikumar-et-al-2024>(38/53 | 160/286) Unveiling Privacy, Memorization, and Input Curvature Links (Deepak Ravikumar et al., 2024)</a></li><li><a href=#3953--161286-learning-associative-memories-with-gradient-descent-vivien-cabannes-et-al-2024>(39/53 | 161/286) Learning Associative Memories with Gradient Descent (Vivien Cabannes et al., 2024)</a></li><li><a href=#4053--162286-implicit-bias-of-next-token-prediction-christos-thrampoulidis-2024>(40/53 | 162/286) Implicit Bias of Next-Token Prediction (Christos Thrampoulidis, 2024)</a></li><li><a href=#4153--163286-evolving-machine-learning-workflows-through-interactive-automl-rafael-barbudo-et-al-2024>(41/53 | 163/286) Evolving machine learning workflows through interactive AutoML (Rafael Barbudo et al., 2024)</a></li><li><a href=#4253--164286-dynamical-regimes-of-diffusion-models-giulio-biroli-et-al-2024>(42/53 | 164/286) Dynamical Regimes of Diffusion Models (Giulio Biroli et al., 2024)</a></li><li><a href=#4353--165286-unveiling-the-potential-of-robustness-in-evaluating-causal-inference-models-yiyan-huang-et-al-2024>(43/53 | 165/286) Unveiling the Potential of Robustness in Evaluating Causal Inference Models (Yiyan Huang et al., 2024)</a></li><li><a href=#4453--166286-ice-search-a-language-model-driven-feature-selection-approach-tianze-et-al-2024>(44/53 | 166/286) ICE-SEARCH: A Language Model-Driven Feature Selection Approach (Tianze et al., 2024)</a></li><li><a href=#4553--167286-catastrophic-overfitting-a-potential-blessing-in-disguise-mengnan-zhao-et-al-2024>(45/53 | 167/286) Catastrophic Overfitting: A Potential Blessing in Disguise (Mengnan Zhao et al., 2024)</a></li><li><a href=#4653--168286-decentralised-traffic-incident-detection-via-network-lasso-qiyuan-zhu-et-al-2024>(46/53 | 168/286) Decentralised Traffic Incident Detection via Network Lasso (Qiyuan Zhu et al., 2024)</a></li><li><a href=#4753--169286-mixer-is-more-than-just-a-model-qingfeng-ji-et-al-2024>(47/53 | 169/286) Mixer is more than just a model (Qingfeng Ji et al., 2024)</a></li><li><a href=#4853--170286-imitation-regularized-optimal-transport-on-networks-provable-robustness-and-application-to-logistics-planning-koshi-oishi-et-al-2024>(48/53 | 170/286) Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning (Koshi Oishi et al., 2024)</a></li><li><a href=#4953--171286-multi-sensor-and-multi-temporal-high-throughput-phenotyping-for-monitoring-and-early-detection-of-water-limiting-stress-in-soybean-sarah-e-jones-et-al-2024>(49/53 | 171/286) Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean (Sarah E. Jones et al., 2024)</a></li><li><a href=#5053--172286-quantifying-human-priors-over-social-and-navigation-networks-gecia-bravo-hermsdorff-2024>(50/53 | 172/286) Quantifying Human Priors over Social and Navigation Networks (Gecia Bravo-Hermsdorff, 2024)</a></li><li><a href=#5153--173286-log-neural-controlled-differential-equations-the-lie-brackets-make-a-difference-benjamin-walker-et-al-2024>(51/53 | 173/286) Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference (Benjamin Walker et al., 2024)</a></li><li><a href=#5253--174286-signature-kernel-conditional-independence-tests-in-causal-discovery-for-stochastic-processes-georg-manten-et-al-2024>(52/53 | 174/286) Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes (Georg Manten et al., 2024)</a></li><li><a href=#5353--175286-escaping-local-optima-in-global-placement-ke-xue-et-al-2024>(53/53 | 175/286) Escaping Local Optima in Global Placement (Ke Xue et al., 2024)</a></li></ul></li><li><a href=#csai-12>cs.AI (12)</a><ul><li><a href=#112--176286-large-language-models-as-evolution-strategies-robert-tjarko-lange-et-al-2024>(1/12 | 176/286) Large Language Models As Evolution Strategies (Robert Tjarko Lange et al., 2024)</a></li><li><a href=#212--177286-from-summary-to-action-enhancing-large-language-models-for-complex-tasks-with-open-world-apis-yulong-liu-et-al-2024>(2/12 | 177/286) From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs (Yulong Liu et al., 2024)</a></li><li><a href=#312--178286-do-large-language-models-mirror-cognitive-language-processing-yuqi-ren-et-al-2024>(3/12 | 178/286) Do Large Language Models Mirror Cognitive Language Processing? (Yuqi Ren et al., 2024)</a></li><li><a href=#412--179286-a-cognitive-evaluation-benchmark-of-image-reasoning-and-description-for-large-vision-language-models-xiujie-song-et-al-2024>(4/12 | 179/286) A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models (Xiujie Song et al., 2024)</a></li><li><a href=#512--180286-data-interpreter-an-llm-agent-for-data-science-sirui-hong-et-al-2024>(5/12 | 180/286) Data Interpreter: An LLM Agent For Data Science (Sirui Hong et al., 2024)</a></li><li><a href=#612--181286-language-models-represent-beliefs-of-self-and-others-wentao-zhu-et-al-2024>(6/12 | 181/286) Language Models Represent Beliefs of Self and Others (Wentao Zhu et al., 2024)</a></li><li><a href=#712--182286-random-silicon-sampling-simulating-human-sub-population-opinion-using-a-large-language-model-based-on-group-level-demographic-information-seungjong-sun-et-al-2024>(7/12 | 182/286) Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information (Seungjong Sun et al., 2024)</a></li><li><a href=#812--183286-commonsense-ontology-micropatterns-andrew-eells-et-al-2024>(8/12 | 183/286) Commonsense Ontology Micropatterns (Andrew Eells et al., 2024)</a></li><li><a href=#912--184286-automated-discovery-of-integral-with-deep-learning-xiaoxin-yin-2024>(9/12 | 184/286) Automated Discovery of Integral with Deep Learning (Xiaoxin Yin, 2024)</a></li><li><a href=#1012--185286-sample-efficient-preference-based-reinforcement-learning-with-dynamics-aware-rewards-katherine-metcalf-et-al-2024>(10/12 | 185/286) Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards (Katherine Metcalf et al., 2024)</a></li><li><a href=#1112--186286-gaia-categorical-foundations-of-generative-ai-sridhar-mahadevan-2024>(11/12 | 186/286) GAIA: Categorical Foundations of Generative AI (Sridhar Mahadevan, 2024)</a></li><li><a href=#1212--187286-a-relational-inductive-bias-for-dimensional-abstraction-in-neural-networks-declan-campbell-et-al-2024>(12/12 | 187/286) A Relational Inductive Bias for Dimensional Abstraction in Neural Networks (Declan Campbell et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--188286-making-them-ask-and-answer-jailbreaking-large-language-models-in-few-queries-via-disguise-and-reconstruction-tong-liu-et-al-2024>(1/9 | 188/286) Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction (Tong Liu et al., 2024)</a></li><li><a href=#29--189286-chatspamdetector-leveraging-large-language-models-for-effective-phishing-email-detection-takashi-koide-et-al-2024>(2/9 | 189/286) ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection (Takashi Koide et al., 2024)</a></li><li><a href=#39--190286-vulmci--code-splicing-based-pixel-row-oversampling-for-more-continuous-vulnerability-image-generation-tao-peng-et-al-2024>(3/9 | 190/286) VulMCI : Code Splicing-based Pixel-row Oversampling for More Continuous Vulnerability Image Generation (Tao Peng et al., 2024)</a></li><li><a href=#49--191286-efficient-fault-detection-architectures-for-modular-exponentiation-targeting-cryptographic-applications-benchmarked-on-fpgas-saeed-aghapour-et-al-2024>(4/9 | 191/286) Efficient Fault Detection Architectures for Modular Exponentiation Targeting Cryptographic Applications Benchmarked on FPGAs (Saeed Aghapour et al., 2024)</a></li><li><a href=#59--192286-a-new-era-in-llm-security-exploring-security-concerns-in-real-world-llm-based-systems-fangzhou-wu-et-al-2024>(5/9 | 192/286) A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems (Fangzhou Wu et al., 2024)</a></li><li><a href=#69--193286-exploring-advanced-methodologies-in-security-evaluation-for-llms-jun-huang-et-al-2024>(6/9 | 193/286) Exploring Advanced Methodologies in Security Evaluation for LLMs (Jun Huang et al., 2024)</a></li><li><a href=#79--194286-living-off-the-land-reverse-shell-detection-by-informed-data-augmentation-dmitrijs-trizna-et-al-2024>(7/9 | 194/286) Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation (Dmitrijs Trizna et al., 2024)</a></li><li><a href=#89--195286-performance-modeling-of-public-permissionless-blockchains-a-survey-molud-esmaili-et-al-2024>(8/9 | 195/286) Performance modeling of public permissionless blockchains: A survey (Molud Esmaili et al., 2024)</a></li><li><a href=#99--196286-on-defeating-graph-analysis-of-anonymous-transactions-christoph-egger-et-al-2024>(9/9 | 196/286) On Defeating Graph Analysis of Anonymous Transactions (Christoph Egger et al., 2024)</a></li></ul></li><li><a href=#csro-20>cs.RO (20)</a><ul><li><a href=#120--197286-cafknet-gnn-empowered-forward-kinematic-modeling-for-cable-driven-parallel-robots-zeqing-zhang-et-al-2024>(1/20 | 197/286) CafkNet: GNN-Empowered Forward Kinematic Modeling for Cable-Driven Parallel Robots (Zeqing Zhang et al., 2024)</a></li><li><a href=#220--198286-solving-multi-entity-robotic-problems-using-permutation-invariant-neural-networks-tianxu-an-et-al-2024>(2/20 | 198/286) Solving Multi-Entity Robotic Problems Using Permutation Invariant Neural Networks (Tianxu An et al., 2024)</a></li><li><a href=#320--199286-decisionnce-embodied-multimodal-representations-via-implicit-preference-learning-jianxiong-li-et-al-2024>(3/20 | 199/286) DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning (Jianxiong Li et al., 2024)</a></li><li><a href=#420--200286-automated-testing-of-spatially-dependent-environmental-hypotheses-through-active-transfer-learning-nicholas-harrison-et-al-2024>(4/20 | 200/286) Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning (Nicholas Harrison et al., 2024)</a></li><li><a href=#520--201286-symmetry-aware-reinforcement-learning-for-robotic-assembly-under-partial-observability-with-a-soft-wrist-hai-nguyen-et-al-2024>(5/20 | 201/286) Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial Observability with a Soft Wrist (Hai Nguyen et al., 2024)</a></li><li><a href=#620--202286-extending-qgroundcontrol-for-automated-mission-planning-of-uavs-cristian-ramirez-atencia-et-al-2024>(6/20 | 202/286) Extending QGroundControl for Automated Mission Planning of UAVs (Cristian Ramirez-Atencia et al., 2024)</a></li><li><a href=#720--203286-articulated-object-manipulation-with-coarse-to-fine-affordance-for-mitigating-the-effect-of-point-cloud-noise-suhan-ling-et-al-2024>(7/20 | 203/286) Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise (Suhan Ling et al., 2024)</a></li><li><a href=#820--204286-robot-body-schema-learning-from-full-body-exteroproprioception-sensors-shuo-jiang-et-al-2024>(8/20 | 204/286) Robot Body Schema Learning from Full-body Extero/Proprioception Sensors (Shuo Jiang et al., 2024)</a></li><li><a href=#920--205286-dual-imu-state-estimation-for-relative-localization-of-two-mobile-agents-wenqian-lai-et-al-2024>(9/20 | 205/286) Dual-IMU State Estimation for Relative Localization of Two Mobile Agents (Wenqian Lai et al., 2024)</a></li><li><a href=#1020--206286-fixture-calibration-with-guaranteed-bounds-from-a-few-correspondence-free-surface-points-rasmus-laurvig-haugaard-et-al-2024>(10/20 | 206/286) Fixture calibration with guaranteed bounds from a few correspondence-free surface points (Rasmus Laurvig Haugaard et al., 2024)</a></li><li><a href=#1120--207286-online-time-optimal-trajectory-generation-for-two-quadrotors-with-multi-waypoints-constraints-fangguo-zhao-et-al-2024>(11/20 | 207/286) Online Time-Optimal Trajectory Generation for Two Quadrotors with Multi-Waypoints Constraints (Fangguo Zhao et al., 2024)</a></li><li><a href=#1220--208286-a-multimodal-handover-failure-detection-dataset-and-baselines-santosh-thoduka-et-al-2024>(12/20 | 208/286) A Multimodal Handover Failure Detection Dataset and Baselines (Santosh Thoduka et al., 2024)</a></li><li><a href=#1320--209286-unifying-f1tenth-autonomous-racing-survey-methods-and-benchmarks-benjamin-david-evans-et-al-2024>(13/20 | 209/286) Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks (Benjamin David Evans et al., 2024)</a></li><li><a href=#1420--210286-a-probabilistic-motion-model-for-skid-steer-wheeled-mobile-robot-navigation-on-off-road-terrains-ananya-trivedi-et-al-2024>(14/20 | 210/286) A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains (Ananya Trivedi et al., 2024)</a></li><li><a href=#1520--211286-the-grasp-reset-mechanism-an-automated-apparatus-for-conducting-grasping-trials-kyle-dufrene-et-al-2024>(15/20 | 211/286) The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials (Kyle DuFrene et al., 2024)</a></li><li><a href=#1620--212286-human-centric-aware-uav-trajectory-planning-in-search-and-rescue-missions-employing-multi-objective-reinforcement-learning-with-ahp-and-similarity-based-experience-replay-mahya-ramezani-et-al-2024>(16/20 | 212/286) Human-Centric Aware UAV Trajectory Planning in Search and Rescue Missions Employing Multi-Objective Reinforcement Learning with AHP and Similarity-Based Experience Replay (Mahya Ramezani et al., 2024)</a></li><li><a href=#1720--213286-whole-body-humanoid-robot-locomotion-with-human-reference-qiang-zhang-et-al-2024>(17/20 | 213/286) Whole-body Humanoid Robot Locomotion with Human Reference (Qiang Zhang et al., 2024)</a></li><li><a href=#1820--214286-generative-ai-for-unmanned-vehicle-swarms-challenges-applications-and-opportunities-guangyuan-liu-et-al-2024>(18/20 | 214/286) Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and Opportunities (Guangyuan Liu et al., 2024)</a></li><li><a href=#1920--215286-generation-of-skill-specific-maps-from-graph-world-models-for-robotic-systems-koen-de-vos-et-al-2024>(19/20 | 215/286) Generation of skill-specific maps from graph world models for robotic systems (Koen de Vos et al., 2024)</a></li><li><a href=#2020--216286-ukf-based-sensor-fusion-for-joint-torque-sensorless-humanoid-robots-ines-sorrentino-et-al-2024>(20/20 | 216/286) UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots (Ines Sorrentino et al., 2024)</a></li></ul></li><li><a href=#csmm-3>cs.MM (3)</a><ul><li><a href=#13--217286-balanced-similarity-with-auxiliary-prompts-towards-alleviating-text-to-image-retrieval-bias-for-clip-in-zero-shot-learning-hanyao-wang-et-al-2024>(1/3 | 217/286) Balanced Similarity with Auxiliary Prompts: Towards Alleviating Text-to-Image Retrieval Bias for CLIP in Zero-shot Learning (Hanyao Wang et al., 2024)</a></li><li><a href=#23--218286-multimodal-interaction-modeling-via-self-supervised-multi-task-learning-for-review-helpfulness-prediction-honglin-gong-et-al-2024>(2/3 | 218/286) Multimodal Interaction Modeling via Self-Supervised Multi-Task Learning for Review Helpfulness Prediction (HongLin Gong et al., 2024)</a></li><li><a href=#33--219286-characterizing-multimedia-information-environment-through-multi-modal-clustering-of-youtube-videos-niloofar-yousefi-et-al-2024>(3/3 | 219/286) Characterizing Multimedia Information Environment through Multi-modal Clustering of YouTube Videos (Niloofar Yousefi et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--220286-prospect-personalized-recommendation-on-large-language-model-based-agent-platform-jizhi-zhang-et-al-2024>(1/4 | 220/286) Prospect Personalized Recommendation on Large Language Model-based Agent Platform (Jizhi Zhang et al., 2024)</a></li><li><a href=#24--221286-sequence-level-semantic-representation-fusion-for-recommender-systems-lanling-xu-et-al-2024>(2/4 | 221/286) Sequence-level Semantic Representation Fusion for Recommender Systems (Lanling Xu et al., 2024)</a></li><li><a href=#34--222286-a-categorization-of-complexity-classes-for-information-retrieval-and-synthesis-using-natural-logic-gregory-coppola-2024>(3/4 | 222/286) A Categorization of Complexity Classes for Information Retrieval and Synthesis Using Natural Logic (Gregory Coppola, 2024)</a></li><li><a href=#44--223286-corpus-steered-query-expansion-with-large-language-models-yibin-lei-et-al-2024>(4/4 | 223/286) Corpus-Steered Query Expansion with Large Language Models (Yibin Lei et al., 2024)</a></li></ul></li><li><a href=#physicsapp-ph-1>physics.app-ph (1)</a><ul><li><a href=#11--224286-physics-informed-machine-learning-for-seismic-response-prediction-of-nonlinear-steel-moment-resisting-frame-structures-r-bailey-bond-et-al-2024>(1/1 | 224/286) Physics-Informed Machine Learning for Seismic Response Prediction OF Nonlinear Steel Moment Resisting Frame Structures (R. Bailey Bond et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--225286-implementing-online-reinforcement-learning-with-clustering-neural-networks-james-e-smith-2024>(1/1 | 225/286) Implementing Online Reinforcement Learning with Clustering Neural Networks (James E. Smith, 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--226286-hemagraph-breaking-barriers-in-hematologic-single-cell-classification-with-graph-attention-lorenzo-bini-et-al-2024>(1/1 | 226/286) HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention (Lorenzo Bini et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--227286-ela-exploited-level-augmentation-for-offline-learning-in-zero-sum-games-shiqi-lei-et-al-2024>(1/1 | 227/286) ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games (Shiqi Lei et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--228286-take-it-leave-it-or-fix-it-measuring-productivity-and-trust-in-human-ai-collaboration-crystal-qian-et-al-2024>(1/3 | 228/286) Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration (Crystal Qian et al., 2024)</a></li><li><a href=#23--229286-hearhere-mitigating-echo-chambers-in-news-consumption-through-an-ai-based-web-system-youngseung-jeon-et-al-2024>(2/3 | 229/286) HearHere: Mitigating Echo Chambers in News Consumption through an AI-based Web System (Youngseung Jeon et al., 2024)</a></li><li><a href=#33--230286-dynamic-explanation-selection-towards-successful-user-decision-support-with-explainable-ai-yosuke-fukuchi-et-al-2024>(3/3 | 230/286) Dynamic Explanation Selection Towards Successful User-Decision Support with Explainable AI (Yosuke Fukuchi et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--231286-investigation-of-adapter-for-automatic-speech-recognition-in-noisy-environment-hao-shi-et-al-2024>(1/2 | 231/286) Investigation of Adapter for Automatic Speech Recognition in Noisy Environment (Hao Shi et al., 2024)</a></li><li><a href=#22--232286-convdtw-acs-audio-segmentation-for-track-type-detection-during-car-manufacturing-álvaro-lópez-chilet-et-al-2024>(2/2 | 232/286) ConvDTW-ACS: Audio Segmentation for Track Type Detection During Car Manufacturing (Álvaro López-Chilet et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--233286-human-simulacra-a-step-toward-the-personification-of-large-language-models-qiuejie-xie-et-al-2024>(1/1 | 233/286) Human Simulacra: A Step toward the Personification of Large Language Models (Qiuejie Xie et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--234286-understanding-random-forests-and-overfitting-a-visualization-and-simulation-study-lasai-barreñada-et-al-2024>(1/1 | 234/286) Understanding random forests and overfitting: a visualization and simulation study (Lasai Barreñada et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--235286-reinforcement-learning-and-graph-neural-networks-for-probabilistic-risk-assessment-joachim-grimstad-et-al-2024>(1/3 | 235/286) Reinforcement Learning and Graph Neural Networks for Probabilistic Risk Assessment (Joachim Grimstad et al., 2024)</a></li><li><a href=#23--236286-timer-based-coverage-control-for-mobile-sensors-federico-m-zegers-et-al-2024>(2/3 | 236/286) Timer-Based Coverage Control for Mobile Sensors (Federico M. Zegers et al., 2024)</a></li><li><a href=#33--237286-exergetic-port-hamiltonian-systems-for-multibody-dynamics-markus-lohmayer-et-al-2024>(3/3 | 237/286) Exergetic Port-Hamiltonian Systems for Multibody Dynamics (Markus Lohmayer et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--238286-a-priori-uncertainty-quantification-of-reacting-turbulence-closure-models-using-bayesian-neural-networks-graham-pash-et-al-2024>(1/1 | 238/286) A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks (Graham Pash et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--239286-bluebell-an-alliance-of-relational-lifting-and-independence-for-probabilistic-reasoning-jialu-bao-et-al-2024>(1/1 | 239/286) Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning (Jialu Bao et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--240286-unveiling-novel-insights-into-kirchhoff-migration-for-effective-object-detection-using-experimental-fresnel-dataset-won-kwang-park-2024>(1/2 | 240/286) Unveiling novel insights into Kirchhoff migration for effective object detection using experimental Fresnel dataset (Won-Kwang Park, 2024)</a></li><li><a href=#22--241286-tensor-network-space-time-spectral-collocation-method-for-time-dependent-convection-diffusion-reaction-equations-dibyendu-adak-et-al-2024>(2/2 | 241/286) Tensor Network Space-Time Spectral Collocation Method for Time Dependent Convection-Diffusion-Reaction Equations (Dibyendu Adak et al., 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--242286-simple-but-effective-rethinking-the-ability-of-deep-learning-in-fnirs-to-exclude-abnormal-input-zhihao-cao-2024>(1/3 | 242/286) Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS to Exclude Abnormal Input (Zhihao Cao, 2024)</a></li><li><a href=#23--243286-joint-activity-delay-detection-and-channel-estimation-for-asynchronous-massive-random-access-a-free-probability-theory-approach-xinyu-bian-et-al-2024>(2/3 | 243/286) Joint Activity-Delay Detection and Channel Estimation for Asynchronous Massive Random Access: A Free Probability Theory Approach (Xinyu Bian et al., 2024)</a></li><li><a href=#33--244286-distributed-intelligent-integrated-sensing-and-communications-the-6g-disac-approach-emilio-calvanese-strinati-et-al-2024>(3/3 | 244/286) Distributed Intelligent Integrated Sensing and Communications: The 6G-DISAC Approach (Emilio Calvanese Strinati et al., 2024)</a></li></ul></li><li><a href=#q-fintr-1>q-fin.TR (1)</a><ul><li><a href=#11--245286-a-multimodal-foundation-agent-for-financial-trading-tool-augmented-diversified-and-generalist-wentao-zhang-et-al-2024>(1/1 | 245/286) A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist (Wentao Zhang et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--246286-a-higher-order-lens-for-social-systems-giulia-preti-et-al-2024>(1/1 | 246/286) A Higher-Order Lens for Social Systems (Giulia Preti et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--247286-deep-confident-steps-to-new-pockets-strategies-for-docking-generalization-gabriele-corso-et-al-2024>(1/1 | 247/286) Deep Confident Steps to New Pockets: Strategies for Docking Generalization (Gabriele Corso et al., 2024)</a></li></ul></li><li><a href=#csar-5>cs.AR (5)</a><ul><li><a href=#15--248286-accelerating-computer-architecture-simulation-through-machine-learning-wajid-ali-et-al-2024>(1/5 | 248/286) Accelerating Computer Architecture Simulation through Machine Learning (Wajid Ali et al., 2024)</a></li><li><a href=#25--249286-pimsim-nn-an-isa-based-simulation-framework-for-processing-in-memory-accelerators-xinyu-wang-et-al-2024>(2/5 | 249/286) PIMSIM-NN: An ISA-based Simulation Framework for Processing-in-Memory Accelerators (Xinyu Wang et al., 2024)</a></li><li><a href=#35--250286-energy-aware-heterogeneous-federated-learning-via-approximate-systolic-dnn-accelerators-kilian-pfeiffer-et-al-2024>(3/5 | 250/286) Energy-Aware Heterogeneous Federated Learning via Approximate Systolic DNN Accelerators (Kilian Pfeiffer et al., 2024)</a></li><li><a href=#45--251286-pimsyn-synthesizing-processing-in-memory-cnn-accelerators-wanqian-li-et-al-2024>(4/5 | 251/286) PIMSYN: Synthesizing Processing-in-memory CNN Accelerators (Wanqian Li et al., 2024)</a></li><li><a href=#55--252286-a-hierarchical-dataflow-driven-heterogeneous-architecture-for-wireless-baseband-processing-limin-jiang-et-al-2024>(5/5 | 252/286) A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing (Limin Jiang et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--253286-integrated-sensing-and-communication-meets-smart-propagation-engineering-opportunities-and-challenges-kaitao-meng-et-al-2024>(1/2 | 253/286) Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges (Kaitao Meng et al., 2024)</a></li><li><a href=#22--254286-precoding-for-multi-cell-isac-from-coordinated-beamforming-to-coordinated-multipoint-and-bi-static-sensing-nithin-babu-et-al-2024>(2/2 | 254/286) Precoding for Multi-Cell ISAC: from Coordinated Beamforming to Coordinated Multipoint and Bi-Static Sensing (Nithin Babu et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#11--255286-a-non-intrusive-machine-learning-framework-for-debiasing-long-time-coarse-resolution-climate-simulations-and-quantifying-rare-events-statistics-benedikt-barthel-sorensen-et-al-2024>(1/1 | 255/286) A non-intrusive machine learning framework for debiasing long-time coarse resolution climate simulations and quantifying rare events statistics (Benedikt Barthel Sorensen et al., 2024)</a></li></ul></li><li><a href=#cset-1>cs.ET (1)</a><ul><li><a href=#11--256286-neuromorphic-event-driven-semantic-communication-in-microgrids-xiaoguang-diao-et-al-2024>(1/1 | 256/286) Neuromorphic Event-Driven Semantic Communication in Microgrids (Xiaoguang Diao et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--257286-utilization-of-reconfigurable-intelligent-surfaces-with-context-information-use-cases-łukasz-kułacz-2024>(1/2 | 257/286) Utilization of Reconfigurable Intelligent Surfaces with Context Information: Use Cases (Łukasz Kułacz, 2024)</a></li><li><a href=#22--258286-hyperfednet-communication-efficient-personalized-federated-learning-via-hypernetwork-xingyun-chen-et-al-2024>(2/2 | 258/286) HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork (Xingyun Chen et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--259286-lemur-log-parsing-with-entropy-sampling-and-chain-of-thought-merging-hongcheng-guo-et-al-2024>(1/2 | 259/286) Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging (Hongcheng Guo et al., 2024)</a></li><li><a href=#22--260286-potentials-of-green-coding----findings-and-recommendations-for-industry-education-and-science----extended-paper-dennis-junger-et-al-2024>(2/2 | 260/286) Potentials of Green Coding &ndash; Findings and Recommendations for Industry, Education and Science &ndash; Extended Paper (Dennis Junger et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#17--261286-a-lightweight-low-light-image-enhancement-network-via-channel-prior-and-gamma-correction-shyang-en-weng-et-al-2024>(1/7 | 261/286) A Lightweight Low-Light Image Enhancement Network via Channel Prior and Gamma Correction (Shyang-En Weng et al., 2024)</a></li><li><a href=#27--262286-passive-snapshot-coded-aperture-dual-pixel-rgb-d-imaging-bhargav-ghanekar-et-al-2024>(2/7 | 262/286) Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging (Bhargav Ghanekar et al., 2024)</a></li><li><a href=#37--263286-mambamir-an-arbitrary-masked-mamba-for-joint-medical-image-reconstruction-and-uncertainty-estimation-jiahao-huang-et-al-2024>(3/7 | 263/286) MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image Reconstruction and Uncertainty Estimation (Jiahao Huang et al., 2024)</a></li><li><a href=#47--264286-nerv-an-enhanced-implicit-neural-video-representation-ahmed-ghorbel-et-al-2024>(4/7 | 264/286) NERV++: An Enhanced Implicit Neural Video Representation (Ahmed Ghorbel et al., 2024)</a></li><li><a href=#57--265286-boosting-neural-representations-for-videos-with-a-conditional-decoder-xinjie-zhang-et-al-2024>(5/7 | 265/286) Boosting Neural Representations for Videos with a Conditional Decoder (Xinjie Zhang et al., 2024)</a></li><li><a href=#67--266286-improvement-of-audiovisual-quality-estimation-using-a-nonlinear-autoregressive-exogenous-neural-network-and-bitstream-parameters-koffi-kossi-et-al-2024>(6/7 | 266/286) Improvement Of Audiovisual Quality Estimation Using A Nonlinear Autoregressive Exogenous Neural Network And Bitstream Parameters (Koffi Kossi et al., 2024)</a></li><li><a href=#77--267286-prediction-of-recurrence-free-survival-of-head-and-neck-cancer-using-petct-radiomics-and-clinical-information-mona-furukawa-et-al-2024>(7/7 | 267/286) Prediction of recurrence free survival of head and neck cancer using PET/CT radiomics and clinical information (Mona Furukawa et al., 2024)</a></li></ul></li><li><a href=#q-finrm-1>q-fin.RM (1)</a><ul><li><a href=#11--268286-modeling-and-analysis-of-crypto-backed-over-collateralized-stable-derivatives-in-defi-zhenbang-feng-et-al-2024>(1/1 | 268/286) Modeling and Analysis of Crypto-Backed Over-Collateralized Stable Derivatives in DeFi (Zhenbang Feng et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--269286-eccbo-an-inherently-safe-bayesian-optimization-with-embedded-constraint-control-for-real-time-optimization-dinesh-krishnamoorthy-2024>(1/1 | 269/286) ECCBO: An Inherently Safe Bayesian Optimization with Embedded Constraint Control for Real-Time Optimization (Dinesh Krishnamoorthy, 2024)</a></li></ul></li><li><a href=#csds-6>cs.DS (6)</a><ul><li><a href=#16--270286-tighter-bounds-for-local-differentially-private-core-decomposition-and-densest-subgraph-monika-henzinger-et-al-2024>(1/6 | 270/286) Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph (Monika Henzinger et al., 2024)</a></li><li><a href=#26--271286-dynamic-deterministic-constant-approximate-distance-oracles-with-nε-worst-case-update-time-bernhard-haeupler-et-al-2024>(2/6 | 271/286) Dynamic Deterministic Constant-Approximate Distance Oracles with $n^ε$ Worst-Case Update Time (Bernhard Haeupler et al., 2024)</a></li><li><a href=#36--272286-polynomial-time-approximation-schemes-for-induced-subgraph-problems-on-fractionally-tree-independence-number-fragile-graphs-esther-galby-et-al-2024>(3/6 | 272/286) Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs (Esther Galby et al., 2024)</a></li><li><a href=#46--273286-online-edge-coloring-is-nearly-as-easy-as-offline-joakim-blikstad-et-al-2024>(4/6 | 273/286) Online Edge Coloring is (Nearly) as Easy as Offline (Joakim Blikstad et al., 2024)</a></li><li><a href=#56--274286-output-sensitive-enumeration-of-potential-maximal-cliques-in-polynomial-space-caroline-brosse-et-al-2024>(5/6 | 274/286) Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space (Caroline Brosse et al., 2024)</a></li><li><a href=#66--275286-computing-minimal-absent-words-and-extended-bispecial-factors-with-cdawg-space-shunsuke-inenaga-et-al-2024>(6/6 | 275/286) Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space (Shunsuke Inenaga et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--276286-block-and-detail-scaffolding-sketch-to-image-generation-vishnu-sarukkai-et-al-2024>(1/1 | 276/286) Block and Detail: Scaffolding Sketch-to-Image Generation (Vishnu Sarukkai et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--277286-constrained-decoding-for-code-language-models-via-efficient-left-and-right-quotienting-of-context-sensitive-grammars-daniel-melcer-et-al-2024>(1/1 | 277/286) Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars (Daniel Melcer et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--278286-exploring-the-space-of-graphs-with-fixed-discrete-curvatures-michelle-roost-et-al-2024>(1/1 | 278/286) Exploring the space of graphs with fixed discrete curvatures (Michelle Roost et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--279286-play-like-a-vertex-a-stackelberg-game-approach-for-streaming-graph-partitioning-zezhong-ding-et-al-2024>(1/2 | 279/286) Play like a Vertex: A Stackelberg Game Approach for Streaming Graph Partitioning (Zezhong Ding et al., 2024)</a></li><li><a href=#22--280286-libfork-portable-continuation-stealing-with-stackless-coroutines-conor-john-williams-et-al-2024>(2/2 | 280/286) Libfork: portable continuation-stealing with stackless coroutines (Conor John Williams et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--281286-fractional-linear-matroid-matching-is-in-quasi-nc-rohit-gurjar-et-al-2024>(1/1 | 281/286) Fractional Linear Matroid Matching is in quasi-NC (Rohit Gurjar et al., 2024)</a></li></ul></li><li><a href=#mathag-1>math.AG (1)</a><ul><li><a href=#11--282286-counting-points-with-riemann-roch-formulas-jorge-martín-morales-2024>(1/1 | 282/286) Counting points with Riemann-Roch formulas (Jorge Martín-Morales, 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--283286-qaoa-with-random-and-subgraph-driver-hamiltonians-anthony-wilkie-et-al-2024>(1/2 | 283/286) QAOA with random and subgraph driver Hamiltonians (Anthony Wilkie et al., 2024)</a></li><li><a href=#22--284286-indirect-job-shop-coding-using-rank-application-to-qaoa-iqaoa-eric-bourreau-et-al-2024>(2/2 | 284/286) Indirect Job-Shop coding using rank: application to QAOA (IQAOA) (Eric Bourreau et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--285286-lower-bounds-for-leaf-rank-of-leaf-powers-svein-høgemo-2024>(1/1 | 285/286) Lower Bounds for Leaf Rank of Leaf Powers (Svein Høgemo, 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--286286-ensemble-methodologyinnovations-in-credit-default-prediction-using-lightgbm-xgboost-and-localensemble-mengran-zhu-et-al-2024>(1/1 | 286/286) Ensemble Methodology:Innovations in Credit Default Prediction Using LightGBM, XGBoost, and LocalEnsemble (Mengran Zhu et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>