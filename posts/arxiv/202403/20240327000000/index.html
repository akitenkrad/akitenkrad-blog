<!doctype html><html><head><title>arXiv @ 2024.03.27</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.27"><meta property="og:description" content="Primary Categories cs.AI (20) cs.AR (3) cs.CG (1) cs.CL (56) cs.CR (3) cs.CV (76) cs.CY (1) cs.DB (1) cs.DC (4) cs.DL (2) cs.DM (1) cs.FL (1) cs.GR (1) cs.GT (1) cs.HC (11) cs.IR (4) cs.IT (6) cs.LG (42) cs.MA (3) cs.NE (2) cs.NI (2) cs.OS (1) cs.PF (1) cs.RO (27) cs.SD (1) cs.SE (15) cs.SI (2) eess.AS (3) eess.IV (13) eess.SP (2) eess.SY (17) math.AG (1) math.MG (1) math.NA (5) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240327000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-27T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.27"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240327000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Mar 27, 2024</p></div><div class=title><h1>arXiv @ 2024.03.27</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csai-20>cs.AI (20)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csar-3>cs.AR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cscl-56>cs.CL (56)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cscv-76>cs.CV (76)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csdc-4>cs.DC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csdl-2>cs.DL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cshc-11>cs.HC (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csir-4>cs.IR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csit-6>cs.IT (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cslg-42>cs.LG (42)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csma-3>cs.MA (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csos-1>cs.OS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csro-27>cs.RO (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#csse-15>cs.SE (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#eessiv-13>eess.IV (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#eesssy-17>eess.SY (17)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#mathag-1>math.AG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#mathmg-1>math.MG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#mathna-5>math.NA (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#mathoc-4>math.OC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#physicsflu-dyn-2>physics.flu-dyn (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/#quant-ph-1>quant-ph (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.HC</th><th>cs.LG</th><th>cs.RO</th><th>cs.SE</th><th>eess.IV</th><th>eess.SY</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td></td><td></td><td>4</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>2</td><td>1</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Benchmarking</td><td>1</td><td>15</td><td>24</td><td>1</td><td>10</td><td>2</td><td>6</td><td></td><td>1</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Box Embedding</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td>2</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>3</td><td>2</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Contextual Embedding</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4</td></tr><tr><td>Contrastive Learning</td><td>1</td><td>2</td><td>3</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>5</td><td></td><td>5</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>4</td><td></td><td>5</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Coreference Resolution</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td>1</td><td></td><td>18</td><td></td><td>2</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Discrete Time</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td>2</td></tr><tr><td>Distribution Shift</td><td></td><td></td><td>2</td><td></td><td>4</td><td></td><td></td><td></td><td></td></tr><tr><td>Doc2vec</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Document Classification</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td>1</td><td></td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>6</td><td></td><td></td><td></td><td></td></tr><tr><td>Few-shot</td><td></td><td>5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>3</td><td>12</td><td>8</td><td></td><td>2</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td><td></td><td>2</td></tr><tr><td>GLUE</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT</td><td>1</td><td>4</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td>3</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>3</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>2</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>3</td><td></td><td>2</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>5</td><td></td><td></td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>Graph</td><td>4</td><td></td><td>3</td><td></td><td>11</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>3</td><td></td><td>7</td><td></td><td></td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Heuristic Approach</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Hierarchical Clustering</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>11</td><td>4</td><td></td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>4</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>6</td><td>9</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td>1</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>20</td><td>47</td><td>16</td><td>8</td><td>2</td><td>5</td><td>12</td><td></td><td>2</td></tr><tr><td>Lemmatization</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Low-Resource</td><td></td><td>5</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td></td><td>10</td><td>6</td><td>1</td><td>5</td><td></td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>7</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>1</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Out-of-domain</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Planning Domain Descrition Language</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>5</td><td>18</td><td>2</td><td>1</td><td>2</td><td>4</td><td>1</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td><td>3</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Question Answering</td><td></td><td>5</td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>2</td><td>5</td><td></td><td></td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Recommendation</td><td>2</td><td></td><td></td><td>1</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Recommender System</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Reconstruction Loss</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>2</td><td></td><td></td><td>3</td><td>6</td><td>1</td><td></td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Relation Extraction</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>2</td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Rerank</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>4</td><td></td><td>1</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>1</td><td>1</td><td>2</td><td>3</td><td>12</td><td>1</td><td></td><td>11</td></tr><tr><td>Simulator</td><td></td><td>1</td><td>1</td><td>2</td><td>3</td><td>12</td><td>1</td><td></td><td>11</td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Style Transfer</td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Summarization</td><td>1</td><td>3</td><td>1</td><td>1</td><td>1</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td>1</td><td>1</td><td>4</td><td></td><td>4</td><td></td><td></td><td>1</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>4</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td>13</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Topic Model</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Topic Modeling</td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>3</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td></td><td>4</td><td>11</td><td></td><td>2</td><td></td><td></td><td>5</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>3</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td>1</td><td>1</td><td>4</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Word Embedding</td><td></td><td>2</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Word2vec</td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Zero Trust</td><td></td><td></td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>2</td><td>3</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-56>cs.CL (56)</h2><h3 id=156--1340-textitlinkprompt-natural-and-universal-adversarial-attacks-on-prompt-based-language-models-yue-xu-et-al-2024>(1/56 | 1/340) $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models (Yue Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Xu, Wenjie Wang. (2024)<br><strong>$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models</strong><br><button class=copy-to-clipboard title="$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Pre-trained Language Model, Pre-trained Language Model, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16432v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16432v2.pdf filename=2403.16432v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-based</b> learning is a new language model training paradigm that adapts the <b>Pre-trained</b> <b>Language</b> <b>Models</b> <b>(PLMs)</b> to downstream tasks, which revitalizes the performance <b>benchmarks</b> across various natural language processing (NLP) tasks. Instead of using a fixed <b>prompt</b> template to <b>fine-tune</b> the model, some research demonstrates the effectiveness of searching for the <b>prompt</b> via optimization. Such <b>prompt</b> optimization process of <b>prompt-based</b> learning on <b>PLMs</b> also gives insight into generating <b>adversarial</b> <b>prompts</b> to mislead the model, raising concerns about the <b>adversarial</b> <b>vulnerability</b> of this paradigm. Recent studies have shown that universal <b>adversarial</b> <b>triggers</b> (UATs) can be generated to alter not only the predictions of the target <b>PLMs</b> but also the prediction of corresponding <b>Prompt-based</b> <b>Fine-tuning</b> Models (PFMs) under the <b>prompt-based</b> learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\textit{LinkPrompt}$, an <b>adversarial</b> <b>attack</b> algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target <b>PLMs</b> and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\textit{LinkPrompt}$ to open-sourced <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> Llama2 and API-accessed <b>LLM</b> <b>GPT-3.5-turbo.</b></p></p class="citation"></blockquote><h3 id=256--2340-the-strong-pull-of-prior-knowledge-in-large-language-models-and-its-impact-on-emotion-recognition-georgios-chochlakis-et-al-2024>(2/56 | 2/340) The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition (Georgios Chochlakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan. (2024)<br><strong>The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition</strong><br><button class=copy-to-clipboard title="The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, Emotion Recognition, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17125v1.pdf filename=2403.17125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>Learning</b> <b>(ICL)</b> has emerged as a powerful paradigm for performing natural language tasks with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> without updating the models&rsquo; parameters, in contrast to the traditional gradient-based <b>finetuning.</b> The promise of <b>ICL</b> is that the <b>LLM</b> can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of <b>LLMs</b> to perform tasks in this <b>few-shot</b> manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, <b>LLMs</b> are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as <b>emotion</b> <b>recognition,</b> where the mapping from text to <b>emotions</b> <b>can</b> differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to explicitly quantify the consistency of proxies of <b>LLM</b> priors and their pull on the posteriors. We show that <b>LLMs</b> have strong yet inconsistent priors in <b>emotion</b> <b>recognition</b> that ossify their predictions. We also find that the larger the model, the stronger these effects become. Our results suggest that caution is needed when using <b>ICL</b> with larger <b>LLMs</b> for affect-centered tasks outside their pre-training domain and when interpreting <b>ICL</b> results.</p></p class="citation"></blockquote><h3 id=356--3340-iterative-refinement-of-project-level-code-context-for-precise-code-generation-with-compiler-feedback-zhangqian-bi-et-al-2024>(3/56 | 3/340) Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback (Zhangqian Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin. (2024)<br><strong>Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback</strong><br><button class=copy-to-clipboard title="Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SE, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16792v1.pdf filename=2403.16792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown remarkable progress in automated <b>code</b> <b>generation.</b> Yet, incorporating <b>LLM-based</b> <b>code</b> <b>generation</b> into real-life software projects poses challenges, as the generated <b>code</b> <b>may</b> contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the <b>prompts</b> of <b>LLMs,</b> we must find ways to allow the model to explore the project-level <b>code</b> <b>context.</b> To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level <b>code</b> <b>context</b> for precise <b>code</b> <b>generation,</b> guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated <b>code</b> <b>and</b> the project&rsquo;s context. It then iteratively aligns and fixes the identified errors using information extracted from the <b>code</b> <b>repository.</b> We integrate ProCoder with two representative <b>LLMs,</b> i.e., <b>GPT-3.5-Turbo</b> and <b>Code</b> <b>Llama</b> (13B), and apply it to Python <b>code</b> <b>generation.</b> Experimental results show that ProCoder significantly improves the vanilla <b>LLMs</b> by over 80% in generating <b>code</b> <b>dependent</b> on project context, and consistently outperforms the existing retrieval-based <b>code</b> <b>generation</b> baselines.</p></p class="citation"></blockquote><h3 id=456--4340-llms-are-few-shot-in-context-low-resource-language-learners-samuel-cahyawijaya-et-al-2024>(4/56 | 4/340) LLMs Are Few-Shot In-Context Low-Resource Language Learners (Samuel Cahyawijaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Cahyawijaya, Holy Lovenia, Pascale Fung. (2024)<br><strong>LLMs Are Few-Shot In-Context Low-Resource Language Learners</strong><br><button class=copy-to-clipboard title="LLMs Are Few-Shot In-Context Low-Resource Language Learners" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, High-Resource, Low-Resource, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16512v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16512v2.pdf filename=2403.16512v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> <b>(ICL)</b> empowers <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to perform diverse tasks in underrepresented languages using only short <b>in-context</b> <b>information,</b> offering a crucial avenue for narrowing the gap between <b>high-resource</b> and <b>low-resource</b> languages. Nonetheless, there is only a handful of works explored <b>ICL</b> for <b>low-resource</b> languages with most of them focusing on relatively <b>high-resource</b> languages, such as French and Spanish. In this work, we extensively study <b>ICL</b> and its cross-lingual variation (X-ICL) on 25 <b>low-resource</b> and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of <b>ICL</b> with <b>LLMs</b> in <b>low-resource</b> languages but also identifies the shortcomings of <b>in-context</b> <b>label</b> alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of <b>ICL</b> for <b>low-resource</b> languages. Our study concludes the significance of <b>few-shot</b> <b>in-context</b> <b>information</b> on enhancing the <b>low-resource</b> understanding quality of <b>LLMs</b> through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted <b>low-resource</b> and the <b>high-resource</b> language that the model is proficient in. Our work highlights the importance of advancing <b>ICL</b> research, particularly for <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=556--5340-a-hybrid-approach-to-aspect-based-sentiment-analysis-using-transfer-learning-gaurav-negi-et-al-2024>(5/56 | 5/340) A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning (Gaurav Negi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Negi, Rajdeep Sarkar, Omnia Zayed, Paul Buitelaar. (2024)<br><strong>A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning</strong><br><button class=copy-to-clipboard title="A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Supervised Learning, Transfer Learning, Weakly-supervised Learning, Aspect-based Sentiment Analysis, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17254v1.pdf filename=2403.17254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-Based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA) aims to identify terms or multiword expressions (MWEs) on which <b>sentiments</b> <b>are</b> expressed and the <b>sentiment</b> <b>polarities</b> associated with them. The development of <b>supervised</b> models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based <b>Sentiment</b> <b>Analysis</b> using <b>transfer</b> <b>learning.</b> The approach focuses on generating <b>weakly-supervised</b> annotations by exploiting the strengths of both <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by <b>LLMs,</b> as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect <b>sentiment</b> <b>classification.</b> Keywords: Aspect Based <b>Sentiment</b> <b>Analysis,</b> Syntactic Parsing, <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b></p></p class="citation"></blockquote><h3 id=656--6340-outcome-constrained-large-language-models-for-countering-hate-speech-lingzi-hong-et-al-2024>(6/56 | 6/340) Outcome-Constrained Large Language Models for Countering Hate Speech (Lingzi Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song. (2024)<br><strong>Outcome-Constrained Large Language Models for Countering Hate Speech</strong><br><button class=copy-to-clipboard title="Outcome-Constrained Large Language Models for Countering Hate Speech" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Reinforcement Learning, Transformer, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17146v1.pdf filename=2403.17146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Counterspeech that challenges or responds to hate speech has been seen as an alternative to mitigate the negative impact of hate speech and foster productive online communications. Research endeavors have been directed to using language models for the automatic generation of counterspeech to assist efforts in combating online hate. Existing research focuses on the generation of counterspeech with certain linguistic attributes, such as being polite, informative, and intent-driven. However, it remains unclear what impact the counterspeech might have in an online environment. We first explore methods that utilize <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to generate counterspeech constrained by potential conversation outcomes. We build two conversation outcome classifiers that predict the incivility level and the hater reentry behavior following replies to hate with Reddit data, then propose four methods to incorporate the desired outcomes, i.e., low conversation incivility and non-hateful hater reentry, into the <b>text</b> <b>generation</b> process, including <b>Prompt</b> with Instructions, <b>Prompt</b> and Select, <b>LLM</b> <b>finetune,</b> and <b>LLM</b> <b>transformer</b> <b>reinforcement</b> <b>learning</b> (TRL). Evaluation results show effective strategies to generate outcome-constrained counterspeech and the linguistic characteristics of <b>texts</b> <b>generated</b> by different methods.</p></p class="citation"></blockquote><h3 id=756--7340-attribute-first-then-generate-locally-attributable-grounded-text-generation-aviv-slobodkin-et-al-2024>(7/56 | 7/340) Attribute First, then Generate: Locally-attributable Grounded Text Generation (Aviv Slobodkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan. (2024)<br><strong>Attribute First, then Generate: Locally-attributable Grounded Text Generation</strong><br><button class=copy-to-clipboard title="Attribute First, then Generate: Locally-attributable Grounded Text Generation" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fact Verification, Fact Verification, Question Answering, Text Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17104v1.pdf filename=2403.17104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent efforts to address hallucinations in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have focused on attributed <b>text</b> <b>generation,</b> which supplements generated <b>texts</b> <b>with</b> citations of supporting sources for post-generation <b>fact-checking</b> <b>and</b> corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable <b>text</b> <b>generation</b> approach, prioritizing concise attributions. Our method, named <code>Attribute First, then Generate'', breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (</code>select first&rsquo;&rsquo;) and then conditioning the generation process on them (<code>then generate''), we ensure these segments also act as the output's fine-grained attributions (</code>select&rsquo;&rsquo; becomes ``attribute&rsquo;&rsquo;). Tested on Multi-document <b>Summarization</b> and Long-form <b>Question-answering,</b> <b>our</b> method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for <b>fact</b> <b>verification</b> by human assessors.</p></p class="citation"></blockquote><h3 id=856--8340-grammatical-vs-spelling-error-correction-an-investigation-into-the-responsiveness-of-transformer-based-language-models-using-bart-and-marianmt-rohit-raju-et-al-2024>(8/56 | 8/340) Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT (Rohit Raju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS. (2024)<br><strong>Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT</strong><br><button class=copy-to-clipboard title="Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Optical Character Recognition, Fine-tuning, Transfer Learning, BART, Transformer, Automatic Speech Recognition, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16655v1.pdf filename=2403.16655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>continues</b> to remain a relevant form of representation for information. <b>Text</b> <b>documents</b> are created either in digital native platforms or through the conversion of other media files such as images and <b>speech.</b> <b>While</b> the digital native <b>text</b> <b>is</b> invariably obtained through physical or virtual keyboards, technologies such as <b>OCR</b> and <b>speech</b> <b>recognition</b> are utilized to transform the images and <b>speech</b> <b>signals</b> into <b>text</b> <b>content.</b> All these variety of mechanisms of <b>text</b> <b>generation</b> also introduce errors into the captured <b>text.</b> <b>This</b> project aims at analyzing different kinds of error that occurs in <b>text</b> <b>documents.</b> The work employs two of the advanced deep neural network-based language models, namely, <b>BART</b> and MarianMT, to rectify the anomalies present in the <b>text.</b> <b>Transfer</b> <b>learning</b> of these models with available dataset is performed to <b>finetune</b> their capacity for error correction. A comparative study is conducted to investigate the effectiveness of these models in handling each of the defined error categories. It is observed that while both models can bring down the erroneous sentences by 20+%, <b>BART</b> can handle spelling errors far better (24.6%) than grammatical errors (8.8%).</p></p class="citation"></blockquote><h3 id=956--9340-lara-linguistic-adaptive-retrieval-augmented-llms-for-multi-turn-intent-classification-liu-junhua-et-al-2024>(9/56 | 9/340) LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification (Liu Junhua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Junhua, Tan Yong Keat, Fu Bin. (2024)<br><strong>LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification</strong><br><button class=copy-to-clipboard title="LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Chatbot, Text Classification, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16504v1.pdf filename=2403.16504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following the significant achievements of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> researchers have employed <b>in-context</b> <b>learning</b> for <b>text</b> <b>classification</b> tasks. However, these studies focused on monolingual, single-turn classification tasks. In this paper, we introduce LARA (Linguistic-Adaptive Retrieval-Augmented Language Models), designed to enhance accuracy in multi-turn classification tasks across six languages, accommodating numerous intents in <b>chatbot</b> interactions. Multi-turn intent classification is notably challenging due to the complexity and evolving nature of conversational contexts. LARA tackles these issues by combining a <b>fine-tuned</b> smaller model with a retrieval-augmented mechanism, integrated within the architecture of <b>LLMs.</b> This integration allows LARA to dynamically utilize past dialogues and relevant intents, thereby improving the understanding of the context. Furthermore, our adaptive retrieval techniques bolster the cross-lingual capabilities of <b>LLMs</b> without extensive retraining and <b>fine-tune.</b> Comprehensive experiments demonstrate that LARA achieves state-of-the-art performance on multi-turn intent classification tasks, enhancing the average accuracy by 3.67% compared to existing methods.</p></p class="citation"></blockquote><h3 id=1056--10340-efficient-information-extraction-in-few-shot-relation-classification-through-contrastive-representation-learning-philipp-borchert-et-al-2024>(10/56 | 10/340) Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning (Philipp Borchert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens. (2024)<br><strong>Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning</strong><br><button class=copy-to-clipboard title="Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 65<br>Keywords: Contrastive Learning, Few-shot, Knowledge Distillation, Low-Resource, Representation Learning, Information Retrieval, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16543v1.pdf filename=2403.16543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in <b>few-shot</b> relation classification. <b>Representations</b> <b>of</b> textual data extract rich <b>information</b> <b>spanning</b> the domain, entities, and relations. In this paper, we introduce a novel approach to enhance <b>information</b> <b>extraction</b> combining multiple sentence <b>representations</b> <b>and</b> <b>contrastive</b> <b>learning.</b> While <b>representations</b> <b>in</b> relation classification are commonly extracted using entity marker tokens, we argue that substantial <b>information</b> <b>within</b> the internal model <b>representations</b> <b>remains</b> untapped. To address this, we propose aligning multiple sentence <b>representations,</b> <b>such</b> as the [CLS] token, the [MASK] token used in <b>prompting,</b> and entity marker tokens. Our method employs <b>contrastive</b> <b>learning</b> to extract complementary discriminative <b>information</b> <b>from</b> these individual <b>representations.</b> <b>This</b> is particularly relevant in <b>low-resource</b> settings where <b>information</b> <b>is</b> scarce. Leveraging multiple sentence <b>representations</b> <b>is</b> especially effective in <b>distilling</b> discriminative <b>information</b> <b>for</b> relation classification when additional <b>information,</b> <b>like</b> relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.</p></p class="citation"></blockquote><h3 id=1156--11340-instupr--instruction-based-unsupervised-passage-reranking-with-large-language-models-chao-wei-huang-et-al-2024>(11/56 | 11/340) InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models (Chao-Wei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao-Wei Huang, Yun-Nung Chen. (2024)<br><strong>InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models</strong><br><button class=copy-to-clipboard title="InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Rerank, Unsupervised Learning, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16435v1.pdf filename=2403.16435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces InstUPR, an <b>unsupervised</b> passage <b>reranking</b> method based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Different from existing approaches that rely on extensive training with query-document pairs or retrieval-specific <b>instructions,</b> <b>our</b> method leverages the <b>instruction-following</b> <b>capabilities</b> of <b>instruction-tuned</b> <b>LLMs</b> for passage <b>reranking</b> without any additional <b>fine-tuning.</b> To achieve this, we introduce a soft score aggregation technique and employ pairwise <b>reranking</b> for <b>unsupervised</b> passage <b>reranking.</b> Experiments on the BEIR <b>benchmark</b> demonstrate that InstUPR outperforms <b>unsupervised</b> baselines as well as an <b>instruction-tuned</b> <b>reranker,</b> highlighting its effectiveness and superiority. Source code to reproduce all experiments is open-sourced at <a href=https://github.com/MiuLab/InstUPR>https://github.com/MiuLab/InstUPR</a></p></p class="citation"></blockquote><h3 id=1256--12340-a-comparison-of-human-gpt-35-and-gpt-4-performance-in-a-university-level-coding-course-will-yeadon-et-al-2024>(12/56 | 12/340) A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course (Will Yeadon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Will Yeadon, Alex Peach, Craig P. Testrow. (2024)<br><strong>A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course</strong><br><button class=copy-to-clipboard title="A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: ChatGPT, GPT, GPT-3, GPT-3.5, GPT-4, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16977v1.pdf filename=2403.16977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates the performance of <b>ChatGPT</b> variants, <b>GPT-3.5</b> and <b>GPT-4,</b> both with and without <b>prompt</b> engineering, against solely student work and a mixed category containing both student and <b>GPT-4</b> contributions in university-level physics coding assignments using the Python language. Comparing 50 student submissions to 50 AI-generated submissions across different categories, and marked blindly by three independent markers, we amassed $n = 300$ data points. Students averaged 91.9% (SE:0.4), surpassing the highest performing AI submission category, <b>GPT-4</b> with <b>prompt</b> engineering, which scored 81.1% (SE:0.8) - a statistically significant difference (p = $2.482 \times 10^{-10}$). <b>Prompt</b> engineering significantly improved scores for both <b>GPT-4</b> (p = $1.661 \times 10^{-4}$) and <b>GPT-3.5</b> (p = $4.967 \times 10^{-9}$). Additionally, the blinded markers were tasked with guessing the authorship of the submissions on a four-point Likert scale from <code>Definitely AI' to </code>Definitely Human&rsquo;. They accurately identified the authorship, with 92.1% of the work categorized as &lsquo;Definitely Human&rsquo; being human-authored. Simplifying this to a binary <code>AI' or </code>Human&rsquo; categorization resulted in an average accuracy rate of 85.3%. These findings suggest that while AI-generated work closely approaches the quality of university students&rsquo; work, it often remains detectable by human evaluators.</p></p class="citation"></blockquote><h3 id=1356--13340-a-comparative-analysis-of-embedding-models-for-patent-similarity-grazia-sveva-ascione-et-al-2024>(13/56 | 13/340) A comparative analysis of embedding models for patent similarity (Grazia Sveva Ascione et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Grazia Sveva Ascione, Valerio Sterzi. (2024)<br><strong>A comparative analysis of embedding models for patent similarity</strong><br><button class=copy-to-clipboard title="A comparative analysis of embedding models for patent similarity" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Doc2vec, Transformer, Word2vec, Contextual Embedding, Domain Adaptation, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16630v1.pdf filename=2403.16630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper makes two contributions to the field of text-based patent similarity. First, it compares the performance of different kinds of patent-specific pretrained embedding models, namely static <b>word</b> <b>embeddings</b> (such as <b>word2vec</b> and <b>doc2vec</b> models) and <b>contextual</b> <b>word</b> <b>embeddings</b> (such as <b>transformers</b> based models), on the task of patent similarity calculation. Second, it compares specifically the performance of Sentence <b>Transformers</b> (SBERT) architectures with different training phases on the patent similarity task. To assess the models&rsquo; performance, we use information about patent interferences, a phenomenon in which two or more patent claims belonging to different patent applications are proven to be overlapping by patent examiners. Therefore, we use these interferences cases as a proxy for maximum similarity between two patents, treating them as ground-truth to evaluate the performance of the different embedding models. Our results point out that, first, Patent SBERT-adapt-ub, the <b>domain</b> <b>adaptation</b> of the pretrained Sentence <b>Transformer</b> architecture proposed in this research, outperforms the current state-of-the-art in patent similarity. Second, they show that, in some cases, large static models performances are still comparable to <b>contextual</b> <b>ones</b> when trained on extensive data; thus, we believe that the superiority in the performance of <b>contextual</b> <b>embeddings</b> may not be related to the actual architecture but rather to the way the training phase is performed.</p></p class="citation"></blockquote><h3 id=1456--14340-visually-guided-generative-text-layout-pre-training-for-document-intelligence-zhiming-mao-et-al-2024>(14/56 | 14/340) Visually Guided Generative Text-Layout Pre-training for Document Intelligence (Zhiming Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong. (2024)<br><strong>Visually Guided Generative Text-Layout Pre-training for Document Intelligence</strong><br><button class=copy-to-clipboard title="Visually Guided Generative Text-Layout Pre-training for Document Intelligence" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 53<br>Keywords: Optical Character Recognition, Benchmarking, Transformer, Document Classification, Information Retrieval, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16516v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16516v2.pdf filename=2403.16516v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior study shows that pre-training techniques can boost the performance of visual <b>document</b> <b>understanding</b> (VDU), which typically requires models to gain abilities to perceive and reason both <b>document</b> <b>texts</b> and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a <b>document</b> <b>image,</b> the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long <b>documents</b> <b>by</b> <b>Transformers,</b> we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive <b>documents</b> <b>of</b> any length. ViTLP can function as a native <b>OCR</b> model to localize and recognize texts of <b>document</b> <b>images.</b> Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on <b>benchmark</b> VDU tasks, including <b>information</b> <b>extraction,</b> <b>document</b> <b>classification,</b> and <b>document</b> <b>question</b> <b>answering.</b></p></p class="citation"></blockquote><h3 id=1556--15340-metaaligner-conditional-weak-to-strong-correction-for-generalizable-multi-objective-alignment-of-language-models-kailai-yang-et-al-2024>(15/56 | 15/340) MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models (Kailai Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailai Yang, Zhiwei Liu, Qianqian Xie, Tianlin Zhang, Nirui Song, Jimin Huang, Ziyan Kuang, Sophia Ananiadou. (2024)<br><strong>MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models</strong><br><button class=copy-to-clipboard title="MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Zero-shot, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17141v1.pdf filename=2403.17141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates <b>zero-shot</b> preference alignment for unseen objectives via <b>in-context</b> <b>learning.</b> Experimental results show that MetaAligner achieves significant and balanced improvements in multi-objective alignments on 11 policy models with up to 63x more parameters, and outperforms previous alignment methods with down to 22.27x less computational resources. The model also accurately aligns with unseen objectives, marking the first step towards generalizable multi-objective preference alignment.</p></p class="citation"></blockquote><h3 id=1656--16340-cross-lingual-contextualized-phrase-retrieval-huayang-li-et-al-2024>(16/56 | 16/340) Cross-lingual Contextualized Phrase Retrieval (Huayang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huayang Li, Deng Cai, Zhi Qu, Qu Cui, Hidetaka Kamigaito, Lemao Liu, Taro Watanabe. (2024)<br><strong>Cross-lingual Contextualized Phrase Retrieval</strong><br><button class=copy-to-clipboard title="Cross-lingual Contextualized Phrase Retrieval" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Dense Retrieval, Neural Machine Translation, BERTScore, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16820v1.pdf filename=2403.16820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Phrase-level <b>dense</b> <b>retrieval</b> has shown many appealing characteristics in downstream NLP tasks by leveraging the fine-grained information that phrases offer. In our work, we propose a new task formulation of <b>dense</b> <b>retrieval,</b> cross-lingual contextualized phrase retrieval, which aims to augment cross-lingual applications by addressing polysemy using context information. However, the lack of specific training data and models are the primary challenges to achieve our goal. As a result, we extract pairs of cross-lingual phrases using word alignment information automatically induced from parallel sentences. Subsequently, we train our Cross-lingual Contextualized Phrase Retriever (CCPR) using <b>contrastive</b> <b>learning,</b> which encourages the hidden representations of phrases with similar contexts and semantics to align closely. Comprehensive experiments on both the cross-lingual phrase retrieval task and a downstream task, i.e, <b>machine</b> <b>translation,</b> demonstrate the effectiveness of CCPR. On the phrase retrieval task, CCPR surpasses baselines by a significant margin, achieving a top-1 accuracy that is at least 13 points higher. When utilizing CCPR to augment the <b>large-language-model-based</b> <b>translator,</b> <b>it</b> achieves average gains of 0.7 and 1.5 in <b>BERTScore</b> for translations from X=>En and vice versa, respectively, on WMT16 dataset. Our code and data are available at \url{https://github.com/ghrua/ccpr_release}.</p></p class="citation"></blockquote><h3 id=1756--17340-few-shot-named-entity-recognition-via-superposition-concept-discrimination-jiawei-chen-et-al-2024>(17/56 | 17/340) Few-shot Named Entity Recognition via Superposition Concept Discrimination (Jiawei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Chen, Hongyu Lin, Xianpei Han, Yaojie Lu, Shanshan Jiang, Bin Dong, Le Sun. (2024)<br><strong>Few-shot Named Entity Recognition via Superposition Concept Discrimination</strong><br><button class=copy-to-clipboard title="Few-shot Named Entity Recognition via Superposition Concept Discrimination" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Active Learning, Few-shot, Named Entity Recognition, Named Entity Recognition, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16463v1.pdf filename=2403.16463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>NER</b> aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, <b>few-shot</b> <b>NER</b> is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity <b>stemming</b> from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an <b>active</b> <b>learning</b> paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the <b>few-shot</b> <b>NER</b> performance with minimal additional efforts.</p></p class="citation"></blockquote><h3 id=1856--18340-kit-19-a-comprehensive-korean-instruction-toolkit-on-19-tasks-for-fine-tuning-korean-large-language-models-dongjun-jang-et-al-2024>(18/56 | 18/340) KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models (Dongjun Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjun Jang, Sungjoo Byun, Hyemi Jo, Hyopil Shin. (2024)<br><strong>KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models</strong><br><button class=copy-to-clipboard title="KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, ChatGPT, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16444v1.pdf filename=2403.16444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>Tuning</b> on <b>Large</b> <b>Language</b> <b>Models</b> is an essential process for model to function well and achieve high performance in specific tasks. Accordingly, in mainstream languages such as English, <b>instruction-based</b> <b>datasets</b> are being constructed and made publicly available. In the case of Korean, publicly available models and datasets all rely on using the output of <b>ChatGPT</b> or translating datasets built in English. In this paper, We introduce \textit{KIT-19} as an <b>instruction</b> <b>dataset</b> for the development of <b>LLM</b> in Korean. \textit{KIT-19} is a dataset created in an <b>instruction</b> <b>format,</b> comprising 19 existing open-source datasets for Korean NLP tasks. In this paper, we train a Korean Pretrained <b>LLM</b> using \textit{KIT-19} to demonstrate its effectiveness. The experimental results show that the model trained on \textit{KIT-19} significantly outperforms existing Korean <b>LLMs.</b> Based on the its quality and empirical results, this paper proposes that \textit{KIT-19} has the potential to make a substantial contribution to the future improvement of Korean <b>LLMs&rsquo;</b> performance.</p></p class="citation"></blockquote><h3 id=1956--19340-ontology-completion-with-natural-language-inference-and-concept-embeddings-an-analysis-na-li-et-al-2024>(19/56 | 19/340) Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis (Na Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Na Li, Thomas Bailleux, Zied Bouraoui, Steven Schockaert. (2024)<br><strong>Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis</strong><br><button class=copy-to-clipboard title="Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Natural Language Inference, Natural Language Inference, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17216v1.pdf filename=2403.17216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of finding plausible knowledge that is missing from a given ontology, as a generalisation of the well-studied taxonomy expansion task. One line of work treats this task as a <b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> problem, thus relying on the knowledge captured by language models to identify the missing knowledge. Another line of work uses concept embeddings to identify what different concepts have in common, taking inspiration from cognitive models for category based induction. These two approaches are intuitively complementary, but their effectiveness has not yet been compared. In this paper, we introduce a <b>benchmark</b> for evaluating ontology completion methods and thoroughly analyse the strengths and weaknesses of both approaches. We find that both approaches are indeed complementary, with hybrid strategies achieving the best overall results. We also find that the task is highly challenging for <b>Large</b> <b>Language</b> <b>Models,</b> even after <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=2056--20340-evaluating-shortest-edit-script-methods-for-contextual-lemmatization-olia-toporkov-et-al-2024>(20/56 | 20/340) Evaluating Shortest Edit Script Methods for Contextual Lemmatization (Olia Toporkov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olia Toporkov, Rodrigo Agerri. (2024)<br><strong>Evaluating Shortest Edit Script Methods for Contextual Lemmatization</strong><br><button class=copy-to-clipboard title="Evaluating Shortest Edit Script Methods for Contextual Lemmatization" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Out-of-domain, Lemmatization, In-context Learning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16968v1.pdf filename=2403.16968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern contextual lemmatizers often rely on automatically induced Shortest Edit Scripts (SES), namely, the number of edit operations to transform a word form into its lemma. In fact, different methods of computing SES have been proposed as an integral component in the architecture of several state-of-the-art contextual lemmatizers currently available. However, previous work has not investigated the direct impact of SES in the final <b>lemmatization</b> performance. In this paper we address this issue by focusing on <b>lemmatization</b> as a token classification task where the only input that the model receives is the word-label pairs in context, where the labels correspond to previously induced SES. Thus, by modifying in our <b>lemmatization</b> system only the SES labels that the model needs to learn, we may then objectively conclude which SES representation produces the best <b>lemmatization</b> results. We experiment with seven languages of different morphological complexity, namely, English, Spanish, Basque, Russian, Czech, Turkish and Polish, using multilingual and language-specific <b>pre-trained</b> <b>masked</b> <b>language</b> encoder-only models as a backbone to build our lemmatizers. Comprehensive experimental results, both in- and <b>out-of-domain,</b> indicate that computing the casing and edit operations separately is beneficial overall, but much more clearly for languages with high-inflected morphology. Notably, multilingual <b>pre-trained</b> <b>language</b> <b>models</b> consistently outperform their language-specific counterparts in every evaluation setting.</p></p class="citation"></blockquote><h3 id=2156--21340-synthetic-data-generation-and-joint-learning-for-robust-code-mixed-translation-kartik-et-al-2024>(21/56 | 21/340) Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation (Kartik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kartik, Sanjana Soni, Anoop Kunchukuttan, Tanmoy Chakraborty, Md Shad Akhtar. (2024)<br><strong>Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation</strong><br><button class=copy-to-clipboard title="Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Low-Resource, Parameter Sharing, Zero-shot, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16771v1.pdf filename=2403.16771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in <b>low-resource</b> setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English <b>machine</b> <b>translation.</b> First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by <b>parameter</b> <b>sharing</b> across clean and noisy words. Further, we show the adaptability of RCMT in a <b>zero-shot</b> setup for Bengalish to English translation. Our evaluation and comprehensive analyses qualitatively and quantitatively demonstrate the superiority of RCMT over state-of-the-art code-mixed and robust translation methods.</p></p class="citation"></blockquote><h3 id=2256--22340-can-large-language-models-or-humans-distill-text-nicolas-audinet-de-pieuchon-et-al-2024>(22/56 | 22/340) Can Large Language Models (or Humans) Distill Text? (Nicolas Audinet de Pieuchon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson. (2024)<br><strong>Can Large Language Models (or Humans) Distill Text?</strong><br><button class=copy-to-clipboard title="Can Large Language Models (or Humans) Distill Text?" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16584v1.pdf filename=2403.16584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to <b>distill</b> text: to remove the textual traces of an undesired forbidden variable. We employ a range of <b>LLMs</b> with varying architectures and training approaches to <b>distill</b> text by identifying and removing information about the target variable while preserving other relevant signals. Our findings shed light on the strengths and limitations of <b>LLMs</b> in addressing the <b>distillation</b> and provide insights into the strategies for leveraging these models in computational social science investigations involving text data. In particular, we show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still clearly detectable to machine learning classifiers post-LLM-distillation. Furthermore, we find that human annotators also struggle to <b>distill</b> sentiment while preserving other semantic content. This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of <b>distillation</b> methods that achieve statistical independence in representation space if this is difficult for human coders operating on raw text to attain.</p></p class="citation"></blockquote><h3 id=2356--23340-is-there-a-one-model-fits-all-approach-to-information-extraction-revisiting-task-definition-biases-wenhao-huang-et-al-2024>(23/56 | 23/340) Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases (Wenhao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao. (2024)<br><strong>Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases</strong><br><button class=copy-to-clipboard title="Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Information Retrieval, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16396v1.pdf filename=2403.16396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Definition bias is a negative phenomenon that can mislead models. Definition bias in <b>information</b> <b>extraction</b> appears not only across datasets from different domains but also within datasets sharing the same domain. We identify two types of definition bias in IE: bias among <b>information</b> <b>extraction</b> datasets and bias between <b>information</b> <b>extraction</b> datasets and <b>instruction</b> <b>tuning</b> datasets. To systematically investigate definition bias, we conduct three probing experiments to quantitatively analyze it and discover the limitations of unified <b>information</b> <b>extraction</b> and <b>large</b> <b>language</b> <b>models</b> in solving definition bias. To mitigate definition bias in <b>information</b> <b>extraction,</b> we propose a multi-stage framework consisting of definition bias measurement, bias-aware <b>fine-tuning,</b> and task-specific bias mitigation. Experimental results demonstrate the effectiveness of our framework in addressing definition bias. Resources of this paper can be found at <a href=https://github.com/EZ-hwh/definition-bias>https://github.com/EZ-hwh/definition-bias</a></p></p class="citation"></blockquote><h3 id=2456--24340-splice-a-singleton-enhanced-pipeline-for-coreference-resolution-yilun-zhu-et-al-2024>(24/56 | 24/340) SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution (Yilun Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes. (2024)<br><strong>SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution</strong><br><button class=copy-to-clipboard title="SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Clustering, Out-of-domain, Coreference Resolution, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17245v1.pdf filename=2403.17245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural <b>coreference</b> <b>resolution</b> for English have been hampered by the lack of singleton mention spans in the OntoNotes <b>benchmark.</b> This paper addresses this limitation by combining predicted mentions from existing nested <b>NER</b> systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and <b>coreference</b> <b>resolution</b> system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the <b>out-of-domain</b> (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields results comparable to end-to-end systems for OntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error analysis for mention detection and delve into its impact on <b>coreference</b> <b>clustering,</b> revealing that precision improvements deliver more substantial benefits than increases in recall for resolving <b>coreference</b> <b>chains.</b></p></p class="citation"></blockquote><h3 id=2556--25340-nsina-a-news-corpus-for-sinhala-hansi-hettiarachchi-et-al-2024>(25/56 | 25/340) NSINA: A News Corpus for Sinhala (Hansi Hettiarachchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe. (2024)<br><strong>NSINA: A News Corpus for Sinhala</strong><br><button class=copy-to-clipboard title="NSINA: A News Corpus for Sinhala" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16571v1.pdf filename=2403.16571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The introduction of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in <b>low-resource</b> languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited <b>benchmarking</b> datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting <b>LLMs</b> to Sinhala, offering valuable resources and <b>benchmarks</b> for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.</p></p class="citation"></blockquote><h3 id=2656--26340-can-machine-translation-bridge-multilingual-pretraining-and-cross-lingual-transfer-learning-shaoxiong-ji-et-al-2024>(26/56 | 26/340) Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning? (Shaoxiong Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoxiong Ji, Timothee Mickus, Vincent Segonne, Jörg Tiedemann. (2024)<br><strong>Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?</strong><br><button class=copy-to-clipboard title="Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Fine-tuning, Representation Learning, Natural Language Understanding, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16777v1.pdf filename=2403.16777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilingual pretraining and <b>fine-tuning</b> have remarkably succeeded in various <b>natural</b> <b>language</b> <b>processing</b> tasks. Transferring <b>representations</b> <b>from</b> one language to another is especially crucial for cross-lingual learning. One can expect <b>machine</b> <b>translation</b> objectives to be well suited to fostering such capabilities, as they involve the explicit alignment of semantically equivalent sentences from different languages. This paper investigates the potential benefits of employing <b>machine</b> <b>translation</b> as a continued training objective to enhance language <b>representation</b> <b>learning,</b> bridging multilingual pretraining and cross-lingual applications. We study this question through two lenses: a quantitative evaluation of the performance of existing models and an analysis of their latent <b>representations.</b> <b>Our</b> results show that, contrary to expectations, <b>machine</b> <b>translation</b> as the continued training fails to enhance cross-lingual <b>representation</b> <b>learning</b> in multiple cross-lingual <b>natural</b> <b>language</b> <b>understanding</b> tasks. We conclude that explicit sentence-level alignment in the cross-lingual scenario is detrimental to cross-lingual transfer pretraining, which has important implications for future cross-lingual transfer studies. We furthermore provide evidence through similarity measures and investigation of parameters that this lack of positive influence is due to output separability &ndash; which we argue is of use for <b>machine</b> <b>translation</b> but detrimental elsewhere.</p></p class="citation"></blockquote><h3 id=2756--27340-semantically-enriched-cross-lingual-sentence-embeddings-for-crisis-related-social-media-texts-rabindra-lamsal-et-al-2024>(27/56 | 27/340) Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts (Rabindra Lamsal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera. (2024)<br><strong>Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts</strong><br><button class=copy-to-clipboard title="Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Clustering, Sentence Embedding, Contextual Embedding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16614v1.pdf filename=2403.16614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tasks such as semantic search and <b>clustering</b> on crisis-related social media texts enhance our comprehension of crisis discourse, aiding decision-making and targeted interventions. <b>Pre-trained</b> <b>language</b> <b>models</b> have advanced performance in crisis informatics, but their <b>contextual</b> <b>embeddings</b> lack semantic meaningfulness. Although the CrisisTransformers family includes a <b>sentence</b> <b>encoder</b> to address the semanticity issue, it remains monolingual, processing only English texts. Furthermore, employing separate models for different languages leads to embeddings in distinct vector spaces, introducing challenges when comparing semantic similarities between multi-lingual texts. Therefore, we propose multi-lingual <b>sentence</b> <b>encoders</b> (CT-XLMR-SE and CT-mBERT-SE) that embed crisis-related social media texts for over 50 languages, such that texts with similar meanings are in close proximity within the same vector space, irrespective of language diversity. Results in <b>sentence</b> <b>encoding</b> and <b>sentence</b> <b>matching</b> tasks are promising, suggesting these models could serve as robust baselines when embedding multi-lingual crisis-related social media texts. The models are publicly available at: <a href=https://huggingface.co/crisistransformers>https://huggingface.co/crisistransformers</a>.</p></p class="citation"></blockquote><h3 id=2856--28340-a-study-on-how-attention-scores-in-the-bert-model-are-aware-of-lexical-categories-in-syntactic-and-semantic-tasks-on-the-glue-benchmark-dongjun-jang-et-al-2024>(28/56 | 28/340) A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark (Dongjun Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjun Jang, Sungjoo Byun, Hyopil Shin. (2024)<br><strong>A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark</strong><br><button class=copy-to-clipboard title="A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, BERT, GLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16447v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16447v1.pdf filename=2403.16447v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study examines whether the attention scores between tokens in the <b>BERT</b> model significantly vary based on lexical categories during the <b>fine-tuning</b> process for downstream tasks. Drawing inspiration from the notion that in human language processing, syntactic and semantic information is parsed differently, we categorize tokens in sentences according to their lexical categories and focus on changes in attention scores among these categories. Our hypothesis posits that in downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases emphasizing syntactic information, attention scores centered on function words are intensified. Through experimentation conducted on six tasks from the <b>GLUE</b> <b>benchmark</b> dataset, we substantiate our hypothesis regarding the <b>fine-tuning</b> process. Furthermore, our additional investigations reveal the presence of <b>BERT</b> layers that consistently assign more bias to specific lexical categories, irrespective of the task, highlighting the existence of task-agnostic lexical category preferences.</p></p class="citation"></blockquote><h3 id=2956--29340-towards-automatic-evaluation-for-llms-clinical-capabilities-metric-data-and-algorithm-lei-liu-et-al-2024>(29/56 | 29/340) Towards Automatic Evaluation for LLMs&rsquo; Clinical Capabilities: Metric, Data, and Algorithm (Lei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu Ming Zhang, Xiaowei Ma, Xiangguo Lyu, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu. (2024)<br><strong>Towards Automatic Evaluation for LLMs&rsquo; Clinical Capabilities: Metric, Data, and Algorithm</strong><br><button class=copy-to-clipboard title="Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Automatic Evaluation, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16446v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16446v1.pdf filename=2403.16446v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are gaining increasing interests to improve clinical efficiency for medical diagnosis, owing to their unprecedented performance in modelling natural language. Ensuring the safe and reliable clinical applications, the evaluation of <b>LLMs</b> indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an <b>automatic</b> <b>evaluation</b> paradigm tailored to assess the <b>LLMs&rsquo;</b> capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a <b>LLM-specific</b> clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the <b>LLMs&rsquo;</b> medical capabilities. Applying such paradigm, we construct an evaluation <b>benchmark</b> in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for <b>LLMs&rsquo;</b> safe and reliable deployments in clinical practice.</p></p class="citation"></blockquote><h3 id=3056--30340-language-rectified-flow-advancing-diffusion-language-generation-with-probabilistic-flows-shujian-zhang-et-al-2024>(30/56 | 30/340) Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows (Shujian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shujian Zhang, Lemeng Wu, Chengyue Gong, Xingchao Liu. (2024)<br><strong>Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows</strong><br><button class=copy-to-clipboard title="Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL, stat-ML<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16995v1.pdf filename=2403.16995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion <b>language</b> <b>model.</b> A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes <b>Language</b> <b>Rectified</b> Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. <b>Language</b> <b>rectified</b> flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our <b>language</b> <b>rectified</b> flow yields fast <b>simulation</b> and effectively decreases the inference time. Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.</p></p class="citation"></blockquote><h3 id=3156--31340-aligning-with-human-judgement-the-role-of-pairwise-preference-in-large-language-model-evaluators-yinhong-liu-et-al-2024>(31/56 | 31/340) Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators (Yinhong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier. (2024)<br><strong>Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators</strong><br><button class=copy-to-clipboard title="Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16950v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16950v2.pdf filename=2403.16950v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, <b>LLMs</b> still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between <b>LLM</b> evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning <b>LLM</b> evaluators. Inspired by the use of preference data in <b>RLHF,</b> we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided search method that employs <b>LLMs</b> to conduct pairwise comparisons and efficiently ranks candidate texts. PairS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthermore, we provide insights into the role of pairwise preference in quantifying the transitivity of <b>LLMs</b> and demonstrate how PairS benefits from calibration.</p></p class="citation"></blockquote><h3 id=3256--32340-toxcl-a-unified-framework-for-toxic-speech-detection-and-explanation-nhat-m-hoang-et-al-2024>(32/56 | 32/340) ToXCL: A Unified Framework for Toxic Speech Detection and Explanation (Nhat M. Hoang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan. (2024)<br><strong>ToXCL: A Unified Framework for Toxic Speech Detection and Explanation</strong><br><button class=copy-to-clipboard title="ToXCL: A Unified Framework for Toxic Speech Detection and Explanation" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16685v1.pdf filename=2403.16685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a <b>text</b> <b>generation</b> problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via <b>knowledge</b> <b>distillation,</b> and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.</p></p class="citation"></blockquote><h3 id=3356--33340-ru22fact-optimizing-evidence-for-multilingual-explainable-fact-checking-on-russia-ukraine-conflict-yirong-zeng-et-al-2024>(33/56 | 33/340) RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict (Yirong Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin. (2024)<br><strong>RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict</strong><br><button class=copy-to-clipboard title="RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fact Verification, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16662v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16662v2.pdf filename=2403.16662v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fact-checking</b> <b>is</b> the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing <b>fact-checking</b> <b>systems</b> and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable <b>fact-checking</b> <b>systems</b> poses a challenge. To tackle this challenge, we propose a method based on a <b>Large</b> <b>Language</b> <b>Model</b> to automatically retrieve and <b>summarize</b> evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable <b>fact-checking</b> <b>dataset</b> on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable <b>fact-checking</b> <b>system</b> to verify claims and generate explanations. Experimental results demonstrate the prospect of optimized evidence in increasing <b>fact-checking</b> <b>performance</b> and also indicate the possibility of further progress in the end-to-end claim verification and explanation generation tasks.</p></p class="citation"></blockquote><h3 id=3456--34340-if-clip-could-talk-understanding-vision-language-model-representations-through-their-preferred-concept-descriptions-reza-esfandiarpoor-et-al-2024>(34/56 | 34/340) If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions (Reza Esfandiarpoor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach. (2024)<br><strong>If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions</strong><br><button class=copy-to-clipboard title="If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16442v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16442v1.pdf filename=2403.16442v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works often assume that <b>Vision-Language</b> Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses <b>reinforcement</b> <b>learning</b> to align a <b>large</b> <b>language</b> <b>model</b> with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations.</p></p class="citation"></blockquote><h3 id=3556--35340-enhanced-facet-generation-with-llm-editing-joosung-lee-et-al-2024>(35/56 | 35/340) Enhanced Facet Generation with LLM Editing (Joosung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joosung Lee, Jinhong Kim. (2024)<br><strong>Enhanced Facet Generation with LLM Editing</strong><br><button class=copy-to-clipboard title="Enhanced Facet Generation with LLM Editing" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 30<br>Keywords: Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16345v1.pdf filename=2403.16345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>information</b> <b>retrieval,</b> facet identification of a user query is an important task. If a search service can recognize the facets of a user&rsquo;s query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional <b>information</b> <b>may</b> change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input without a search engine. The first strategy is multi-task learning to predict SERP. By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules. The second strategy is to enhance the facets by combining <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> and the small model. Overall performance improves when small model and <b>LLM</b> are combined rather than facet generation individually.</p></p class="citation"></blockquote><h3 id=3656--36340-codes-natural-language-to-code-repository-via-multi-layer-sketch-daoguang-zan-et-al-2024>(36/56 | 36/340) CodeS: Natural Language to Code Repository via Multi-Layer Sketch (Daoguang Zan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui. (2024)<br><strong>CodeS: Natural Language to Code Repository via Multi-Layer Sketch</strong><br><button class=copy-to-clipboard title="CodeS: Natural Language to Code Repository via Multi-Layer Sketch" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-SE, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16443v1.pdf filename=2403.16443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The impressive performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository&rsquo;s directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automated <b>benchmarking</b> and manual feedback analysis. For <b>benchmark-based</b> evaluation, we craft a repository-oriented <b>benchmark,</b> SketchEval, and design an evaluation metric, SketchBLEU. For feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30 participants in conducting empirical studies. Extensive experiments prove the effectiveness and practicality of CodeS on the NL2Repo task.</p></p class="citation"></blockquote><h3 id=3756--37340-procqa-a-large-scale-community-based-programming-question-answering-dataset-for-code-search-zehan-li-et-al-2024>(37/56 | 37/340) ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search (Zehan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong. (2024)<br><strong>ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search</strong><br><button class=copy-to-clipboard title="ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-SE, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16702v1.pdf filename=2403.16702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-based code <b>question</b> <b>answering</b> seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming <b>question</b> <b>answering</b> dataset extracted from the StackOverflow community, offering naturally structured mixed-modal <b>QA</b> pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=3856--38340-the-role-of-n-gram-smoothing-in-the-age-of-neural-networks-luca-malagutti-et-al-2024>(38/56 | 38/340) The Role of $n$-gram Smoothing in the Age of Neural Networks (Luca Malagutti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Malagutti, Andrius Buinovskij, Anej Svete, Clara Meister, Afra Amini, Ryan Cotterell. (2024)<br><strong>The Role of $n$-gram Smoothing in the Age of Neural Networks</strong><br><button class=copy-to-clipboard title="The Role of $n$-gram Smoothing in the Age of Neural Networks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Label Smoothing, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17240v1.pdf filename=2403.17240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between <b>label</b> <b>smoothing,</b> a popular regularization technique for neural language models, and add-$\lambda$ smoothing. Second, we derive a generalized framework for converting \emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results find that our novel regularizers are comparable to and, indeed, sometimes outperform <b>label</b> <b>smoothing</b> on language modeling and <b>machine</b> <b>translation.</b></p></p class="citation"></blockquote><h3 id=3956--39340-extracting-social-support-and-social-isolation-information-from-clinical-psychiatry-notes-comparing-a-rule-based-nlp-system-and-a-large-language-model-braja-gopal-patra-et-al-2024>(39/56 | 39/340) Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model (Braja Gopal Patra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Braja Gopal Patra, Lauren A. Lepow, Praneet Kasi Reddy Jagadeesh Kumar, Veer Vekaria, Mohit Manoj Sharma, Prakash Adekkanattu, Brian Fennessy, Gavin Hynes, Isotta Landi, Jorge A. Sanchez-Ruiz, Euijung Ryu, Joanna M. Biernacka, Girish N. Nadkarni, Ardesheer Talati, Myrna Weissman, Mark Olfson, J. John Mann, Alexander W. Charney, Jyotishman Pathak. (2024)<br><strong>Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model</strong><br><button class=copy-to-clipboard title="Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17199v1.pdf filename=2403.17199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Social support (SS) and social isolation (SI) are social determinants of health (SDOH) associated with psychiatric outcomes. In electronic health records (EHRs), individual-level SS/SI is typically documented as narrative clinical notes rather than structured coded data. Natural language processing (NLP) algorithms can automate the otherwise labor-intensive process of data extraction. Data and Methods: Psychiatric encounter notes from Mount Sinai Health System (MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and established a gold standard corpus. A rule-based system (RBS) involving lexicons and a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> using FLAN-T5-XL were developed to identify mentions of SS and SI and their subcategories (e.g., social network, instrumental support, and loneliness). Results: For extracting SS/SI, the RBS obtained higher macro-averaged f-scores than the <b>LLM</b> at both MSHS (0.89 vs. 0.65) and WCM (0.85 vs. 0.82). For extracting subcategories, the RBS also outperformed the <b>LLM</b> at both MSHS (0.90 vs. 0.62) and WCM (0.82 vs. 0.81). Discussion and Conclusion: Unexpectedly, the RBS outperformed the <b>LLMs</b> across all metrics. Intensive review demonstrates that this finding is due to the divergent approach taken by the RBS and <b>LLM.</b> The RBS were designed and refined to follow the same specific rules as the gold standard annotations. Conversely, the <b>LLM</b> were more inclusive with categorization and conformed to common English-language understanding. Both approaches offer advantages and are made available open-source for future testing.</p></p class="citation"></blockquote><h3 id=4056--40340-gpt-4-understands-discourse-at-least-as-well-as-humans-do-thomas-shultz-et-al-2024>(40/56 | 40/340) GPT-4 Understands Discourse at Least as Well as Humans Do (Thomas Shultz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Shultz, Jamie Wise, Ardavan Salehi Nobandegani. (2024)<br><strong>GPT-4 Understands Discourse at Least as Well as Humans Do</strong><br><button class=copy-to-clipboard title="GPT-4 Understands Discourse at Least as Well as Humans Do" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17196v1.pdf filename=2403.17196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We test whether a leading AI system <b>GPT-4</b> understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). <b>GPT-4</b> performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both <b>GPT-4</b> and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.</p></p class="citation"></blockquote><h3 id=4156--41340-task-agnostic-detector-for-insertion-based-backdoor-attacks-weimin-lyu-et-al-2024>(41/56 | 41/340) Task-Agnostic Detector for Insertion-Based Backdoor Attacks (Weimin Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen. (2024)<br><strong>Task-Agnostic Detector for Insertion-Based Backdoor Attacks</strong><br><button class=copy-to-clipboard title="Task-Agnostic Detector for Insertion-Based Backdoor Attacks" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 20<br>Keywords: Named Entity Recognition, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17155v1.pdf filename=2403.17155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like <b>question</b> <b>answering</b> and <b>named</b> <b>entity</b> <b>recognition.</b> We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.</p></p class="citation"></blockquote><h3 id=4256--42340-guided-distant-supervision-for-multilingual-relation-extraction-data-adapting-to-a-new-language-alistair-plum-et-al-2024>(42/56 | 42/340) Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language (Alistair Plum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alistair Plum, Tharindu Ranasinghe, Christoph Purschke. (2024)<br><strong>Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language</strong><br><button class=copy-to-clipboard title="Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Low-Resource, Relation Extraction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17143v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17143v2.pdf filename=2403.17143v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Relation</b> <b>extraction</b> is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as well. Furthermore, we experiment with multilingual and cross-lingual experiments that could benefit many <b>low-resource</b> languages.</p></p class="citation"></blockquote><h3 id=4356--43340-exploring-the-generalization-of-cancer-clinical-trial-eligibility-classifiers-across-diseases-yumeng-yang-et-al-2024>(43/56 | 43/340) Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases (Yumeng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yumeng Yang, Ashley Gilliam, Ethan B Ludmir, Kirk Roberts. (2024)<br><strong>Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases</strong><br><button class=copy-to-clipboard title="Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, q-bio-QM<br>Keyword Score: 20<br>Keywords: Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17135v1.pdf filename=2403.17135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disproportionately prevalent in cancer trials, like prior malignancy. We also experiment with <b>few-shot</b> <b>learning,</b> demonstrating that a limited number of disease-specific examples can partially overcome this performance gap. We are releasing this new dataset of annotated eligibility statements to promote the development of cross-disease generalization in clinical trial classification.</p></p class="citation"></blockquote><h3 id=4456--44340-strum-llm-attributed-and-structured-contrastive-summarization-beliz-gunel-et-al-2024>(44/56 | 44/340) STRUM-LLM: Attributed and Structured Contrastive Summarization (Beliz Gunel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beliz Gunel, James B. Wendt, Jing Xie, Yichao Zhou, Nguyen Vo, Zachary Fisher, Sandeep Tata. (2024)<br><strong>STRUM-LLM: Attributed and Structured Contrastive Summarization</strong><br><button class=copy-to-clipboard title="STRUM-LLM: Attributed and Structured Contrastive Summarization" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19710v1.pdf filename=2403.19710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user&rsquo;s decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM <b>Distilled</b> has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations for our method and lay out future directions for our currently deployed system.</p></p class="citation"></blockquote><h3 id=4556--45340-data-mixing-laws-optimizing-data-mixtures-by-predicting-language-modeling-performance-jiasheng-ye-et-al-2024>(45/56 | 45/340) Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance (Jiasheng Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, Xipeng Qiu. (2024)<br><strong>Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance</strong><br><button class=copy-to-clipboard title="Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16952v1.pdf filename=2403.16952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining data of <b>large</b> <b>language</b> <b>models</b> composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the <b>scaling</b> <b>laws</b> of training steps, model sizes, and our data mixing law to enable predicting the performance of <b>large</b> <b>models</b> <b>trained</b> on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 1B model trained for 100B tokens in RedPajama, reaching a performance comparable to the one trained for 48% more steps on the default mixture. Extending the application of data mixing laws to continual training accurately predicts the critical mixture proportion that avoids catastrophic forgetting and outlooks the potential for dynamic data schedules</p></p class="citation"></blockquote><h3 id=4656--46340-conversational-grounding-annotation-and-analysis-of-grounding-acts-and-grounding-units-biswesh-mohapatra-et-al-2024>(46/56 | 46/340) Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units (Biswesh Mohapatra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell. (2024)<br><strong>Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units</strong><br><button class=copy-to-clipboard title="Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Grounding, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16609v1.pdf filename=2403.16609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Successful conversations often rest on common understanding, where all parties are on the same page about the information being shared. This process, known as conversational <b>grounding,</b> is crucial for building trustworthy dialog systems that can accurately keep track of and recall the shared information. The proficiencies of an agent in <b>grounding</b> the conveyed information significantly contribute to building a reliable dialog system. Despite recent advancements in dialog systems, there exists a noticeable deficit in their <b>grounding</b> capabilities. Traum provided a framework for conversational <b>grounding</b> introducing <b>Grounding</b> Acts and <b>Grounding</b> Units, but substantial progress, especially in the realm of <b>Large</b> <b>Language</b> <b>Models,</b> remains lacking. To bridge this gap, we present the annotation of two dialog corpora employing <b>Grounding</b> Acts, <b>Grounding</b> Units, and a measure of their degree of <b>grounding.</b> We discuss our key findings during the annotation and also provide a baseline model to test the performance of current Language Models in categorizing the <b>grounding</b> acts of the dialogs. Our work aims to provide a useful resource for further research in making conversations with machines better understood and more reliable in natural day-to-day collaborative dialogs.</p></p class="citation"></blockquote><h3 id=4756--47340-trustai-at-semeval-2024-task-8-a-comprehensive-analysis-of-multi-domain-machine-generated-text-detection-techniques-ashok-urlana-et-al-2024>(47/56 | 47/340) TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques (Ashok Urlana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala. (2024)<br><strong>TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques</strong><br><button class=copy-to-clipboard title="TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16592v1.pdf filename=2403.16592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> exhibit remarkable ability to generate fluent content across a wide spectrum of user queries. However, this capability has raised concerns regarding misinformation and personal information leakage. In this paper, we present our methods for the SemEval2024 Task8, aiming to detect machine-generated text across various domains in both mono-lingual and multi-lingual contexts. Our study comprehensively analyzes various methods to detect machine-generated text, including statistical, neural, and pre-trained model approaches. We also detail our experimental setup and perform a in-depth error analysis to evaluate the effectiveness of these methods. Our methods obtain an accuracy of 86.9% on the test set of subtask-A mono and 83.7% for subtask-B. Furthermore, we also highlight the challenges and essential factors for consideration in future studies.</p></p class="citation"></blockquote><h3 id=4856--48340-skews-in-the-phenomenon-space-hinder-generalization-in-text-to-image-generation-yingshan-chang-et-al-2024>(48/56 | 48/340) Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation (Yingshan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk, Feng Gao. (2024)<br><strong>Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16394v1.pdf filename=2403.16394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The literature on <b>text-to-image</b> generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of <b>text-to-image</b> generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demonstrate that systematically controlled metrics are strongly predictive of generalization performance. Then we move to natural images and show that simple distribution perturbations in light of our theories boost generalization without enlarging the absolute data size. This work informs an important direction towards quality-enhancing the data diversity or balance orthogonal to scaling up the absolute size. Our discussions point out important open questions on 1) Evaluation of generated entity-relation compositions, and 2) Better models for <b>reasoning</b> with abstract relations.</p></p class="citation"></blockquote><h3 id=4956--49340-pe-a-poincare-explanation-method-for-fast-text-hierarchy-generation-qian-chen-et-al-2024>(49/56 | 49/340) PE: A Poincare Explanation Method for Fast Text Hierarchy Generation (Qian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi. (2024)<br><strong>PE: A Poincare Explanation Method for Fast Text Hierarchy Generation</strong><br><button class=copy-to-clipboard title="PE: A Poincare Explanation Method for Fast Text Hierarchy Generation" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 18<br>Keywords: Black Box, Clustering, Hierarchical Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16554v1.pdf filename=2403.16554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>black-box</b> <b>nature</b> of deep learning models in NLP hinders their widespread application. The research focus has shifted to <b>Hierarchical</b> <b>Attribution</b> (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic <b>hierarchical</b> <b>structures.</b> Eventually, we prove that the <b>hierarchical</b> <b>clustering</b> process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=5056--50340-numtemp-a-real-world-benchmark-to-verify-claims-with-statistical-and-temporal-expressions-venktesh-v-et-al-2024>(50/56 | 50/340) NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions (Venktesh V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Venktesh V, Abhijit Anand, Avishek Anand, Vinay Setty. (2024)<br><strong>NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions</strong><br><button class=copy-to-clipboard title="NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17169v1.pdf filename=2403.17169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated <b>fact</b> <b>checking</b> has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp serves as a challenging evaluation set for numerical claim verification.</p></p class="citation"></blockquote><h3 id=5156--51340-new-intent-discovery-with-attracting-and-dispersing-prototype-shun-zhang-et-al-2024>(51/56 | 51/340) New Intent Discovery with Attracting and Dispersing Prototype (Shun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shun Zhang, Jian Yang, Jiaqi Bai, Chaoran Yan, Tongliang Li, Zhao Yan, Zhoujun Li. (2024)<br><strong>New Intent Discovery with Attracting and Dispersing Prototype</strong><br><button class=copy-to-clipboard title="New Intent Discovery with Attracting and Dispersing Prototype" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16913v1.pdf filename=2403.16913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>New Intent Discovery (NID) aims to recognize known and infer new intent categories with the help of limited labeled and <b>large-scale</b> <b>unlabeled</b> <b>data.</b> The task is addressed as a feature-clustering problem and recent studies augment instance representation. However, existing methods fail to capture cluster-friendly representations, since they show less capability to effectively control and coordinate within-cluster and between-cluster distances. Tailored to the NID problem, we propose a Robust and Adaptive Prototypical learning (RAP) framework for globally distinct decision boundaries for both known and new intent categories. Specifically, a robust prototypical attracting learning (RPAL) method is designed to compel instances to gravitate toward their corresponding prototype, achieving greater within-cluster compactness. To attain larger between-cluster separation, another adaptive prototypical dispersing learning (APDL) method is devised to maximize the between-cluster distance from the prototype-to-prototype perspective. Experimental results evaluated on three challenging <b>benchmarks</b> (CLINC, BANKING, and StackOverflow) of our method with better cluster-friendly representation demonstrate that RAP brings in substantial improvements over the current state-of-the-art methods (even <b>large</b> <b>language</b> <b>model)</b> by a <b>large</b> <b>margin</b> <b>(average</b> +5.5% improvement).</p></p class="citation"></blockquote><h3 id=5256--52340-an-expert-is-worth-one-token-synergizing-multiple-expert-llms-as-generalist-via-expert-token-routing-ziwei-chai-et-al-2024>(52/56 | 52/340) An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing (Ziwei Chai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziwei Chai, Guoyin Wang, Jing Su, Tianjie Zhang, Xuanwen Huang, Xuwu Wang, Jingjing Xu, Jianbo Yuan, Hongxia Yang, Fei Wu, Yang Yang. (2024)<br><strong>An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing</strong><br><button class=copy-to-clipboard title="An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16854v1.pdf filename=2403.16854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert <b>LLMs.</b> Our framework represents expert <b>LLMs</b> as special expert tokens within the vocabulary of a meta <b>LLM.</b> The meta <b>LLM</b> can route to an expert <b>LLM</b> like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert <b>LLMs</b> from existing instruction dataset but also allows for dynamic extension of new expert <b>LLMs</b> in a plug-and-play manner. It also conceals the detailed collaboration process from the user&rsquo;s perspective, facilitating interaction as though it were a singular <b>LLM.</b> Our framework outperforms various existing multi-LLM collaboration paradigms across <b>benchmarks</b> that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist <b>LLM</b> system via synergizing multiple expert <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=5356--53340-making-sentence-embeddings-robust-to-user-generated-content-lydia-nishimwe-et-al-2024>(53/56 | 53/340) Making Sentence Embeddings Robust to User-Generated Content (Lydia Nishimwe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lydia Nishimwe, Benoît Sagot, Rachel Bawden. (2024)<br><strong>Making Sentence Embeddings Robust to User-Generated Content</strong><br><button class=copy-to-clipboard title="Making Sentence Embeddings Robust to User-Generated Content" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17220v1.pdf filename=2403.17220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>NLP models have been known to perform poorly on user-generated content (UGC), mainly because it presents a lot of lexical variations and deviates from the standard texts on which most of these models were trained. In this work, we focus on the robustness of LASER, a <b>sentence</b> <b>embedding</b> model, to UGC data. We evaluate this robustness by LASER&rsquo;s ability to represent non-standard <b>sentences</b> <b>and</b> their standard counterparts close to each other in the embedding space. Inspired by previous works extending LASER to other languages and modalities, we propose RoLASER, a robust English encoder trained using a teacher-student approach to reduce the distances between the representations of standard and UGC <b>sentences.</b> <b>We</b> show that with training only on standard and synthetic UGC-like data, RoLASER significantly improves LASER&rsquo;s robustness to both natural and artificial UGC data by achieving up to 2x and 11x better scores. We also perform a fine-grained analysis on artificial UGC data and find that our model greatly outperforms LASER on its most challenging UGC phenomena such as keyboard typos and social media abbreviations. Evaluation on downstream tasks shows that RoLASER performs comparably to or better than LASER on standard data, while consistently outperforming it on UGC data.</p></p class="citation"></blockquote><h3 id=5456--54340-reflecting-the-male-gaze-quantifying-female-objectification-in-19th-and-20th-century-novels-kexin-luo-et-al-2024>(54/56 | 54/340) Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels (Kexin Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexin Luo, Yue Mao, Bei Zhang, Sophie Hao. (2024)<br><strong>Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels</strong><br><button class=copy-to-clipboard title="Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17158v1.pdf filename=2403.17158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the concept of the male gaze (Mulvey, 1975) in literature and media studies, this paper proposes a framework for analyzing gender bias in terms of female objectification: the extent to which a text portrays female individuals as objects of visual pleasure. Our framework measures female objectification along two axes. First, we compute an agency bias score that indicates whether male entities are more likely to appear in the text as grammatical agents than female entities. Next, by analyzing the <b>word</b> <b>embedding</b> space induced by a text (Caliskan et al., 2017), we compute an appearance bias score that indicates whether female entities are more closely associated with appearance-related <b>words</b> <b>than</b> male entities. Applying our framework to 19th and 20th century novels reveals evidence of female objectification in literature: we find that novels written from a male perspective systematically objectify female characters, while novels written from a female perspective do not exhibit statistically significant objectification of any gender.</p></p class="citation"></blockquote><h3 id=5556--55340-encoding-of-lexical-tone-in-self-supervised-models-of-spoken-language-gaofei-shen-et-al-2024>(55/56 | 55/340) Encoding of lexical tone in self-supervised models of spoken language (Gaofei Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza, Grzegorz Chrupała. (2024)<br><strong>Encoding of lexical tone in self-supervised models of spoken language</strong><br><button class=copy-to-clipboard title="Encoding of lexical tone in self-supervised models of spoken language" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16865v1.pdf filename=2403.16865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interpretability research has shown that <b>self-supervised</b> Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world&rsquo;s languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.</p></p class="citation"></blockquote><h3 id=5656--56340-towards-explainability-in-legal-outcome-prediction-models-josef-valvoda-et-al-2024>(56/56 | 56/340) Towards Explainability in Legal Outcome Prediction Models (Josef Valvoda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josef Valvoda, Ryan Cotterell. (2024)<br><strong>Towards Explainability in Legal Outcome Prediction Models</strong><br><button class=copy-to-clipboard title="Towards Explainability in Legal Outcome Prediction Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16852v1.pdf filename=2403.16852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current legal outcome prediction models - a staple of legal NLP - do not explain their <b>reasoning.</b> However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=13--57340-cygent-a-cybersecurity-conversational-agent-with-log-summarization-powered-by-gpt-3-prasasthy-balasubramanian-et-al-2024>(1/3 | 57/340) CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3 (Prasasthy Balasubramanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prasasthy Balasubramanian, Justin Seby, Panos Kostakos. (2024)<br><strong>CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3</strong><br><button class=copy-to-clipboard title="CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, Generative AI, GPT, GPT-3, GPT-3.5, BERTScore, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17160v1.pdf filename=2403.17160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the escalating cyber-attacks in the modern IT and IoT landscape, we developed CYGENT, a conversational agent framework powered by <b>GPT-3.5</b> turbo model, designed to aid system administrators in ensuring optimal performance and uninterrupted resource availability. This study focuses on <b>fine-tuning</b> <b>GPT-3</b> models for cybersecurity tasks, including conversational AI and <b>generative</b> <b>AI</b> tailored specifically for cybersecurity operations. CYGENT assists users by providing cybersecurity information, analyzing and summarizing uploaded log files, detecting specific events, and delivering essential instructions. The conversational agent was developed based on the <b>GPT-3.5</b> turbo model. We <b>fine-tuned</b> and validated summarizer models <b>(GPT3)</b> using manually generated data points. Using this approach, we achieved a <b>BERTscore</b> of over 97%, indicating <b>GPT-3&rsquo;s</b> enhanced capability in summarizing log files into human-readable formats and providing necessary information to users. Furthermore, we conducted a comparative analysis of <b>GPT-3</b> models with other <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> including CodeT5-small, CodeT5-base, and CodeT5-base-multi-sum, with the objective of analyzing log analysis techniques. Our analysis consistently demonstrated that Davinci <b>(GPT-3)</b> model outperformed all other <b>LLMs,</b> showcasing higher performance. These findings are crucial for improving human comprehension of logs, particularly in light of the increasing numbers of IoT devices. Additionally, our research suggests that the CodeT5-base-multi-sum model exhibits comparable performance to Davinci to some extent in summarizing logs, indicating its potential as an offline model for this task.</p></p class="citation"></blockquote><h3 id=23--58340-semantic-ranking-for-automated-adversarial-technique-annotation-in-security-text-udesh-kumarasinghe-et-al-2024>(2/3 | 58/340) Semantic Ranking for Automated Adversarial Technique Annotation in Security Text (Udesh Kumarasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Udesh Kumarasinghe, Ahmed Lekssays, Husrev Taha Sencar, Sabri Boughorbel, Charitha Elvitigala, Preslav Nakov. (2024)<br><strong>Semantic Ranking for Automated Adversarial Technique Annotation in Security Text</strong><br><button class=copy-to-clipboard title="Semantic Ranking for Automated Adversarial Technique Annotation in Security Text" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17068v1.pdf filename=2403.17068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new method for extracting structured threat behaviors from threat intelligence text. Our method is based on a multi-stage ranking architecture that allows jointly optimizing for efficiency and effectiveness. Therefore, we believe this problem formulation better aligns with the real-world nature of the task considering the <b>large</b> <b>number</b> <b>of</b> adversary techniques and the extensive body of threat intelligence created by security analysts. Our findings show that the proposed system yields state-of-the-art performance results for this task. Results show that our method has a top-3 recall performance of 81% in identifying the relevant technique among 193 top-level techniques. Our tests also demonstrate that our system performs significantly better (+40%) than the widely used <b>large</b> <b>language</b> <b>models</b> when tested under a <b>zero-shot</b> setting.</p></p class="citation"></blockquote><h3 id=33--59340-cipherformer-efficient-transformer-private-inference-with-low-round-complexity-weize-wang-et-al-2024>(3/3 | 59/340) CipherFormer: Efficient Transformer Private Inference with Low Round Complexity (Weize Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weize Wang, Yi Kuang. (2024)<br><strong>CipherFormer: Efficient Transformer Private Inference with Low Round Complexity</strong><br><button class=copy-to-clipboard title="CipherFormer: Efficient Transformer Private Inference with Low Round Complexity" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Transformer, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16860v1.pdf filename=2403.16860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing trend to outsource the inference task of large <b>transformer</b> models to cloud servers. However, this poses a severe threat to users&rsquo; private data as they are exposed to cloud servers after uploading. Although several works attempted to provide private inference for <b>transformer</b> models, their hundreds of communication rounds limit the application scenarios. Motivated by the desire to minimize round complexity, we propose CipherFormer, a novel <b>transformer</b> private inference scheme using homomorphic encryption and garbled circuits. We present a protocol for quickly computing homomorphic matrix multiplications. We then modify the attention mechanism and design the corresponding garbled circuits. Furthermore, we show how to use a lightweight attention mechanism and mixed-bitwidth to reduce the inference latency while maintaining accuracy. In comparison with an advanced homomorphic encryption scheme on <b>text</b> <b>classification</b> tasks, our model improves accuracy by 3% to 11% while performing private inference with a 7.7x-11.9x speedup.</p></p class="citation"></blockquote><h2 id=cslg-42>cs.LG (42)</h2><h3 id=142--60340-graph-augmentation-for-recommendation-qianru-zhang-et-al-2024>(1/42 | 60/340) Graph Augmentation for Recommendation (Qianru Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen. (2024)<br><strong>Graph Augmentation for Recommendation</strong><br><button class=copy-to-clipboard title="Graph Augmentation for Recommendation" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 93<br>Keywords: Message-Passing, Graph, Graph Contrastive Learning, Graph Neural Network, Graph Neural Network, Contrastive Learning, Knowledge Distillation, Recommendation, Recommender System, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16656v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16656v1.pdf filename=2403.16656v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>augmentation</b> <b>with</b> <b>contrastive</b> <b>learning</b> has gained significant attention in the field of <b>recommendation</b> systems due to its ability to learn expressive user representations, even when labeled data is limited. However, directly applying existing <b>GCL</b> models to real-world <b>recommendation</b> environments poses challenges. There are two primary issues to address. Firstly, the lack of consideration for data noise in <b>contrastive</b> <b>learning</b> can result in noisy <b>self-supervised</b> signals, leading to degraded performance. Secondly, many existing <b>GCL</b> approaches rely on <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> architectures, which can suffer from over-smoothing problems due to non-adaptive message passing. To address these challenges, we propose a principled framework called GraphAug. This framework introduces a robust data augmentor that generates denoised <b>self-supervised</b> signals, enhancing <b>recommender</b> <b>systems.</b> The GraphAug framework incorporates a <b>graph</b> <b>information</b> <b>bottleneck</b> (GIB)-regularized augmentation paradigm, which automatically <b>distills</b> informative self-supervision information and adaptively adjusts <b>contrastive</b> <b>view</b> generation. Through rigorous experimentation on real-world datasets, we thoroughly assessed the performance of our novel GraphAug model. The outcomes consistently unveil its superiority over existing baseline methods. The source code for our model is publicly available at: <a href=https://github.com/HKUDS/GraphAug>https://github.com/HKUDS/GraphAug</a>.</p></p class="citation"></blockquote><h3 id=242--61340-do-llm-agents-have-regret-a-case-study-in-online-learning-and-games-chanwoo-park-et-al-2024>(2/42 | 61/340) Do LLM Agents Have Regret? A Case Study in Online Learning and Games (Chanwoo Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang. (2024)<br><strong>Do LLM Agents Have Regret? A Case Study in Online Learning and Games</strong><br><button class=copy-to-clipboard title="Do LLM Agents Have Regret? A Case Study in Online Learning and Games" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Supervised Learning, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16843v1.pdf filename=2403.16843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been increasingly employed for (interactive) decision-making, via the development of <b>LLM-based</b> autonomous agents. Despite their emerging successes, the performance of <b>LLM</b> agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world <b>LLM-agent</b> applications. To better understand the limits of <b>LLM</b> agents in these interactive environments, we propose to study their interactions in <b>benchmark</b> decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of <b>LLMs</b> in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when <b>LLM</b> agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of <b>LLM</b> agents, under certain assumptions on the <b>supervised</b> pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced <b>LLMs</b> such as <b>GPT-4</b> fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the <b>supervised</b> pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable&rsquo;&rsquo; cases.</p></p class="citation"></blockquote><h3 id=342--62340-learning-from-reduced-labels-for-long-tailed-data-meng-wei-et-al-2024>(3/42 | 62/340) Learning from Reduced Labels for Long-Tailed Data (Meng Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Wei, Zhongnian Li, Yong Zhou, Xinzheng Xu. (2024)<br><strong>Learning from Reduced Labels for Long-Tailed Data</strong><br><button class=copy-to-clipboard title="Learning from Reduced Labels for Long-Tailed Data" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Benchmarking, Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16469v1.pdf filename=2403.16469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-tailed data is prevalent in real-world classification tasks and heavily relies on <b>supervised</b> <b>information,</b> which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing <b>weakly</b> <b>supervised</b> <b>learning</b> methods struggle to adequately preserve <b>supervised</b> <b>information</b> for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel <b>weakly</b> <b>supervised</b> <b>labeling</b> setting called Reduced Label. The proposed labeling setting not only avoids the decline of <b>supervised</b> <b>information</b> for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on <b>benchmark</b> datasets including ImageNet validate the effectiveness of our approach, surpassing the performance of state-of-the-art <b>weakly</b> <b>supervised</b> <b>methods.</b></p></p class="citation"></blockquote><h3 id=442--63340-the-anatomy-of-adversarial-attacks-concept-based-xai-dissection-georgii-mikriukov-et-al-2024>(4/42 | 63/340) The Anatomy of Adversarial Attacks: Concept-based XAI Dissection (Georgii Mikriukov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgii Mikriukov, Gesina Schwalbe, Franz Motzkus, Korinna Bade. (2024)<br><strong>The Anatomy of Adversarial Attacks: Concept-based XAI Dissection</strong><br><button class=copy-to-clipboard title="The Anatomy of Adversarial Attacks: Concept-based XAI Dissection" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16782v1.pdf filename=2403.16782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>attacks</b> (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the <b>adversarial</b> <b>perturbation</b> itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack&rsquo;s success. Notably, we discover that these components are target-specific, i.e., are similar for a given target class throughout different AA techniques and starting classes. Our findings provide valuable insights into the nature of AAs and their impact on learned representations, paving the way for the development of more robust and interpretable deep learning models, as well as effective defenses against <b>adversarial</b> <b>threats.</b></p></p class="citation"></blockquote><h3 id=542--64340-learning-action-based-representations-using-invariance-max-rudolph-et-al-2024>(5/42 | 64/340) Learning Action-based Representations Using Invariance (Max Rudolph et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Rudolph, Caleb Chuck, Kevin Black, Misha Lvovsky, Scott Niekum, Amy Zhang. (2024)<br><strong>Learning Action-based Representations Using Invariance</strong><br><button class=copy-to-clipboard title="Learning Action-based Representations Using Invariance" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Mutual Information, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16369v1.pdf filename=2403.16369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robust <b>reinforcement</b> <b>learning</b> agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and <b>mutual</b> <b>information</b> capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts distant state features that are relevant for control. We demonstrate that action-bisimulation pretraining on reward-free, uniformly random data improves sample efficiency in several environments, including a photorealistic 3D <b>simulation</b> domain, Habitat. Additionally, we provide theoretical analysis and qualitative results demonstrating the information captured by action-bisimulation.</p></p class="citation"></blockquote><h3 id=642--65340-deepknowledge-generalisation-driven-deep-learning-testing-sondess-missaoui-et-al-2024>(6/42 | 65/340) DeepKnowledge: Generalisation-Driven Deep Learning Testing (Sondess Missaoui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sondess Missaoui, Simos Gerasimou, Nikolaos Matragkas. (2024)<br><strong>DeepKnowledge: Generalisation-Driven Deep Learning Testing</strong><br><button class=copy-to-clipboard title="DeepKnowledge: Generalisation-Driven Deep Learning Testing" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SE, cs.LG<br>Keyword Score: 38<br>Keywords: MNIST, Benchmarking, Black Box, Distribution Shift, Distribution Shift, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16768v1.pdf filename=2403.16768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data <b>distribution,</b> <b>demanding</b> effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN&rsquo;s capability to generalise and operate comparably beyond data in their training <b>distribution.</b> <b>We</b> address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of <b>&lsquo;black</b> <b>box&rsquo;</b> models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data <b>distribution</b> <b>shifts</b> and uses this information to instrument a generalisation-informed test adequacy criterion to check the transfer knowledge capacity of a test set. Our empirical evaluation of several DNNs, across multiple datasets and state-of-the-art <b>adversarial</b> <b>generation</b> techniques demonstrates the usefulness and effectiveness of DeepKnowledge and its ability to support the engineering of more dependable DNNs. We report improvements of up to 10 percentage points over state-of-the-art coverage criteria for detecting <b>adversarial</b> <b>attacks</b> on several <b>benchmarks,</b> including <b>MNIST,</b> SVHN, and CIFAR.</p></p class="citation"></blockquote><h3 id=742--66340-less-is-more----on-the-importance-of-sparsification-for-transformers-and-graph-neural-networks-for-tsp-attila-lischka-et-al-2024>(7/42 | 66/340) Less Is More &ndash; On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP (Attila Lischka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Attila Lischka, Jiaming Wu, Rafael Basso, Morteza Haghir Chehreghani, Balázs Kulcsár. (2024)<br><strong>Less Is More &ndash; On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP</strong><br><button class=copy-to-clipboard title="Less Is More -- On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17159v1.pdf filename=2403.17159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a <b>transformer</b> or <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose <b>graph</b> <b>sparsification</b> <b>for</b> TSP <b>graph</b> <b>representations</b> <b>passed</b> to <b>GNNs</b> and attention masking for TSP instances passed to <b>transformers</b> where the masks correspond to the adjacency matrices of the sparse TSP <b>graph</b> <b>representations.</b> <b>Furthermore,</b> we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that for <b>GNNs</b> appropriate sparsification and ensembles of different sparsification levels lead to substantial performance increases of the overall architecture. We also design a new, state-of-the-art <b>transformer</b> encoder with ensembles of attention masking. These <b>transformers</b> increase model performance from a gap of $0.16%$ to $0.10%$ for TSP instances of size 100 and from $0.02%$ to $0.00%$ for TSP instances of size 50.</p></p class="citation"></blockquote><h3 id=842--67340-stochastic-gradient-langevin-unlearning-eli-chien-et-al-2024>(8/42 | 67/340) Stochastic Gradient Langevin Unlearning (Eli Chien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eli Chien, Haoyu Wang, Ziang Chen, Pan Li. (2024)<br><strong>Stochastic Gradient Langevin Unlearning</strong><br><button class=copy-to-clipboard title="Stochastic Gradient Langevin Unlearning" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Machine Unlearning, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17105v1.pdf filename=2403.17105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>``The right to be forgotten&rsquo;&rsquo; ensured by laws for user data privacy becomes increasingly important. <b>Machine</b> <b>unlearning</b> aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes <b>stochastic</b> <b>gradient</b> <b>Langevin</b> unlearning, the first unlearning framework based on noisy <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on <b>benchmark</b> datasets compared against prior works. Our approach achieves a similar utility under the same privacy constraint while using $2%$ and $10%$ of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively.</p></p class="citation"></blockquote><h3 id=942--68340-multiple-source-localization-from-a-single-snapshot-observation-using-graph-bayesian-optimization-zonghan-zhang-et-al-2024>(9/42 | 68/340) Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization (Zonghan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zonghan Zhang, Zijian Zhang, Zhiqian Chen. (2024)<br><strong>Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16818v1.pdf filename=2403.16818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the significance of its various applications, source localization has garnered considerable attention as one of the most important means to confront <b>diffusion</b> <b>hazards.</b> Multi-source localization from a single-snapshot observation is especially relevant due to its prevalence. However, the inherent complexities of this problem, such as limited information, interactions among sources, and dependence on <b>diffusion</b> <b>models,</b> pose challenges to resolution. Current methods typically utilize heuristics and greedy selection, and they are usually bonded with one <b>diffusion</b> <b>model.</b> Consequently, their effectiveness is constrained. To address these limitations, we propose a <b>simulation-based</b> method termed BOSouL. Bayesian optimization (BO) is adopted to approximate the results for its sample efficiency. A surrogate function models uncertainty from the limited information. It takes sets of nodes as the input instead of individual nodes. BOSouL can incorporate any <b>diffusion</b> <b>model</b> in the data acquisition process through <b>simulations.</b> Empirical studies demonstrate that its performance is robust across <b>graph</b> structures and <b>diffusion</b> <b>models.</b> The code is available at <a href=https://github.com/XGraph-Team/BOSouL>https://github.com/XGraph-Team/BOSouL</a>.</p></p class="citation"></blockquote><h3 id=1042--69340-lsttn-a-long-short-term-transformer-based-spatio-temporal-neural-network-for-traffic-flow-forecasting-qinyao-luo-et-al-2024>(10/42 | 69/340) LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting (Qinyao Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinyao Luo, Silu He, Xing Han, Yuhan Wang, Haifeng Li. (2024)<br><strong>LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting</strong><br><button class=copy-to-clipboard title="LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16495v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16495v1.pdf filename=2403.16495v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal <b>graph</b> <b>neural</b> <b>networks</b> (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term <b>Transformer-based</b> Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries <b>Transformer</b> to infer the content of masked subseries from a small portion of unmasked subseries and their temporal context in a pretraining manner, forcing the model to efficiently learn compressed and contextual subseries temporal representations from long historical series. Then, based on the learned representations, long-term trend is extracted by using stacked 1D dilated <b>convolution</b> layers, and periodic features are extracted by dynamic <b>graph</b> <b>convolution</b> <b>layers.</b> For the difficulties in making time-step level prediction, LSTTN adopts a short-term trend extractor to learn fine-grained short-term temporal features. Finally, LSTTN fuses the long-term trend, periodic features and short-term features to obtain the prediction results. Experiments on four real-world datasets show that in 60-minute-ahead long-term forecasting, the LSTTN model achieves a minimum improvement of 5.63% and a maximum improvement of 16.78% over baseline models. The source code is available at <a href=https://github.com/GeoX-Lab/LSTTN>https://github.com/GeoX-Lab/LSTTN</a>.</p></p class="citation"></blockquote><h3 id=1142--70340-rethinking-the-representation-in-federated-unsupervised-learning-with-non-iid-data-xinting-liao-et-al-2024>(11/42 | 70/340) Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data (Xinting Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinting Liao, Weiming Liu, Chaochao Chen, Pengyang Zhou, Fengyuan Yu, Huabin Zhu, Binhui Yao, Tao Wang, Xiaolin Zheng, Yanchao Tan. (2024)<br><strong>Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data</strong><br><button class=copy-to-clipboard title="Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Federated Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16398v1.pdf filename=2403.16398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for <b>federated</b> <b>unsupervised</b> <b>learning</b> (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each client avoids representation collapse via dispersing samples uniformly, and EUA in server promotes unified representation by constraining consistent client model updating. To extensively validate the performance of FedU2, we conduct both cross-device and cross-silo evaluation experiments on two <b>benchmark</b> datasets, i.e., CIFAR10 and CIFAR100.</p></p class="citation"></blockquote><h3 id=1242--71340-fligan-enhancing-federated-learning-with-incomplete-data-using-gan-paul-joe-maliakel-et-al-2024>(12/42 | 71/340) FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN (Paul Joe Maliakel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic. (2024)<br><strong>FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN</strong><br><button class=copy-to-clipboard title="FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16930v1.pdf filename=2403.16930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) provides a privacy-preserving mechanism for distributed training of machine learning models on networked devices (e.g., mobile devices, IoT edge nodes). It enables Artificial Intelligence (AI) at the edge by creating models without sharing the actual data across the network. Existing research works typically focus on generic aspects of non-IID data and heterogeneity in client&rsquo;s system characteristics, but they often neglect the issue of insufficient data for model development, which can arise from uneven class label distribution and highly variable data volumes across edge nodes. In this work, we propose FLIGAN, a novel approach to address the issue of data incompleteness in FL. First, we leverage <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> to adeptly capture complex data distributions and generate synthetic data that closely resemble the real-world data. Then, we use synthetic data to enhance the robustness and completeness of datasets across nodes. Our methodology adheres to FL&rsquo;s privacy requirements by generating synthetic data in a <b>federated</b> <b>manner</b> without sharing the actual data in the process. We incorporate techniques such as classwise sampling and node grouping, designed to improve the <b>federated</b> <b>GAN&rsquo;s</b> performance, enabling the creation of high-quality synthetic datasets and facilitating efficient FL training. Empirical results from our experiments demonstrate that FLIGAN significantly improves the model accuracy, especially in scenarios with high class imbalances, achieving up to a 20% increase in model accuracy over traditional FL baselines.</p></p class="citation"></blockquote><h3 id=1342--72340-convergence-of-a-model-free-entropy-regularized-inverse-reinforcement-learning-algorithm-titouan-renard-et-al-2024>(13/42 | 72/340) Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm (Titouan Renard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Titouan Renard, Andreas Schlaginhaufen, Tingting Ni, Maryam Kamgarpour. (2024)<br><strong>Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm</strong><br><button class=copy-to-clipboard title="Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Reinforcement Learning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16829v1.pdf filename=2403.16829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a dataset of expert demonstrations, inverse <b>reinforcement</b> <b>learning</b> (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a <b>stochastic</b> <b>gradient</b> <b>descent</b> update for the reward and a <b>stochastic</b> <b>soft</b> <b>policy</b> iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the <b>Markov</b> <b>decision</b> <b>process</b> (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.</p></p class="citation"></blockquote><h3 id=1442--73340-symmetric-basis-convolutions-for-learning-lagrangian-fluid-mechanics-rene-winchenbach-et-al-2024>(14/42 | 73/340) Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics (Rene Winchenbach et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rene Winchenbach, Nils Thuerey. (2024)<br><strong>Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics</strong><br><button class=copy-to-clipboard title="Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-comp-ph<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16680v1.pdf filename=2403.16680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning physical <b>simulations</b> has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, whereas Neural solvers aim to address both concerns through machine learning. We propose a general formulation for continuous <b>convolutions</b> using separable basis functions as a superset of existing methods and evaluate a large set of basis functions in the context of (a) a compressible 1D SPH <b>simulation,</b> (b) a weakly compressible 2D SPH <b>simulation,</b> and (c) an incompressible 2D SPH <b>Simulation.</b> We demonstrate that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Our broad evaluation shows that Fourier-based continuous <b>convolutions</b> outperform all other architectures regarding accuracy and generalization. Finally, using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary. An implementation of our approach, as well as complete datasets and solver implementations, is available at <a href=https://github.com/tum-pbs/SFBC>https://github.com/tum-pbs/SFBC</a>.</p></p class="citation"></blockquote><h3 id=1542--74340-revealing-vulnerabilities-of-neural-networks-in-parameter-learning-and-defense-against-explanation-aware-backdoors-md-abdul-kadir-et-al-2024>(15/42 | 74/340) Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors (Md Abdul Kadir et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abdul Kadir, GowthamKrishna Addluri, Daniel Sonntag. (2024)<br><strong>Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors</strong><br><button class=copy-to-clipboard title="Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Automatic Speech Recognition, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16569v1.pdf filename=2403.16569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm&rsquo;s prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model&rsquo;s accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in <b>CNN</b> weights within a <b>CNN</b> following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware <b>adversarial</b> <b>attacks,</b> achieving an approximate decrease of ~99% in the Attack Success Rate <b>(ASR)</b> and a ~91% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.</p></p class="citation"></blockquote><h3 id=1642--75340-fedac-an-adaptive-clustered-federated-learning-framework-for-heterogeneous-data-yuxin-zhang-et-al-2024>(16/42 | 75/340) FedAC: An Adaptive Clustered Federated Learning Framework for Heterogeneous Data (Yuxin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao. (2024)<br><strong>FedAC: An Adaptive Clustered Federated Learning Framework for Heterogeneous Data</strong><br><button class=copy-to-clipboard title="FedAC: An Adaptive Clustered Federated Learning Framework for Heterogeneous Data" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Fine-tuning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16460v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16460v2.pdf filename=2403.16460v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clustered <b>federated</b> <b>learning</b> (CFL) is proposed to mitigate the performance deterioration <b>stemming</b> from data heterogeneity in <b>federated</b> <b>learning</b> (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number <b>fine-tuning</b> module for improved adaptability and scalability in complex, heterogeneous environments. Extensive experiments show that FedAC achieves superior empirical performance, increasing the test accuracy by around 1.82% and 12.67% on CIFAR-10 and CIFAR-100 datasets, respectively, under different non-IID settings compared to SOTA methods.</p></p class="citation"></blockquote><h3 id=1742--76340-on-the-rates-of-convergence-for-learning-with-convolutional-neural-networks-yunfei-yang-et-al-2024>(17/42 | 76/340) On the rates of convergence for learning with convolutional neural networks (Yunfei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunfei Yang, Han Feng, Ding-Xuan Zhou. (2024)<br><strong>On the rates of convergence for learning with convolutional neural networks</strong><br><button class=copy-to-clipboard title="On the rates of convergence for learning with convolutional neural networks" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16459v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16459v1.pdf filename=2403.16459v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the approximation and learning capacities of <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> Our first result proves a new approximation bound for <b>CNNs</b> with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include <b>CNNs</b> as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on <b>CNNs</b> in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on <b>CNNs</b> for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for <b>CNN</b> classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.</p></p class="citation"></blockquote><h3 id=1842--77340-diffusion-based-negative-sampling-on-graphs-for-link-prediction-trung-kien-nguyen-et-al-2024>(18/42 | 77/340) Diffusion-based Negative Sampling on Graphs for Link Prediction (Trung-Kien Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Trung-Kien Nguyen, Yuan Fang. (2024)<br><strong>Diffusion-based Negative Sampling on Graphs for Link Prediction</strong><br><button class=copy-to-clipboard title="Diffusion-based Negative Sampling on Graphs for Link Prediction" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 26<br>Keywords: Diffusion Model, Graph, Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17259v1.pdf filename=2403.17259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link prediction is a fundamental task for <b>graph</b> analysis with important applications on the Web, such as social network analysis and <b>recommendation</b> systems, etc. Modern <b>graph</b> link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the <b>graph,</b> missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness&rsquo;&rsquo; levels from the latent space. Our method, called Conditional <b>Diffusion-based</b> <b>Multi-level</b> Negative Sampling (DMNS), leverages the Markov chain property of <b>diffusion</b> <b>models</b> to generate negative nodes in multiple levels of variable hardness and reconcile them for effective <b>graph</b> link prediction. We further demonstrate that DMNS follows the sub-linear positivity principle for robust negative sampling. Extensive experiments on several <b>benchmark</b> datasets demonstrate the effectiveness of DMNS.</p></p class="citation"></blockquote><h3 id=1942--78340-neural-image-compression-with-quantization-rectifier-wei-luo-et-al-2024>(19/42 | 78/340) Neural Image Compression with Quantization Rectifier (Wei Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Luo, Bo Chen. (2024)<br><strong>Neural Image Compression with Quantization Rectifier</strong><br><button class=copy-to-clipboard title="Neural Image Compression with Quantization Rectifier" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17236v1.pdf filename=2403.17236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural image compression has been shown to outperform traditional image codecs in terms of rate-distortion performance. However, <b>quantization</b> introduces errors in the compression process, which can degrade the quality of the compressed image. Existing approaches address the train-test mismatch problem incurred during <b>quantization,</b> the random impact of <b>quantization</b> on the expressiveness of image features is still unsolved. This paper presents a novel <b>quantization</b> rectifier (QR) method for image compression that leverages image feature correlation to mitigate the impact of <b>quantization.</b> Our method designs a neural network architecture that predicts unquantized features from the <b>quantized</b> ones, preserving feature expressiveness for better image reconstruction quality. We develop a soft-to-predictive training technique to integrate QR into existing neural image codecs. In evaluation, we integrate QR into state-of-the-art neural image codecs and compare enhanced models and baselines on the widely-used Kodak <b>benchmark.</b> The results show consistent coding efficiency improvement by QR with a negligible increase in the running time.</p></p class="citation"></blockquote><h3 id=2042--79340-greedy-and-cody-counterfactual-explainers-for-dynamic-graphs-zhan-qu-et-al-2024>(20/42 | 79/340) GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs (Zhan Qu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhan Qu, Daniel Gomm, Michael Färber. (2024)<br><strong>GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs</strong><br><button class=copy-to-clipboard title="GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16846v1.pdf filename=2403.16846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>Graph</b> <b>Neural</b> <b>Networks</b> (TGNNs), crucial for modeling dynamic <b>graphs</b> <b>with</b> <b>time-varying</b> interactions, face a significant challenge in explainability due to their complex model structure. <b>Counterfactual</b> explanations, crucial for understanding model decisions, examine how input <b>graph</b> <b>changes</b> <b>affect</b> outcomes. This paper introduces two novel <b>counterfactual</b> explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic <b>Graphs)</b> <b>and</b> <b>CoDy</b> <b>(Counterfactual</b> Explainer for Dynamic <b>Graphs).</b> <b>They</b> <b>treat</b> explanations as a search problem, seeking input <b>graph</b> <b>alterations</b> <b>that</b> alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59% higher success rate in finding significant <b>counterfactual</b> inputs. This highlights CoDy&rsquo;s potential in clarifying TGNN decision-making, increasing their transparency and trustworthiness in practice.</p></p class="citation"></blockquote><h3 id=2142--80340-cluster-based-normalization-layer-for-neural-networks-bilal-faye-et-al-2024>(21/42 | 80/340) Cluster-Based Normalization Layer for Neural Networks (Bilal Faye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bilal Faye, Hanane Azzag, Mustapha Lebbah. (2024)<br><strong>Cluster-Based Normalization Layer for Neural Networks</strong><br><button class=copy-to-clipboard title="Cluster-Based Normalization Layer for Neural Networks" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 23<br>Keywords: Clustering, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16798v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16798v1.pdf filename=2403.16798v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions. This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - <b>Supervised</b> Cluster-Based Normalization (SCB-Norm) and <b>Unsupervised</b> Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration. For SCB-Norm, a <b>supervised</b> variant, the novel mechanism involves introducing predefined data partitioning, termed clusters, to normalize activations based on the assigned cluster. This cluster-driven approach creates a space that conforms to a Gaussian mixture model. On the other hand, UCB-Norm, an <b>unsupervised</b> counterpart, dynamically clusters neuron activations during training, adapting to task-specific challenges without relying on predefined data partitions (clusters). This dual approach ensures flexibility in addressing diverse learning scenarios. CB-Norm innovatively uses a one-step normalization approach, where parameters of each mixture component (cluster in activation space) serve as weights for deep neural networks. This adaptive <b>clustering</b> process tackles both <b>clustering</b> and resolution of deep neural network tasks concurrently during training, signifying a notable advancement in the field.</p></p class="citation"></blockquote><h3 id=2242--81340-graphs-generalization-under-distribution-shifts-qin-tian-et-al-2024>(22/42 | 81/340) Graphs Generalization under Distribution Shifts (Qin Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qin Tian, Wenjun Wang, Chen Zhao, Minglai Shao, Wang Zhang, Dong Li. (2024)<br><strong>Graphs Generalization under Distribution Shifts</strong><br><button class=copy-to-clipboard title="Graphs Generalization under Distribution Shifts" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Distribution Shift, Distribution Shift, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16334v1.pdf filename=2403.16334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional machine learning methods heavily rely on the independent and identically <b>distribution</b> <b>assumption,</b> which imposes limitations when the test <b>distribution</b> <b>deviates</b> from the training <b>distribution.</b> <b>To</b> address this crucial issue, <b>out-of-distribution</b> (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown <b>distribution</b> <b>shifts,</b> has made a significant process. However, the OOD method for <b>graph-structured</b> data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, <b>distribution</b> <b>shifts</b> on <b>graphs</b> often occur simultaneously on node attributes and <b>graph</b> topology. Secondly, capturing invariant information amidst diverse <b>distribution</b> <b>shifts</b> proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely <b>Graph</b> Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations across domains by modeling the potential seen or unseen variations of attribute <b>distribution</b> <b>and</b> topological structure and (2) minimize the discrepancy of the variation in a representation space where the target is to predict semantic labels. Extensive experiment results indicate that our model outperforms baseline methods on node-level OOD generalization across domains in <b>distribution</b> <b>shift</b> on node features and topological structures simultaneously.</p></p class="citation"></blockquote><h3 id=2342--82340-exploring-the-potential-of-prototype-based-soft-labels-data-distillation-for-imbalanced-data-classification-radu-andrei-rosu-et-al-2024>(23/42 | 82/340) Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification (Radu-Andrei Rosu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Radu-Andrei Rosu, Mihaela-Elena Breaban, Henri Luchian. (2024)<br><strong>Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification</strong><br><button class=copy-to-clipboard title="Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17130v1.pdf filename=2403.17130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data <b>distillation</b> methods are usually tied to a specific ML algorithm. While recent literature deals mainly with <b>distillation</b> of large collections of images in the context of neural network models, tabular data <b>distillation</b> is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple <b>distillation</b> technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels <b>distillation</b> in terms of classification accuracy, by integrating optimization steps in the <b>distillation</b> process. The analysis is performed on real-world data sets with various degrees of imbalance. Experimental studies trace the capability of the method to <b>distill</b> the data, but also the opportunity to act as an augmentation method, i.e. to generate new data that is able to increase model accuracy when used in conjunction with - as opposed to instead of - the original data.</p></p class="citation"></blockquote><h3 id=2442--83340-enhancing-uav-security-through-zero-trust-architecture-an-advanced-deep-learning-and-explainable-ai-analysis-ekramul-haque-et-al-2024>(24/42 | 83/340) Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis (Ekramul Haque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul Islam. (2024)<br><strong>Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis</strong><br><button class=copy-to-clipboard title="Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Explainable AI, Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17093v1.pdf filename=2403.17093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures. This study highlights the necessity of implementing a <b>Zero</b> <b>Trust</b> Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities. The <b>Zero</b> <b>Trust</b> Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications. The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59%. This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method. Precise identification is crucial in <b>Zero</b> <b>Trust</b> Architecture (ZTA), as it determines network access. In addition, the use of <b>eXplainable</b> <b>Artificial</b> Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) contributes to the improvement of the model&rsquo;s transparency and interpretability. Adherence to <b>Zero</b> <b>Trust</b> Architecture (ZTA) standards guarantees that the classifications of unmanned aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security within the UAV field.</p></p class="citation"></blockquote><h3 id=2542--84340-offline-reinforcement-learning-role-of-state-aggregation-and-trajectory-data-zeyu-jia-et-al-2024>(25/42 | 84/340) Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data (Zeyu Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Jia, Alexander Rakhlin, Ayush Sekhari, Chen-Yu Wei. (2024)<br><strong>Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data</strong><br><button class=copy-to-clipboard title="Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17091v1.pdf filename=2403.17091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We revisit the problem of <b>offline</b> <b>reinforcement</b> <b>learning</b> with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based <b>offline</b> <b>data</b> <b>admits</b> a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of <b>offline</b> <b>policy</b> <b>evaluation.</b> In addition to addressing this question, we provide a rather complete picture for <b>offline</b> <b>policy</b> <b>evaluation</b> with only value function realizability. Our primary findings are threefold: 1) The sample complexity of <b>offline</b> <b>policy</b> <b>evaluation</b> is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the <b>offline</b> <b>data</b> <b>distribution,</b> rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the <b>offline</b> <b>data</b> <b>is</b> admissible (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data. These three pieces jointly resolve the open problem, though each of them could be of independent interest.</p></p class="citation"></blockquote><h3 id=2642--85340-iso-diffusion-improving-diffusion-probabilistic-models-using-the-isotropy-of-the-additive-gaussian-noise-dilum-fernando-et-al-2024>(26/42 | 85/340) Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise (Dilum Fernando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dilum Fernando, Dhananjaya jayasundara, Roshan Godaliyadda, Chaminda Bandara, Parakrama Ekanayake, Vijitha Herath. (2024)<br><strong>Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise</strong><br><button class=copy-to-clipboard title="Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative AI, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16790v1.pdf filename=2403.16790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Denoising Diffusion <b>Probabilistic</b> <b>Models</b> (DDPMs) have accomplished much in the realm of <b>generative</b> <b>AI.</b> Despite their high performance, there is room for improvement, especially in terms of sample fidelity by utilizing statistical properties that impose structural integrity, such as isotropy. Minimizing the mean squared error between the additive and predicted noise alone does not impose constraints on the predicted noise to be isotropic. Thus, we were motivated to utilize the isotropy of the additive noise as a constraint on the objective function to enhance the fidelity of DDPMs. Our approach is simple and can be applied to any DDPM variant. We validate our approach by presenting experiments conducted on four synthetic 2D datasets as well as on unconditional image generation. As demonstrated by the results, the incorporation of this constraint improves the fidelity metrics, Precision and Density for the 2D datasets as well as for the unconditional image generation.</p></p class="citation"></blockquote><h3 id=2742--86340-enhancing-industrial-transfer-learning-with-style-filter-cost-reduction-and-defect-focus-chen-li-et-al-2024>(27/42 | 86/340) Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus (Chen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Li, Ruijie Ma, Xiang Qian, Xiaohao Wang, Xinghui Li. (2024)<br><strong>Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus</strong><br><button class=copy-to-clipboard title="Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Transfer, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16607v1.pdf filename=2403.16607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenge of data scarcity in industrial domains, <b>transfer</b> <b>learning</b> emerges as a pivotal paradigm. This work introduces Style Filter, a tailored methodology for industrial contexts. By selectively filtering source domain data before <b>knowledge</b> <b>transfer,</b> <b>Style</b> Filter reduces the quantity of data while maintaining or even enhancing the performance of <b>transfer</b> <b>learning</b> strategy. Offering label-free operation, minimal reliance on prior <b>knowledge,</b> <b>independence</b> from specific models, and re-utilization, Style Filter is evaluated on authentic industrial datasets, highlighting its effectiveness when employed before conventional <b>transfer</b> <b>strategies</b> in the deep learning domain. The results underscore the effectiveness of Style Filter in real-world industrial applications.</p></p class="citation"></blockquote><h3 id=2842--87340-differentially-private-online-federated-learning-with-correlated-noise-jiaojiao-zhang-et-al-2024>(28/42 | 87/340) Differentially Private Online Federated Learning with Correlated Noise (Jiaojiao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson. (2024)<br><strong>Differentially Private Online Federated Learning with Correlated Noise</strong><br><button class=copy-to-clipboard title="Differentially Private Online Federated Learning with Correlated Noise" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Federated Learning, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16542v1.pdf filename=2403.16542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel differentially private algorithm for online <b>federated</b> <b>learning</b> that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges <b>stemming</b> from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.</p></p class="citation"></blockquote><h3 id=2942--88340-determined-multi-label-learning-via-similarity-based-prompt-meng-wei-et-al-2024>(29/42 | 88/340) Determined Multi-Label Learning via Similarity-Based Prompt (Meng Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Wei, Zhongnian Li, Peng Ying, Yong Zhou, Xinzheng Xu. (2024)<br><strong>Determined Multi-Label Learning via Similarity-Based Prompt</strong><br><button class=copy-to-clipboard title="Determined Multi-Label Learning via Similarity-Based Prompt" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16482v1.pdf filename=2403.16482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In multi-label classification, each training instance is associated with multiple class labels simultaneously. Unfortunately, collecting the fully precise class labels for each training instance is time- and labor-consuming for real-world applications. To alleviate this problem, a novel labeling setting termed \textit{Determined Multi-Label Learning} (DMLL) is proposed, aiming to effectively alleviate the labeling cost inherent in multi-label tasks. In this novel labeling setting, each training instance is associated with a \textit{determined label} (either &ldquo;Yes&rdquo; or &ldquo;No&rdquo;), which indicates whether the training instance contains the provided class label. The provided class label is randomly and uniformly selected from the whole candidate labels set. Besides, each training instance only need to be determined once, which significantly reduce the annotation cost of the labeling task for multi-label datasets. In this paper, we theoretically derive an risk-consistent estimator to learn a multi-label classifier from these determined-labeled training data. Additionally, we introduce a similarity-based <b>prompt</b> <b>learning</b> method for the first time, which minimizes the risk-consistent loss of large-scale pre-trained models to learn a supplemental <b>prompt</b> <b>with</b> richer semantic information. Extensive experimental validation underscores the efficacy of our approach, demonstrating superior performance compared to existing state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3042--89340-proin-learning-to-predict-trajectory-based-on-progressive-interactions-for-autonomous-driving-yinke-dong-et-al-2024>(30/42 | 89/340) ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving (Yinke Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinke Dong, Haifeng Yuan, Hongkun Liu, Wei Jing, Fangzhen Li, Hongmin Liu, Bin Fan. (2024)<br><strong>ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving</strong><br><button class=copy-to-clipboard title="ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-RO, cs.LG<br>Keyword Score: 19<br>Keywords: Graph, Benchmarking, Convolution, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16374v1.pdf filename=2403.16374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate motion prediction of pedestrians, cyclists, and other surrounding vehicles (all called agents) is very important for autonomous driving. Most existing works capture map information through an one-stage interaction with map by vector-based attention, to provide map constraints for social interaction and <b>multi-modal</b> differentiation. However, these methods have to encode all required map rules into the focal agent&rsquo;s feature, so as to retain all possible intentions&rsquo; paths while at the meantime to adapt to potential social interaction. In this work, a progressive interaction network is proposed to enable the agent&rsquo;s feature to progressively focus on relevant maps, in order to better learn agents&rsquo; feature representation capturing the relevant map constraints. The network progressively encode the complex influence of map constraints into the agent&rsquo;s feature through <b>graph</b> <b>convolutions</b> at the following three stages: after historical trajectory encoder, after social interaction, and after <b>multi-modal</b> differentiation. In addition, a weight allocation mechanism is proposed for <b>multi-modal</b> training, so that each mode can obtain learning opportunities from a single-mode ground truth. Experiments have validated the superiority of progressive interactions to the existing one-stage interaction, and demonstrate the effectiveness of each component. Encouraging results were obtained in the challenging <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=3142--90340-manufacturing-service-capability-prediction-with-graph-neural-networks-yunqing-li-et-al-2024>(31/42 | 90/340) Manufacturing Service Capability Prediction with Graph Neural Networks (Yunqing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunqing Li, Xiaorui Liu, Binil Starly. (2024)<br><strong>Manufacturing Service Capability Prediction with Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Manufacturing Service Capability Prediction with Graph Neural Networks" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 18<br>Keywords: Graph, Graph Neural Network, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17239v1.pdf filename=2403.17239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current landscape, the predominant methods for identifying manufacturing capabilities from manufacturers rely heavily on keyword matching and semantic matching. However, these methods often fall short by either overlooking valuable hidden information or misinterpreting critical data. Consequently, such approaches result in an incomplete identification of manufacturers&rsquo; capabilities. This underscores the pressing need for data-driven solutions to enhance the accuracy and completeness of manufacturing capability identification. To address the need, this study proposes a <b>Graph</b> <b>Neural</b> <b>Network-based</b> method for manufacturing service capability identification over a <b>knowledge</b> <b>graph.</b> <b>To</b> <b>enhance</b> the identification performance, this work introduces a novel approach that involves aggregating information from the <b>graph</b> <b>nodes&rsquo;</b> <b>neighborhoods</b> as well as oversampling the <b>graph</b> <b>data,</b> <b>which</b> can be effectively applied across a wide range of practical scenarios. Evaluations conducted on a Manufacturing Service <b>Knowledge</b> <b>Graph</b> <b>and</b> <b>subsequent</b> ablation studies demonstrate the efficacy and robustness of the proposed approach. This study not only contributes a innovative method for inferring manufacturing service capabilities but also significantly augments the quality of Manufacturing Service <b>Knowledge</b> <b>Graphs.</b></p></p class="citation"></blockquote><h3 id=3242--91340-cadgl-context-aware-deep-graph-learning-for-predicting-drug-drug-interactions-azmine-toushik-wasi-et-al-2024>(32/42 | 91/340) CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions (Azmine Toushik Wasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo, Dong-Kyu Chae. (2024)<br><strong>CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions</strong><br><button class=copy-to-clipboard title="CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs.LG, q-bio-BM, q-bio-MN<br>Keyword Score: 13<br>Keywords: Graph, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17210v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17210v2.pdf filename=2403.17210v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug&rsquo;s properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep <b>graph</b> learning by introducing a novel framework named CADGL. Based on a customized variational <b>graph</b> <b>autoencoder</b> (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Our customized VGAE consists of a <b>graph</b> encoder, a latent information encoder, and an MLP decoder. CADGL surpasses other state-of-the-art DDI prediction models, excelling in predicting clinically valuable novel DDIs, supported by rigorous case studies.</p></p class="citation"></blockquote><h3 id=3342--92340-fedfixer-mitigating-heterogeneous-label-noise-in-federated-learning-xinyuan-ji-et-al-2024>(33/42 | 92/340) FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning (Xinyuan Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyuan Ji, Zhaowei Zhu, Wei Xi, Olga Gadyatskaya, Zilong Song, Yong Cai, Yang Liu. (2024)<br><strong>FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning</strong><br><button class=copy-to-clipboard title="FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16561v1.pdf filename=2403.16561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models&rsquo; performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident predictions caused by label noise. Secondly, a distance regularizer is implemented to constrain the disparity between the personalized and global models. We validate the effectiveness of FedFixer through extensive experiments on <b>benchmark</b> datasets. The results demonstrate that FedFixer can perform well in filtering noisy label samples on different clients, especially in highly heterogeneous label noise scenarios.</p></p class="citation"></blockquote><h3 id=3442--93340-scod-from-heuristics-to-theory-vojtech-franc-et-al-2024>(34/42 | 93/340) SCOD: From Heuristics to Theory (Vojtech Franc et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vojtech Franc, Jakub Paplham, Daniel Prusa. (2024)<br><strong>SCOD: From Heuristics to Theory</strong><br><button class=copy-to-clipboard title="SCOD: From Heuristics to Theory" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16916v1.pdf filename=2403.16916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of designing reliable prediction models that abstain from predictions when faced with uncertain or <b>out-of-distribution</b> samples - a recently proposed problem known as Selective Classification in the presence of <b>Out-of-Distribution</b> data (SCOD). We make three key contributions to SCOD. Firstly, we demonstrate that the optimal SCOD strategy involves a Bayes classifier for in-distribution (ID) data and a selector represented as a stochastic linear classifier in a 2D space, using i) the conditional risk of the ID classifier, and ii) the likelihood ratio of ID and <b>out-of-distribution</b> (OOD) data as input. This contrasts with suboptimal strategies from current OOD detection methods and the Softmax Information Retaining Combination (SIRC), specifically developed for SCOD. Secondly, we establish that in a distribution-free setting, the SCOD problem is not Probably Approximately Correct learnable when relying solely on an ID data sample. Third, we introduce POSCOD, a simple method for learning a plugin estimate of the optimal SCOD strategy from both an ID data sample and an unlabeled mixture of ID and OOD data. Our empirical results confirm the theoretical findings and demonstrate that our proposed method, POSCOD, out performs existing OOD methods in effectively addressing the SCOD problem.</p></p class="citation"></blockquote><h3 id=3542--94340-deciphering-the-interplay-between-local-differential-privacy-average-bayesian-privacy-and-maximum-bayesian-privacy-xiaojin-zhang-et-al-2024>(35/42 | 94/340) Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy (Xiaojin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin. (2024)<br><strong>Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy</strong><br><button class=copy-to-clipboard title="Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16591v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16591v2.pdf filename=2403.16591v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local <b>differential</b> <b>privacy</b> (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary&rsquo;s background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between LDP and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. The relationship between LDP and Maximum Bayesian Privacy (MBP) is first revealed, demonstrating that under uniform prior distribution, a mechanism satisfying $\xi$-LDP will satisfy $\xi$-MBP and conversely $\xi$-MBP also confers 2$\xi$-LDP. Our next theoretical contribution are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Maximum Bayesian Privacy (MBP), encapsulated by equations $\epsilon_{p,a} \leq \frac{1}{\sqrt{2}}\sqrt{(\epsilon_{p,m} + \epsilon)\cdot(e^{\epsilon_{p,m} + \epsilon} - 1)}$. These relationships fortify our understanding of the privacy guarantees provided by various mechanisms. Our work not only lays the groundwork for future empirical exploration but also promises to facilitate the design of privacy-preserving algorithms, thereby fostering the development of trustworthy machine learning solutions.</p></p class="citation"></blockquote><h3 id=3642--95340-in-the-search-for-optimal-multi-view-learning-models-for-crop-classification-with-global-remote-sensing-data-francisco-mena-et-al-2024>(36/42 | 95/340) In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data (Francisco Mena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Mena, Diego Arenas, Andreas Dengel. (2024)<br><strong>In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data</strong><br><button class=copy-to-clipboard title="In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16582v1.pdf filename=2403.16582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) and five temporal encoder architectures <b>(LSTM,</b> GRU, TempCNN, TAE, L-TAE) as possible MVL model configurations. The validation is on the CropHarvest dataset that provides optical, radar, and weather time series, and topographic information as input data. We found that in scenarios with a limited number of labeled samples, a unique configuration is insufficient for all the cases. Instead, a specialized combination, including encoder and fusion strategy, should be meticulously sought. To streamline this search process, we suggest initially identifying the optimal encoder architecture tailored for a particular fusion strategy, and then determining the most suitable fusion strategy for the classification task. We provide a technical framework for researchers exploring crop classification or related tasks through a MVL approach.</p></p class="citation"></blockquote><h3 id=3742--96340-accelerating-federated-learning-by-selecting-beneficial-herd-of-local-gradients-ping-luo-et-al-2024>(37/42 | 96/340) Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients (Ping Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ping Luo, Xiaoge Deng, Ziqing Wen, Tao Sun, Dongsheng Li. (2024)<br><strong>Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients</strong><br><button class=copy-to-clipboard title="Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16557v1.pdf filename=2403.16557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) is a distributed machine learning framework in communication network systems. However, the systems&rsquo; Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples are beneficial for model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, We propose the BHerd strategy which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portion of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset, thus accelerating model convergence.</p></p class="citation"></blockquote><h3 id=3842--97340-human-understanding-ai-paper-challenge-2024----dataset-design-se-won-oh-et-al-2024>(38/42 | 97/340) Human Understanding AI Paper Challenge 2024 &ndash; Dataset Design (Se Won Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Se Won Oh, Hyuntae Jeong, Jeong Mook Lim, Seungeun Chung, Kyoung Ju Noh. (2024)<br><strong>Human Understanding AI Paper Challenge 2024 &ndash; Dataset Design</strong><br><button class=copy-to-clipboard title="Human Understanding AI Paper Challenge 2024 -- Dataset Design" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: J-7; E-m, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16509v1.pdf filename=2403.16509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 2024, we will hold a research paper competition (the third Human Understanding AI Paper Challenge) for the research and development of artificial intelligence technologies to understand human daily life. This document introduces the datasets that will be provided to participants in the competition, and <b>summarizes</b> the issues to consider in data processing and learning model development.</p></p class="citation"></blockquote><h3 id=3942--98340-deepmachining-online-prediction-of-machining-errors-of-lathe-machines-xiang-li-lu-et-al-2024>(39/42 | 98/340) DeepMachining: Online Prediction of Machining Errors of Lathe Machines (Xiang-Li Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee, Sheng-Mao Cheng. (2024)<br><strong>DeepMachining: Online Prediction of Machining Errors of Lathe Machines</strong><br><button class=copy-to-clipboard title="DeepMachining: Online Prediction of Machining Errors of Lathe Machines" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16451v4 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16451v4.pdf filename=2403.16451v4.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine&rsquo;s operations to learn the salient features of machining states. Then, we <b>fine-tune</b> the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.</p></p class="citation"></blockquote><h3 id=4042--99340-ensemble-adversarial-defense-via-integration-of-multiple-dispersed-low-curvature-models-kaikang-zhao-et-al-2024>(40/42 | 99/340) Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models (Kaikang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaikang Zhao, Xi Chen, Wei Huang, Liuxin Ding, Xianglong Kong, Fan Zhang. (2024)<br><strong>Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models</strong><br><button class=copy-to-clipboard title="Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16405v1.pdf filename=2403.16405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of an ensemble of deep learning models has been extensively explored to enhance defense against <b>adversarial</b> <b>attacks.</b> The diversity among sub-models increases the attack cost required to deceive the majority of the ensemble, thereby improving the <b>adversarial</b> <b>robustness.</b> While existing approaches mainly center on increasing diversity in feature representations or dispersion of first-order gradients with respect to input, the limited correlation between these diversity metrics and <b>adversarial</b> <b>robustness</b> constrains the performance of ensemble <b>adversarial</b> <b>defense.</b> In this work, we aim to enhance ensemble diversity by reducing attack transferability. We identify second-order gradients, which depict the loss curvature, as a key factor in <b>adversarial</b> <b>robustness.</b> Computing the Hessian matrix involved in second-order gradients is computationally expensive. To address this, we approximate the Hessian-vector product using differential approximation. Given that low curvature provides better robustness, our ensemble model was designed to consider the influence of curvature among different sub-models. We introduce a novel regularizer to train multiple more-diverse low-curvature network models. Extensive experiments across various datasets demonstrate that our ensemble model exhibits superior robustness against a range of attacks, underscoring the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=4142--100340-signsgd-with-federated-voting-chanho-park-et-al-2024>(41/42 | 100/340) SignSGD with Federated Voting (Chanho Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanho Park, H. Vincent Poor, Namyoon Lee. (2024)<br><strong>SignSGD with Federated Voting</strong><br><button class=copy-to-clipboard title="SignSGD with Federated Voting" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16372v1.pdf filename=2403.16372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed learning is commonly used for accelerating model training by harnessing the computational capabilities of multiple-edge devices. However, in practical applications, the communication delay emerges as a bottleneck due to the substantial information exchange required between workers and a central parameter server. SignSGD with majority voting (signSGD-MV) is an effective distributed learning algorithm that can significantly reduce communication costs by one-bit <b>quantization.</b> However, due to heterogeneous computational capabilities, it fails to converge when the mini-batch sizes differ among workers. To overcome this, we propose a novel signSGD optimizer with \textit{federated voting} (signSGD-FV). The idea of federated voting is to exploit learnable weights to perform weighted majority voting. The server learns the weights assigned to the edge devices in an online fashion based on their computational capabilities. Subsequently, these weights are employed to decode the signs of the aggregated local gradients in such a way to minimize the sign decoding error probability. We provide a unified convergence rate analysis framework applicable to scenarios where the estimated weights are known to the parameter server either perfectly or imperfectly. We demonstrate that the proposed signSGD-FV algorithm has a theoretical convergence guarantee even when edge devices use heterogeneous mini-batch sizes. Experimental results show that signSGD-FV outperforms signSGD-MV, exhibiting a faster convergence rate, especially in heterogeneous mini-batch sizes.</p></p class="citation"></blockquote><h3 id=4242--101340-discrete-latent-graph-generative-modeling-with-diffusion-bridges-van-khoa-nguyen-et-al-2024>(42/42 | 101/340) Discrete Latent Graph Generative Modeling with Diffusion Bridges (Van Khoa Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Van Khoa Nguyen, Yoann Boget, Frantzeska Lavda, Alexandros Kalousis. (2024)<br><strong>Discrete Latent Graph Generative Modeling with Diffusion Bridges</strong><br><button class=copy-to-clipboard title="Discrete Latent Graph Generative Modeling with Diffusion Bridges" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16883v1.pdf filename=2403.16883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning <b>graph</b> generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space <b>graph</b> generative model. Unlike most previous latent space <b>graph</b> generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the <b>graph</b> structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of <b>graph</b> <b>benchmark</b> datasets which clearly show the superiority of the discrete latent space and obtain state of the art <b>graph</b> generative performance, making GLAD the first latent space <b>graph</b> generative model with competitive performance. Our source code is published at: \url{https://github.com/v18nguye/GLAD}.</p></p class="citation"></blockquote><h2 id=csai-20>cs.AI (20)</h2><h3 id=120--102340-clha-a-simple-yet-effective-contrastive-learning-framework-for-human-alignment-feiteng-fang-et-al-2024>(1/20 | 102/340) CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment (Feiteng Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao, Chengming Li, Xiping Hu, Ruifeng Xu. (2024)<br><strong>CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment</strong><br><button class=copy-to-clipboard title="CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 90<br>Keywords: Automatic Evaluation, Contrastive Learning, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16649v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16649v2.pdf filename=2403.16649v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> is a crucial technique in aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human preferences, ensuring these <b>LLMs</b> behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on <b>reinforcement</b> <b>learning</b> <b>lies</b> <b>in</b> <b>their</b> inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective <b>Contrastive</b> <b>Learning</b> Framework for Human Alignment (CLHA) to align <b>LLMs</b> with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise <b>contrastive</b> <b>loss</b> and adaptive <b>supervised</b> <b>fine-tuning</b> loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses other algorithms, showcasing superior performance in terms of reward model scores, <b>automatic</b> <b>evaluations,</b> and human assessments on the widely used ``Helpful and Harmless&rsquo;&rsquo; dataset.</p></p class="citation"></blockquote><h3 id=220--103340-re2llm-reflective-reinforcement-large-language-model-for-session-based-recommendation-ziyan-wang-et-al-2024>(2/20 | 103/340) Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation (Ziyan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang. (2024)<br><strong>Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation</strong><br><button class=copy-to-clipboard title="Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Recommendation, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16427v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16427v3.pdf filename=2403.16427v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are emerging as promising approaches to enhance session-based <b>recommendation</b> (SBR), where both <b>prompt-based</b> and <b>fine-tuning-based</b> methods have been widely investigated to align <b>LLMs</b> with SBR. However, the former methods struggle with optimal <b>prompts</b> to elicit the correct <b>reasoning</b> of <b>LLMs</b> due to the lack of task-specific feedback, leading to unsatisfactory <b>recommendations.</b> Although the latter methods attempt to <b>fine-tune</b> <b>LLMs</b> with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones. To address such issues, we propose a Reflective Reinforcement <b>Large</b> <b>Language</b> <b>Model</b> (Re2LLM) for SBR, guiding <b>LLMs</b> to focus on specialized knowledge essential for more accurate <b>recommendations</b> effectively and efficiently. In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by <b>LLMs.</b> To be specific, we direct <b>LLMs</b> to examine <b>recommendation</b> errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors. To efficiently elicit the correct <b>reasoning</b> of <b>LLMs,</b> we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent. It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct <b>LLMs</b> <b>reasoning</b> for better <b>recommendations.</b> Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=320--104340-hallucination-detection-in-foundation-models-for-decision-making-a-flexible-definition-and-review-of-the-state-of-the-art-neeloy-chakraborty-et-al-2024>(3/20 | 104/340) Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art (Neeloy Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell. (2024)<br><strong>Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art</strong><br><button class=copy-to-clipboard title="Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-RO, cs.AI<br>Keyword Score: 60<br>Keywords: Foundation Model, Out-of-distribution, Common-sense Reasoning, Hallucination Detection, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16527v1.pdf filename=2403.16527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, <b>out-of-distribution</b> scenarios that will undoubtedly arise at test-time. The rise of <b>foundation</b> <b>models</b> trained on multiple tasks with impressively <b>large</b> <b>datasets</b> <b>from</b> a variety of fields has led researchers to believe that these models may provide common sense <b>reasoning</b> that existing planners are missing. Researchers posit that this common sense <b>reasoning</b> will bridge the gap between algorithm development and deployment to <b>out-of-distribution</b> tasks, like how humans adapt to unexpected scenarios. <b>Large</b> <b>language</b> <b>models</b> have already penetrated the robotics and autonomous systems domains as researchers are scrambling to showcase their potential use cases in deployment. While this application direction is very promising empirically, <b>foundation</b> <b>models</b> are known to hallucinate and generate decisions that may sound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model&rsquo;s decision, and detect when it may be hallucinating. In this work, we discuss the current use cases of <b>foundation</b> <b>models</b> for decision-making tasks, provide a general definition for <b>hallucinations</b> <b>with</b> examples, discuss existing approaches to <b>hallucination</b> <b>detection</b> and mitigation with a focus on decision problems, and explore areas for further research in this exciting field.</p></p class="citation"></blockquote><h3 id=420--105340-twostep-multi-agent-task-planning-using-classical-planners-and-large-language-models-ishika-singh-et-al-2024>(4/20 | 105/340) TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models (Ishika Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishika Singh, David Traum, Jesse Thomason. (2024)<br><strong>TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models</strong><br><button class=copy-to-clipboard title="TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-MA, cs-RO, cs.AI<br>Keyword Score: 50<br>Keywords: Planning Domain Descrition Language, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17246v1.pdf filename=2403.17246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classical planning formulations like the Planning Domain Definition Language <b>(PDDL)</b> admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, <b>reasoning</b> problems defined in <b>PDDL</b> do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> used for directly inferring plan steps do not guarantee execution success, but do leverage <b>commonsense</b> <b>reasoning</b> to assemble action sequences. We combine the strengths of classical planning and <b>LLMs</b> by approximating human intuitions for two-agent planning goal decomposition. We demonstrate that <b>LLM-based</b> goal decomposition leads to faster planning times than solving multi-agent <b>PDDL</b> problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone and preserving execution success. Additionally, we find that <b>LLM-based</b> approximations of subgoals can achieve similar multi-agent execution steps than those specified by human experts. Website and resources at <a href=https://glamor-usc.github.io/twostep>https://glamor-usc.github.io/twostep</a></p></p class="citation"></blockquote><h3 id=520--106340-generation-of-asset-administration-shell-with-large-language-model-agents-interoperability-in-digital-twins-with-semantic-node-yuchen-xia-et-al-2024>(5/20 | 106/340) Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node (Yuchen Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich. (2024)<br><strong>Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node</strong><br><button class=copy-to-clipboard title="Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-IR, cs-MA, cs-SE, cs.AI<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17209v1.pdf filename=2403.17209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a &ldquo;semantic node&rdquo; data structure to capture the semantic essence of textual data. Then, a system powered by <b>large</b> <b>language</b> <b>models</b> is designed and implemented to process &ldquo;semantic node&rdquo; and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different <b>LLMs</b> and an in-depth ablation study of <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> mechanisms provide insights into the effectiveness of <b>LLM</b> systems for interpreting technical concepts. Our findings emphasize <b>LLMs&rsquo;</b> capability in automating AAS instance creation, enhancing semantic interoperability, and contributing to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are released on our GitHub Repository with the link: <a href=https://github.com/YuchenXia/AASbyLLM>https://github.com/YuchenXia/AASbyLLM</a></p></p class="citation"></blockquote><h3 id=620--107340-how-reliable-is-your-simulator-analysis-on-the-limitations-of-current-llm-based-user-simulators-for-conversational-recommendation-lixi-zhu-et-al-2024>(6/20 | 107/340) How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation (Lixi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lixi Zhu, Xiaowen Huang, Jitao Sang. (2024)<br><strong>How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation</strong><br><button class=copy-to-clipboard title="How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 50<br>Keywords: Recommendation, Recommender System, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16416v1.pdf filename=2403.16416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational <b>Recommender</b> <b>System</b> (CRS) interacts with users through natural language to understand their preferences and provide personalized <b>recommendations</b> in real-time. CRS has demonstrated significant potential, <b>prompting</b> researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on <b>LLMs.</b> While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using <b>LLMs</b> in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational <b>recommendation,</b> we highlight several issues with the current evaluation methods for user simulators based on <b>LLMs:</b> (1) Data leakage, which occurs in conversational history and the user simulator&rsquo;s replies, results in inflated evaluation results. (2) The success of CRS <b>recommendations</b> depends more on the availability and quality of conversational history than on the responses from user simulators. (3) Controlling the output of the user simulator through a single <b>prompt</b> template proves challenging. To overcome these limitations, we propose SimpleUserSim, employing a straightforward strategy to guide the topic toward the target items. Our study validates the ability of CRS models to utilize the interaction information, significantly improving the <b>recommendation</b> results.</p></p class="citation"></blockquote><h3 id=720--108340-concurrent-linguistic-error-detection-cled-for-large-language-models-jinhua-zhu-et-al-2024>(7/20 | 108/340) Concurrent Linguistic Error Detection (CLED) for Large Language Models (Jinhua Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhua Zhu, Javier Conde, Zhen Gao, Pedro Reviriego, Shanshan Liu, Fabrizio Lombardi. (2024)<br><strong>Concurrent Linguistic Error Detection (CLED) for Large Language Models</strong><br><button class=copy-to-clipboard title="Concurrent Linguistic Error Detection (CLED) for Large Language Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 45<br>Keywords: Black Box, T5, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16393v1.pdf filename=2403.16393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The wide adoption of <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for <b>LLMs</b> is an important issue. In many settings, the <b>LLM</b> is considered as a <b>black</b> <b>box</b> with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model&rsquo;s internal nodes. An interesting observation is that the output of <b>LLMs</b> in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the <b>LLM</b> and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only relies on the outputs of the model, then it can be used on <b>LLMs</b> in which there is no access to the internal nodes. The proposed CLED scheme has been evaluated on the <b>T5</b> model when used for news <b>summarization</b> and on the OPUS-MT model when used for translation. In both cases, the same set of linguistic features has been used for error detection to illustrate the applicability of the proposed scheme beyond a specific case. The results show that CLED can detect most of the errors at a low overhead penalty. The use of the concurrent classifier also enables a trade-off between error detection effectiveness and its associated overhead, so providing flexibility to a designer.</p></p class="citation"></blockquote><h3 id=820--109340-towards-algorithmic-fidelity-mental-health-representation-across-demographics-in-synthetic-vs-human-generated-data-shinka-mori-et-al-2024>(8/20 | 109/340) Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data (Shinka Mori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shinka Mori, Oana Ignat, Andrew Lee, Rada Mihalcea. (2024)<br><strong>Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data</strong><br><button class=copy-to-clipboard title="Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs.AI<br>Keyword Score: 30<br>Keywords: GPT, GPT-3, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16909v1.pdf filename=2403.16909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Synthetic data generation has the potential to impact applications and domains with scarce data. However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it. In our paper, we analyze the potential of producing synthetic data using <b>GPT-3</b> by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using <b>LLMs</b> for data generation. Using <b>GPT-3,</b> we develop HEADROOM, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19). Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset. We present the procedures to generate queries to develop depression data using <b>GPT-3,</b> and conduct analyzes to uncover the types of stressors it assigns to demographic groups, which could be used to test the limitations of <b>LLMs</b> for synthetic data generation for depression data. Our findings show that synthetic data mimics some of the human-generated data distribution for the predominant depression stressors across diverse demographics.</p></p class="citation"></blockquote><h3 id=920--110340-harnessing-the-power-of-llms-for-normative-reasoning-in-mass-bastin-tony-roy-savarimuthu-et-al-2024>(9/20 | 110/340) Harnessing the power of LLMs for normative reasoning in MASs (Bastin Tony Roy Savarimuthu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Stephen Cranefield. (2024)<br><strong>Harnessing the power of LLMs for normative reasoning in MASs</strong><br><button class=copy-to-clipboard title="Harnessing the power of LLMs for normative reasoning in MASs" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16524v1.pdf filename=2403.16524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic <b>reasoning.</b> The advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative <b>reasoning</b> and decision-making. This paper examines the potential of <b>LLM-based</b> agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and <b>LLM</b> research. We present our vision for creating normative <b>LLM</b> agents. In particular, we discuss how the recently proposed <b>&ldquo;LLM</b> agent&rdquo; approaches can be extended to implement such normative <b>LLM</b> agents. We also highlight challenges in this emerging field. This paper thus aims to foster collaboration between MAS, NLP and <b>LLM</b> researchers in order to advance the field of normative agents.</p></p class="citation"></blockquote><h3 id=1020--111340-an-experiment-with-the-use-of-chatgpt-for-lcsh-subject-assignment-on-electronic-theses-and-dissertations-eric-h-c-chow-et-al-2024>(10/20 | 111/340) An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations (Eric H. C. Chow et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric H. C. Chow, TJ Kao, Xiaoli Li. (2024)<br><strong>An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations</strong><br><button class=copy-to-clipboard title="An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DL, cs-IR, cs.AI<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16424v1.pdf filename=2403.16424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study delves into the potential use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for generating Library of Congress Subject Headings (LCSH). The authors employed <b>ChatGPT</b> to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that <b>LLMs</b> can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1120--112340-all-artificial-less-intelligence-genai-through-the-lens-of-formal-verification-deepak-narayan-gadde-et-al-2024>(11/20 | 112/340) All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification (Deepak Narayan Gadde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deepak Narayan Gadde, Aman Kumar, Thomas Nalapat, Evgenii Rezunov, Fabio Cappellini. (2024)<br><strong>All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification</strong><br><button class=copy-to-clipboard title="All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16750v1.pdf filename=2403.16750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern hardware designs have grown increasingly efficient and complex. However, they are often susceptible to Common Weakness Enumerations (CWEs). This paper is focused on the formal verification of CWEs in a dataset of hardware designs written in SystemVerilog from Regenerative Artificial Intelligence (AI) powered by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> We applied formal verification to categorize each hardware design as vulnerable or CWE-free. This dataset was generated by 4 different <b>LLMs</b> and features a unique set of designs for each of the 10 CWEs we target in our paper. We have associated the identified vulnerabilities with CWE numbers for a dataset of 60,000 generated SystemVerilog Register Transfer Level (RTL) code. It was also found that most <b>LLMs</b> are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code. Our study reveals that approximately 60% of the hardware designs generated by <b>LLMs</b> are prone to CWEs, posing potential safety and security risks. The dataset could be ideal for training <b>LLMs</b> and Machine Learning (ML) algorithms to abstain from generating CWE-prone hardware designs.</p></p class="citation"></blockquote><h3 id=1220--113340-graph-protection-under-multiple-simultaneous-attacks-a-heuristic-approach-marko-djukanovic-et-al-2024>(12/20 | 113/340) Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach (Marko Djukanovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marko Djukanovic, Stefan Kapunac, Aleksandar Kartelj, Dragan Matic. (2024)<br><strong>Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach</strong><br><button class=copy-to-clipboard title="Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Graph, Heuristic Approach<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17108v1.pdf filename=2403.17108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on developing an effective meta-heuristic approach to protect against simultaneous attacks on nodes of a network modeled using a <b>graph.</b> Specifically, we focus on the $k$-strong Roman domination problem, a generalization of the well-known Roman domination problem on <b>graphs.</b> This general problem is about assigning integer weights to nodes that represent the number of field armies stationed at each node in order to satisfy the protection constraints while minimizing the total weights. These constraints concern the protection of a <b>graph</b> against any simultaneous attack consisting of $k \in \mathbb{N}$ nodes. An attack is considered repelled if each node labeled 0 can be defended by borrowing an army from one of its neighboring nodes, ensuring that the neighbor retains at least one army for self-defense. The $k$-SRD problem has practical applications in various areas, such as developing counter-terrorism strategies or managing supply chain disruptions. The solution to this problem is notoriously difficult to find, as even checking the feasibility of the proposed solution requires an exponential number of steps. We propose a variable neighborhood search algorithm in which the feasibility of the solution is checked by introducing the concept of quasi-feasibility, which is realized by careful sampling within the set of all possible attacks. Extensive experimental evaluations show the scalability and robustness of the proposed approach compared to the two exact approaches from the literature. Experiments are conducted with random networks from the literature and newly introduced random wireless networks as well as with real-world networks. A practical application scenario, using real-world networks, involves applying our approach to <b>graphs</b> extracted from GeoJSON files containing geographic features of hundreds of cities or larger regions.</p></p class="citation"></blockquote><h3 id=1320--114340-enhancing-graph-representation-learning-with-attention-driven-spiking-neural-networks-huifeng-yin-et-al-2024>(13/20 | 114/340) Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks (Huifeng Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huifeng Yin, Mingkun Xu, Jing Pei, Lei Deng. (2024)<br><strong>Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-NE, cs.AI<br>Keyword Score: 11<br>Keywords: Graph, Benchmarking, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17040v1.pdf filename=2403.17040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>representation</b> <b>learning</b> has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for <b>graph</b> learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve <b>graph</b> <b>representation</b> <b>learning.</b> Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a <b>graph</b> during the learning process. We evaluate our proposed method on several <b>benchmark</b> datasets and show that it achieves comparable performance compared to existing <b>graph</b> learning techniques.</p></p class="citation"></blockquote><h3 id=1420--115340-speeding-up-path-planning-via-reinforcement-learning-in-mcts-for-automated-parking-xinlong-zheng-et-al-2024>(14/20 | 115/340) Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking (Xinlong Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinlong Zheng, Xiaozhou Zhang, Donghao Xu. (2024)<br><strong>Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking</strong><br><button class=copy-to-clipboard title="Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17234v1.pdf filename=2403.17234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address a method that integrates <b>reinforcement</b> <b>learning</b> into the Monte Carlo tree search to boost online path planning under fully observable environments for automated parking tasks. Sampling-based planning methods under high-dimensional space can be computationally expensive and time-consuming. State evaluation methods are useful by leveraging the prior knowledge into the search steps, making the process faster in a real-time system. Given the fact that automated parking tasks are often executed under complex environments, a solid but lightweight heuristic guidance is challenging to compose in a traditional analytical way. To overcome this limitation, we propose a <b>reinforcement</b> <b>learning</b> pipeline with a Monte Carlo tree search under the path planning framework. By iteratively learning the value of a state and the best action among samples from its previous cycle&rsquo;s outcomes, we are able to model a value estimator and a policy generator for given states. By doing that, we build up a balancing mechanism between exploration and exploitation, speeding up the path planning process while maintaining its quality without using human expert driver data.</p></p class="citation"></blockquote><h3 id=1520--116340-xaiport-a-service-framework-for-the-early-adoption-of-xai-in-ai-model-development-zerui-wang-et-al-2024>(15/20 | 116/340) XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development (Zerui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zerui Wang, Yan Liu, Abishek Arumugam Thiruselvi, Abdelwahab Hamou-Lhadj. (2024)<br><strong>XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development</strong><br><button class=copy-to-clipboard title="XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16858v1.pdf filename=2403.16858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we propose the early adoption of <b>Explainable</b> <b>AI</b> (XAI) with a focus on three properties: Quality of explanation, the explanation summaries should be consistent across multiple XAI methods; Architectural Compatibility, for effective integration in XAI, the architecture styles of both the XAI methods and the models to be explained must be compatible with the framework; Configurable operations, XAI explanations are operable, akin to machine learning operations. Thus, an explanation for AI models should be reproducible and tractable to be trustworthy. We present XAIport, a framework of XAI microservices encapsulated into Open APIs to deliver early explanations as observation for learning model quality assurance. XAIport enables configurable XAI operations along with machine learning development. We quantify the operational costs of incorporating XAI with three cloud computer vision services on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and Amazon Rekognition. Our findings show comparable operational costs between XAI and traditional machine learning, with XAIport significantly improving both cloud AI model performance and explanation stability.</p></p class="citation"></blockquote><h3 id=1620--117340-improving-diffusion-modelss-data-corruption-resistance-using-scheduled-pseudo-huber-loss-artem-khrapov-et-al-2024>(16/20 | 117/340) Improving Diffusion Models&rsquo;s Data-Corruption Resistance using Scheduled Pseudo-Huber Loss (Artem Khrapov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artem Khrapov, Vadim Popov, Tasnima Sadekova, Assel Yermekova, Mikhail Kudinov. (2024)<br><strong>Improving Diffusion Models&rsquo;s Data-Corruption Resistance using Scheduled Pseudo-Huber Loss</strong><br><button class=copy-to-clipboard title="Improving Diffusion Models's Data-Corruption Resistance using Scheduled Pseudo-Huber Loss" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16728v1.pdf filename=2403.16728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are known to be vulnerable to outliers in training data. In this paper we study an alternative <b>diffusion</b> <b>loss</b> function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers. We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps. We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains. In addition, the loss function we propose can potentially help <b>diffusion</b> <b>models</b> to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms.</p></p class="citation"></blockquote><h3 id=1720--118340-deep-reinforcement-learning-and-mean-variance-strategies-for-responsible-portfolio-optimization-fernando-acero-et-al-2024>(17/20 | 118/340) Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization (Fernando Acero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Acero, Parisa Zehtabi, Nicolas Marchesotti, Michael Cashmore, Daniele Magazzeni, Manuela Veloso. (2024)<br><strong>Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16667v1.pdf filename=2403.16667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective. Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep <b>reinforcement</b> <b>learning</b> formulations have been explored. Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed. In this work, we study the use of deep <b>reinforcement</b> <b>learning</b> for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches. Our results show that deep <b>reinforcement</b> <b>learning</b> policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive and multiplicative utility functions of financial and ESG responsibility objectives.</p></p class="citation"></blockquote><h3 id=1820--119340-learning-to-guide-human-decision-makers-with-vision-language-models-debodeep-banerjee-et-al-2024>(18/20 | 119/340) Learning To Guide Human Decision Makers With Vision-Language Models (Debodeep Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debodeep Banerjee, Stefano Teso, Burcu Sayin, Andrea Passerini. (2024)<br><strong>Learning To Guide Human Decision Makers With Vision-Language Models</strong><br><button class=copy-to-clipboard title="Learning To Guide Human Decision Makers With Vision-Language Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16501v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16501v2.pdf filename=2403.16501v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is increasing interest in developing AIs for assisting human decision-making in high-stakes tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain. Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention. his separation of responsibilities setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine&rsquo;s decisions due to anchoring bias, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained. As a remedy, we introduce learning to guide (LTG), an alternative framework in which - rather than taking control from the human expert - the machine provides guidance useful for decision making, and the human is entirely responsible for coming up with a decision. In order to ensure guidance is interpretable} and task-specific, we develop SLOG, an approach for turning any <b>vision-language</b> model into a capable generator of textual guidance by leveraging a modicum of human feedback. Our empirical evaluation highlights the promise of \method on a challenging, real-world medical diagnosis task.</p></p class="citation"></blockquote><h3 id=1920--120340-towards-trustworthy-automated-driving-through-qualitative-scene-understanding-and-explanations-nassim-belmecheri-et-al-2024>(19/20 | 120/340) Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations (Nassim Belmecheri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nassim Belmecheri, Arnaud Gotlieb, Nadjib Lazaar, Helge Spieker. (2024)<br><strong>Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations</strong><br><button class=copy-to-clipboard title="Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16908v1.pdf filename=2403.16908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding driving scenes and communicating automated vehicle decisions are key requirements for trustworthy automated driving. In this article, we introduce the Qualitative Explainable <b>Graph</b> (QXG), which is a unified symbolic and qualitative representation for scene understanding in urban mobility. The QXG enables interpreting an automated vehicle&rsquo;s environment using sensor data and machine learning models. It utilizes spatio-temporal <b>graphs</b> and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an interpretable scene model. A QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations across various sensor types. Our research showcases the potential of QXG, particularly in the context of automated driving, where it can rationalize decisions by linking the <b>graph</b> with observed actions. These explanations can serve diverse purposes, from informing passengers and alerting vulnerable road users to enabling post-hoc analysis of prior behaviors.</p></p class="citation"></blockquote><h3 id=2020--121340-return-to-tradition-learning-reliable-heuristics-with-classical-machine-learning-dillon-z-chen-et-al-2024>(20/20 | 121/340) Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning (Dillon Z. Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dillon Z. Chen, Felipe Trevizan, Sylvie Thiébaux. (2024)<br><strong>Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning</strong><br><button class=copy-to-clipboard title="Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16508v1.pdf filename=2403.16508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance. In this work, we construct novel <b>graph</b> representations of lifted planning tasks and use the WL algorithm to generate features from them. These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models. Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\text{FF}}$ heuristic in a fair competition setting. It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality. WL-GOOSE is the first learning for planning model which achieves these feats. Furthermore, we study the connections between our novel WL feature generation method, previous theoretically flavoured learning architectures, and Description Logic Features for planning.</p></p class="citation"></blockquote><h2 id=cscv-76>cs.CV (76)</h2><h3 id=176--122340-synthesize-step-by-step-tools-templates-and-llms-as-data-generators-for-reasoning-based-chart-vqa-zhuowan-li-et-al-2024>(1/76 | 122/340) Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA (Zhuowan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuowan Li, Bhavan Jasani, Peng Tang, Shabnam Ghadar. (2024)<br><strong>Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA</strong><br><button class=copy-to-clipboard title="Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Data Augmentation, Question Answering, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16385v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16385v2.pdf filename=2403.16385v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding <b>data</b> <b>visualizations</b> like charts and plots requires <b>reasoning</b> about both <b>visual</b> <b>elements</b> <b>and</b> numerics. Although strong in extractive <b>questions,</b> <b>current</b> chart <b>visual</b> <b>question</b> <b>answering</b> (chart <b>VQA)</b> models suffer on complex <b>reasoning</b> <b>questions.</b> <b>In</b> this work, we address the lack of <b>reasoning</b> ability by <b>data</b> <b>augmentation.</b> We leverage <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> which have shown to have strong <b>reasoning</b> ability, as an automatic <b>data</b> <b>annotator</b> that generates <b>question-answer</b> <b>annotations</b> for chart images. The key innovation in our method lies in the Synthesize Step-by-Step strategy: our <b>LLM-based</b> <b>data</b> <b>generator</b> learns to decompose the complex <b>question</b> <b>into</b> step-by-step sub-questions (rationales), which are then used to derive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic <b>data</b> <b>generated</b> using a template-based <b>QA</b> generation pipeline. Experimental results highlight the significance of the proposed step-by-step generation. By training with the <b>LLM-augmented</b> <b>data</b> <b>(LAMENDA),</b> we significantly enhance the chart <b>VQA</b> models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written <b>questions</b> <b>in</b> the ChartQA dataset, which needs strong <b>reasoning.</b> We hope our work underscores the potential of synthetic <b>data</b> <b>and</b> encourages further exploration of <b>data</b> <b>augmentation</b> using <b>LLMs</b> for <b>reasoning-heavy</b> tasks.</p></p class="citation"></blockquote><h3 id=276--123340-proptest-automatic-property-testing-for-improved-visual-programming-jaywon-koo-et-al-2024>(2/76 | 123/340) PropTest: Automatic Property Testing for Improved Visual Programming (Jaywon Koo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaywon Koo, Ziyan Yang, Paola Cascante-Bonilla, Baishakhi Ray, Vicente Ordonez. (2024)<br><strong>PropTest: Automatic Property Testing for Improved Visual Programming</strong><br><button class=copy-to-clipboard title="PropTest: Automatic Property Testing for Improved Visual Programming" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 68<br>Keywords: Benchmarking, Black Box, Fine-tuning, Question Answering, Reasoning, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16921v1.pdf filename=2403.16921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Visual</b> <b>Programming</b> <b>has</b> emerged as an alternative to end-to-end <b>black-box</b> <b>visual</b> <b>reasoning</b> <b>models.</b> This type of methods leverage <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to decompose a problem and generate the source code for an executable computer program. This strategy has the advantage of offering an interpretable <b>reasoning</b> path and does not require <b>finetuning</b> a model with task-specific data. We propose PropTest, a general strategy that improves <b>visual</b> <b>programming</b> <b>by</b> further using an <b>LLM</b> to generate code that tests for <b>visual</b> <b>properties</b> <b>in</b> an initial round of proposed solutions. Particularly, our method tests for data-type consistency, as well as syntactic and semantic properties in the generated solutions. Our proposed solution outperforms baselines and achieves comparable results to state-of-the-art methods while using smaller and publicly available <b>LLMs</b> (CodeLlama-7B and WizardCoder-15B). This is demonstrated across different <b>benchmarks</b> on <b>visual</b> <b>question</b> <b>answering</b> and referring expression comprehension, showing the efficacy of our approach in enhancing the performance and generalization of <b>visual</b> <b>reasoning</b> <b>tasks.</b> Specifically, PropTest improves ViperGPT by obtaining 48.66% accuracy (+8.3%) on the A-OKVQA <b>benchmark</b> and 52.8% (+3.3%) on the RefCOCO+ <b>benchmark</b> using CodeLlama-7B.</p></p class="citation"></blockquote><h3 id=376--124340-vp3d-unleashing-2d-visual-prompt-for-text-to-3d-generation-yang-chen-et-al-2024>(3/76 | 124/340) VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation (Yang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei. (2024)<br><strong>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</strong><br><button class=copy-to-clipboard title="VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation, Zero-shot, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17001v1.pdf filename=2403.17001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent innovations on text-to-3D generation have featured Score <b>Distillation</b> Sampling (SDS), which enables the <b>zero-shot</b> <b>learning</b> of implicit 3D models (NeRF) by directly <b>distilling</b> prior knowledge from 2D <b>diffusion</b> <b>models.</b> However, current SDS-based models still struggle with intricate text <b>prompts</b> and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual <b>Prompt-guided</b> text-to-3D <b>diffusion</b> <b>model</b> (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual <b>prompt</b> to boost text-to-3D generation. Instead of solely supervising SDS with text <b>prompt,</b> VP3D first capitalizes on 2D <b>diffusion</b> <b>model</b> to generate a high-quality image from input text, which subsequently acts as visual <b>prompt</b> to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual <b>prompt</b> and semantically match with text <b>prompt.</b> Through extensive experiments, we show that the 2D Visual <b>Prompt</b> in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual <b>prompt</b> with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at <a href=https://vp3d-cvpr24.github.io>https://vp3d-cvpr24.github.io</a>.</p></p class="citation"></blockquote><h3 id=476--125340-comp4d-llm-guided-compositional-4d-scene-generation-dejia-xu-et-al-2024>(4/76 | 125/340) Comp4D: LLM-Guided Compositional 4D Scene Generation (Dejia Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang. (2024)<br><strong>Comp4D: LLM-Guided Compositional 4D Scene Generation</strong><br><button class=copy-to-clipboard title="Comp4D: LLM-Guided Compositional 4D Scene Generation" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Knowledge Distillation, Text2image, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16993v1.pdf filename=2403.16993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>diffusion</b> <b>models</b> for 2D and 3D content creation have sparked a surge of interest in generating 4D content. However, the scarcity of 3D scene datasets constrains current methodologies to primarily object-centric generation. To overcome this limitation, we present Comp4D, a novel framework for Compositional 4D Generation. Unlike conventional methods that generate a singular 4D representation of the entire scene, Comp4D innovatively constructs each 4D object within the scene separately. Utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> the framework begins by decomposing an input text <b>prompt</b> into distinct entities and maps out their trajectories. It then constructs the compositional 4D scene by accurately positioning these objects along their designated paths. To refine the scene, our method employs a compositional score <b>distillation</b> technique guided by the pre-defined trajectories, utilizing pre-trained <b>diffusion</b> <b>models</b> across <b>text-to-image,</b> text-to-video, and text-to-3D domains. Extensive experiments demonstrate our outstanding 4D content creation capability compared to prior arts, showcasing superior visual quality, motion fidelity, and enhanced object interactions.</p></p class="citation"></blockquote><h3 id=576--126340-satsynth-augmenting-image-mask-pairs-through-diffusion-models-for-aerial-semantic-segmentation-aysim-toker-et-al-2024>(5/76 | 126/340) SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation (Aysim Toker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aysim Toker, Marvin Eisenberger, Daniel Cremers, Laura Leal-Taixé. (2024)<br><strong>SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation</strong><br><button class=copy-to-clipboard title="SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Diffusion Model, Data Augmentation, Generative Adversarial Network, Probabilistic Model, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16605v1.pdf filename=2403.16605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, semantic segmentation has become a pivotal tool in processing and interpreting satellite imagery. Yet, a prevalent limitation of <b>supervised</b> <b>learning</b> techniques remains the need for extensive manual annotations by experts. In this work, we explore the potential of generative image <b>diffusion</b> <b>to</b> address the scarcity of annotated <b>data</b> <b>in</b> earth observation tasks. The main idea is to learn the joint <b>data</b> <b>manifold</b> of images and labels, leveraging recent advancements in denoising <b>diffusion</b> <b>probabilistic</b> <b>models.</b> To the best of our knowledge, we are the first to generate both images and corresponding masks for satellite segmentation. We find that the obtained pairs not only display high quality in fine-scale features but also ensure a wide sampling diversity. Both aspects are crucial for earth observation <b>data,</b> <b>where</b> semantic classes can vary severely in scale and occurrence frequency. We employ the novel <b>data</b> <b>instances</b> for downstream segmentation, as a form of <b>data</b> <b>augmentation.</b> In our experiments, we provide comparisons to prior works based on discriminative <b>diffusion</b> <b>models</b> or <b>GANs.</b> We demonstrate that integrating generated samples yields significant quantitative improvements for satellite semantic segmentation &ndash; both compared to baselines and when training only on the original data.</p></p class="citation"></blockquote><h3 id=676--127340-segicl-a-universal-in-context-learning-framework-for-enhanced-segmentation-in-medical-imaging-lingdong-shen-et-al-2024>(6/76 | 127/340) SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging (Lingdong Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shining Xiang. (2024)<br><strong>SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging</strong><br><button class=copy-to-clipboard title="SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Fine-tuning, Out-of-distribution, In-context Learning, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16578v1.pdf filename=2403.16578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation models adapting to new tasks in a training-free manner through <b>in-context</b> <b>learning</b> is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to <b>out-of-distribution</b> (OOD) data modalities and tasks, requiring intricate <b>fine-tuning</b> of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct <b>in-context</b> <b>learning</b> with a small set of image-mask pairs, eliminating the need for training the model from scratch or <b>fine-tuning</b> for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of <b>prompt</b> samples and segmentation performance on OOD modalities and tasks. This indicates that SegICL effectively address new segmentation tasks based on contextual information. Additionally, SegICL also exhibits comparable segmentation performance to mainstream models on OOD and in-distribution tasks. Our code will be released soon.</p></p class="citation"></blockquote><h3 id=776--128340-refining-text-to-image-generation-towards-accurate-training-free-glyph-enhanced-image-generation-sanyam-lakhanpal-et-al-2024>(7/76 | 128/340) Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation (Sanyam Lakhanpal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanyam Lakhanpal, Shivang Chopra, Vinija Jain, Aman Chadha, Man Luo. (2024)<br><strong>Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation</strong><br><button class=copy-to-clipboard title="Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Diffusion Model, Optical Character Recognition, Benchmarking, Text2image, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16422v1.pdf filename=2403.16422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, <b>Text-to-Image</b> (T2I) generation approaches based on <b>diffusion</b> <b>models</b> have gained significant attention. However, vanilla <b>diffusion</b> <b>models</b> often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, <b>prompting</b> us to develop a testbed to facilitate future research. We introduce a <b>benchmark,</b> LenCom-Eval, specifically designed for testing models&rsquo; capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce a training-free framework to enhance the two-stage generation approaches. We examine the effectiveness of our approach on both LenCom-Eval and MARIO-Eval <b>benchmarks</b> and demonstrate notable improvements across a range of evaluation metrics, including CLIPScore, <b>OCR</b> precision, recall, F1 score, accuracy, and edit distance scores. For instance, our proposed framework improves the backbone model, TextDiffuser, by more than 23% and 13.5% in terms of <b>OCR</b> word F1 on LenCom-Eval and MARIO-Eval, respectively. Our work makes a unique contribution to the field by focusing on generating images with long and rare text sequences, a niche previously unexplored by existing literature</p></p class="citation"></blockquote><h3 id=876--129340-diffusionact-controllable-diffusion-autoencoder-for-one-shot-face-reenactment-stella-bounareli-et-al-2024>(8/76 | 129/340) DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment (Stella Bounareli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos. (2024)<br><strong>DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment</strong><br><button class=copy-to-clipboard title="DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Autoencoder, Fine-tuning, Generative Adversarial Network, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17217v1.pdf filename=2403.17217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing <b>GAN-based</b> methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in <b>Diffusion</b> <b>Probabilistic</b> <b>Models</b> (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of <b>diffusion</b> <b>models</b> to perform neural face reenactment. Specifically, we propose to control the semantic space of a <b>Diffusion</b> <b>Autoencoder</b> (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation and the facial expressions. Our method allows one-shot, self, and cross-subject reenactment, without requiring subject-specific <b>fine-tuning.</b> We compare against state-of-the-art <b>GAN-,</b> StyleGAN2-, and <b>diffusion-based</b> <b>methods,</b> showing better or on-par reenactment performance.</p></p class="citation"></blockquote><h3 id=976--130340-isolated-diffusion-optimizing-multi-concept-text-to-image-generation-training-freely-with-isolated-diffusion-guidance-jingyuan-zhu-et-al-2024>(9/76 | 130/340) Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance (Jingyuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan. (2024)<br><strong>Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance</strong><br><button class=copy-to-clipboard title="Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Object Detection, Text2image, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16954v1.pdf filename=2403.16954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>text-to-image</b> <b>diffusion</b> <b>models</b> have achieved great success in synthesizing high-quality and diverse images given target text <b>prompts.</b> Despite the revolutionary image generation ability, current state-of-the-art models still struggle to deal with multi-concept generation accurately in many cases. This phenomenon is known as ``concept bleeding" and displays as the unexpected overlapping or merging of various concepts. This paper presents a general approach for <b>text-to-image</b> <b>diffusion</b> <b>models</b> to address the mutual interference between different subjects and their attachments in complex scenes, pursuing better <b>text-image</b> consistency. The core idea is to isolate the synthesizing processes of different concepts. We propose to bind each attachment to corresponding subjects separately with split text <b>prompts.</b> Besides, we introduce a revision method to fix the concept bleeding problem in multi-subject synthesis. We first depend on pre-trained <b>object</b> <b>detection</b> and segmentation models to obtain the layouts of subjects. Then we isolate and resynthesize each subject individually with corresponding text <b>prompts</b> to avoid mutual interference. Overall, we achieve a training-free strategy, named Isolated <b>Diffusion,</b> <b>to</b> optimize multi-concept <b>text-to-image</b> synthesis. It is compatible with the latest Stable <b>Diffusion</b> <b>XL</b> (SDXL) and prior Stable <b>Diffusion</b> <b>(SD)</b> models. We compare our approach with alternative methods using a variety of multi-concept text <b>prompts</b> and demonstrate its effectiveness with clear advantages in <b>text-image</b> consistency and user study.</p></p class="citation"></blockquote><h3 id=1076--131340-dora-3d-visual-grounding-with-order-aware-referring-tung-yu-wu-et-al-2024>(10/76 | 131/340) DOrA: 3D Visual Grounding with Order-Aware Referring (Tung-Yu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang. (2024)<br><strong>DOrA: 3D Visual Grounding with Order-Aware Referring</strong><br><button class=copy-to-clipboard title="DOrA: 3D Visual Grounding with Order-Aware Referring" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Low-Resource, Transformer, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16539v1.pdf filename=2403.16539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D visual <b>grounding</b> aims to identify the target object within a 3D point cloud scene referred to by a natural language description. While previous works attempt to exploit the verbo-visual relation with proposed cross-modal <b>transformers,</b> unstructured natural utterances and scattered objects might lead to undesirable performances. In this paper, we introduce DOrA, a novel 3D visual <b>grounding</b> framework with Order-Aware referring. DOrA is designed to leverage <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to parse language description, suggesting a referential order of anchor objects. Such ordered anchor objects allow DOrA to update visual features and locate the target object during the <b>grounding</b> process. Experimental results on the NR3D and ScanRefer datasets demonstrate our superiority in both <b>low-resource</b> and full-data scenarios. In particular, DOrA surpasses current state-of-the-art frameworks by 9.3% and 7.8% <b>grounding</b> accuracy under 1% data and 10% data settings, respectively.</p></p class="citation"></blockquote><h3 id=1176--132340-dia-llama-towards-large-language-model-driven-ct-report-generation-zhixuan-chen-et-al-2024>(11/76 | 132/340) Dia-LLaMA: Towards Large Language Model-driven CT Report Generation (Zhixuan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixuan Chen, Luyang Luo, Yequan Bie, Hao Chen. (2024)<br><strong>Dia-LLaMA: Towards Large Language Model-driven CT Report Generation</strong><br><button class=copy-to-clipboard title="Dia-LLaMA: Towards Large Language Model-driven CT Report Generation" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Language Generation, Natural Language Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16386v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16386v1.pdf filename=2403.16386v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical report generation has achieved remarkable advancements yet has still been faced with several challenges. First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses. Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information. Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs. Recently, <b>LLM</b> has shown a great ability to generate reliable answers with appropriate <b>prompts,</b> which shed light on addressing the aforementioned challenges. In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance <b>prompts.</b> Considering the high dimension of CT, we leverage a pre-trained ViT3D with perceiver to extract the visual information. To tailor the <b>LLM</b> for report generation and emphasize abnormality, we extract additional diagnostic information by referring to a disease prototype memory bank, which is updated during training to capture common disease representations. Furthermore, we introduce disease-aware attention to enable the model to adjust attention for different diseases. Experiments on the chest CT dataset demonstrated that our proposed method outperformed previous methods and achieved state-of-the-art on both clinical efficacy performance and <b>natural</b> <b>language</b> <b>generation</b> metrics. The code will be made publically available.</p></p class="citation"></blockquote><h3 id=1276--133340-flasheval-towards-fast-and-accurate-evaluation-of-text-to-image-diffusion-generative-models-lin-zhao-et-al-2024>(12/76 | 133/340) FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models (Lin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, Yu Wang. (2024)<br><strong>FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models</strong><br><button class=copy-to-clipboard title="FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Quantization, Text2image, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16379v1.pdf filename=2403.16379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been significant progress in the development of <b>text-to-image</b> generative models. Evaluating the quality of the generative models is one essential step in the development process. Unfortunately, the evaluation process could consume a significant amount of computational resources, making the required periodic evaluation of model performance (e.g., monitoring training progress) impractical. Therefore, we seek to improve the evaluation efficiency by selecting the representative subset of the <b>text-image</b> dataset. We systematically investigate the design choices, including the selection criteria (textural features or image-based metrics) and the selection granularity <b>(prompt-level</b> or set-level). We find that the insights from prior work on subset selection for training data do not generalize to this problem, and we propose FlashEval, an iterative search algorithm tailored to evaluation data selection. We demonstrate the effectiveness of FlashEval on ranking <b>diffusion</b> <b>models</b> with various configurations, including architectures, <b>quantization</b> levels, and sampler schedules on COCO and DiffusionDB datasets. Our searched 50-item subset could achieve comparable evaluation quality to the randomly sampled 500-item subset for COCO annotations on unseen models, achieving a 10x evaluation speedup. We release the condensed subset of these commonly used datasets to help facilitate <b>diffusion</b> <b>algorithm</b> design and evaluation, and open-source FlashEval as a tool for condensing future datasets, accessible at <a href=https://github.com/thu-nics/FlashEval>https://github.com/thu-nics/FlashEval</a>.</p></p class="citation"></blockquote><h3 id=1376--134340-chebmixer-efficient-graph-representation-learning-with-mlp-mixer-xiaoyan-kui-et-al-2024>(13/76 | 134/340) ChebMixer: Efficient Graph Representation Learning with MLP Mixer (Xiaoyan Kui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyan Kui, Haonan Yan, Qinsong Li, Liming Chen, Beiji Zou. (2024)<br><strong>ChebMixer: Efficient Graph Representation Learning with MLP Mixer</strong><br><button class=copy-to-clipboard title="ChebMixer: Efficient Graph Representation Learning with MLP Mixer" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Node Classification, Graph, Graph Neural Network, Representation Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16358v1.pdf filename=2403.16358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> have achieved remarkable success in learning <b>graph</b> <b>representations,</b> <b>especially</b> <b>graph</b> <b>Transformer,</b> <b>which</b> has recently shown superior performance on various <b>graph</b> <b>mining</b> <b>tasks.</b> However, <b>graph</b> <b>Transformer</b> <b>generally</b> treats <b>nodes</b> <b>as</b> tokens, which results in quadratic complexity regarding the number of <b>nodes</b> <b>during</b> <b>self-attention</b> computation. The <b>graph</b> <b>MLP</b> <b>Mixer</b> addresses this challenge by using the efficient MLP Mixer technique from computer vision. However, the time-consuming process of extracting <b>graph</b> <b>tokens</b> <b>limits</b> its performance. In this paper, we present a novel architecture named ChebMixer, a newly <b>graph</b> <b>MLP</b> <b>Mixer</b> that uses fast Chebyshev polynomials-based spectral filtering to extract a sequence of tokens. Firstly, we produce multiscale <b>representations</b> <b>of</b> <b>graph</b> <b>nodes</b> <b>via</b> fast Chebyshev polynomial-based spectral filtering. Next, we consider each <b>node&rsquo;s</b> <b>multiscale</b> <b>representations</b> <b>as</b> a sequence of tokens and refine the <b>node</b> <b>representation</b> <b>with</b> an effective MLP Mixer. Finally, we aggregate the multiscale <b>representations</b> <b>of</b> <b>nodes</b> <b>through</b> Chebyshev interpolation. Owing to the powerful <b>representation</b> <b>capabilities</b> and fast computational properties of MLP Mixer, we can quickly extract more informative <b>node</b> <b>representations</b> <b>to</b> improve the performance of downstream tasks. The experimental results prove our significant improvements in a variety of scenarios ranging from <b>graph</b> <b>node</b> <b>classification</b> to medical image segmentation.</p></p class="citation"></blockquote><h3 id=1476--135340-an-intermediate-fusion-vit-enables-efficient-text-image-alignment-in-diffusion-models-zizhao-hu-et-al-2024>(14/76 | 135/340) An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models (Zizhao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zizhao Hu, Shaochong Jia, Mohammad Rostami. (2024)<br><strong>An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models</strong><br><button class=copy-to-clipboard title="An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Diffusion Model, Multi-modal, Multi-modal, Text2image, Text2image, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16530v1.pdf filename=2403.16530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have been widely used for conditional data cross-modal generation tasks such as <b>text-to-image</b> and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a <b>multimodal</b> data fusion perspective and investigate how different fusion strategies can affect <b>vision-language</b> alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost <b>text-to-image</b> alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank <b>text-to-image</b> attention calculations. We perform experiments using a <b>text-to-image</b> generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fusion mechanism on two common conditioning methods on a U-shaped ViT backbone. Our intermediate fusion model achieves a higher CLIP Score and lower FID, with 20% reduced FLOPs, and 50% increased training speed compared to a strong U-ViT baseline with an early fusion.</p></p class="citation"></blockquote><h3 id=1576--136340-make-it-vivid-dressing-your-animatable-biped-cartoon-characters-from-text-junshu-tang-et-al-2024>(15/76 | 136/340) Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text (Junshu Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junshu Tang, Yanhong Zeng, Ke Fan, Xuheng Wang, Bo Dai, Kai Chen, Lizhuang Ma. (2024)<br><strong>Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</strong><br><button class=copy-to-clipboard title="Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 45<br>Keywords: Adversarial Learning, Geometry, Out-of-domain, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16897v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16897v1.pdf filename=2403.16897v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating and animating 3D biped cartoon characters is crucial and valuable in various applications. Compared with <b>geometry,</b> the diverse texture design plays an important role in making 3D biped cartoon characters vivid and charming. Therefore, we focus on automatic texture design for cartoon characters based on input instructions. This is challenging for domain-specific requirements and a lack of high-quality data. To address this challenge, we propose Make-It-Vivid, the first attempt to enable high-quality texture generation from text in UV space. We prepare a detailed text-texture paired data for 3D characters by using vision-question-answering agents. Then we customize a pretrained <b>text-to-image</b> model to generate texture map with template structure while preserving the natural 2D image knowledge. Furthermore, to enhance fine-grained details, we propose a novel <b>adversarial</b> <b>learning</b> scheme to shorten the domain gap between original dataset and realistic texture domain. Extensive experiments show that our approach outperforms current texture generation methods, resulting in efficient character texturing and faithful generation with <b>prompts.</b> Besides, we showcase various applications such as out of domain generation and texture stylization. We also provide an efficient generation system for automatic text-guided textured character generation and animation.</p></p class="citation"></blockquote><h3 id=1676--137340-engagement-measurement-based-on-facial-landmarks-and-spatial-temporal-graph-convolutional-networks-ali-abedi-et-al-2024>(16/76 | 137/340) Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks (Ali Abedi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Abedi, Shehroz S. Khan. (2024)<br><strong>Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks</strong><br><button class=copy-to-clipboard title="Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17175v1.pdf filename=2403.17175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Engagement in virtual learning is crucial for a variety of factors including learner satisfaction, performance, and compliance with learning programs, but measuring it is a challenging task. There is therefore considerable interest in utilizing artificial intelligence and affective computing to measure engagement in natural settings as well as on a large scale. This paper introduces a novel, privacy-preserving method for engagement measurement from videos. It uses facial landmarks, which carry no personally identifiable information, extracted from videos via the MediaPipe deep learning solution. The extracted facial landmarks are fed to a Spatial-Temporal <b>Graph</b> <b>Convolutional</b> <b>Network</b> (ST-GCN) to output the engagement level of the learner in the video. To integrate the ordinal nature of the engagement variable into the training process, ST-GCNs undergo training in a novel ordinal learning framework based on <b>transfer</b> <b>learning.</b> Experimental results on two video student engagement measurement datasets show the superiority of the proposed method compared to previous methods with improved state-of-the-art on the EngageNet dataset with a %3.1 improvement in four-class engagement level classification accuracy and on the Online Student Engagement dataset with a %1.5 improvement in binary engagement classification accuracy. The relatively lightweight ST-GCN and its integration with the real-time MediaPipe deep learning solution make the proposed approach capable of being deployed on virtual learning platforms and measuring engagement in real time.</p></p class="citation"></blockquote><h3 id=1776--138340-continuous-subject-specific-attribute-control-in-t2i-models-by-identifying-semantic-directions-stefan-andreas-baumann-et-al-2024>(17/76 | 138/340) Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions (Stefan Andreas Baumann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, Björn Ommer. (2024)<br><strong>Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions</strong><br><button class=copy-to-clipboard title="Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17064v1.pdf filename=2403.17064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, advances in <b>text-to-image</b> <b>(T2I)</b> <b>diffusion</b> <b>models</b> have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language <b>prompts</b> (such as no continuous set of intermediate descriptions existing between <code>person'' and </code>old person&rsquo;&rsquo;). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP <b>text</b> <b>embeddings</b> that enable fine-grained subject-specific control of high-level attributes in <b>text-to-image</b> <b>models.</b> Based on this observation, we introduce one efficient optimization-free and one robust optimization-based method to identify these directions for specific attributes from contrastive <b>text</b> <b>prompts.</b> We demonstrate that these directions can be used to augment the <b>prompt</b> <b>text</b> <b>input</b> with fine-grained control over attributes of specific subjects in a compositional manner (control over multiple attributes of a single subject) without having to adapt the <b>diffusion</b> <b>model.</b> Project page: <a href=https://compvis.github.io/attribute-control>https://compvis.github.io/attribute-control</a>. Code is available at <a href=https://github.com/CompVis/attribute-control>https://github.com/CompVis/attribute-control</a>.</p></p class="citation"></blockquote><h3 id=1876--139340-dreamlip-language-image-pre-training-with-long-captions-kecheng-zheng-et-al-2024>(18/76 | 139/340) DreamLIP: Language-Image Pre-training with Long Captions (Kecheng Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, Yujun Shen. (2024)<br><strong>DreamLIP: Language-Image Pre-training with Long Captions</strong><br><button class=copy-to-clipboard title="DreamLIP: Language-Image Pre-training with Long Captions" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Self-supervised Learning, Image2text, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17007v1.pdf filename=2403.17007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language-image pre-training largely relies on how precisely and thoroughly a text describes its paired image. In practice, however, the contents of an image can be so rich that well describing them requires lengthy captions (e.g., with 10 sentences), which are usually missing in existing datasets. Consequently, there are currently no clear evidences on whether and how language-image pre-training could benefit from long captions. To figure this out, we first re-caption 30M images with detailed descriptions using a pre-trained Multi-modality <b>Large</b> <b>Language</b> <b>Model</b> (MLLM), and then study the usage of the resulting captions under a <b>contrastive</b> <b>learning</b> framework. We observe that, each sentence within a long caption is very likely to describe the image partially (e.g., an object). Motivated by this, we propose to dynamically sample sub-captions from the text label to construct multiple positive pairs, and introduce a grouping loss to match the embeddings of each sub-caption with its corresponding local image patches in a <b>self-supervised</b> manner. Experimental results on a wide rage of downstream tasks demonstrate the consistent superiority of our method, termed DreamLIP, over previous alternatives, highlighting its fine-grained representational capacity. It is noteworthy that, on the tasks of <b>image-text</b> retrieval and semantic segmentation, our model trained with 30M <b>image-text</b> pairs achieves on par or even better performance than CLIP trained with 400M pairs. Project page is available at <a href=https://zyf0619sjtu.github.io/dream-lip>https://zyf0619sjtu.github.io/dream-lip</a>.</p></p class="citation"></blockquote><h3 id=1976--140340-sd-dit-unleashing-the-power-of-self-supervised-discrimination-in-diffusion-transformer-rui-zhu-et-al-2024>(19/76 | 140/340) SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer (Rui Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, Chang Wen Chen. (2024)<br><strong>SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer</strong><br><button class=copy-to-clipboard title="SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Reconstruction Loss, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17004v1.pdf filename=2403.17004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>Transformer</b> (DiT) has emerged as the new trend of generative <b>diffusion</b> <b>models</b> on image generation. In view of extremely slow convergence in typical DiT, recent breakthroughs have been driven by mask strategy that significantly improves the training efficiency of DiT with additional intra-image contextual learning. Despite this progress, mask strategy still suffers from two inherent limitations: (a) training-inference discrepancy and (b) fuzzy relations between mask <b>reconstruction</b> <b>&</b> generative <b>diffusion</b> <b>process,</b> resulting in sub-optimal training of DiT. In this work, we address these limitations by novelly unleashing the <b>self-supervised</b> discrimination knowledge to boost DiT training. Technically, we frame our DiT in a teacher-student manner. The teacher-student discriminative pairs are built on the <b>diffusion</b> <b>noises</b> along the same Probability Flow Ordinary Differential Equation (PF-ODE). Instead of applying mask <b>reconstruction</b> <b>loss</b> over both DiT encoder and decoder, we decouple DiT encoder and decoder to separately tackle discriminative and generative objectives. In particular, by encoding discriminative pairs with student and teacher DiT encoders, a new discriminative loss is designed to encourage the inter-image alignment in the <b>self-supervised</b> embedding space. After that, student samples are fed into student DiT decoder to perform the typical generative <b>diffusion</b> <b>task.</b> Extensive experiments are conducted on ImageNet dataset, and our method achieves a competitive balance between training cost and generative capacity.</p></p class="citation"></blockquote><h3 id=2076--141340-multiple-object-tracking-as-id-prediction-ruopeng-gao-et-al-2024>(20/76 | 141/340) Multiple Object Tracking as ID Prediction (Ruopeng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruopeng Gao, Yijun Zhang, Limin Wang. (2024)<br><strong>Multiple Object Tracking as ID Prediction</strong><br><button class=copy-to-clipboard title="Multiple Object Tracking as ID Prediction" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Transformer, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16848v1.pdf filename=2403.16848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Multiple <b>Object</b> <b>Tracking</b> (MOT), tracking-by-detection methods have stood the test for a long time, which split the process into two parts according to the definition: <b>object</b> <b>detection</b> and association. They leverage robust single-frame detectors and treat <b>object</b> <b>association</b> as a post-processing step through hand-crafted heuristic algorithms and surrogate tasks. However, the nature of heuristic techniques prevents end-to-end exploitation of training data, leading to increasingly cumbersome and challenging manual modification while facing complicated or novel scenarios. In this paper, we regard this <b>object</b> <b>association</b> task as an End-to-End <b>in-context</b> ID prediction problem and propose a streamlined baseline called MOTIP. Specifically, we form the target embeddings into historical trajectory information while considering the corresponding IDs as <b>in-context</b> <b>prompts,</b> then directly predict the ID labels for the <b>objects</b> <b>in</b> the current frame. Thanks to this end-to-end process, MOTIP can learn tracking capabilities straight from training data, freeing itself from burdensome hand-crafted algorithms. Without bells and whistles, our method achieves impressive state-of-the-art performance in complex scenarios like DanceTrack and SportsMOT, and it performs competitively with other <b>transformer-based</b> methods on MOT17. We believe that MOTIP demonstrates remarkable potential and can serve as a starting point for future research. The code is available at <a href=https://github.com/MCG-NJU/MOTIP>https://github.com/MCG-NJU/MOTIP</a>.</p></p class="citation"></blockquote><h3 id=2176--142340-from-two-stream-to-one-stream-efficient-rgb-t-tracking-via-mutual-prompt-learning-and-knowledge-distillation-yang-luo-et-al-2024>(21/76 | 142/340) From Two Stream to One Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation (Yang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Luo, Xiqing Guo, Hao Li. (2024)<br><strong>From Two Stream to One Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation</strong><br><button class=copy-to-clipboard title="From Two Stream to One Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16834v1.pdf filename=2403.16834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the complementary nature of visible light and thermal in-frared modalities, object tracking based on the fusion of visible light images and thermal images (referred to as RGB-T tracking) has received increasing attention from researchers in recent years. How to achieve more comprehensive fusion of information from the two modalities at a lower cost has been an issue that re-searchers have been exploring. Inspired by visual <b>prompt</b> <b>learn-ing,</b> we designed a novel two-stream RGB-T tracking architecture based on cross-modal mutual <b>prompt</b> <b>learning,</b> and used this model as a teacher to guide a one-stream student model for rapid learning through <b>knowledge</b> <b>distillation</b> techniques. Extensive experiments have shown that, compared to similar RGB-T track-ers, our designed teacher model achieved the highest precision rate, while the student model, with comparable precision rate to the teacher model, realized an inference speed more than three times faster than the teacher model.(Codes will be available if accepted.)</p></p class="citation"></blockquote><h3 id=2276--143340-let-real-images-be-as-a-judger-spotting-fake-images-synthesized-with-generative-models-ziyou-liang-et-al-2024>(22/76 | 143/340) Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models (Ziyou Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyou Liang, Run Wang, Weifeng Liu, Yuyang Zhang, Wenyuan Yang, Lina Wang, Xingkai Wang. (2024)<br><strong>Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models</strong><br><button class=copy-to-clipboard title="Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Contrastive Learning, Generative Adversarial Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16513v1.pdf filename=2403.16513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the last few years, generative models have shown their powerful capabilities in synthesizing realistic images in both quality and diversity (i.e., facial images, and natural subjects). Unfortunately, the artifact patterns in fake images synthesized by different generative models are inconsistent, leading to the failure of previous research that relied on spotting subtle differences between real and fake. In our preliminary experiments, we find that the artifacts in fake images always change with the development of the generative model, while natural images exhibit stable statistical properties. In this paper, we employ natural traces shared only by real images as an additional predictive target in the detector. Specifically, the natural traces are learned from the wild real images and we introduce extended <b>supervised</b> <b>contrastive</b> <b>learning</b> to bring them closer to real images and further away from fake ones. This motivates the detector to make decisions based on the proximity of images to the natural traces. To conduct a comprehensive experiment, we built a high-quality and diverse dataset that includes generative models comprising 6 <b>GAN</b> and 6 <b>diffusion</b> <b>models,</b> to evaluate the effectiveness in generalizing unknown forgery techniques and robustness in surviving different transformations. Experimental results show that our proposed method gives 96.1% mAP significantly outperforms the baselines. Extensive experiments conducted on the widely recognized platform Midjourney reveal that our proposed method achieves an accuracy exceeding 78.4%, underscoring its practicality for real-world application deployment. The source code and partial self-built dataset are available in supplementary material.</p></p class="citation"></blockquote><h3 id=2376--144340-cmvim-contrastive-masked-vim-autoencoder-for-3d-multi-modal-representation-learning-for-ad-classification-guangqian-yang-et-al-2024>(23/76 | 144/340) CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification (Guangqian Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangqian Yang, Kangrui Du, Zhihan Yang, Ye Du, Yongping Zheng, Shujun Wang. (2024)<br><strong>CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification</strong><br><button class=copy-to-clipboard title="CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 38<br>Keywords: Autoencoder, Contrastive Learning, Multi-modal, Representation Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16520v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16520v1.pdf filename=2403.16520v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Alzheimer&rsquo;s disease (AD) is an incurable neurodegenerative condition leading to cognitive and functional deterioration. Given the lack of a cure, <b>prompt</b> and precise AD diagnosis is vital, a complex process dependent on multiple factors and <b>multi-modal</b> data. While successful efforts have been made to integrate <b>multi-modal</b> <b>representation</b> <b>learning</b> into medical datasets, scant attention has been given to 3D medical images. In this paper, we propose <b>Contrastive</b> <b>Masked</b> Vim <b>Autoencoder</b> (CMViM), the first efficient <b>representation</b> <b>learning</b> method tailored for 3D <b>multi-modal</b> data. Our proposed framework is built on a masked Vim <b>autoencoder</b> to learn a unified <b>multi-modal</b> <b>representation</b> <b>and</b> long-dependencies contained in 3D medical images. We also introduce an intra-modal <b>contrastive</b> <b>learning</b> module to enhance the capability of the <b>multi-modal</b> Vim encoder for modeling the discriminative features in the same modality, and an inter-modal <b>contrastive</b> <b>learning</b> module to alleviate misaligned <b>representation</b> <b>among</b> modalities. Our framework consists of two main steps: 1) incorporate the Vision Mamba (Vim) into the mask <b>autoencoder</b> to reconstruct 3D masked <b>multi-modal</b> data efficiently. 2) align the <b>multi-modal</b> <b>representations</b> <b>with</b> <b>contrastive</b> <b>learning</b> mechanisms from both intra-modal and inter-modal aspects. Our framework is pre-trained and validated ADNI2 dataset and validated on the downstream task for AD classification. The proposed CMViM yields 2.7% AUC performance improvement compared with other state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=2476--145340-self-supervised-learning-for-medical-image-data-with-anatomy-oriented-imaging-planes-tianwei-zhang-et-al-2024>(24/76 | 145/340) Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes (Tianwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianwei Zhang, Dong Wei, Mengmeng Zhua, Shi Gu, Yefeng Zheng. (2024)<br><strong>Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes</strong><br><button class=copy-to-clipboard title="Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 35<br>Keywords: Representation Learning, Self-supervised Learning, Self-supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16499v1.pdf filename=2403.16499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> has emerged as a powerful tool for pretraining deep networks on unlabeled data, prior to <b>transfer</b> <b>learning</b> of target tasks with limited annotation. The relevance between the pretraining pretext and target tasks is crucial to the success of <b>transfer</b> <b>learning.</b> Various pretext tasks have been proposed to utilize properties of medical image data (e.g., three dimensionality), which are more relevant to medical image analysis than generic ones for natural images. However, previous work rarely paid attention to data with anatomy-oriented imaging planes, e.g., standard cardiac magnetic resonance imaging views. As these imaging planes are defined according to the anatomy of the imaged organ, pretext tasks effectively exploiting this information can pretrain the networks to gain knowledge on the organ of interest. In this work, we propose two complementary pretext tasks for this group of medical image data based on the spatial relationship of the imaging planes. The first is to learn the relative orientation between the imaging planes and implemented as regressing their intersecting lines. The second exploits parallel imaging planes to regress their relative slice locations within a stack. Both pretext tasks are conceptually straightforward and easy to implement, and can be combined in multitask learning for better <b>representation</b> <b>learning.</b> Thorough experiments on two anatomical structures (heart and knee) and representative target tasks (semantic segmentation and classification) demonstrate that the proposed pretext tasks are effective in pretraining deep networks for remarkably boosted performance on the target tasks, and superior to other recent approaches.</p></p class="citation"></blockquote><h3 id=2576--146340-animateme-4d-facial-expressions-via-diffusion-models-dimitrios-gerogiannis-et-al-2024>(25/76 | 146/340) AnimateMe: 4D Facial Expressions via Diffusion Models (Dimitrios Gerogiannis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimitrios Gerogiannis, Foivos Paraperas Papantoniou, Rolandos Alexandros Potamias, Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Stefanos Zafeiriou. (2024)<br><strong>AnimateMe: 4D Facial Expressions via Diffusion Models</strong><br><button class=copy-to-clipboard title="AnimateMe: 4D Facial Expressions via Diffusion Models" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17213v1.pdf filename=2403.17213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of photorealistic 3D avatar reconstruction and generation has garnered significant attention in recent years; however, animating such avatars remains challenging. Recent advances in <b>diffusion</b> <b>models</b> have notably enhanced the capabilities of generative models in 2D animation. In this work, we directly utilize these models within the 3D domain to achieve controllable and high-fidelity 4D facial animation. By integrating the strengths of <b>diffusion</b> <b>processes</b> and geometric deep learning, we employ <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> as denoising <b>diffusion</b> <b>models</b> in a novel approach, formulating the <b>diffusion</b> <b>process</b> directly on the mesh space and enabling the generation of 3D facial expressions. This facilitates the generation of facial deformations through a mesh-diffusion-based model. Additionally, to ensure temporal coherence in our animations, we propose a consistent noise sampling method. Under a series of both quantitative and qualitative experiments, we showcase that the proposed method outperforms prior work in 4D expression synthesis by generating high-fidelity extreme expressions. Furthermore, we applied our method to textured 4D facial expression generation, implementing a straightforward extension that involves training on a large-scale textured 4D facial expression database.</p></p class="citation"></blockquote><h3 id=2676--147340-dpstyler-dynamic-promptstyler-for-source-free-domain-generalization-yunlong-tang-et-al-2024>(26/76 | 147/340) DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization (Yunlong Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunlong Tang, Yuxuan Wan, Lei Qi, Xin Geng. (2024)<br><strong>DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization</strong><br><button class=copy-to-clipboard title="DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16697v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16697v1.pdf filename=2403.16697v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Source-Free Domain Generalization (SFDG) aims to develop a model that works for unseen target domains without relying on any source domain. Recent work, PromptStyler, employs text <b>prompts</b> to simulate different <b>distribution</b> <b>shifts</b> in the joint <b>vision-language</b> space, allowing the model to generalize effectively to unseen domains without using any images. However, 1) PromptStyler&rsquo;s style generation strategy has limitations, as all style patterns are fixed after the first training phase. This leads to the training set in the second training phase being restricted to a limited set of styles. Additionally, 2) the frozen text encoder in PromptStyler result in the encoder&rsquo;s output varying with the style of the input text <b>prompts,</b> making it difficult for the model to learn domain-invariant features. In this paper, we introduce Dynamic PromptStyler (DPStyler), comprising Style Generation and Style Removal modules to address these issues. The Style Generation module refreshes all styles at every training epoch, while the Style Removal module eliminates variations in the encoder&rsquo;s output features caused by input styles. Moreover, since the Style Generation module, responsible for generating style word vectors using random sampling or style mixing, makes the model sensitive to input text <b>prompts,</b> we introduce a model ensemble method to mitigate this sensitivity. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=2776--148340-ai-generated-video-detection-via-spatio-temporal-anomaly-learning-jianfa-bai-et-al-2024>(27/76 | 148/340) AI-Generated Video Detection via Spatio-Temporal Anomaly Learning (Jianfa Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianfa Bai, Man Lin, Gang Cao. (2024)<br><strong>AI-Generated Video Detection via Spatio-Temporal Anomaly Learning</strong><br><button class=copy-to-clipboard title="AI-Generated Video Detection via Spatio-Temporal Anomaly Learning" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16638v1.pdf filename=2403.16638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of generation models has led to the emergence of highly realistic artificial intelligence (AI)-generated videos. Malicious users can easily create non-existent videos to spread false information. This letter proposes an effective AI-generated video detection (AIGVDet) scheme by capturing the forensic traces with a two-branch spatio-temporal <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN).</b> Specifically, two ResNet sub-detectors are learned separately for identifying the anomalies in spatical and optical flow domains, respectively. Results of such sub-detectors are fused to further enhance the discrimination ability. A large-scale generated video dataset (GVD) is constructed as a <b>benchmark</b> for model training and evaluation. Extensive experimental results verify the high generalization and robustness of our AIGVDet scheme. Code and dataset will be available at <a href=https://github.com/multimediaFor/AIGVDet>https://github.com/multimediaFor/AIGVDet</a>.</p></p class="citation"></blockquote><h3 id=2876--149340-goodsam-bridging-domain-and-capacity-gaps-via-segment-anything-model-for-distortion-aware-panoramic-semantic-segmentation-weiming-zhang-et-al-2024>(28/76 | 149/340) GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation (Weiming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiming Zhang, Yexin Liu, Xu Zheng, Lin Wang. (2024)<br><strong>GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation</strong><br><button class=copy-to-clipboard title="GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Transfer, Zero-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16370v1.pdf filename=2403.16370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles a novel yet challenging problem: how to transfer <b>knowledge</b> <b>from</b> the emerging Segment Anything Model (SAM) &ndash; which reveals impressive <b>zero-shot</b> instance segmentation capacity &ndash; to learn a compact panoramic semantic segmentation model, i.e., student, without requiring any labeled data. This poses considerable challenges due to SAM&rsquo;s inability to provide semantic labels and the large capacity gap between SAM and the student. To this end, we propose a novel framework, called GoodSAM, that introduces a teacher assistant (TA) to provide semantic information, integrated with SAM to generate ensemble logits to achieve <b>knowledge</b> <b>transfer.</b> Specifically, we propose a Distortion-Aware Rectification (DAR) module that first addresses the distortion problem of panoramic images by imposing prediction-level consistency and boundary enhancement. This subtly enhances TA&rsquo;s prediction capacity on panoramic images. DAR then incorporates a cross-task complementary fusion block to adaptively merge the predictions of SAM and TA to obtain more reliable ensemble logits. Moreover, we introduce a Multi-level <b>Knowledge</b> <b>Adaptation</b> (MKA) module to efficiently transfer the multi-level feature <b>knowledge</b> <b>from</b> TA and ensemble logits to learn a compact student model. Extensive experiments on two <b>benchmarks</b> show that our GoodSAM achieves a remarkable +3.75% mIoU improvement over the state-of-the-art (SOTA) <b>domain</b> <b>adaptation</b> methods. Also, our most lightweight model achieves comparable performance to the SOTA methods with only 3.7M parameters.</p></p class="citation"></blockquote><h3 id=2976--150340-flashface-human-image-personalization-with-high-fidelity-identity-preservation-shilong-zhang-et-al-2024>(29/76 | 150/340) FlashFace: Human Image Personalization with High-fidelity Identity Preservation (Shilong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shilong Zhang, Lianghua Huang, Xi Chen, Yifei Zhang, Zhi-Fan Wu, Yutong Feng, Wei Wang, Yujun Shen, Yu Liu, Ping Luo. (2024)<br><strong>FlashFace: Human Image Personalization with High-fidelity Identity Preservation</strong><br><button class=copy-to-clipboard title="FlashFace: Human Image Personalization with High-fidelity Identity Preservation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Instruction Following, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17008v1.pdf filename=2403.17008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents FlashFace, a practical tool with which users can easily personalize their own photos on the fly by providing one or a few reference face images and a text <b>prompt.</b> Our approach is distinguishable from existing human photo customization methods by higher-fidelity identity preservation and better <b>instruction</b> <b>following,</b> benefiting from two subtle designs. First, we encode the face identity into a series of feature maps instead of one image token as in prior arts, allowing the model to retain more details of the reference faces (e.g., scars, tattoos, and face shape ). Second, we introduce a disentangled integration strategy to balance the text and image guidance during the <b>text-to-image</b> generation process, alleviating the conflict between the reference faces and the text <b>prompts</b> (e.g., personalizing an adult into a &ldquo;child&rdquo; or an &ldquo;elder&rdquo;). Extensive experimental results demonstrate the effectiveness of our method on various applications, including human image personalization, face swapping under language <b>prompts,</b> making virtual characters into real people, etc. Project Page: <a href=https://jshilong.github.io/flashface-page>https://jshilong.github.io/flashface-page</a>.</p></p class="citation"></blockquote><h3 id=3076--151340-invertible-diffusion-models-for-compressed-sensing-bin-chen-et-al-2024>(30/76 | 151/340) Invertible Diffusion Models for Compressed Sensing (Bin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Chen, Zhenyu Zhang, Weiqi Li, Chen Zhao, Jiwen Yu, Shijie Zhao, Jie Chen, Jian Zhang. (2024)<br><strong>Invertible Diffusion Models for Compressed Sensing</strong><br><button class=copy-to-clipboard title="Invertible Diffusion Models for Compressed Sensing" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17006v1.pdf filename=2403.17006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained <b>diffusion</b> <b>models</b> for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible <b>Diffusion</b> <b>Models</b> (IDM), a novel efficient, end-to-end <b>diffusion-based</b> <b>CS</b> method. IDM repurposes a large-scale <b>diffusion</b> <b>sampling</b> process as a reconstruction model, and <b>finetunes</b> it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end <b>finetuning,</b> we propose a novel two-level invertible design to transform both (1) the multi-step sampling process and (2) the noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent <b>diffusion</b> <b>model-based</b> approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference.</p></p class="citation"></blockquote><h3 id=3176--152340-be-yourself-bounded-attention-for-multi-subject-text-to-image-generation-omer-dahary-et-al-2024>(31/76 | 152/340) Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation (Omer Dahary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Dahary, Or Patashnik, Kfir Aberman, Daniel Cohen-Or. (2024)<br><strong>Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16990v1.pdf filename=2403.16990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> <b>diffusion</b> <b>models</b> have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input <b>prompts</b> that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the <b>diffusion</b> <b>model&rsquo;s</b> attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for bounding the information flow in the sampling process. Bounded Attention prevents detrimental leakage among subjects and enables guiding the generation to promote each subject&rsquo;s individuality, even with complex multi-subject conditioning. Through extensive experimentation, we demonstrate that our method empowers the generation of multiple subjects that better align with given <b>prompts</b> and layouts.</p></p class="citation"></blockquote><h3 id=3276--153340-urbanvlp-a-multi-granularity-vision-language-pre-trained-foundation-model-for-urban-indicator-prediction-xixuan-hao-et-al-2024>(32/76 | 153/340) UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction (Xixuan Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xixuan Hao, Wei Chen, Yibo Yan, Siru Zhong, Kun Wang, Qingsong Wen, Yuxuan Liang. (2024)<br><strong>UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction</strong><br><button class=copy-to-clipboard title="UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Text Generation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16831v1.pdf filename=2403.16831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel <b>Vision-Language</b> Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic <b>text</b> <b>generation</b> and calibration, elevating interpretability in downstream applications by producing high-quality <b>text</b> <b>descriptions</b> of urban imagery. Rigorous experiments conducted across six socio-economic tasks underscore UrbanVLP&rsquo;s superior performance. We also deploy a web platform to verify its practicality.</p></p class="citation"></blockquote><h3 id=3376--154340-hpl-ess-hybrid-pseudo-labeling-for-unsupervised-event-based-semantic-segmentation-linglin-jing-et-al-2024>(33/76 | 154/340) HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation (Linglin Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang, Gerald Schaefer, Hui Fang, Bin Zhao, Xuelong Li. (2024)<br><strong>HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation</strong><br><button class=copy-to-clipboard title="HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16788v1.pdf filename=2403.16788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event-based semantic segmentation has gained popularity due to its capability to deal with scenarios under high-speed motion and extreme lighting conditions, which cannot be addressed by conventional RGB cameras. Since it is hard to annotate event data, previous approaches rely on event-to-image reconstruction to obtain pseudo labels for training. However, this will inevitably introduce noise, and learning from noisy pseudo labels, especially when generated from a single source, may reinforce the errors. This drawback is also called confirmation bias in pseudo-labeling. In this paper, we propose a novel hybrid pseudo-labeling framework for <b>unsupervised</b> event-based semantic segmentation, HPL-ESS, to alleviate the influence of noisy pseudo labels. In particular, we first employ a plain <b>unsupervised</b> <b>domain</b> <b>adaptation</b> framework as our baseline, which can generate a set of pseudo labels through self-training. Then, we incorporate offline event-to-image reconstruction into the framework, and obtain another set of pseudo labels by predicting segmentation maps on the reconstructed images. A noisy label learning strategy is designed to mix the two sets of pseudo labels and enhance the quality. Moreover, we propose a soft prototypical alignment module to further improve the consistency of target <b>domain</b> <b>features.</b> Extensive experiments show that our proposed method outperforms existing state-of-the-art methods by a large margin on the DSEC-Semantic dataset (+5.88% accuracy, +10.32% mIoU), which even surpasses several <b>supervised</b> methods.</p></p class="citation"></blockquote><h3 id=3476--155340-sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions-yuda-song-et-al-2024>(34/76 | 155/340) SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions (Yuda Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuda Song, Zehao Sun, Xuanwu Yin. (2024)<br><strong>SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions</strong><br><button class=copy-to-clipboard title="SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16627v1.pdf filename=2403.16627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>diffusion</b> <b>models</b> have positioned them at the forefront of image generation. Despite their superior performance, <b>diffusion</b> <b>models</b> are not without drawbacks; they are characterized by complex architectures and substantial computational demands, resulting in significant latency due to their iterative sampling process. To mitigate these limitations, we introduce a dual approach involving model miniaturization and a reduction in sampling steps, aimed at significantly decreasing model latency. Our methodology leverages <b>knowledge</b> <b>distillation</b> to streamline the U-Net and image decoder architectures, and introduces an innovative one-step DM training technique that utilizes feature matching and score <b>distillation.</b> We present two models, SDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS (30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU, respectively. Moreover, our training approach offers promising applications in image-conditioned control, facilitating efficient image-to-image translation.</p></p class="citation"></blockquote><h3 id=3576--156340-vmrnn-integrating-vision-mamba-and-lstm-for-efficient-and-accurate-spatiotemporal-forecasting-yujin-tang-et-al-2024>(35/76 | 156/340) VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting (Yujin Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujin Tang, Peijie Dong, Zhenheng Tang, Xiaowen Chu, Junwei Liang. (2024)<br><strong>VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting</strong><br><button class=copy-to-clipboard title="VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16536v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16536v2.pdf filename=2403.16536v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining <b>CNNs</b> or ViTs, with <b>RNNs</b> for spatiotemporal forecasting, has yielded unparalleled results in predicting temporal and spatial dynamics. However, modeling extensive global information remains a formidable challenge; <b>CNNs</b> are limited by their narrow receptive fields, and ViTs struggle with the intensive computational demands of their attention mechanisms. The emergence of recent Mamba-based architectures has been met with enthusiasm for their exceptional long-sequence modeling capabilities, surpassing established vision models in efficiency and accuracy, which motivates us to develop an innovative architecture tailored for spatiotemporal forecasting. In this paper, we propose the VMRNN cell, a new recurrent unit that integrates the strengths of Vision Mamba blocks with <b>LSTM.</b> We construct a network centered on VMRNN cells to tackle spatiotemporal prediction tasks effectively. Our extensive evaluations show that our proposed approach secures competitive results on a variety of tasks while maintaining a smaller model size. Our code is available at <a href=https://github.com/yyyujintang/VMRNN-PyTorch>https://github.com/yyyujintang/VMRNN-PyTorch</a>.</p></p class="citation"></blockquote><h3 id=3676--157340-ct-bound-fast-boundary-estimation-from-noisy-images-via-hybrid-convolution-and-transformer-neural-networks-wei-xu-et-al-2024>(36/76 | 157/340) CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks (Wei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Xu, Junjie Luo, Qi Guo. (2024)<br><strong>CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks</strong><br><button class=copy-to-clipboard title="CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16494v1.pdf filename=2403.16494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CT-Bound, a fast boundary estimation method for noisy images using a hybrid <b>Convolution</b> and <b>Transformer</b> neural network. The proposed architecture decomposes boundary estimation into two tasks: local detection and global regularization of image boundaries. It first estimates a parametric representation of boundary structures only using the input image within a small receptive field and then refines the boundary structure in the parameter domain without accessing the input image. Because of this, a part of the network can be easily trained using naive, synthetic images and still generalized to real images, and the entire architecture is computationally efficient as the boundary refinement is non-iterative and not in the image domain. Compared with the previous highest accuracy methods, our experiment shows that CT-Bound is 100 times faster, producing comparably accurate, high-quality boundary and color maps. We also demonstrate that CT-Bound can produce boundary and color maps on real captured images without extra <b>fine-tuning</b> and real-time boundary map and color map videos at ten frames per second.</p></p class="citation"></blockquote><h3 id=3776--158340-distilling-semantic-priors-from-sam-to-efficient-image-restoration-models-quan-zhang-et-al-2024>(37/76 | 158/340) Distilling Semantic Priors from SAM to Efficient Image Restoration Models (Quan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quan Zhang, Xiaoyu Liu, Wei Li, Hanting Chen, Junchao Liu, Jie Hu, Zhiwei Xiong, Chun Yuan, Yunhe Wang. (2024)<br><strong>Distilling Semantic Priors from SAM to Efficient Image Restoration Models</strong><br><button class=copy-to-clipboard title="Distilling Semantic Priors from SAM to Efficient Image Restoration Models" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Self-Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16368v1.pdf filename=2403.16368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In image restoration (IR), leveraging semantic priors from segmentation models has been a common approach to improve performance. The recent segment anything model (SAM) has emerged as a powerful tool for extracting advanced semantic priors to enhance IR tasks. However, the computational cost of SAM is prohibitive for IR, compared to existing smaller IR models. The incorporation of SAM for extracting semantic priors considerably hampers the model inference efficiency. To address this issue, we propose a general framework to <b>distill</b> SAM&rsquo;s semantic knowledge to boost exiting IR models without interfering with their inference process. Specifically, our proposed framework consists of the semantic priors fusion (SPF) scheme and the semantic priors <b>distillation</b> (SPD) scheme. SPF fuses two kinds of information between the restored image predicted by the original IR model and the semantic mask predicted by SAM for the refined restored image. SPD leverages a <b>self-distillation</b> manner to <b>distill</b> the fused semantic priors to boost the performance of original IR models. Additionally, we design a semantic-guided relation (SGR) module for SPD, which ensures semantic feature representation space consistency to fully <b>distill</b> the priors. We demonstrate the effectiveness of our framework across multiple IR models and tasks, including deraining, deblurring, and denoising.</p></p class="citation"></blockquote><h3 id=3876--159340-understanding-long-videos-in-one-multimodal-language-model-pass-kanchana-ranasinghe-et-al-2024>(38/76 | 159/340) Understanding Long Videos in One Multimodal Language Model Pass (Kanchana Ranasinghe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo. (2024)<br><strong>Understanding Long Videos in One Multimodal Language Model Pass</strong><br><button class=copy-to-clipboard title="Understanding Long Videos in One Multimodal Language Model Pass" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16998v1.pdf filename=2403.16998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> known to contain a strong awareness of world knowledge, have allowed recent approaches to achieve excellent performance on Long-Video Understanding <b>benchmarks,</b> but at high inference costs. In this work, we first propose Likelihood Selection, a simple technique that unlocks faster inference in autoregressive <b>LLMs</b> for multiple-choice tasks common in long-video <b>benchmarks.</b> In addition to faster inference, we discover the resulting models to yield surprisingly good accuracy on long-video tasks, even with no video specific information. Building on this, we inject video-specific object-centric information extracted from off-the-shelf pre-trained models and utilize natural language as a medium for information fusion. Our resulting <b>Multimodal</b> Video Understanding (MVU) framework demonstrates state-of-the-art performance across long-video and fine-grained action recognition <b>benchmarks.</b> Code available at: <a href=https://github.com/kahnchana/mvu>https://github.com/kahnchana/mvu</a></p></p class="citation"></blockquote><h3 id=3976--160340-visual-cot-unleashing-chain-of-thought-reasoning-in-multi-modal-language-models-hao-shao-et-al-2024>(39/76 | 160/340) Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models (Hao Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, Hongsheng Li. (2024)<br><strong>Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models</strong><br><button class=copy-to-clipboard title="Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16999v1.pdf filename=2403.16999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Visual CoT, a novel pipeline that leverages the <b>reasoning</b> capabilities of <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) by incorporating visual Chain-of-Thought (CoT) <b>reasoning.</b> While MLLMs have shown promise in various visual tasks, they often lack interpretability and struggle with complex visual inputs. To address these challenges, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We collect and introduce the Visual CoT dataset comprising 373k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Importantly, the introduced <b>benchmark</b> is capable of evaluating MLLMs in scenarios requiring specific local region identification. Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, <b>benchmark,</b> and pre-trained models are available to foster further research in this direction.</p></p class="citation"></blockquote><h3 id=4076--161340-rcbevdet-radar-camera-fusion-in-birds-eye-view-for-3d-object-detection-zhiwei-lin-et-al-2024>(40/76 | 161/340) RCBEVDet: Radar-camera Fusion in Bird&rsquo;s Eye View for 3D Object Detection (Zhiwei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiwei Lin, Zhe Liu, Zhongyu Xia, Xinhao Wang, Yongtao Wang, Shengxiang Qi, Yang Dong, Nan Dong, Le Zhang, Ce Zhu. (2024)<br><strong>RCBEVDet: Radar-camera Fusion in Bird&rsquo;s Eye View for 3D Object Detection</strong><br><button class=copy-to-clipboard title="RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Object Detection, Benchmarking, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16440v1.pdf filename=2403.16440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Three-dimensional <b>object</b> <b>detection</b> is one of the key tasks in autonomous driving. To reduce costs in practice, low-cost multi-view cameras for 3D <b>object</b> <b>detection</b> are proposed to replace the expansive LiDAR sensors. However, relying solely on cameras is difficult to achieve highly accurate and robust 3D <b>object</b> <b>detection.</b> An effective solution to this issue is combining multi-view cameras with the economical millimeter-wave radar sensor to achieve more reliable <b>multi-modal</b> 3D <b>object</b> <b>detection.</b> In this paper, we introduce RCBEVDet, a radar-camera fusion 3D <b>object</b> <b>detection</b> method in the bird&rsquo;s eye view (BEV). Specifically, we first design RadarBEVNet for radar BEV feature extraction. RadarBEVNet consists of a dual-stream radar backbone and a Radar Cross-Section (RCS) aware BEV encoder. In the dual-stream radar backbone, a point-based encoder and a <b>transformer-based</b> encoder are proposed to extract radar features, with an injection and extraction module to facilitate communication between the two encoders. The RCS-aware BEV encoder takes RCS as the <b>object</b> <b>size</b> prior to scattering the point feature in BEV. Besides, we present the Cross-Attention Multi-layer Fusion module to automatically align the <b>multi-modal</b> BEV feature from radar and camera with the deformable attention mechanism, and then fuse the feature with channel and spatial fusion layers. Experimental results show that RCBEVDet achieves new state-of-the-art radar-camera fusion results on nuScenes and view-of-delft (VoD) 3D <b>object</b> <b>detection</b> <b>benchmarks.</b> Furthermore, RCBEVDet achieves better 3D detection results than all real-time camera-only and radar-camera 3D <b>object</b> <b>detectors</b> with a faster inference speed at 21~28 FPS. The source code will be released at <a href=https://github.com/VDIGPKU/RCBEVDet>https://github.com/VDIGPKU/RCBEVDet</a>.</p></p class="citation"></blockquote><h3 id=4176--162340-task2box-box-embeddings-for-modeling-asymmetric-task-relationships-rangel-daroya-et-al-2024>(41/76 | 162/340) Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships (Rangel Daroya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rangel Daroya, Aaron Sun, Subhransu Maji. (2024)<br><strong>Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships</strong><br><button class=copy-to-clipboard title="Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Transfer Learning, Box Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17173v1.pdf filename=2403.17173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling and visualizing relationships between tasks or datasets is an important step towards solving various meta-tasks such as dataset discovery, multi-tasking, and <b>transfer</b> <b>learning.</b> However, many relationships, such as containment and transferability, are naturally asymmetric and current approaches for representation and visualization (e.g., t-SNE do not readily support this. We propose Task2Box, an approach to represent tasks using <b>box</b> <b>embeddings</b> &ndash; axis-aligned hyperrectangles in low dimensional spaces &ndash; that can capture asymmetric relationships between them through volumetric overlaps. We show that Task2Box accurately predicts unseen hierarchical relationships between nodes in ImageNet and iNaturalist datasets, as well as transferability between tasks in the Taskonomy <b>benchmark.</b> We also show that <b>box</b> <b>embeddings</b> estimated from task representations (e.g., CLIP, Task2Vec, or attribute based) can be used to predict relationships between unseen tasks more accurately than classifiers trained on the same representations, as well as handcrafted asymmetric distances (e.g., KL divergence). This suggests that low-dimensional <b>box</b> <b>embeddings</b> can effectively capture these task relationships and have the added advantage of being interpretable. We use the approach to visualize relationships among publicly available image classification datasets on popular dataset hosting platform called Hugging Face.</p></p class="citation"></blockquote><h3 id=4276--163340-optimizing-lidar-placements-for-robust-driving-perception-in-adverse-conditions-ye-li-et-al-2024>(42/76 | 163/340) Optimizing LiDAR Placements for Robust Driving Perception in Adverse Conditions (Ye Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, Xiaonan Huang. (2024)<br><strong>Optimizing LiDAR Placements for Robust Driving Perception in Adverse Conditions</strong><br><button class=copy-to-clipboard title="Optimizing LiDAR Placements for Robust Driving Perception in Adverse Conditions" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17009v1.pdf filename=2403.17009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The robustness of driving perception systems under unprecedented conditions is crucial for safety-critical usages. Latest advancements have <b>prompted</b> increasing interests towards multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce a Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 364,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional robustness in both 3D <b>object</b> <b>detection</b> and LiDAR semantic segmentation tasks, under diverse adverse weather and sensor failure conditions. Code and <b>benchmark</b> toolkit are publicly available.</p></p class="citation"></blockquote><h3 id=4376--164340-elysium-exploring-object-level-perception-in-videos-via-mllm-han-wang-et-al-2024>(43/76 | 164/340) Elysium: Exploring Object-level Perception in Videos via MLLM (Han Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang. (2024)<br><strong>Elysium: Exploring Object-level Perception in Videos via MLLM</strong><br><button class=copy-to-clipboard title="Elysium: Exploring Object-level Perception in Videos via MLLM" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16558v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16558v2.pdf filename=2403.16558v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on <b>large-scale</b> <b>video</b> <b>datasets</b> is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a <b>large</b> <b>number</b> <b>of</b> frames within the context window of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a <b>large-scale</b> <b>video</b> <b>dataset</b> supported for three tasks: Single Object Tracking (SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that attempts to conduct object-level tasks in videos without requiring any additional plug-in or expert models. All codes and datasets are available at <a href=https://github.com/Hon-Wong/Elysium>https://github.com/Hon-Wong/Elysium</a>.</p></p class="citation"></blockquote><h3 id=4476--165340-pathotune-adapting-visual-foundation-model-to-pathological-specialists-jiaxuan-lu-et-al-2024>(44/76 | 165/340) PathoTune: Adapting Visual Foundation Model to Pathological Specialists (Jiaxuan Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxuan Lu, Fang Yan, Xiaofan Zhang, Yue Gao, Shaoting Zhang. (2024)<br><strong>PathoTune: Adapting Visual Foundation Model to Pathological Specialists</strong><br><button class=copy-to-clipboard title="PathoTune: Adapting Visual Foundation Model to Pathological Specialists" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Foundation Model, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16497v1.pdf filename=2403.16497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological <b>foundation</b> <b>models,</b> how to adapt <b>foundation</b> <b>models</b> to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the <b>Foundation-Task</b> <b>Gap</b> and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual <b>foundation</b> <b>models</b> to pathology-specific tasks via <b>multi-modal</b> <b>prompt</b> tuning. The proposed framework leverages Task-specific Visual <b>Prompts</b> and Task-specific Textual <b>Prompts</b> to identify task-relevant features, along with Instance-specific Visual <b>Prompts</b> for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality <b>prompt</b> tuning approaches. Significantly, PathoTune facilitates the direct adaptation of natural visual <b>foundation</b> <b>models</b> to pathological tasks, drastically outperforming pathological <b>foundation</b> <b>models</b> with simple linear probing. The code will be available upon acceptance.</p></p class="citation"></blockquote><h3 id=4576--166340-dreampolisher-towards-high-quality-text-to-3d-generation-via-geometric-diffusion-yuanze-lin-et-al-2024>(45/76 | 166/340) DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion (Yuanze Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanze Lin, Ronald Clark, Philip Torr. (2024)<br><strong>DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion</strong><br><button class=copy-to-clipboard title="DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: ControlNet, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17237v1.pdf filename=2403.17237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present DreamPolisher, a novel Gaussian Splatting based method with geometric guidance, tailored to learn cross-view consistency and intricate detail from textual descriptions. While recent progress on text-to-3D generation methods have been promising, prevailing methods often fail to ensure view-consistency and textural richness. This problem becomes particularly noticeable for methods that work with text input alone. To address this, we propose a two-stage Gaussian Splatting based approach that enforces geometric consistency among views. Initially, a coarse 3D generation undergoes refinement via geometric optimization. Subsequently, we use a <b>ControlNet</b> driven refiner coupled with the geometric consistency term to improve both texture fidelity and overall consistency of the generated 3D asset. Empirical evaluations across diverse textual <b>prompts</b> spanning various object categories demonstrate the efficacy of DreamPolisher in generating consistent and realistic 3D objects, aligning closely with the semantics of the textual instructions.</p></p class="citation"></blockquote><h3 id=4676--167340-synfog-a-photo-realistic-synthetic-fog-dataset-based-on-end-to-end-imaging-simulation-for-advancing-real-world-defogging-in-autonomous-driving-yiming-xie-et-al-2024>(46/76 | 167/340) SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving (Yiming Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Xie, Henglu Wei, Zhenyi Liu, Xiaoyu Wang, Xiangyang Ji. (2024)<br><strong>SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving</strong><br><button class=copy-to-clipboard title="SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17094v1.pdf filename=2403.17094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end <b>simulation</b> pipeline designed to generate photo-realistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior performance in visual perception and detection accuracy compared to others when applied to real-world foggy images.</p></p class="citation"></blockquote><h3 id=4776--168340-trip-temporal-residual-learning-with-image-noise-prior-for-image-to-video-diffusion-models-zhongwei-zhang-et-al-2024>(47/76 | 168/340) TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models (Zhongwei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei. (2024)<br><strong>TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models</strong><br><button class=copy-to-clipboard title="TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17005v1.pdf filename=2403.17005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in text-to-video generation have demonstrated the utility of powerful <b>diffusion</b> <b>models.</b> Nevertheless, the problem is not trivial when shaping <b>diffusion</b> <b>models</b> to animate static image (i.e., image-to-video generation). The difficulty originates from the aspect that the <b>diffusion</b> <b>process</b> of subsequent animated frames should not only preserve the faithful alignment with the given image but also pursue temporal coherence among adjacent frames. To alleviate this, we present TRIP, a new recipe of image-to-video <b>diffusion</b> <b>paradigm</b> that pivots on image noise prior derived from static image to jointly trigger inter-frame relational <b>reasoning</b> and ease the coherent temporal modeling via temporal residual learning. Technically, the image noise prior is first attained through one-step backward <b>diffusion</b> <b>process</b> based on both static image and noised video latent codes. Next, TRIP executes a residual-like dual-path scheme for noise prediction: 1) a shortcut path that directly takes image noise prior as the reference noise of each frame to amplify the alignment between the first frame and subsequent frames; 2) a residual path that employs 3D-UNet over noised video and static image latent codes to enable inter-frame relational <b>reasoning,</b> thereby easing the learning of the residual noise for each frame. Furthermore, both reference and residual noise of each frame are dynamically merged via attention mechanism for final video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT datasets demonstrate the effectiveness of our TRIP for image-to-video generation. Please see our project page at <a href=https://trip-i2v.github.io/TRIP/>https://trip-i2v.github.io/TRIP/</a>.</p></p class="citation"></blockquote><h3 id=4876--169340-learning-spatial-adaptation-and-temporal-coherence-in-diffusion-models-for-video-super-resolution-zhikai-chen-et-al-2024>(48/76 | 169/340) Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution (Zhikai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei. (2024)<br><strong>Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution</strong><br><button class=copy-to-clipboard title="Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17000v1.pdf filename=2403.17000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are just at a tipping point for image super-resolution task. Nevertheless, it is not trivial to capitalize on <b>diffusion</b> <b>models</b> for video super-resolution which necessitates not only the preservation of visual appearance from low-resolution to high-resolution videos, but also the temporal consistency across video frames. In this paper, we propose a novel approach, pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video super-resolution. SATeCo pivots on learning spatial-temporal guidance from low-resolution videos to calibrate both latent-space high-resolution video denoising and pixel-space video reconstruction. Technically, SATeCo freezes all the parameters of the pre-trained UNet and VAE, and only optimizes two deliberately-designed spatial feature adaptation (SFA) and temporal feature alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame features via adaptively estimating affine parameters for each pixel, guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA delves into feature interaction within a 3D local window (tubelet) through <b>self-attention,</b> and executes cross-attention between tubelet and its low-resolution counterpart to guide temporal feature alignment. Extensive experiments conducted on the REDS4 and Vid4 datasets demonstrate the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=4976--170340-make-your-anchor-a-diffusion-based-2d-avatar-generation-framework-ziyao-huang-et-al-2024>(49/76 | 170/340) Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework (Ziyao Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, Tong-Yee Lee. (2024)<br><strong>Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</strong><br><button class=copy-to-clipboard title="Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16510v1.pdf filename=2403.16510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable process of talking-head-based avatar-creating solutions, directly generating anchor-style videos with full-body motions remains challenging. In this study, we propose Make-Your-Anchor, a novel system necessitating only a one-minute video clip of an individual for training, subsequently enabling the automatic generation of anchor-style videos with precise torso and hand movements. Specifically, we <b>finetune</b> a proposed structure-guided <b>diffusion</b> <b>model</b> on input video to render 3D mesh conditions into human appearances. We adopt a two-stage training strategy for the <b>diffusion</b> <b>model,</b> effectively binding movements with specific appearances. To produce arbitrary long temporal video, we extend the 2D U-Net in the frame-wise <b>diffusion</b> <b>model</b> to a 3D style without additional training cost, and a simple yet effective batch-overlapped temporal denoising module is proposed to bypass the constraints on video length during inference. Finally, a novel identity-specific face enhancement module is introduced to improve the visual quality of facial regions in the output videos. Comparative experiments demonstrate the effectiveness and superiority of the system in terms of visual quality, temporal coherence, and identity preservation, outperforming SOTA diffusion/non-diffusion methods. Project page: \url{https://github.com/ICTMCG/Make-Your-Anchor}.</p></p class="citation"></blockquote><h3 id=5076--171340-elite360d-towards-efficient-360-depth-estimation-via-semantic--and-distance-aware-bi-projection-fusion-hao-ai-et-al-2024>(50/76 | 171/340) Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion (Hao Ai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Ai, Lin Wang. (2024)<br><strong>Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion</strong><br><button class=copy-to-clipboard title="Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Geometry, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16376v1.pdf filename=2403.16376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>360 depth estimation has recently received great attention for 3D reconstruction owing to its omnidirectional field of view (FoV). Recent approaches are predominantly focused on cross-projection fusion with <b>geometry-based</b> re-projection: they fuse 360 images with equirectangular projection (ERP) and another projection type, e.g., cubemap projection to estimate depth with the ERP format. However, these methods suffer from 1) limited local receptive fields, making it hardly possible to capture large FoV scenes, and 2) prohibitive computational cost, caused by the complex cross-projection fusion module design. In this paper, we propose Elite360D, a novel framework that inputs the ERP image and icosahedron projection (ICOSAP) point set, which is undistorted and spatially continuous. Elite360D is superior in its capacity in learning a representation from a local-with-global perspective. With a flexible ERP image encoder, it includes an ICOSAP point encoder, and a Bi-projection Bi-attention Fusion (B2F) module (totally ~1M parameters). Specifically, the ERP image encoder can take various perspective image-trained backbones (e.g., ResNet, <b>Transformer)</b> to extract local features. The point encoder extracts the global features from the ICOSAP. Then, the B2F module captures the semantic- and distance-aware dependencies between each pixel of the ERP feature and the entire ICOSAP feature set. Without specific backbone design and obvious computational cost increase, Elite360D outperforms the prior arts on several <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=5176--172340-co-occurring-of-object-detection-and-identification-towards-unlabeled-object-discovery-binay-kumar-singh-et-al-2024>(51/76 | 172/340) Co-Occurring of Object Detection and Identification towards unlabeled object discovery (Binay Kumar Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binay Kumar Singh, Niels Da Vitoria Lobo. (2024)<br><strong>Co-Occurring of Object Detection and Identification towards unlabeled object discovery</strong><br><button class=copy-to-clipboard title="Co-Occurring of Object Detection and Identification towards unlabeled object discovery" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17223v1.pdf filename=2403.17223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel deep learning based approach for identifying co-occurring <b>objects</b> <b>in</b> conjunction with base <b>objects</b> <b>in</b> multilabel <b>object</b> <b>categories.</b> Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring <b>objects</b> <b>with</b> respect to base <b>object</b> <b>for</b> various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results on public <b>benchmark</b> dataset is reported in Sec 4. Further we extend this work by considering all frequently <b>objects</b> <b>as</b> unlabeled and what if they are occluded as well.</p></p class="citation"></blockquote><h3 id=5276--173340-histogram-layers-for-neural-engineered-features-joshua-peeples-et-al-2024>(52/76 | 173/340) Histogram Layers for Neural Engineered Features (Joshua Peeples et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Peeples, Salim Al Kharsa, Luke Saleh, Alina Zare. (2024)<br><strong>Histogram Layers for Neural Engineered Features</strong><br><button class=copy-to-clipboard title="Histogram Layers for Neural Engineered Features" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17176v1.pdf filename=2403.17176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the computer vision literature, many effective histogram-based features have been developed. These engineered features include local binary patterns and edge histogram descriptors among others and they have been shown to be informative features for a variety of computer vision tasks. In this paper, we explore whether these features can be learned through histogram layers embedded in a neural network and, therefore, be leveraged within deep learning frameworks. By using histogram features, local statistics of the feature maps from the <b>convolution</b> neural networks can be used to better represent the data. We present neural versions of local binary pattern and edge histogram descriptors that jointly improve the feature representation and perform image classification. Experiments are presented on <b>benchmark</b> and real-world datasets.</p></p class="citation"></blockquote><h3 id=5376--174340-calib3d-calibrating-model-preferences-for-reliable-3d-scene-understanding-lingdong-kong-et-al-2024>(53/76 | 174/340) Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding (Lingdong Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu. (2024)<br><strong>Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding</strong><br><button class=copy-to-clipboard title="Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17010v1.pdf filename=2403.17010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to <b>benchmark</b> and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates &ndash; a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D <b>data</b> <b>augmentation</b> techniques, we correlate these aspects directly with the model calibration efficacy. Furthermore, we introduce DeptS, a novel depth-aware scaling approach aimed at enhancing 3D model calibration. Extensive experiments across a wide range of configurations validate the superiority of our method. We hope this work could serve as a cornerstone for fostering reliable 3D scene understanding. Code and <b>benchmark</b> toolkits are publicly available.</p></p class="citation"></blockquote><h3 id=5476--175340-drivecot-integrating-chain-of-thought-reasoning-with-end-to-end-driving-tianqi-wang-et-al-2024>(54/76 | 175/340) DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving (Tianqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianqi Wang, Enze Xie, Ruihang Chu, Zhenguo Li, Ping Luo. (2024)<br><strong>DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving</strong><br><button class=copy-to-clipboard title="DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16996v1.pdf filename=2403.16996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-end driving has made significant progress in recent years, demonstrating benefits such as system simplicity and competitive driving performance under both open-loop and closed-loop settings. Nevertheless, the lack of interpretability and controllability in its driving decisions hinders real-world deployment for end-to-end driving systems. In this paper, we collect a comprehensive end-to-end driving dataset named DriveCoT, leveraging the CARLA simulator. It contains sensor data, control decisions, and chain-of-thought labels to indicate the <b>reasoning</b> process. We utilize the challenging driving scenarios from the CARLA leaderboard 2.0, which involve high-speed driving and lane-changing, and propose a rule-based expert policy to control the vehicle and generate ground truth labels for its <b>reasoning</b> process across different driving aspects and the final decisions. This dataset can serve as an open-loop end-to-end driving <b>benchmark,</b> enabling the evaluation of accuracy in various chain-of-thought aspects and the final decision. In addition, we propose a baseline model called DriveCoT-Agent, trained on our dataset, to generate chain-of-thought predictions and final decisions. The trained model exhibits strong performance in both open-loop and closed-loop evaluations, demonstrating the effectiveness of our proposed dataset.</p></p class="citation"></blockquote><h3 id=5576--176340-v2x-pc-vehicle-to-everything-collaborative-perception-via-point-cluster-si-liu-et-al-2024>(55/76 | 176/340) V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster (Si Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Si Liu, Zihan Ding, Jiahui Fu, Hongyu Li, Siheng Chen, Shifeng Zhang, Xu Zhou. (2024)<br><strong>V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster</strong><br><button class=copy-to-clipboard title="V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16635v1.pdf filename=2403.16635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The objective of the collaborative vehicle-to-everything perception task is to enhance the individual vehicle&rsquo;s perception capability through message communication among neighboring traffic agents. Previous methods focus on achieving optimal performance within bandwidth limitations and typically adopt BEV maps as the basic collaborative message units. However, we demonstrate that collaboration with dense representations is plagued by object feature destruction during message packing, inefficient message aggregation for long-range collaboration, and implicit structure representation communication. To tackle these issues, we introduce a brand new message unit, namely point cluster, designed to represent the scene sparsely with a combination of low-level structure information and high-level semantic information. The point cluster inherently preserves object information while packing messages, with weak relevance to the collaboration range, and supports explicit structure modeling. Building upon this representation, we propose a novel framework V2X-PC for collaborative perception. This framework includes a Point Cluster Packing (PCP) module to keep object feature and manage bandwidth through the manipulation of cluster point numbers. As for effective message aggregation, we propose a Point Cluster Aggregation (PCA) module to match and merge point clusters associated with the same object. To further handle time latency and pose errors encountered in real-world scenarios, we propose parameter-free solutions that can adapt to different noisy levels without <b>finetuning.</b> Experiments on two widely recognized collaborative perception <b>benchmarks</b> showcase the superior performance of our method compared to the previous state-of-the-art approaches relying on BEV maps.</p></p class="citation"></blockquote><h3 id=5676--177340-open-set-recognition-in-the-age-of-vision-language-models-dimity-miller-et-al-2024>(56/76 | 177/340) Open-Set Recognition in the Age of Vision-Language Models (Dimity Miller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dimity Miller, Niko Sünderhauf, Alex Kenna, Keita Mason. (2024)<br><strong>Open-Set Recognition in the Age of Vision-Language Models</strong><br><button class=copy-to-clipboard title="Open-Set Recognition in the Age of Vision-Language Models" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16528v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16528v1.pdf filename=2403.16528v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Are <b>vision-language</b> models (VLMs) open-set models because they are trained on internet-scale datasets? We answer this question with a clear no - VLMs introduce closed-set assumptions via their finite query set, making them vulnerable to open-set conditions. We systematically evaluate VLMs for open-set recognition and find they frequently misclassify objects not contained in their query set, leading to alarmingly low precision when tuned for high recall and vice versa. We show that naively increasing the size of the query set to contain more and more classes does not mitigate this problem, but instead causes diminishing task performance and open-set performance. We establish a revised definition of the open-set problem for the age of VLMs, define a new <b>benchmark</b> and evaluation protocol to facilitate standardised evaluation and research in this important area, and evaluate promising baseline approaches based on predictive uncertainty and dedicated negative embeddings on a range of VLM classifiers and object detectors.</p></p class="citation"></blockquote><h3 id=5776--178340-camera-aware-label-refinement-for-unsupervised-person-re-identification-pengna-li-et-al-2024>(57/76 | 178/340) Camera-aware Label Refinement for Unsupervised Person Re-identification (Pengna Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengna Li, Kangyi Wu, Wenli Huang, Sanping Zhou, Jinjun Wang. (2024)<br><strong>Camera-aware Label Refinement for Unsupervised Person Re-identification</strong><br><button class=copy-to-clipboard title="Camera-aware Label Refinement for Unsupervised Person Re-identification" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16450v1.pdf filename=2403.16450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> person re-identification aims to retrieve images of a specified person without identity labels. Many recent <b>unsupervised</b> Re-ID approaches adopt <b>clustering-based</b> methods to measure cross-camera feature similarity to roughly divide images into clusters. They ignore the feature distribution discrepancy induced by camera domain gap, resulting in the unavoidable performance degradation. Camera information is usually available, and the feature distribution in the single camera usually focuses more on the appearance of the individual and has less intra-identity variance. Inspired by the observation, we introduce a \textbf{C}amera-\textbf{A}ware \textbf{L}abel \textbf{R}efinement~(CALR) framework that reduces camera discrepancy by <b>clustering</b> intra-camera similarity. Specifically, we employ intra-camera training to obtain reliable local pseudo labels within each camera, and then refine global labels generated by inter-camera <b>clustering</b> and train the discriminative model using more reliable global pseudo labels in a self-paced manner. Meanwhile, we develop a camera-alignment module to align feature distributions under different cameras, which could help deal with the camera variance further. Extensive experiments validate the superiority of our proposed method over state-of-the-art approaches. The code is accessible at <a href=https://github.com/leeBooMla/CALR>https://github.com/leeBooMla/CALR</a>.</p></p class="citation"></blockquote><h3 id=5876--179340-benchmarks-and-challenges-in-pose-estimation-for-egocentric-hand-interactions-with-objects-zicong-fan-et-al-2024>(58/76 | 179/340) Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects (Zicong Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zicong Fan, Takehiko Ohkawa, Linlin Yang, Nie Lin, Zhishan Zhou, Shihao Zhou, Jiajun Liang, Zhong Gao, Xuanyang Zhang, Xue Zhang, Fei Li, Liu Zheng, Feng Lu, Karim Abou Zeid, Bastian Leibe, Jeongwan On, Seungryul Baek, Aditya Prakash, Saurabh Gupta, Kun He, Yoichi Sato, Otmar Hilliges, Hyung Jin Chang, Angela Yao. (2024)<br><strong>Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects</strong><br><button class=copy-to-clipboard title="Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16428v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16428v1.pdf filename=2403.16428v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity <b>transformers</b> to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community&rsquo;s knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.</p></p class="citation"></blockquote><h3 id=5976--180340-composed-video-retrieval-via-enriched-context-and-discriminative-embeddings-omkar-thawakar-et-al-2024>(59/76 | 180/340) Composed Video Retrieval via Enriched Context and Discriminative Embeddings (Omkar Thawakar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omkar Thawakar, Muzammal Naseer, Rao Muhammad Anwer, Salman Khan, Michael Felsberg, Mubarak Shah, Fahad Shahbaz Khan. (2024)<br><strong>Composed Video Retrieval via Enriched Context and Discriminative Embeddings</strong><br><button class=copy-to-clipboard title="Composed Video Retrieval via Enriched Context and Discriminative Embeddings" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16997v1.pdf filename=2403.16997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Composed video retrieval (CoVR) is a challenging problem in computer vision which has recently highlighted the integration of modification text with visual queries for more sophisticated video search in large databases. Existing works predominantly rely on visual queries combined with modification text to distinguish relevant videos. However, such a strategy struggles to fully preserve the rich query-specific context in retrieved target videos and only represents the target video using visual embedding. We introduce a novel CoVR framework that leverages detailed language descriptions to explicitly encode query-specific contextual information and learns discriminative embeddings of vision only, text only and vision-text for better alignment to accurately retrieve matched target videos. Our proposed framework can be flexibly employed for both composed video (CoVR) and image (CoIR) retrieval tasks. Experiments on three datasets show that our approach obtains state-of-the-art performance for both CovR and <b>zero-shot</b> CoIR tasks, achieving gains as high as around 7% in terms of recall@K=1 score. Our code, models, detailed language descriptions for WebViD-CoVR dataset are available at \url{https://github.com/OmkarThawakar/composed-video-retrieval}</p></p class="citation"></blockquote><h3 id=6076--181340-twinlitenetplus-a-stronger-model-for-real-time-drivable-area-and-lane-segmentation-quang-huy-che-et-al-2024>(60/76 | 181/340) TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane Segmentation (Quang-Huy Che et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Quang-Huy Che, Duc-Tri Le, Minh-Quan Pham, Vinh-Tiep Nguyen, Duc-Khai Lam. (2024)<br><strong>TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane Segmentation</strong><br><button class=copy-to-clipboard title="TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane Segmentation" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16958v1.pdf filename=2403.16958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic segmentation is crucial for autonomous driving, particularly for Drivable Area and Lane Segmentation, ensuring safety and navigation. To address the high computational costs of current state-of-the-art (SOTA) models, this paper introduces TwinLiteNetPlus (TwinLiteNet$^+$), a model adept at balancing efficiency and accuracy. TwinLiteNet$^+$ incorporates standard and depth-wise separable dilated <b>convolutions,</b> reducing complexity while maintaining high accuracy. It is available in four configurations, from the robust 1.94 million-parameter TwinLiteNet$^+<em>{\text{Large}}$ to the ultra-compact 34K-parameter TwinLiteNet$^+</em>{\text{Nano}}$. Notably, TwinLiteNet$^+_{\text{Large}}$ attains a 92.9% mIoU for Drivable Area Segmentation and a 34.2% IoU for Lane Segmentation. These results notably outperform those of current SOTA models while requiring a computational cost that is approximately 11 times lower in terms of Floating Point Operations (FLOPs) compared to the existing SOTA model. Extensively tested on various embedded devices, TwinLiteNet$^+$ demonstrates promising latency and power efficiency, underscoring its suitability for real-world autonomous vehicle applications.</p></p class="citation"></blockquote><h3 id=6176--182340-cvt-xrf-contrastive-in-voxel-transformer-for-3d-consistent-radiance-fields-from-sparse-inputs-yingji-zhong-et-al-2024>(61/76 | 182/340) CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs (Yingji Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingji Zhong, Lanqing Hong, Zhenguo Li, Dan Xu. (2024)<br><strong>CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs</strong><br><button class=copy-to-clipboard title="CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16885v1.pdf filename=2403.16885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Fields (NeRF) have shown impressive capabilities for photorealistic novel view synthesis when trained on dense inputs. However, when trained on sparse inputs, NeRF typically encounters issues of incorrect density or color predictions, mainly due to insufficient coverage of the scene causing partial and sparse supervision, thus leading to significant performance degradation. While existing works mainly consider ray-level consistency to construct 2D learning regularization based on rendered color, depth, or semantics on image planes, in this paper we propose a novel approach that models 3D spatial field consistency to improve NeRF&rsquo;s performance with sparse inputs. Specifically, we first adopt a voxel-based ray sampling strategy to ensure that the sampled rays intersect with a certain voxel in 3D space. We then randomly sample additional points within the voxel and apply a <b>Transformer</b> to infer the properties of other points on each ray, which are then incorporated into the volume rendering. By backpropagating through the rendering loss, we enhance the consistency among neighboring points. Additionally, we propose to use a contrastive loss on the encoder output of the <b>Transformer</b> to further improve consistency within each voxel. Experiments demonstrate that our method yields significant improvement over different radiance fields in the sparse inputs setting, and achieves comparable performance with current works.</p></p class="citation"></blockquote><h3 id=6276--183340-modetv2-gpu-accelerated-motion-decomposition-transformer-for-pairwise-optimization-in-medical-image-registration-haiqiao-wang-et-al-2024>(62/76 | 183/340) ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise Optimization in Medical Image Registration (Haiqiao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiqiao Wang, Zhuoyuan Wang, Dong Ni, Yi Wang. (2024)<br><strong>ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise Optimization in Medical Image Registration</strong><br><button class=copy-to-clipboard title="ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise Optimization in Medical Image Registration" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16526v1.pdf filename=2403.16526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deformable image registration plays a crucial role in medical imaging, aiding in disease diagnosis and image-guided interventions. Traditional iterative methods are slow, while deep learning (DL) accelerates solutions but faces usability and precision challenges. This study introduces a pyramid network with the enhanced motion decomposition <b>Transformer</b> (ModeTv2) operator, showcasing superior pairwise optimization (PO) akin to traditional methods. We re-implement ModeT operator with CUDA extensions to enhance its computational efficiency. We further propose RegHead module which refines deformation fields, improves the realism of deformation and reduces parameters. By adopting the PO, the proposed network balances accuracy, efficiency, and generalizability. Extensive experiments on two public brain MRI datasets and one abdominal CT dataset demonstrate the network&rsquo;s suitability for PO, providing a DL model with enhanced usability and interpretability. The code is publicly available.</p></p class="citation"></blockquote><h3 id=6376--184340-medical-image-registration-and-its-application-in-retinal-images-a-review-qiushi-nie-et-al-2024>(63/76 | 184/340) Medical Image Registration and Its Application in Retinal Images: A Review (Qiushi Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiushi Nie, Xiaoqing Zhang, Yan Hu, Mingdao Gong, Jiang Liu. (2024)<br><strong>Medical Image Registration and Its Application in Retinal Images: A Review</strong><br><button class=copy-to-clipboard title="Medical Image Registration and Its Application in Retinal Images: A Review" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16502v1.pdf filename=2403.16502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image registration is vital for disease diagnosis and treatment with its ability to merge diverse information of images, which may be captured under different times, angles, or modalities. Although several surveys have reviewed the development of medical image registration, these surveys have not systematically <b>summarized</b> methodologies of existing medical image registration methods. To this end, we provide a comprehensive review of these methods from traditional and deep learning-based directions, aiming to help audiences understand the development of medical image registration quickly. In particular, we review recent advances in retinal image registration at the end of each section, which has not attracted much attention. Additionally, we also discuss the current challenges of retinal image registration and provide insights and prospects for future research.</p></p class="citation"></blockquote><h3 id=6476--185340-doctr-disentangled-object-centric-transformer-for-point-scene-understanding-xiaoxuan-yu-et-al-2024>(64/76 | 185/340) DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding (Xiaoxuan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxuan Yu, Hao Wang, Weiming Li, Qiang Wang, Soonyong Cho, Younghun Sung. (2024)<br><strong>DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding</strong><br><button class=copy-to-clipboard title="DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16431v1.pdf filename=2403.16431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric <b>TRansformer</b> (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a <b>Transformer</b> decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to attend separately to semantic information and geometric information relevant to the corresponding sub-tasks. A hybrid bipartite matching module is employed to well use the supervisions from all the sub-tasks during training. Qualitative and quantitative experimental results demonstrate that our method achieves state-of-the-art performance on the challenging ScanNet dataset. Code is available at <a href=https://github.com/SAITPublic/DOCTR>https://github.com/SAITPublic/DOCTR</a>.</p></p class="citation"></blockquote><h3 id=6576--186340-unsupervised-template-assisted-point-cloud-shape-correspondence-network-jiacheng-deng-et-al-2024>(65/76 | 186/340) Unsupervised Template-assisted Point Cloud Shape Correspondence Network (Jiacheng Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Deng, Jiahao Lu, Tianzhu Zhang. (2024)<br><strong>Unsupervised Template-assisted Point Cloud Shape Correspondence Network</strong><br><button class=copy-to-clipboard title="Unsupervised Template-assisted Point Cloud Shape Correspondence Network" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16412v1.pdf filename=2403.16412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> point cloud shape correspondence aims to establish point-wise correspondences between source and target point clouds. Existing methods obtain correspondences directly by computing point-wise feature similarity between point clouds. However, non-rigid objects possess strong deformability and unusual shapes, making it a longstanding challenge to directly establish correspondences between point clouds with unconventional shapes. To address this challenge, we propose an <b>unsupervised</b> Template-Assisted point cloud shape correspondence Network, termed TANet, including a template generation module and a template assistance module. The proposed TANet enjoys several merits. Firstly, the template generation module establishes a set of learnable templates with explicit structures. Secondly, we introduce a template assistance module that extensively leverages the generated templates to establish more accurate shape correspondences from multiple perspectives. Extensive experiments on four human and animal datasets demonstrate that TANet achieves favorable performance against state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=6676--187340-asdf-assembly-state-detection-utilizing-late-fusion-by-integrating-6d-pose-estimation-hannah-schieber-et-al-2024>(66/76 | 187/340) ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation (Hannah Schieber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannah Schieber, Shiyu Li, Niklas Corell, Philipp Beckerle, Julian Kreimeier, Daniel Roth. (2024)<br><strong>ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation</strong><br><button class=copy-to-clipboard title="ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16400v1.pdf filename=2403.16400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In medical and industrial domains, providing guidance for assembly processes is critical to ensure efficiency and safety. Errors in assembly can lead to significant consequences such as extended surgery times, and prolonged manufacturing or maintenance times in industry. Assembly scenarios can benefit from in-situ AR visualization to provide guidance, reduce assembly times and minimize errors. To enable in-situ visualization 6D pose estimation can be leveraged. Existing 6D pose estimation techniques primarily focus on individual <b>objects</b> <b>and</b> static captures. However, assembly scenarios have various dynamics including occlusion during assembly and dynamics in the assembly <b>objects</b> <b>appearance.</b> Existing work, combining <b>object</b> <b>detection/6D</b> pose estimation and assembly state detection focuses either on pure deep learning-based approaches, or limit the assembly state detection to building blocks. To address the challenges of 6D pose estimation in combination with assembly state detection, our approach ASDF builds upon the strengths of YOLOv8, a real-time capable <b>object</b> <b>detection</b> framework. We extend this framework, refine the <b>object</b> <b>pose</b> and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. Our evaluation on our ASDF dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network, and even outperform the hybrid and pure tracking-based approaches.</p></p class="citation"></blockquote><h3 id=6776--188340-impact-of-video-compression-artifacts-on-fisheye-camera-visual-perception-tasks-madhumitha-sakthi-et-al-2024>(67/76 | 188/340) Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks (Madhumitha Sakthi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Madhumitha Sakthi, Louis Kerofsky, Varun Ravi Kumar, Senthil Yogamani. (2024)<br><strong>Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks</strong><br><button class=copy-to-clipboard title="Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16338v1.pdf filename=2403.16338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system. The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle&rsquo;s life cycle). Lossless compression doesn&rsquo;t provide sufficient compression ratios, hence, lossy video compression has been explored. It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms. However, there is limited work in this area to provide a solid conclusion. In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts. Fisheye cameras are commonly used in automotive systems for 3D <b>object</b> <b>detection</b> task. In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fisheye camera images. We demonstrate that the achievable compression with negligible impact depends on the dataset and temporal prediction of the video codec. We propose a radial distortion-aware zonal metric to evaluate the performance of artifacts in fisheye images. In addition, we present a novel method for estimating affine mode parameters of the latest VVC codec, and suggest some areas for improvement in video codecs for the application to fisheye imagery.</p></p class="citation"></blockquote><h3 id=6876--189340-benchmarking-video-frame-interpolation-simon-kiefhaber-et-al-2024>(68/76 | 189/340) Benchmarking Video Frame Interpolation (Simon Kiefhaber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Kiefhaber, Simon Niklaus, Feng Liu, Simone Schaub-Meyer. (2024)<br><strong>Benchmarking Video Frame Interpolation</strong><br><button class=copy-to-clipboard title="Benchmarking Video Frame Interpolation" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17128v1.pdf filename=2403.17128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video frame interpolation, the task of synthesizing new frames in between two or more given ones, is becoming an increasingly popular research target. However, the current evaluation of frame interpolation techniques is not ideal. Due to the plethora of test datasets available and inconsistent computation of error metrics, a coherent and fair comparison across papers is very challenging. Furthermore, new test sets have been proposed as part of method papers so they are unable to provide the in-depth evaluation of a dedicated <b>benchmarking</b> paper. Another severe downside is that these test sets violate the assumption of linearity when given two input frames, making it impossible to solve without an oracle. We hence strongly believe that the community would greatly benefit from a <b>benchmarking</b> paper, which is what we propose. Specifically, we present a <b>benchmark</b> which establishes consistent error metrics by utilizing a submission website that computes them, provides insights by analyzing the interpolation quality with respect to various per-pixel attributes such as the motion magnitude, contains a carefully designed test set adhering to the assumption of linearity by utilizing synthetic data, and evaluates the computational efficiency in a coherent manner.</p></p class="citation"></blockquote><h3 id=6976--190340-clustering-propagation-for-universal-medical-image-segmentation-yuhang-ding-et-al-2024>(69/76 | 190/340) Clustering Propagation for Universal Medical Image Segmentation (Yuhang Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Ding, Liulei Li, Wenguan Wang, Yi Yang. (2024)<br><strong>Clustering Propagation for Universal Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Clustering Propagation for Universal Medical Image Segmentation" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16646v1.pdf filename=2403.16646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prominent solutions for medical image segmentation are typically tailored for automatic or interactive setups, posing challenges in facilitating progress achieved in one task to another.$<em>{!}$ This$</em>{!}$ also$<em>{!}$ necessitates$</em>{!}$ separate$<em>{!}$ models for each task, duplicating both training time and parameters.$</em>{!}$ To$<em>{!}$ address$</em>{!}$ above$<em>{!}$ issues,$</em>{!}$ we$<em>{!}$ introduce$</em>{!}$ S2VNet,$<em>{!}$ a$</em>{!}$ universal$<em>{!}$ framework$</em>{!}$ that$<em>{!}$ leverages$</em>{!}$ Slice-to-Volume$<em>{!}$ propagation$</em>{!}$ to$<em>{!}$ unify automatic/interactive segmentation within a single model and one training session. Inspired by <b>clustering-based</b> segmentation techniques, S2VNet makes full use of the slice-wise structure of volumetric data by initializing cluster centers from the cluster$</em>{!}$ results$<em>{!}$ of$</em>{!}$ previous$<em>{!}$ slice.$</em>{!}$ This enables knowledge acquired from prior slices to assist in the segmentation of the current slice, further efficiently bridging the communication between remote slices using mere 2D networks. Moreover, such a framework readily accommodates interactive segmentation with no architectural change, simply by initializing centroids from user inputs. S2VNet distinguishes itself by swift inference speeds and reduced memory consumption compared to prevailing 3D solutions. It can also handle multi-class interactions with each of them serving to initialize different centroids. Experiments on three <b>benchmarks</b> demonstrate S2VNet surpasses task-specified solutions on both automatic/interactive setups.</p></p class="citation"></blockquote><h3 id=7076--191340-gsdf-3dgs-meets-sdf-for-improved-rendering-and-reconstruction-mulin-yu-et-al-2024>(70/76 | 191/340) GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction (Mulin Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, Bo Dai. (2024)<br><strong>GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction</strong><br><button class=copy-to-clipboard title="GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16964v1.pdf filename=2403.16964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Presenting a 3D scene from multiview images remains a core and long-standing challenge in computer vision and computer graphics. Two main requirements lie in rendering and reconstruction. Notably, SOTA rendering quality is usually achieved with neural volumetric rendering techniques, which rely on aggregated point/primitive-wise color and neglect the underlying scene <b>geometry.</b> Learning of neural implicit surfaces is sparked from the success of neural rendering. Current works either constrain the distribution of density fields or the shape of primitives, resulting in degraded rendering quality and flaws on the learned scene surfaces. The efficacy of such methods is limited by the inherent constraints of the chosen neural representation, which struggles to capture fine surface details, especially for larger, more intricate scenes. To address these issues, we introduce GSDF, a novel dual-branch architecture that combines the benefits of a flexible and efficient 3D Gaussian Splatting (3DGS) representation with neural Signed Distance Fields (SDF). The core idea is to leverage and enhance the strengths of each branch while alleviating their limitation through mutual guidance and joint supervision. We show on diverse scenes that our design unlocks the potential for more accurate and detailed surface reconstructions, and at the meantime benefits 3DGS rendering with structures that are more aligned with the underlying <b>geometry.</b></p></p class="citation"></blockquote><h3 id=7176--192340-towards-balanced-rgb-tsdf-fusion-for-consistent-semantic-scene-completion-by-3d-rgb-feature-completion-and-a-classwise-entropy-loss-function-laiyan-ding-et-al-2024>(71/76 | 192/340) Towards Balanced RGB-TSDF Fusion for Consistent Semantic Scene Completion by 3D RGB Feature Completion and a Classwise Entropy Loss Function (Laiyan Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laiyan Ding, Panwen Hu, Jie Li, Rui Huang. (2024)<br><strong>Towards Balanced RGB-TSDF Fusion for Consistent Semantic Scene Completion by 3D RGB Feature Completion and a Classwise Entropy Loss Function</strong><br><button class=copy-to-clipboard title="Towards Balanced RGB-TSDF Fusion for Consistent Semantic Scene Completion by 3D RGB Feature Completion and a Classwise Entropy Loss Function" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16888v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16888v1.pdf filename=2403.16888v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Scene Completion (SSC) aims to jointly infer semantics and occupancies of 3D scenes. Truncated Signed Distance Function (TSDF), a 3D encoding of depth, has been a common input for SSC. Furthermore, RGB-TSDF fusion, seems promising since these two modalities provide color and <b>geometry</b> information, respectively. Nevertheless, RGB-TSDF fusion has been considered nontrivial and commonly-used naive addition will result in inconsistent results. We argue that the inconsistency comes from the sparsity of RGB features upon projecting into 3D space, while TSDF features are dense, leading to imbalanced feature maps when summed up. To address this RGB-TSDF distribution difference, we propose a two-stage network with a 3D RGB feature completion module that completes RGB features with meaningful values for occluded areas. Moreover, we propose an effective classwise entropy loss function to punish inconsistency. Extensive experiments on public datasets verify that our method achieves state-of-the-art performance among methods that do not adopt extra data.</p></p class="citation"></blockquote><h3 id=7276--193340-creating-a-digital-twin-of-spinal-surgery-a-proof-of-concept-jonas-hein-et-al-2024>(72/76 | 193/340) Creating a Digital Twin of Spinal Surgery: A Proof of Concept (Jonas Hein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Hein, Frederic Giraud, Lilian Calvet, Alexander Schwarz, Nicola Alessandro Cavalcanti, Sergey Prokudin, Mazda Farshad, Siyu Tang, Marc Pollefeys, Fabio Carrillo, Philipp Fürnstahl. (2024)<br><strong>Creating a Digital Twin of Spinal Surgery: A Proof of Concept</strong><br><button class=copy-to-clipboard title="Creating a Digital Twin of Spinal Surgery: A Proof of Concept" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16736v1.pdf filename=2403.16736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surgery digitalization is the process of creating a virtual replica of real-world surgery, also referred to as a surgical digital twin (SDT). It has significant applications in various fields such as education and training, surgical planning, and automation of surgical tasks. Given their detailed representations of surgical procedures, SDTs are an ideal foundation for machine learning methods, enabling automatic generation of training data. In robotic surgery, SDTs can provide realistic virtual environments in which robots may learn through trial and error. In this paper, we present a proof of concept (PoC) for surgery digitalization that is applied to an ex-vivo spinal surgery performed in realistic conditions. The proposed digitalization focuses on the acquisition and modelling of the <b>geometry</b> and appearance of the entire surgical scene. We employ five RGB-D cameras for dynamic 3D reconstruction of the surgeon, a high-end camera for 3D reconstruction of the anatomy, an infrared stereo camera for surgical instrument tracking, and a laser scanner for 3D reconstruction of the operating room and data fusion. We justify the proposed methodology, discuss the challenges faced and further extensions of our prototype. While our PoC partially relies on manual data curation, its high quality and great potential motivate the development of automated methods for the creation of SDTs. The quality of our SDT can be assessed in a rendered video available at <a href=https://youtu.be/LqVaWGgaTMY>https://youtu.be/LqVaWGgaTMY</a> .</p></p class="citation"></blockquote><h3 id=7376--194340-inpc-implicit-neural-point-clouds-for-radiance-field-rendering-florian-hahlbohm-et-al-2024>(73/76 | 194/340) INPC: Implicit Neural Point Clouds for Radiance Field Rendering (Florian Hahlbohm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Hahlbohm, Linus Franke, Moritz Kappel, Susana Castillo, Marc Stamminger, Marcus Magnor. (2024)<br><strong>INPC: Implicit Neural Point Clouds for Radiance Field Rendering</strong><br><button class=copy-to-clipboard title="INPC: Implicit Neural Point Clouds for Radiance Field Rendering" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16862v1.pdf filename=2403.16862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common <b>benchmark</b> datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.</p></p class="citation"></blockquote><h3 id=7476--195340-curbnet-curb-detection-framework-based-on-lidar-point-cloud-segmentation-guoyang-zhao-et-al-2024>(74/76 | 195/340) CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation (Guoyang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoyang Zhao, Fulong Ma, Yuxuan Liu, Weiqing Qi, Ming Liu. (2024)<br><strong>CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation</strong><br><button class=copy-to-clipboard title="CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16794v1.pdf filename=2403.16794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Curb detection is an important function in intelligent driving and can be used to determine drivable areas of the road. However, curbs are difficult to detect due to the complex road environment. This paper introduces CurbNet, a novel framework for curb detection, leveraging point cloud segmentation. Addressing the dearth of comprehensive curb datasets and the absence of 3D annotations, we have developed the 3D-Curb dataset, encompassing 7,100 frames, which represents the largest and most categorically diverse collection of curb point clouds currently available. Recognizing that curbs are primarily characterized by height variations, our approach harnesses spatially-rich 3D point clouds for training. To tackle the challenges presented by the uneven distribution of curb features on the xy-plane and their reliance on z-axis high-frequency features, we introduce the multi-scale and channel attention (MSCA) module, a bespoke solution designed to optimize detection performance. Moreover, we propose an adaptive weighted loss function group, specifically formulated to counteract the imbalance in the distribution of curb point clouds relative to other categories. Our extensive experimentation on 2 major datasets has yielded results that surpass existing <b>benchmarks</b> set by leading curb detection and point cloud segmentation models. By integrating multi-clustering and curve fitting techniques in our post-processing stage, we have substantially reduced noise in curb detection, thereby enhancing precision to 0.8744. Notably, CurbNet has achieved an exceptional average metrics of over 0.95 at a tolerance of just 0.15m, thereby establishing a new <b>benchmark.</b> Furthermore, corroborative real-world experiments and dataset analyzes mutually validate each other, solidifying CurbNet&rsquo;s superior detection proficiency and its robust generalizability.</p></p class="citation"></blockquote><h3 id=7576--196340-multi-attention-associate-prediction-network-for-visual-tracking-xinglong-sun-et-al-2024>(75/76 | 196/340) Multi-attention Associate Prediction Network for Visual Tracking (Xinglong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Xilai Wei, Zhonghe Hu. (2024)<br><strong>Multi-attention Associate Prediction Network for Visual Tracking</strong><br><button class=copy-to-clipboard title="Multi-attention Associate Prediction Network for Visual Tracking" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16395v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16395v1.pdf filename=2403.16395v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classification-regression prediction networks have realized impressive success in several modern deep trackers. However, there is an inherent difference between classification and regression tasks, so they have diverse even opposite demands for feature matching. Existed models always ignore the key issue and only employ a unified matching block in two task branches, decaying the decision quality. Besides, these models also struggle with decision misalignment situation. In this paper, we propose a multi-attention associate prediction network (MAPNet) to tackle the above problems. Concretely, two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are first designed for feature comparison by integrating self, cross, channel or spatial attentions organically. They are capable of fully capturing the category-related semantics for classification and the local spatial contexts for regression, respectively. Then, we present a dual alignment module to enhance the correspondences between two branches, which is useful to find the optimal tracking solution. Finally, we describe a Siamese tracker built upon the proposed prediction network, which achieves the leading performance on five tracking <b>benchmarks,</b> consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and UAV123, and surpasses other state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=7676--197340-text-if-leveraging-semantic-text-guidance-for-degradation-aware-and-interactive-image-fusion-xunpeng-yi-et-al-2024>(76/76 | 197/340) Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion (Xunpeng Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, Jiayi Ma. (2024)<br><strong>Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion</strong><br><button class=copy-to-clipboard title="Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16387v1.pdf filename=2403.16387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image fusion aims to combine information from different source images to create a comprehensively representative image. Existing fusion methods are typically helpless in dealing with degradations in low-quality source images and non-interactive to multiple subjective and objective needs. To solve them, we introduce a novel approach that leverages semantic text guidance image fusion model for degradation-aware and interactive image fusion task, termed as Text-IF. It innovatively extends the classical image fusion to the text guided image fusion along with the ability to harmoniously address the degradation and interaction issues during fusion. Through the text semantic encoder and semantic interaction fusion decoder, Text-IF is accessible to the all-in-one infrared and visible image degradation-aware processing and the interactive flexible fusion outcomes. In this way, Text-IF achieves not only <b>multi-modal</b> image fusion, but also <b>multi-modal</b> information fusion. Extensive experiments prove that our proposed text guided image fusion strategy has obvious advantages over SOTA methods in the image fusion performance and degradation treatment. The code is available at <a href=https://github.com/XunpengYi/Text-IF>https://github.com/XunpengYi/Text-IF</a>.</p></p class="citation"></blockquote><h2 id=cshc-11>cs.HC (11)</h2><h3 id=111--198340-golf-goal-oriented-long-term-life-tasks-supported-by-human-ai-collaboration-ben-wang-2024>(1/11 | 198/340) GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration (Ben Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Wang. (2024)<br><strong>GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration</strong><br><button class=copy-to-clipboard title="GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-IR, cs.HC<br>Keyword Score: 73<br>Keywords: Benchmarking, Simulation, Simulator, ChatGPT, Information Retrieval, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17089v1.pdf filename=2403.17089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>ChatGPT</b> and similar <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has revolutionized the human-AI interaction and <b>information-seeking</b> <b>process.</b> Leveraging <b>LLMs</b> as an alternative to search engines, users can now access <b>summarized</b> <b>information</b> <b>tailored</b> to their queries, significantly reducing the cognitive load associated with navigating vast <b>information</b> <b>resources.</b> This shift underscores the potential of <b>LLMs</b> in redefining <b>information</b> <b>access</b> paradigms. Drawing on the foundation of task-focused <b>information</b> <b>retrieval</b> and <b>LLMs&rsquo;</b> task planning ability, this research extends the scope of <b>LLM</b> capabilities beyond routine task automation to support users in navigating long-term and significant life tasks. It introduces the GOLF framework (Goal-Oriented Long-term liFe tasks), which focuses on enhancing <b>LLMs&rsquo;</b> ability to assist in significant life decisions through goal orientation and long-term planning. The methodology encompasses a comprehensive <b>simulation</b> study to test the framework&rsquo;s efficacy, followed by model and human evaluations to develop a dataset <b>benchmark</b> for long-term life tasks, and experiments across different models and settings. By shifting the focus from short-term tasks to the broader spectrum of long-term life goals, this research underscores the transformative potential of <b>LLMs</b> in enhancing human decision-making processes and task management, marking a significant step forward in the evolution of human-AI collaboration.</p></p class="citation"></blockquote><h3 id=211--199340-sesame-a-framework-to-simulate-self-reported-ground-truth-for-mental-health-sensing-studies-akshat-choube-et-al-2024>(2/11 | 199/340) SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies (Akshat Choube et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshat Choube, Vedant Das Swain, Varun Mishra. (2024)<br><strong>SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies</strong><br><button class=copy-to-clipboard title="SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs.HC<br>Keyword Score: 70<br>Keywords: Simulation, Simulator, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17219v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17219v2.pdf filename=2403.17219v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in mobile and wearable technologies have enabled the potential to passively monitor a person&rsquo;s mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores <b>Simulation</b> using Mental Models (SeSaMe) framework to alleviate participants&rsquo; burden in digital mental health studies. By leveraging pre-trained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> SeSaMe enables the <b>simulation</b> of participants&rsquo; responses on psychological scales. In SeSaMe, researchers can <b>prompt</b> <b>LLMs</b> with information on participants&rsquo; internal behavioral dispositions, enabling <b>LLMs</b> to construct mental models of participants to simulate their responses on psychological scales. We demonstrate an application of SeSaMe, where we use <b>GPT-4</b> to simulate responses on one scale using responses from another as behavioral information. We also evaluate the alignment between human and SeSaMe-simulated responses to psychological scales. Then, we present experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training ML models by replicating established depression and anxiety screening tasks from a previous study. Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives. We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies.</p></p class="citation"></blockquote><h3 id=311--200340-virtual-co-pilot-multimodal-large-language-model-enabled-quick-access-procedures-for-single-pilot-operations-fan-li-et-al-2024>(3/11 | 200/340) Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations (Fan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Li, Shanshan Feng, Yuqi Yan, Ching-Hung Lee, Yew Soon Ong. (2024)<br><strong>Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations</strong><br><button class=copy-to-clipboard title="Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16645v1.pdf filename=2403.16645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advancements in technology, pilot shortages, and cost pressures are driving a trend towards single-pilot and even remote operations in aviation. Considering the extensive workload and huge risks associated with single-pilot operations, the development of a Virtual Co-Pilot (V-CoP) is expected to be a potential way to ensure aviation safety. This study proposes a V-CoP concept and explores how humans and virtual assistants can effectively collaborate. A preliminary case study is conducted to explore a critical role of V-CoP, namely automated quick procedures searching, using the <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> The <b>LLM-enabled</b> V-CoP integrates the pilot instruction and real-time cockpit instrumental data to <b>prompt</b> applicable aviation manuals and operation procedures. The results showed that the <b>LLM-enabled</b> V-CoP achieved high accuracy in situational analysis and effective retrieval of procedure information. The results showed that the <b>LLM-enabled</b> V-CoP achieved high accuracy in situational analysis (90.5%) and effective retrieval of procedure information (86.5%). The proposed V-CoP is expected to provide a foundation for future virtual intelligent assistant development, improve the performance of single pilots, and reduce the risk of human errors in aviation.</p></p class="citation"></blockquote><h3 id=411--201340-towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making-shuai-ma-et-al-2024>(4/11 | 201/340) Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (Shuai Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Ma, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming Yin, Xiaojuan Ma. (2024)<br><strong>Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</strong><br><button class=copy-to-clipboard title="Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Explainable AI, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16812v1.pdf filename=2403.16812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In AI-assisted decision-making, humans often passively review AI&rsquo;s suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional <b>explainable</b> <b>AI</b> (XAI) assistants in improving humans&rsquo; appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.</p></p class="citation"></blockquote><h3 id=511--202340-compressing-and-interpreting-word-embeddings-with-latent-space-regularization-and-interactive-semantics-probing-haoyu-li-et-al-2024>(5/11 | 202/340) Compressing and Interpreting Word Embeddings with Latent Space Regularization and Interactive Semantics Probing (Haoyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Li, Junpeng Wang, Yan Zheng, Liang Wang, Wei Zhang, Han-Wei Shen. (2024)<br><strong>Compressing and Interpreting Word Embeddings with Latent Space Regularization and Interactive Semantics Probing</strong><br><button class=copy-to-clipboard title="Compressing and Interpreting Word Embeddings with Latent Space Regularization and Interactive Semantics Probing" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Autoencoder, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16815v1.pdf filename=2403.16815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Word</b> <b>embedding,</b> a high-dimensional (HD) numerical representation of <b>words</b> <b>generated</b> by machine learning models, has been used for different natural language processing tasks, e.g., translation between two languages. Recently, there has been an increasing trend of transforming the HD embeddings into a latent space (e.g., via <b>autoencoders)</b> for further tasks, exploiting various merits the latent representations could bring. To preserve the embeddings&rsquo; quality, these works often map the embeddings into an even higher-dimensional latent space, making the already complicated embeddings even less interpretable and consuming more storage space. In this work, we borrow the idea of $\beta$VAE to regularize the HD latent space. Our regularization implicitly condenses information from the HD latent space into a much lower-dimensional space, thus compressing the embeddings. We also show that each dimension of our regularized latent space is more semantically salient, and validate our assertion by interactively probing the encoding-level of user-proposed semantics in the dimensions. To the end, we design a visual analytics system to monitor the regularization process, explore the HD latent space, and interpret latent dimensions&rsquo; semantics. We validate the effectiveness of our embedding regularization and interpretation approach through both quantitative and qualitative evaluations.</p></p class="citation"></blockquote><h3 id=611--203340-enhancing-cross-dataset-eeg-emotion-recognition-a-novel-approach-with-emotional-eeg-style-transfer-network-yijin-zhou-et-al-2024>(6/11 | 203/340) Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network (Yijin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijin Zhou, Fu Li, Yang Li, Youshuo Ji, Lijian Zhang, Yuanfang Chen. (2024)<br><strong>Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network</strong><br><button class=copy-to-clipboard title="Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Emotion Recognition, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16540v1.pdf filename=2403.16540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognizing the pivotal role of EEG <b>emotion</b> <b>recognition</b> in the development of affective Brain-Computer Interfaces (aBCIs), considerable research efforts have been dedicated to this field. While prior methods have demonstrated success in intra-subject EEG <b>emotion</b> <b>recognition,</b> a critical challenge persists in addressing the <b>style</b> <b>mismatch</b> between EEG signals from the source domain (training data) and the target domain (test data). To tackle the significant inter-domain differences in cross-dataset EEG <b>emotion</b> <b>recognition,</b> this paper introduces an innovative solution known as the <b>Emotional</b> <b>EEG</b> <b>Style</b> <b>Transfer</b> Network (E$^2$STN). The primary objective of this network is to effectively capture content information from the source domain and the <b>style</b> <b>characteristics</b> from the target domain, enabling the reconstruction of stylized EEG <b>emotion</b> <b>representations.</b> These representations prove highly beneficial in enhancing cross-dataset discriminative prediction. Concretely, E$^2$STN consists of three key modules\textemdash transfer module, transfer evaluation module, and discriminative prediction module\textemdash which address the domain <b>style</b> <b>transfer,</b> transfer quality evaluation, and discriminative prediction, respectively. Extensive experiments demonstrate that E$^2$STN achieves state-of-the-art performance in cross-dataset EEG <b>emotion</b> <b>recognition</b> tasks.</p></p class="citation"></blockquote><h3 id=711--204340-as-good-as-a-coin-toss-human-detection-of-ai-generated-images-videos-audio-and-audiovisual-stimuli-di-cooke-et-al-2024>(7/11 | 204/340) As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli (Di Cooke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly. (2024)<br><strong>As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli</strong><br><button class=copy-to-clipboard title="As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: 68T01, I-2, cs-AI, cs-HC, cs-SD, cs.HC, eess-AS<br>Keyword Score: 16<br>Keywords: Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16760v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16760v2.pdf filename=2403.16760v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from publicly accessible <b>generative</b> <b>AI</b> technology. We find that overall, participants struggled to meaningfully discern between synthetic and authentic content. We also find that detection performance worsens when the stimuli contains synthetic content as compared to authentic content, images featuring human faces as compared to non face objects, a single modality as compared to <b>multimodal</b> stimuli, mixed authenticity as compared to being fully synthetic for audiovisual stimuli, and features foreign languages as compared to languages the observer is fluent in. Finally, we also find that prior knowledge of synthetic media does not meaningfully impact their detection performance. Collectively, these results indicate that people are highly susceptible to being tricked by synthetic media in their daily lives and that human perceptual detection capabilities can no longer be relied upon as an effective counterdefense.</p></p class="citation"></blockquote><h3 id=811--205340-behind-the-counter-exploring-the-motivations-and-barriers-of-online-counterspeech-writing-kaike-ping-et-al-2024>(8/11 | 205/340) Behind the Counter: Exploring the Motivations and Barriers of Online Counterspeech Writing (Kaike Ping et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaike Ping, Anisha Kumar, Xiaohan Ding, Eugenia Rho. (2024)<br><strong>Behind the Counter: Exploring the Motivations and Barriers of Online Counterspeech Writing</strong><br><button class=copy-to-clipboard title="Behind the Counter: Exploring the Motivations and Barriers of Online Counterspeech Writing" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17116v1.pdf filename=2403.17116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current research mainly explores the attributes and impact of online counterspeech, leaving a gap in understanding of who engages in online counterspeech or what motivates or deters users from participating. To investigate this, we surveyed 458 English-speaking U.S. participants, analyzing key motivations and barriers underlying online counterspeech engagement. We presented each participant with three hate speech examples from a set of 900, spanning race, gender, religion, sexual orientation, and disability, and requested counterspeech responses. Subsequent questions assessed their satisfaction, perceived difficulty, and the effectiveness of their counterspeech. Our findings show that having been a target of online hate is a key driver of frequent online counterspeech engagement. People differ in their motivations and barriers towards engaging in online counterspeech across different demographic groups. Younger individuals, women, those with higher education levels, and regular witnesses to online hate are more reluctant to engage in online counterspeech due to concerns around public exposure, retaliation, and third-party harassment. Varying motivation and barriers in counterspeech engagement also shape how individuals view their own self-authored counterspeech and the difficulty experienced writing it. Additionally, our work explores people&rsquo;s willingness to use AI technologies like <b>ChatGPT</b> for counterspeech writing. Through this work we introduce a multi-item scale for understanding counterspeech motivation and barriers and a more nuanced understanding of the factors shaping online counterspeech engagement.</p></p class="citation"></blockquote><h3 id=911--206340-it-is-there-and-you-need-it-so-why-do-you-not-use-it-achieving-better-adoption-of-ai-systems-by-domain-experts-in-the-case-study-of-natural-science-research-auste-simkute-et-al-2024>(9/11 | 206/340) &lsquo;It is there, and you need it, so why do you not use it?&rsquo; Achieving better adoption of AI systems by domain experts, in the case study of natural science research (Auste Simkute et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Auste Simkute, Ewa Luger, Michael Evans, Rhianne Jones. (2024)<br><strong>&lsquo;It is there, and you need it, so why do you not use it?&rsquo; Achieving better adoption of AI systems by domain experts, in the case study of natural science research</strong><br><button class=copy-to-clipboard title="'It is there, and you need it, so why do you not use it?' Achieving better adoption of AI systems by domain experts, in the case study of natural science research" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16895v1.pdf filename=2403.16895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial Intelligence (AI) is becoming ubiquitous in domains such as medicine and natural science research. However, when AI systems are implemented in practice, domain experts often refuse them. Low acceptance hinders effective human-AI collaboration, even when it is essential for progress. In natural science research, scientists&rsquo; ineffective use of AI-enabled systems can impede them from analysing their data and advancing their research. We conducted an ethnographically informed study of 10 in-depth interviews with AI practitioners and natural scientists at the organisation facing low adoption of algorithmic systems. Results were consolidated into <b>recommendations</b> for better AI adoption: i) actively supporting experts during the initial stages of system use, ii) communicating the capabilities of a system in a user-relevant way, and iii) following predefined collaboration rules. We discuss the broader implications of our findings and expand on how our proposed requirements could support practitioners and experts across domains.</p></p class="citation"></blockquote><h3 id=1011--207340-we-have-no-idea-how-models-will-behave-in-production-until-production-how-engineers-operationalize-machine-learning-shreya-shankar-et-al-2024>(10/11 | 207/340) &lsquo;We Have No Idea How Models will Behave in Production until Production&rsquo;: How Engineers Operationalize Machine Learning (Shreya Shankar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shreya Shankar, Rolando Garcia, Joseph M Hellerstein, Aditya G Parameswaran. (2024)<br><strong>&lsquo;We Have No Idea How Models will Behave in Production until Production&rsquo;: How Engineers Operationalize Machine Learning</strong><br><button class=copy-to-clipboard title="'We Have No Idea How Models will Behave in Production until Production': How Engineers Operationalize Machine Learning" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16795v1.pdf filename=2403.16795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Organizations rely on machine learning engineers (MLEs) to deploy models and maintain ML pipelines in production. Due to models&rsquo; extensive reliance on fresh data, the operationalization of machine learning, or MLOps, requires MLEs to have proficiency in data science and engineering. When considered holistically, the job seems staggering &ndash; how do MLEs do MLOps, and what are their unaddressed challenges? To address these questions, we conducted semi-structured ethnographic interviews with 18 MLEs working on various applications, including <b>chatbots,</b> autonomous vehicles, and finance. We find that MLEs engage in a workflow of (i) data preparation, (ii) experimentation, (iii) evaluation throughout a multi-staged deployment, and (iv) continual monitoring and response. Throughout this workflow, MLEs collaborate extensively with data scientists, product stakeholders, and one another, supplementing routine verbal exchanges with communication tools ranging from Slack to organization-wide ticketing and reporting systems. We introduce the 3Vs of MLOps: velocity, visibility, and versioning &ndash; three virtues of successful ML deployments that MLEs learn to balance and grow as they mature. Finally, we discuss design implications and opportunities for future work.</p></p class="citation"></blockquote><h3 id=1111--208340-explora-a-teacher-apprentice-methodology-for-eliciting-natural-child-computer-interactions-vanessa-figueiredo-et-al-2024>(11/11 | 208/340) EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions (Vanessa Figueiredo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vanessa Figueiredo, Catherine Ann Cameron. (2024)<br><strong>EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions</strong><br><button class=copy-to-clipboard title="EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-IR, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17264v1.pdf filename=2403.17264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Investigating child-computer interactions within their contexts is vital for designing technology that caters to children&rsquo;s needs. However, determining what aspects of context are relevant for designing child-centric technology remains a challenge. We introduce EXPLORA, a <b>multimodal,</b> multistage online methodology comprising three pivotal stages: (1) building a teacher-apprentice relationship,(2) learning from child-teachers, and (3) assessing and reinforcing researcher-apprentice learning. Central to EXPLORA is the collection of attitudinal data through pre-observation interviews, offering researchers a deeper understanding of children&rsquo;s characteristics and contexts. This informs subsequent online observations, allowing researchers to focus on frequent interactions. Furthermore, researchers can validate preliminary assumptions with children. A means-ends analysis framework aids in the systematic analysis of data, shedding light on context, agency and homework-information searching processes children employ in their activities. To illustrate EXPLORA&rsquo;s capabilities, we present nine single case studies investigating Brazilian child-caregiver dyads&rsquo; (children ages 9-11) use of technology in homework information-searching.</p></p class="citation"></blockquote><h2 id=csse-15>cs.SE (15)</h2><h3 id=115--209340-a-comprehensive-study-of-the-capabilities-of-large-language-models-for-vulnerability-detection-benjamin-steenhoek-et-al-2024>(1/15 | 209/340) A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection (Benjamin Steenhoek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza Sanjida Alam, Earl T. Barr, Wei Le. (2024)<br><strong>A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection</strong><br><button class=copy-to-clipboard title="A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-LG, cs-SE, cs.SE<br>Keyword Score: 70<br>Keywords: Code Generation, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17218v1.pdf filename=2403.17218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated great potential for <b>code</b> <b>generation</b> and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires <b>reasoning</b> about the <b>code,</b> <b>making</b> it a good case study for exploring the limits of <b>LLMs&rsquo;</b> <b>reasoning</b> capabilities. Although recent work has applied <b>LLMs</b> to vulnerability detection using generic <b>prompting</b> techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear. In this paper, we surveyed eleven <b>LLMs</b> that are state-of-the-art in <b>code</b> <b>generation</b> and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing <b>prompts,</b> incorporating techniques such as <b>in-context</b> <b>learning</b> and chain-of-thought, and proposed three of our own <b>prompting</b> methods. Our results show that while our <b>prompting</b> methods improved the models&rsquo; performance, <b>LLMs</b> generally struggled with vulnerability detection. They reported 0.5-0.63 Balanced Accuracy and failed to distinguish between buggy and fixed versions of programs in 76% of cases on average. By comprehensively analyzing and categorizing 287 instances of model <b>reasoning,</b> we found that 57% of <b>LLM</b> responses contained errors, and the models frequently predicted incorrect locations of buggy <b>code</b> <b>and</b> misidentified bug types. <b>LLMs</b> only correctly localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted correctly by 70-100% of human participants. These findings suggest that despite their potential for other tasks, <b>LLMs</b> may fail to properly comprehend critical <b>code</b> <b>structures</b> and security-related concepts. Our data and <b>code</b> <b>are</b> available at <a href=https://figshare.com/s/78fe02e56e09ec49300b>https://figshare.com/s/78fe02e56e09ec49300b</a>.</p></p class="citation"></blockquote><h3 id=215--210340-repairagent-an-autonomous-llm-based-agent-for-program-repair-islem-bouzenia-et-al-2024>(2/15 | 210/340) RepairAgent: An Autonomous, LLM-Based Agent for Program Repair (Islem Bouzenia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Islem Bouzenia, Premkumar Devanbu, Michael Pradel. (2024)<br><strong>RepairAgent: An Autonomous, LLM-Based Agent for Program Repair</strong><br><button class=copy-to-clipboard title="RepairAgent: An Autonomous, LLM-Based Agent for Program Repair" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17134v1.pdf filename=2403.17134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated program repair has emerged as a powerful technique to mitigate the impact of software bugs on system reliability and user experience. This paper introduces RepairAgent, the first work to address the program repair challenge through an autonomous agent based on a <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> Unlike existing deep learning-based approaches, which <b>prompt</b> a model with a fixed <b>prompt</b> or in a fixed feedback loop, our work treats the <b>LLM</b> as an agent capable of autonomously planning and executing actions to fix bugs by invoking suitable tools. RepairAgent freely interleaves gathering information about the bug, gathering repair ingredients, and validating fixes, while deciding which tools to invoke based on the gathered information and feedback from previous fix attempts. Key contributions that enable RepairAgent include a set of tools that are useful for program repair, a dynamically updated <b>prompt</b> format that allows the <b>LLM</b> to interact with these tools, and a finite state machine that guides the agent in invoking the tools. Our evaluation on the popular Defects4J dataset demonstrates RepairAgent&rsquo;s effectiveness in autonomously repairing 164 bugs, including 39 bugs not fixed by prior techniques. Interacting with the <b>LLM</b> imposes an average cost of 270,000 tokens per bug, which, under the current pricing of OpenAI&rsquo;s <b>GPT-3.5</b> model, translates to 14 cents of USD per bug. To the best of our knowledge, this work is the first to present an autonomous, <b>LLM-based</b> agent for program repair, paving the way for future agent-based techniques in software engineering.</p></p class="citation"></blockquote><h3 id=315--211340-chatgpt-incorrectness-detection-in-software-reviews-minaoar-hossain-tanzil-et-al-2024>(3/15 | 211/340) ChatGPT Incorrectness Detection in Software Reviews (Minaoar Hossain Tanzil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minaoar Hossain Tanzil, Junaed Younus Khan, Gias Uddin. (2024)<br><strong>ChatGPT Incorrectness Detection in Software Reviews</strong><br><button class=copy-to-clipboard title="ChatGPT Incorrectness Detection in Software Reviews" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-SE, cs.SE<br>Keyword Score: 43<br>Keywords: Benchmarking, Generative AI, ChatGPT, Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16347v1.pdf filename=2403.16347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We conducted a survey of 135 software engineering (SE) practitioners to understand how they use <b>Generative</b> <b>AI-based</b> <b>chatbots</b> like <b>ChatGPT</b> for SE tasks. We find that they want to use <b>ChatGPT</b> for SE tasks like software library selection but often worry about the truthfulness of <b>ChatGPT</b> responses. We developed a suite of techniques and a tool called CID <b>(ChatGPT</b> Incorrectness Detector) to automatically test and detect the incorrectness in <b>ChatGPT</b> responses. CID is based on the iterative <b>prompting</b> to <b>ChatGPT</b> by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a <b>benchmark</b> study of library selection, we show that CID can detect incorrect responses from <b>ChatGPT</b> with an F1-score of 0.74 - 0.75.</p></p class="citation"></blockquote><h3 id=415--212340-exploring-the-impact-of-the-output-format-on-the-evaluation-of-large-language-models-for-code-translation-marcos-macedo-et-al-2024>(4/15 | 212/340) Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation (Marcos Macedo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcos Macedo, Yuan Tian, Filipe R. Cogo, Bram Adams. (2024)<br><strong>Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation</strong><br><button class=copy-to-clipboard title="Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17214v1.pdf filename=2403.17214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code translation between programming languages is a long-existing and critical task in software engineering, facilitating the modernization of legacy systems, ensuring cross-platform compatibility, and enhancing software performance. With the recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and their applications to code translation, there is an increasing need for comprehensive evaluation of these models. In this study, we empirically analyze the generated outputs of eleven popular instruct-tuned <b>LLMs</b> with parameters ranging from 1B up to 46.7B on 3,820 translation pairs across five languages, including C, C++, Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code translations produced by our evaluated <b>LLMs</b> necessitate post-processing, as these translations often include a mix of code, quotes, and text rather than being purely source code. Overlooking the output format of these models can inadvertently lead to underestimation of their actual performance. This is particularly evident when evaluating them with execution-based metrics such as Computational Accuracy (CA). Our results demonstrate that a strategic combination of <b>prompt</b> engineering and regular expression can effectively extract the source code from the model generation output. In particular, our method can help eleven selected models achieve an average Code Extraction Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future research to conduct more reliable <b>benchmarks</b> of <b>LLMs</b> for code translation.</p></p class="citation"></blockquote><h3 id=515--213340-evaluating-large-language-models-with-runtime-behavior-of-program-execution-junkai-chen-et-al-2024>(5/15 | 213/340) Evaluating Large Language Models with Runtime Behavior of Program Execution (Junkai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia. (2024)<br><strong>Evaluating Large Language Models with Runtime Behavior of Program Execution</strong><br><button class=copy-to-clipboard title="Evaluating Large Language Models with Runtime Behavior of Program Execution" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16437v1.pdf filename=2403.16437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> for code (i.e., code <b>LLMs)</b> have shown strong code understanding and generation capabilities. To evaluate the capabilities of code <b>LLMs</b> in various aspects, many <b>benchmarks</b> have been proposed (e.g., HumanEval and ClassEval). Code <b>reasoning</b> is one of the most essential abilities of code <b>LLMs,</b> but existing <b>benchmarks</b> for code <b>reasoning</b> are not sufficient. Typically, they focus on predicting the input and output of a program, ignoring the evaluation of the intermediate behavior during program execution, as well as the logical consistency (e.g., the model should not give the correct output if the prediction of execution path is wrong) when performing the <b>reasoning.</b> To address these problems, in this paper, we propose a framework, namely REval, for evaluating code <b>reasoning</b> abilities and consistency of code <b>LLMs</b> with program execution. We utilize existing code <b>benchmarks</b> and adapt them to new <b>benchmarks</b> within our framework. A <b>large-scale</b> <b>empirical</b> <b>study</b> is conducted and most <b>LLMs</b> show unsatisfactory performance on both Runtime Behavior <b>Reasoning</b> (i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation (i.e., an average IC score of 10.3). Evaluation results of current code <b>LLMs</b> reflect the urgent need for the community to strengthen the code <b>reasoning</b> capability of code <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=615--214340-agentfl-scaling-llm-based-fault-localization-to-project-level-context-yihao-qin-et-al-2024>(6/15 | 214/340) AgentFL: Scaling LLM-based Fault Localization to Project-Level Context (Yihao Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, Xiaoguang Mao. (2024)<br><strong>AgentFL: Scaling LLM-based Fault Localization to Project-Level Context</strong><br><button class=copy-to-clipboard title="AgentFL: Scaling LLM-based Fault Localization to Project-Level Context" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16362v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16362v1.pdf filename=2403.16362v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fault Localization (FL) is an essential step during the debugging process. With the strong capabilities of code comprehension, the recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated promising performance in diagnosing bugs in the code. Nevertheless, due to <b>LLMs&rsquo;</b> limited performance in handling long contexts, existing <b>LLM-based</b> fault localization remains on localizing bugs within a small code scope (i.e., a method or a class), which struggles to diagnose bugs for a <b>large</b> <b>code</b> <b>scope</b> (i.e., an entire software system). To address the limitation, this paper presents AgentFL, a multi-agent system based on <b>ChatGPT</b> for automated fault localization. By simulating the behavior of a human developer, AgentFL models the FL task as a three-step process, which involves comprehension, navigation, and confirmation. Within each step, AgentFL hires agents with diversified expertise, each of which utilizes different tools to handle specific tasks. Particularly, we adopt a series of auxiliary strategies such as Test Behavior Tracking, Document-Guided Search, and Multi-Round Dialogue to overcome the challenges in each step. The evaluation on the widely used Defects4J-V1.2.0 <b>benchmark</b> shows that AgentFL can localize 157 out of 395 bugs within Top-1, which outperforms the other <b>LLM-based</b> approaches and exhibits complementarity to the state-of-the-art learning-based techniques. Additionally, we confirm the indispensability of the components in AgentFL with the ablation study and demonstrate the usability of AgentFL through a user study. Finally, the cost analysis shows that AgentFL spends an average of only 0.074 dollars and 97 seconds for a single bug.</p></p class="citation"></blockquote><h3 id=715--215340-on-the-impact-of-black-box-deployment-strategies-for-edge-ai-on-latency-and-model-performance-jaskirat-singh-et-al-2024>(7/15 | 215/340) On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance (Jaskirat Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaskirat Singh, Bram Adams, Ahmed E. Hassan. (2024)<br><strong>On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance</strong><br><button class=copy-to-clipboard title="On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 25<br>Keywords: Black Box, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17154v1.pdf filename=2403.17154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers. This study aims to empirically assess the accuracy vs inference time trade-off of different <b>black-box</b> <b>Edge</b> AI deployment strategies, i.e., combinations of deployment operators and deployment tiers. In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, <b>Quantization,</b> Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers. Our findings suggest that Edge deployment using the hybrid <b>Quantization</b> + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss. However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a <b>Quantization</b> operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and <b>Quantized</b> Early Exit (on edge) operators. In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment. For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies. For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies.</p></p class="citation"></blockquote><h3 id=815--216340-design-patterns-for-multilevel-modeling-and-simulation-luca-serena-et-al-2024>(8/15 | 216/340) Design Patterns for Multilevel Modeling and Simulation (Luca Serena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Serena, Moreno Marzolla, Gabriele D&rsquo;Angelo, Stefano Ferretti. (2024)<br><strong>Design Patterns for Multilevel Modeling and Simulation</strong><br><button class=copy-to-clipboard title="Design Patterns for Multilevel Modeling and Simulation" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: 68, D-2-10; I-6-5, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16713v1.pdf filename=2403.16713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilevel modeling and <b>simulation</b> (M&amp;S) is becoming increasingly relevant due to the benefits that this methodology offers. Multilevel models allow users to describe a system at multiple levels of detail. From one side, this can make better use of computational resources, since the more detailed and time-consuming models can be executed only when/where required. From the other side, multilevel models can be assembled from existing components, cutting down development and verification/validation time. A downside of multilevel M&amp;S is that the development process becomes more complex due to some recurrent issues caused by the very nature of multilevel models: how to make sub-models interoperate, how to orchestrate execution, how state variables are to be updated when changing scale, and so on. In this paper, we address some of these issues by presenting a set of design patterns that provide a systematic approach for designing and implementing multilevel models. The proposed design patterns cover multiple aspects, including how to represent different levels of detail, how to combine incompatible models, how to exchange data across models, and so on. Some of the patterns are derived from the general software engineering literature, while others are specific to the multilevel M&amp;S application area.</p></p class="citation"></blockquote><h3 id=915--217340-a-mixed-method-study-of-devops-challenges-minaoar-hossain-tanzil-et-al-2024>(9/15 | 217/340) A Mixed Method Study of DevOps Challenges (Minaoar Hossain Tanzil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minaoar Hossain Tanzil, Masud Sarker, Gias Uddin, Anindya Iqbal. (2024)<br><strong>A Mixed Method Study of DevOps Challenges</strong><br><button class=copy-to-clipboard title="A Mixed Method Study of DevOps Challenges" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-HC, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Topic Model, Topic Modeling<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16436v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16436v1.pdf filename=2403.16436v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: DevOps practices combine software development and IT operations. There is a growing number of DevOps related posts in popular online developer forum Stack Overflow (SO). While previous research analyzed SO posts related to build/release engineering, we are aware of no research that specifically focused on DevOps related discussions. Objective: To learn the challenges developers face while using the currently available DevOps tools and techniques along with the organizational challenges in DevOps practices. Method: We conduct an empirical study by applying <b>topic</b> <b>modeling</b> on 174K SO posts that contain DevOps discussions. We then validate and extend the empirical study findings with a survey of 21 professional DevOps practitioners. Results: We find that: (1) There are 23 DevOps <b>topics</b> <b>grouped</b> into four categories: Cloud & CI/CD Tools, Infrastructure as Code, Container & Orchestration, and Quality Assurance. (2) The <b>topic</b> <b>category</b> Cloud & CI/CD Tools contains the highest number of <b>topics</b> <b>(10)</b> which cover 48.6% of all questions in our dataset, followed by the category Infrastructure as Code (28.9%). (3) File management is the most popular <b>topic</b> <b>followed</b> by Jenkins Pipeline, while infrastructural Exception Handling and Jenkins Distributed Architecture are the most difficult <b>topics</b> <b>(with</b> least accepted answers). (4) In the survey, developers mention that it requires hands-on experience before current DevOps tools can be considered easy. They raised the needs for better documentation and learning resources to learn the rapidly changing DevOps tools and techniques. Practitioners also emphasized on the formal training approach by the organizations for DevOps skill development. Conclusion: Architects and managers can use the findings of this research to adopt appropriate DevOps technologies, and organizations can design tool or process specific DevOps training programs.</p></p class="citation"></blockquote><h3 id=1015--218340-chatdbg-an-ai-powered-debugging-assistant-kyla-levin-et-al-2024>(10/15 | 218/340) ChatDBG: An AI-Powered Debugging Assistant (Kyla Levin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund. (2024)<br><strong>ChatDBG: An AI-Powered Debugging Assistant</strong><br><button class=copy-to-clipboard title="ChatDBG: An AI-Powered Debugging Assistant" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16354v1.pdf filename=2403.16354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like &ldquo;why is x null?&rdquo;. To handle these queries, ChatDBG grants the <b>LLM</b> autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded nearly 30,000 times.</p></p class="citation"></blockquote><h3 id=1115--219340-model-less-is-the-best-model-generating-pure-code-implementations-to-replace-on-device-dl-models-mingyi-zhou-et-al-2024>(11/15 | 219/340) Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models (Mingyi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyi Zhou, Xiang Gao, Pei Liu, John Grundy, Chunyang Chen, Xiao Chen, Li Li. (2024)<br><strong>Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models</strong><br><button class=copy-to-clipboard title="Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Graph, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16479v1.pdf filename=2403.16479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies show that deployed deep learning (DL) models such as those of Tensor Flow Lite (TFLite) can be easily extracted from real-world applications and devices by attackers to generate many kinds of attacks like <b>adversarial</b> <b>attacks.</b> Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent the aforementioned threats. Traditional software protection techniques have been widely explored, if on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate the DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract the on-device model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing units, configures the computing units to a <b>graph,</b> and then generates customized code to implement and deploy the ML solution without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0% and 24.3% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8% and 36.0% on x86-64 and ARM64 platforms, respectively.</p></p class="citation"></blockquote><h3 id=1215--220340-concerned-with-data-contamination-assessing-countermeasures-in-code-language-model-jialun-cao-et-al-2024>(12/15 | 220/340) Concerned with Data Contamination? Assessing Countermeasures in Code Language Model (Jialun Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialun Cao, Wuqi Zhang, Shing-Chi Cheung. (2024)<br><strong>Concerned with Data Contamination? Assessing Countermeasures in Code Language Model</strong><br><button class=copy-to-clipboard title="Concerned with Data Contamination? Assessing Countermeasures in Code Language Model" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16898v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16898v2.pdf filename=2403.16898v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various techniques have been proposed to leverage the capabilities of code language models (CLMs) for SE tasks. While these techniques typically evaluate their effectiveness using publicly available datasets, the evaluation can be subject to data contamination threats where the evaluation datasets have already been used to train the concerned CLMs. This can significantly affect the reliability of the evaluation. Different countermeasures have been suggested to mitigate the data contamination threat. Countermeasures include using more recent data, curating new data, and refactoring existing data are introduced, yet it is unclear whether these countermeasures could really mitigate data contamination threats to model evaluation. To fill the gap, we systematically study to quantify the impacts of these countermeasures on CLMs&rsquo; performance. To facilitate the study, we collected over 2 million Python functions with timestamps ranging from January 1st, 2018, to December 31st, 2023. The data created before the models&rsquo; cut-off date are considered &ldquo;contaminated data&rdquo;, while the data where the countermeasures are taken are regarded as &ldquo;cleansed data&rdquo;. We study the impact of these countermeasures by investigating the difference in CLMs&rsquo; performance on contaminated and cleansed data derived from different countermeasures. Our experiments yield several interesting observations. For instance, CLMs do not necessarily perform worse on data after the models&rsquo; cut-off date; on the contrary, they sometimes perform better. In addition, refactoring did not always result in decreased performance; it could lead to improvements instead. Furthermore, existing metrics such as <b>perplexity</b> cannot distinguish contaminated/cleansed data. We hope that the results and observations could help deepen the understanding of CLMs&rsquo; capabilities and inform the community about data contamination.</p></p class="citation"></blockquote><h3 id=1315--221340-seeking-enlightenment-incorporating-evidence-based-practice-techniques-in-a-research-software-engineering-team-reed-milewicz-et-al-2024>(13/15 | 221/340) Seeking Enlightenment: Incorporating Evidence-Based Practice Techniques in a Research Software Engineering Team (Reed Milewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reed Milewicz, Jon Bisila, Miranda Mundt, Joshua Teves. (2024)<br><strong>Seeking Enlightenment: Incorporating Evidence-Based Practice Techniques in a Research Software Engineering Team</strong><br><button class=copy-to-clipboard title="Seeking Enlightenment: Incorporating Evidence-Based Practice Techniques in a Research Software Engineering Team" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-0; J-2; K-7-m, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16827v1.pdf filename=2403.16827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evidence-based practice (EBP) in software engineering aims to improve decision-making in software development by complementing practitioners&rsquo; professional judgment with high-quality evidence from research. We believe the use of EBP techniques may be helpful for research software engineers (RSEs) in their work to bring software engineering best practices to scientific software development. In this study, we present an experience report on the use of a particular EBP technique, rapid reviews, within an RSE team at Sandia National Laboratories, and present practical <b>recommendations</b> for how to address barriers to EBP adoption within the RSE community.</p></p class="citation"></blockquote><h3 id=1415--222340-enhancing-software-effort-estimation-through-reinforcement-learning-based-project-management-oriented-feature-selection-haoyang-chen-et-al-2024>(14/15 | 222/340) Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection (Haoyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyang Chen, Botong Xu, Kaiyang Zhong. (2024)<br><strong>Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection</strong><br><button class=copy-to-clipboard title="Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16749v1.pdf filename=2403.16749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: The study aims to investigate the application of the data element market in software project management, focusing on improving effort estimation by addressing challenges faced by traditional methods. Design/methodology/approach: This study proposes a solution based on feature selection, utilizing the data element market and <b>reinforcement</b> <b>learning-based</b> algorithms to enhance the accuracy of software effort estimation. It explores the application of the MARLFS algorithm, customizing improvements to the algorithm and reward function. Findings: This study demonstrates that the proposed approach achieves more precise estimation compared to traditional methods, leveraging feature selection to guide project management in software development. Originality/value: This study contributes to the field by offering a novel approach that combines the data element market, machine learning, and feature selection to improve software effort estimation, addressing limitations of traditional methods and providing insights for future research in project management.</p></p class="citation"></blockquote><h3 id=1515--223340-disl-fueling-research-with-a-large-dataset-of-solidity-smart-contracts-gabriele-morello-et-al-2024>(15/15 | 223/340) DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts (Gabriele Morello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus. (2024)<br><strong>DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts</strong><br><button class=copy-to-clipboard title="DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-DC, cs-LG, cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16861v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16861v2.pdf filename=2403.16861v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for <b>benchmarking</b> software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.</p></p class="citation"></blockquote><h2 id=eesssy-17>eess.SY (17)</h2><h3 id=117--224340-an-llm-based-digital-twin-for-optimizing-human-in-the-loop-systems-hanqing-yang-et-al-2024>(1/17 | 224/340) An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems (Hanqing Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqing Yang, Marie Siew, Carlee Joe-Wong. (2024)<br><strong>An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems</strong><br><button class=copy-to-clipboard title="An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 70<br>Keywords: Foundation Model, Reinforcement Learning, Simulation, Simulator, human-in-the-loop, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16809v1.pdf filename=2403.16809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and <b>Foundation</b> <b>Models</b> are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such <b>human-in-the-loop</b> (HITL) systems, however, is difficult in practice. We propose the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs <b>LLM</b> agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based <b>reinforcement</b> <b>learning</b> algorithm AitL-RL, which employs the <b>LLM</b> as a dynamic <b>simulation</b> of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that <b>LLMs</b> are capable of simulating complex population movements within <b>large</b> <b>open</b> <b>spaces.</b> Besides, AitL-RL demonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced <b>Foundation</b> <b>Models</b> like <b>LLMs</b> into CPS-IoT to enhance system adaptability and efficiency. The project&rsquo;s code can be found on our GitHub repository.</p></p class="citation"></blockquote><h3 id=217--225340-state-space-models-as-foundation-models-a-control-theoretic-overview-carmen-amo-alonso-et-al-2024>(2/17 | 225/340) State Space Models as Foundation Models: A Control Theoretic Overview (Carmen Amo Alonso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger. (2024)<br><strong>State Space Models as Foundation Models: A Control Theoretic Overview</strong><br><button class=copy-to-clipboard title="State Space Models as Foundation Models: A Control Theoretic Overview" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CL, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 53<br>Keywords: Benchmarking, Foundation Model, GPT, GPT-4, Transformer, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16899v1.pdf filename=2403.16899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of <b>foundation</b> <b>models.</b> This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art <b>Transformer</b> architectures in language tasks. <b>Foundation</b> <b>models,</b> like e.g. <b>GPT-4,</b> aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and <b>summarizes</b> the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized <b>benchmark</b> designed for assessing a model&rsquo;s efficiency at learning long sequences.</p></p class="citation"></blockquote><h3 id=317--226340-a-discrete-time-least-squares-adaptive-state-tracking-control-scheme-with-a-mobile-robot-system-study-qianhong-zhao-et-al-2024>(3/17 | 226/340) A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study (Qianhong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianhong Zhao, Gang Tao. (2024)<br><strong>A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study</strong><br><button class=copy-to-clipboard title="A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 40<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17235v1.pdf filename=2403.17235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper develops an adaptive state tracking control scheme for <b>discrete-time</b> <b>systems,</b> using the least-squares algorithm, as the new solution to the long-standing <b>discrete-time</b> <b>adaptive</b> state tracking control problem to which the Lyapunov method (well-developed for the <b>continuous-time</b> <b>adaptive</b> state tracking problem) is not applicable. The new adaptive state tracking scheme is based on a recently-developed new <b>discrete-time</b> <b>error</b> model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation. The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation. The system stability and output tracking properties are studied. Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis. The developed adaptive control scheme is applied to a <b>discrete-time</b> <b>multiple</b> mobile robot system to meet an adaptive state tracking objective. In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process. <b>Simulation</b> results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme.</p></p class="citation"></blockquote><h3 id=417--227340-physics-informed-rl-for-maximal-safety-probability-estimation-hikaru-hoshino-et-al-2024>(4/17 | 227/340) Physics-informed RL for Maximal Safety Probability Estimation (Hikaru Hoshino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hikaru Hoshino, Yorie Nakahira. (2024)<br><strong>Physics-informed RL for Maximal Safety Probability Estimation</strong><br><button class=copy-to-clipboard title="Physics-informed RL for Maximal Safety Probability Estimation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16391v1.pdf filename=2403.16391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate risk quantification and reachability analysis are crucial for safe control and learning, but sampling from rare events, risky states, or long-term trajectories can be prohibitively costly. Motivated by this, we study how to estimate the long-term safety probability of maximally safe actions without sufficient coverage of samples from risky states and long-term trajectories. The use of maximal safety probability in control and learning is expected to avoid conservative behaviors due to over-approximation of risk. Here, we first show that long-term safety probability, which is multiplicative in time, can be converted into additive costs and be solved using standard <b>reinforcement</b> <b>learning</b> methods. We then derive this probability as solutions of partial differential equations (PDEs) and propose Physics-Informed <b>Reinforcement</b> <b>Learning</b> (PIRL) algorithm. The proposed method can learn using sparse rewards because the physics constraints help propagate risk information through neighbors. This suggests that, for the purpose of extracting more information for efficient learning, physics constraints can serve as an alternative to reward shaping. The proposed method can also estimate long-term risk using short-term samples and deduce the risk of unsampled states. This feature is in stark contrast with the unconstrained deep RL that demands sufficient data coverage. These merits of the proposed method are demonstrated in numerical <b>simulation.</b></p></p class="citation"></blockquote><h3 id=517--228340-optimal-operation-of-reconfigurable-active-distribution-networks-aiming-at-resiliency-improvement-saeed-behzadi-et-al-2024>(5/17 | 228/340) Optimal Operation of Reconfigurable Active Distribution Networks Aiming at Resiliency Improvement (Saeed Behzadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Behzadi, Amir Bagheri, Abbas Rabiee. (2024)<br><strong>Optimal Operation of Reconfigurable Active Distribution Networks Aiming at Resiliency Improvement</strong><br><button class=copy-to-clipboard title="Optimal Operation of Reconfigurable Active Distribution Networks Aiming at Resiliency Improvement" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17272v1.pdf filename=2403.17272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As natural disasters bring about power outage and financial losses, network resiliency is an important challenge for distribution network operators (DNOs). On the other side, power loss reduction during normal operating condition is a major concern of DNOs. In this paper, optimal scheduling of active distribution network (ADN) is addressed through simultaneous minimization of power loss in normal condition and load shedding in critical condition after natural disasters. A new formulation is developed for the network reconfiguration to optimize the system operation in both normal and emergency conditions in the presence of conventional and renewable-energy-based distributed generation (DG) as well as energy storage systems (ESSs). The line flow based (LFB) algorithm is used for the AC power flow calculations, and all the developed relations have been convexified to construct a mixed-integer quadratically-constrained programming (MIQCP) optimization model. The <b>simulations</b> have been implemented on the IEEE 33-bus system in GAMS, and the results are investigated.</p></p class="citation"></blockquote><h3 id=617--229340-spline-trajectory-tracking-and-obstacle-avoidance-for-mobile-agents-via-convex-optimization-akua-dickson-et-al-2024>(6/17 | 229/340) Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via Convex Optimization (Akua Dickson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akua Dickson, Christos G. Cassandras, Roberto Tron. (2024)<br><strong>Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via Convex Optimization</strong><br><button class=copy-to-clipboard title="Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via Convex Optimization" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16900v1.pdf filename=2403.16900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an output feedback control-based motion planning technique for agents to enable them to converge to a specified polynomial trajectory while imposing a set of safety constraints on our controller to avoid collisions within the free configuration space (polygonal environment). To achieve this, we 1) decompose our polygonal environment into different overlapping cells 2) write out our polynomial trajectories as the output of a reference dynamical system with given initial conditions 3) formulate convergence and safety constraints as Linear Matrix Inequalities (LMIs) on our controller using Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4) solve a semi-definite programming (SDP) problem with convergence and safety constraints imposed to synthesize a controller for each convex cell. Extensive <b>simulations</b> are included to test our motion planning method under different initial conditions and different reference trajectories. The synthesized controller is robust to changes in initial conditions and is always safe relative to the boundaries of the polygonal environment.</p></p class="citation"></blockquote><h3 id=717--230340-semantic-aware-remote-estimation-of-multiple-markov-sources-under-constraints-jiping-luo-et-al-2024>(7/17 | 230/340) Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints (Jiping Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiping Luo, Nikolaos Pappas. (2024)<br><strong>Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints</strong><br><button class=copy-to-clipboard title="Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-IT, cs-LG, cs-NI, cs-SY, eess-SY, eess.SY, math-IT<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16855v1.pdf filename=2403.16855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies semantic-aware communication for remote estimation of multiple <b>Markov</b> <b>sources</b> <b>over</b> a lossy and rate-constrained channel. Unlike most existing studies that treat all source states equally, we exploit the semantics of information and consider that the remote actuator has different tolerances for the estimation errors of different states. We aim to find an optimal scheduling policy that minimizes the long-term state-dependent costs of estimation errors under a transmission frequency constraint. We theoretically show the structure of the optimal policy by leveraging the average-cost Constrained <b>Markov</b> <b>Decision</b> <b>Process</b> (CMDP) theory and the Lagrangian dynamic programming. By exploiting the optimal structural results, we develop a novel policy search algorithm, termed intersection search plus relative value iteration (Insec-RVI), that can find the optimal policy using only a few iterations. To avoid the ``curse of dimensionality&rsquo;&rsquo; of <b>MDPs,</b> we propose an online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on the Lyapunov optimization theorem. We also design an efficient average-cost Q-learning algorithm to estimate the optimal policy without knowing a priori the channel and source statistics. Numerical results show that continuous transmission is inefficient, and remarkably, our semantic-aware policies can attain the optimum by strategically utilizing fewer transmissions by exploiting the timing of the important information.</p></p class="citation"></blockquote><h3 id=817--231340-energy-efficiency-optimization-method-of-wdm-visible-light-communication-system-for-indoor-broadcasting-networks-dayu-shi-et-al-2024>(8/17 | 231/340) Energy Efficiency Optimization Method of WDM Visible Light Communication System for Indoor Broadcasting Networks (Dayu Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dayu Shi, Xun Zhang, Ziqi Liu, Xuanbang Chen, Jianghao Li, Xiaodong Liu, William Shieh. (2024)<br><strong>Energy Efficiency Optimization Method of WDM Visible Light Communication System for Indoor Broadcasting Networks</strong><br><button class=copy-to-clipboard title="Energy Efficiency Optimization Method of WDM Visible Light Communication System for Indoor Broadcasting Networks" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, physics-optics<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16836v1.pdf filename=2403.16836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to optimize energy efficiency in wavelength division multiplexing (WDM) Visible Light Communication (VLC) systems designed for indoor broadcasting networks. A physics-based LED model is integrated into system energy efficiency optimization, enabling quantitative analysis of the critical issue of VLC energy efficiency: the nonlinear interplay between illumination and communication performance. The optimization jointly incorporates constraints on communication quality of each channel, and illumination performance, standardized by the International Commission on Illumination (CIE). The formulated nonlinear optimization problem is solved by the Sequential Quadratic Programming (SQP) algorithm in an experiment-based <b>simulation.</b> An integrated Red-Green-Blue-Yellow Light Emitting Diode (RGBY-LED) is measured for model calibration and three different scenarios are simulated to evaluate the generality of the proposed method. Results demonstrate a double enhancement in performance and a high versatility in accommodating various scenarios. Furthermore, it highlights the importance of balancing communication and illumination imperatives in VLC systems, challenging conventional perceptions focused solely on minimizing power consumption.</p></p class="citation"></blockquote><h3 id=917--232340-policy-gradient-based-model-free-optimal-lqg-control-with-a-probabilistic-risk-constraint-arunava-naha-et-al-2024>(9/17 | 232/340) Policy Gradient-based Model Free Optimal LQG Control with a Probabilistic Risk Constraint (Arunava Naha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arunava Naha, Subhrakanti Dey. (2024)<br><strong>Policy Gradient-based Model Free Optimal LQG Control with a Probabilistic Risk Constraint</strong><br><button class=copy-to-clipboard title="Policy Gradient-based Model Free Optimal LQG Control with a Probabilistic Risk Constraint" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16767v1.pdf filename=2403.16767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate a model-free optimal control design that minimizes an infinite horizon average expected quadratic cost of states and control actions subject to a probabilistic risk or chance constraint using input-output data. In particular, we consider linear time-invariant systems and design an optimal controller within the class of linear state feedback control. Three different policy gradient (PG) based algorithms, natural policy gradient (NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy gradient (DDPG), are developed, and compared with the optimal risk-neutral linear-quadratic regulator (LQR) and a scenario-based model predictive control (MPC) technique via numerical <b>simulations.</b> The convergence properties and the accuracy of all the algorithms are compared numerically. We also establish analytical convergence properties of the NPG and GNPG algorithms under the known model scenario, while the proof of convergence for the unknown model scenario is part of our ongoing work.</p></p class="citation"></blockquote><h3 id=1017--233340-a-branch-and-bound-method-for-the-exact-parameter-identification-of-the-pkpd-model-for-anesthetic-drugs-giulia-di-credico-et-al-2024>(10/17 | 233/340) A Branch and Bound method for the exact parameter identification of the PK/PD model for anesthetic drugs (Giulia Di Credico et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giulia Di Credico, Luca Consolini, Mattia Laurini, Marco Locatelli, Marco Milanesi, Michele Schiavo, Antonio Visioli. (2024)<br><strong>A Branch and Bound method for the exact parameter identification of the PK/PD model for anesthetic drugs</strong><br><button class=copy-to-clipboard title="A Branch and Bound method for the exact parameter identification of the PK/PD model for anesthetic drugs" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16742v1.pdf filename=2403.16742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of parameter identification for the standard pharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs. Our main contribution is the development of a global optimization method that guarantees finding the parameters that minimize the one-step ahead prediction error. The method is based on a branch-and-bound algorithm, that can be applied to solve a more general class of nonlinear regression problems. We present some <b>simulation</b> results, based on a dataset of twelve patients. In these <b>simulations,</b> we are always able to identify the exact parameters, despite the non-convexity of the overall identification problem.</p></p class="citation"></blockquote><h3 id=1117--234340-guided-bayesian-optimization-data-efficient-controller-tuning-with-digital-twin-mahdi-nobar-et-al-2024>(11/17 | 234/340) Guided Bayesian Optimization: Data-Efficient Controller Tuning with Digital Twin (Mahdi Nobar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahdi Nobar, Jürg Keller, Alisa Rupenyan, Mohammad Khosravi, John Lygeros. (2024)<br><strong>Guided Bayesian Optimization: Data-Efficient Controller Tuning with Digital Twin</strong><br><button class=copy-to-clipboard title="Guided Bayesian Optimization: Data-Efficient Controller Tuning with Digital Twin" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16619v1.pdf filename=2403.16619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents the guided Bayesian optimization algorithm as an efficient data-driven method for iteratively tuning closed-loop controller parameters using an event-triggered digital twin of the system based on available closed-loop data. We define a controller tuning framework independent of the controller or the plant structure. Our proposed methodology is model-free, making it suitable for nonlinear and unmodelled plants with measurement noise. The objective function consists of performance metrics modeled by Gaussian processes. We utilize the available information in the closed-loop system to identify and progressively maintain a digital twin that guides the optimizer, improving the data efficiency of our method. Switching the digital twin on and off is triggered by data-driven criteria related to the digital twin&rsquo;s uncertainty estimations in the BO tuning framework. Effectively, it replaces much of the exploration of the real system with exploration performed on the digital twin. We analyze the properties of our method in <b>simulation</b> and demonstrate its performance on two real closed-loop systems with different plant and controller structures. The experimental results show that our method requires fewer experiments on the physical plant than Bayesian optimization to find the optimal controller parameters.</p></p class="citation"></blockquote><h3 id=1217--235340-sparsity-constrained-linear-quadratic-regulation-problem-greedy-approach-with-performance-guarantee-shumpei-nishida-et-al-2024>(12/17 | 235/340) Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy Approach with Performance Guarantee (Shumpei Nishida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shumpei Nishida, Kunihisa Okano. (2024)<br><strong>Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy Approach with Performance Guarantee</strong><br><button class=copy-to-clipboard title="Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy Approach with Performance Guarantee" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16585v1.pdf filename=2403.16585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a linear quadratic regulation problem with a constraint where the control input can be nonzero only at a limited number of times. Given that this constraint leads to a combinational optimization problem, we adopt a greedy method to find a suboptimal solution. To quantify the performance of the greedy algorithm, we employ two metrics that reflect the submodularity level of the objective function: The submodularity ratio and curvature. We first present an explicit form of the optimal control input that is amenable to evaluating these metrics. Subsequently, we establish bounds on the submodularity ratio and curvature, which enable us to offer a practical performance guarantee for the greedy algorithm. The effectiveness of our guarantee is further demonstrated through numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=1317--236340-ensuring-disturbance-rejection-performance-by-synthesizing-grid-following-and-grid-forming-inverters-in-power-systems-fuyilong-ma-et-al-2024>(13/17 | 236/340) Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems (Fuyilong Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fuyilong Ma, Huanhai Xin, Zhiyi Li, Linbin Huang. (2024)<br><strong>Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems</strong><br><button class=copy-to-clipboard title="Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16488v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16488v3.pdf filename=2403.16488v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To satisfy dynamic requirements of power systems, it is imperative for grid-tied inverters to ensure good disturbance rejection performance (DRP) under variable grid conditions. This letter discovers and theoretically proves that for general networks, synthesizing grid-following (GFL) inverters and grid-forming (GFM) inverters can always more effectively ensure the DRP of multiple inverters, as compared to homogeneous inverter-based systems that solely utilize either GFL or GFM inverters. The synthesis of GFL inverters and GFM inverters can concurrently increase the smallest eigenvalue and decrease the largest eigenvalue of the network grounded Laplacian matrix. This can be equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM inverters, thereby ensuring the DRP of inverters both in weak and strong grids. The results reveal the necessity of synthesizing diverse inverter control schemes from the network-based perspective. Sensitivity function-based tests and real-time <b>simulations</b> validate our results.</p></p class="citation"></blockquote><h3 id=1417--237340-active-learning-of-dynamics-using-prior-domain-knowledge-in-the-sampling-process-kevin-s-miller-et-al-2024>(14/17 | 237/340) Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process (Kevin S. Miller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu. (2024)<br><strong>Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process</strong><br><button class=copy-to-clipboard title="Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17233v1.pdf filename=2403.17233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an <b>active</b> <b>learning</b> algorithm for learning dynamics that leverages side information by explicitly incorporating prior domain knowledge into the sampling process. Our proposed algorithm guides the exploration toward regions that demonstrate high empirical discrepancy between the observed data and an imperfect prior model of the dynamics derived from side information. Through numerical experiments, we demonstrate that this strategy explores regions of high discrepancy and accelerates learning while simultaneously reducing model uncertainty. We rigorously prove that our <b>active</b> <b>learning</b> algorithm yields a consistent estimate of the underlying dynamics by providing an explicit rate of convergence for the maximum predictive variance. We demonstrate the efficacy of our approach on an under-actuated pendulum system and on the half-cheetah MuJoCo environment.</p></p class="citation"></blockquote><h3 id=1517--238340-sis-epidemics-on-open-networks-a-replacement-based-approximation-renato-vizuete-et-al-2024>(15/17 | 238/340) SIS epidemics on open networks: A replacement-based approximation (Renato Vizuete et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renato Vizuete, Paolo Frasca, Elena Panteley. (2024)<br><strong>SIS epidemics on open networks: A replacement-based approximation</strong><br><button class=copy-to-clipboard title="SIS epidemics on open networks: A replacement-based approximation" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC, math-PR<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16727v1.pdf filename=2403.16727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we analyze <b>continuous-time</b> <b>SIS</b> epidemics subject to arrivals and departures of agents, by using an approximated process based on replacements. In defining the SIS dynamics in an open network, we consider a stochastic setting in which arrivals and departures take place according to Poisson processes with similar rates, and the new value of the infection probability of an arriving agent is drawn from a <b>continuous</b> <b>distribution.</b> Since the system size changes with time, we define an approximated process, in which replacements take place instead of arrivals and departures, and we focus on the evolution of an aggregate measure of the level of infection. So long as the reproduction number is less than one, the long-term behavior of this function measures the impact of the changes of the set of agents in the epidemic. We derive upper bounds for the expectation and variance of this function and we include a numerical example to show that the approximated process is close to the original SIS process.</p></p class="citation"></blockquote><h3 id=1617--239340-predictable-interval-mdps-through-entropy-regularization-menno-van-zutphen-et-al-2024>(16/17 | 239/340) Predictable Interval MDPs through Entropy Regularization (Menno van Zutphen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Menno van Zutphen, Giannis Delimpaltadakis, Maurice Heemels, Duarte Antunes. (2024)<br><strong>Predictable Interval MDPs through Entropy Regularization</strong><br><button class=copy-to-clipboard title="Predictable Interval MDPs through Entropy Regularization" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16711v1.pdf filename=2403.16711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regularization of control policies using entropy can be instrumental in adjusting predictability of real-world systems. Applications benefiting from such approaches range from, e.g., cybersecurity, which aims at maximal unpredictability, to human-robot interaction, where predictable behavior is highly desirable. In this paper, we consider entropy regularization for interval Markov decision processes (IMDPs). IMDPs are uncertain <b>MDPs,</b> where transition probabilities are only known to belong to intervals. Lately, IMDPs have gained significant popularity in the context of abstracting stochastic systems for control design. In this work, we address robust minimization of the linear combination of entropy and a standard cumulative cost in IMDPs, thereby establishing a trade-off between optimality and predictability. We show that optimal deterministic policies exist, and devise a value-iteration algorithm to compute them. The algorithm solves a number of convex programs at each step. Finally, through an illustrative example we show the benefits of penalizing entropy in IMDPs.</p></p class="citation"></blockquote><h3 id=1717--240340-a-geometric-perspective-on-fusing-gaussian-distributions-on-lie-groups-yixiao-ge-et-al-2024>(17/17 | 240/340) A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups (Yixiao Ge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixiao Ge, Pieter van Goor, Robert Mahony. (2024)<br><strong>A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups</strong><br><button class=copy-to-clipboard title="A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16411v1.pdf filename=2403.16411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stochastic inference on Lie groups plays a key role in state estimation problems, such as inertial navigation, visual inertial odometry, pose estimation in virtual reality, etc. A key problem is fusing independent concentrated Gaussian distributions defined at different reference points on the group. In this paper we approximate distributions at different points in the group in a single set of exponential coordinates and then use classical Gaussian fusion to obtain the fused posteriori in those coordinates. We consider several approximations including the exact Jacobian of the change of coordinate map, first and second order Taylor&rsquo;s expansions of the Jacobian, and parallel transport with and without curvature correction associated with the underlying <b>geometry</b> of the Lie group. Preliminary results on SO(3) demonstrate that a novel approximation using parallel transport with curvature correction achieves similar accuracy to the state-of-the-art optimisation based algorithms at a fraction of the computational cost.</p></p class="citation"></blockquote><h2 id=csro-27>cs.RO (27)</h2><h3 id=127--241340-grounding-language-plans-in-demonstrations-through-counterfactual-perturbations-yanwei-wang-et-al-2024>(1/27 | 241/340) Grounding Language Plans in Demonstrations Through Counterfactual Perturbations (Yanwei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah. (2024)<br><strong>Grounding Language Plans in Demonstrations Through Counterfactual Perturbations</strong><br><button class=copy-to-clipboard title="Grounding Language Plans in Demonstrations Through Counterfactual Perturbations" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Counter-factual, Common-sense Reasoning, Grounding, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17124v1.pdf filename=2403.17124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Grounding</b> the <b>common-sense</b> <b>reasoning</b> of <b>Large</b> <b>Language</b> <b>Models</b> in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging <b>LLMs</b> directly for planning in symbolic spaces, this work uses <b>LLMs</b> to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an <b>LLM</b> and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations&rsquo; state space with additional successful executions as well as <b>counterfactuals</b> that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned <b>grounding</b> classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: <a href=https://sites.google.com/view/grounding-plans>https://sites.google.com/view/grounding-plans</a></p></p class="citation"></blockquote><h3 id=227--242340-exploring-causalworld-enhancing-robotic-manipulation-via-knowledge-transfer-and-curriculum-learning-xinrui-wang-et-al-2024>(2/27 | 242/340) Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning (Xinrui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrui Wang, Yan Jin. (2024)<br><strong>Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning</strong><br><button class=copy-to-clipboard title="Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Curriculum Learning, Fine-tuning, Knowledge Transfer, Reinforcement Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17266v1.pdf filename=2403.17266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing <b>reinforcement</b> <b>learning,</b> we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two <b>knowledge</b> <b>transfer</b> strategies, <b>fine-tuning</b> and <b>curriculum</b> <b>learning,</b> were utilized within the soft actor-critic architecture. <b>Fine-tuning</b> allows the agent to leverage pre-trained <b>knowledge</b> <b>and</b> adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, <b>curriculum</b> <b>learning</b> decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameters. The key factors of two learning strategies and corresponding effects were explored in context-aware and context-unaware scenarios, enabling us to identify the scenarios where the methods demonstrate optimal performance, derive conclusive insights, and contribute to a broader range of learning-based engineering applications.</p></p class="citation"></blockquote><h3 id=327--243340-temporal-and-semantic-evaluation-metrics-for-foundation-models-in-post-hoc-analysis-of-robotic-sub-tasks-jonathan-salfity-et-al-2024>(3/27 | 243/340) Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks (Jonathan Salfity et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Salfity, Selma Wanna, Minkyu Choi, Mitch Pryor. (2024)<br><strong>Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks</strong><br><button class=copy-to-clipboard title="Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Foundation Model, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17238v1.pdf filename=2403.17238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent <b>prompting</b> strategies for <b>Foundation</b> <b>Models</b> (FMs) including both <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics measure the temporal alignment and semantic fidelity of language descriptions between two sub-task decompositions, namely an FM sub-task decomposition prediction and a ground-truth sub-task decomposition. We present scores for temporal similarity and semantic similarity above 90%, compared to 30% of a randomized baseline, for multiple robotic environments, demonstrating the effectiveness of our proposed framework. Our results enable building diverse, <b>large-scale,</b> <b>language-supervised</b> <b>datasets</b> for improved robotic TAMP.</p></p class="citation"></blockquote><h3 id=427--244340-a-robotic-skill-learning-system-built-upon-diffusion-policies-and-foundation-models-nils-ingelhag-et-al-2024>(4/27 | 244/340) A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models (Nils Ingelhag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nils Ingelhag, Jesper Munkeby, Jonne van Haastregt, Anastasia Varava, Michael C. Welle, Danica Kragic. (2024)<br><strong>A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models</strong><br><button class=copy-to-clipboard title="A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 46<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16730v1.pdf filename=2403.16730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained <b>multimodal</b> <b>foundational</b> <b>models</b> to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. <b>Foundational</b> <b>models</b> are being used to perform skill selection given the user&rsquo;s <b>prompt</b> in natural language. Before executing a skill the <b>foundational</b> <b>model</b> performs a precondition check given an observation of the workspace. We compare the performance of different <b>foundational</b> <b>models</b> to this end as well as give a detailed experimental evaluation of the skills taught by the user in <b>simulation</b> and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in <b>simulation</b> and the real world, are available on the project&rsquo;s website.</p></p class="citation"></blockquote><h3 id=527--245340-domain-adaptive-detection-of-mavs-a-benchmark-and-noise-suppression-network-yin-zhang-et-al-2024>(5/27 | 245/340) Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network (Yin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yin Zhang, Jinhong Deng, Peidong Liu, Wen Li, Shiyu Zhao. (2024)<br><strong>Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network</strong><br><button class=copy-to-clipboard title="Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Benchmarking, Curriculum Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16669v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16669v1.pdf filename=2403.16669v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both <b>simulation</b> and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new <b>benchmark</b> for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based <b>curriculum</b> <b>learning</b> module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of <b>simulation-to-real</b> adaptation, cross-scene adaptation, and cross-camera adaptation, respectively.</p></p class="citation"></blockquote><h3 id=627--246340-towards-cooperative-maneuver-planning-in-mixed-traffic-at-urban-intersections-marvin-klimke-et-al-2024>(6/27 | 246/340) Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections (Marvin Klimke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marvin Klimke, Max Bastian Mertens, Benjamin Völz, Michael Buchholz. (2024)<br><strong>Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections</strong><br><button class=copy-to-clipboard title="Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Graph, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16478v1.pdf filename=2403.16478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas. Apart from sharing of awareness and perception information over wireless communication links, cooperative maneuver planning may facilitate active guidance of connected automated vehicles at urban intersections. Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments. In this work, we present two cooperative planning approaches that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans. Firstly, we propose an optimization-based planner trained on real driving data that cyclically selects the most efficient out of multiple predicted coordinated maneuvers. Additionally, we present a cooperative planning approach based on <b>graph-based</b> <b>reinforcement</b> <b>learning,</b> which conquers the lack of ground truth data for cooperative maneuvers. We present evaluation results of both cooperative planners in high-fidelity <b>simulation</b> and real-world traffic. Simulative experiments in fully automated traffic and mixed traffic show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops. In real-world experiments with three prototype connected automated vehicles in public traffic, both planners demonstrate their ability to perform efficient cooperative maneuvers.</p></p class="citation"></blockquote><h3 id=727--247340-dyna-lflh-learning-agile-navigation-in-dynamic-environments-from-learned-hallucination-saad-abdul-ghani-et-al-2024>(7/27 | 247/340) Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination (Saad Abdul Ghani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao. (2024)<br><strong>Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination</strong><br><button class=copy-to-clipboard title="Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17231v1.pdf filename=2403.17231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a <b>self-supervised</b> <b>learning</b> method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while <b>reinforcement</b> <b>learning</b> becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample dynamic obstacles from it, so the generated training data can be used to learn a motion planner to navigate in dynamic environments. Dyna-LfLH is evaluated on a ground robot in both simulated and physical environments and achieves up to 25% better success rate compared to baselines.</p></p class="citation"></blockquote><h3 id=827--248340-adaptive-step-duration-for-precise-foot-placement-achieving-robust-bipedal-locomotion-on-terrains-with-restricted-footholds-zhaoyang-xiang-et-al-2024>(8/27 | 248/340) Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds (Zhaoyang Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyang Xiang, Victor Paredes, Ayonga Hereid. (2024)<br><strong>Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds</strong><br><button class=copy-to-clipboard title="Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Discrete Time, Discrete Time, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17136v1.pdf filename=2403.17136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel multi-step preview foot placement planning algorithm designed to enhance the robustness of bipedal robotic walking across challenging terrains with restricted footholds. Traditional one-step preview planning struggles to maintain stability when stepping areas are severely limited, such as with random stepping stones. In this work, we developed a <b>discrete-time</b> <b>Model</b> Predictive Control (MPC) based on the step-to-step <b>discrete</b> <b>evolution</b> of the Divergent Component of Motion (DCM) of bipedal locomotion. This approach adaptively changes the step duration for optimal foot placement under constraints, thereby ensuring the robot&rsquo;s operational viability over multiple future steps and significantly improving its ability to navigate through environments with tight constraints on possible footholds. The effectiveness of this planning algorithm is demonstrated through <b>simulations</b> that include a variety of complex stepping-stone configurations and external perturbations. These tests underscore the algorithm&rsquo;s improved performance for navigating foothold-restricted environments, even with the presence of external disturbances.</p></p class="citation"></blockquote><h3 id=927--249340-proprioception-is-all-you-need-terrain-classification-for-boreal-forests-damien-larocque-et-al-2024>(9/27 | 249/340) Proprioception Is All You Need: Terrain Classification for Boreal Forests (Damien LaRocque et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Damien LaRocque, William Guimont-Martin, David-Alexandre Duclos, Philippe Giguère, François Pomerleau. (2024)<br><strong>Proprioception Is All You Need: Terrain Classification for Boreal Forests</strong><br><button class=copy-to-clipboard title="Proprioception Is All You Need: Terrain Classification for Boreal Forests" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO, eess-SP<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16877v1.pdf filename=2403.16877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and the novel state space model (SSM)-based Mamba architecture on a TC task. Interestingly, we show that while <b>CNN</b> outperforms Mamba on each separate dataset, Mamba achieves greater accuracy when trained on a combination of both. In addition, we demonstrate that Mamba&rsquo;s learning capacity is greater than a <b>CNN</b> for increasing amounts of data. We show that the combination of two TC datasets yields a latent space that can be interpreted with the properties of the terrains. We also discuss the implications of merging datasets on classification. Our source code and dataset are publicly available online: <a href=https://github.com/norlab-ulaval/BorealTC>https://github.com/norlab-ulaval/BorealTC</a>.</p></p class="citation"></blockquote><h3 id=1027--250340-exploiting-priors-from-3d-diffusion-models-for-rgb-based-one-shot-view-planning-sicong-pan-et-al-2024>(10/27 | 250/340) Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning (Sicong Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz. (2024)<br><strong>Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning</strong><br><button class=copy-to-clipboard title="Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Diffusion Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16803v1.pdf filename=2403.16803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Object reconstruction is relevant for many autonomous robotic tasks that require interaction with the environment. A key challenge in such scenarios is planning view configurations to collect informative measurements for reconstructing an initially unknown object. One-shot view planning enables efficient data collection by predicting view configurations and planning the globally shortest path connecting all views at once. However, geometric priors about the object are required to conduct one-shot view planning. In this work, we propose a novel one-shot view planning approach that utilizes the powerful 3D generation capabilities of <b>diffusion</b> <b>models</b> as priors. By incorporating such geometric priors into our pipeline, we achieve effective one-shot view planning starting with only a single RGB image of the object to be reconstructed. Our planning experiments in <b>simulation</b> and real-world setups indicate that our approach balances well between object reconstruction quality and movement cost.</p></p class="citation"></blockquote><h3 id=1127--251340-skill-q-network-learning-adaptive-skill-ensemble-for-mapless-navigation-in-unknown-environments-hyunki-seong-et-al-2024>(11/27 | 251/340) Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments (Hyunki Seong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunki Seong, David Hyunchul Shim. (2024)<br><strong>Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments</strong><br><button class=copy-to-clipboard title="Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Out-of-distribution, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16664v1.pdf filename=2403.16664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on the acquisition of mapless navigation skills within unknown environments. We introduce the Skill Q-Network (SQN), a novel <b>reinforcement</b> <b>learning</b> method featuring an adaptive skill ensemble mechanism. Unlike existing methods, our model concurrently learns a high-level skill decision process alongside multiple low-level navigation skills, all without the need for prior knowledge. Leveraging a tailored reward function for mapless navigation, the SQN is capable of learning adaptive maneuvers that incorporate both exploration and goal-directed skills, enabling effective navigation in new environments. Our experiments demonstrate that our SQN can effectively navigate complex environments, exhibiting a 40% higher performance compared to baseline models. Without explicit guidance, SQN discovers how to combine low-level skill policies, showcasing both goal-directed navigations to reach destinations and exploration maneuvers to escape from local minimum regions in challenging scenarios. Remarkably, our adaptive skill ensemble method enables <b>zero-shot</b> transfer to <b>out-of-distribution</b> domains, characterized by unseen observations from non-convex obstacles or uneven, subterranean-like environments.</p></p class="citation"></blockquote><h3 id=1227--252340-bipedal-safe-navigation-over-uncertain-rough-terrain-unifying-terrain-mapping-and-locomotion-stability-kasidit-muenprasitivej-et-al-2024>(12/27 | 252/340) Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability (Kasidit Muenprasitivej et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasidit Muenprasitivej, Jesse Jiang, Abdulaziz Shamsah, Samuel Coogan, Ye Zhao. (2024)<br><strong>Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability</strong><br><button class=copy-to-clipboard title="Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16356v1.pdf filename=2403.16356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for <b>Gaussian</b> <b>process</b> (GP) regression to learn the terrain elevation. We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics. We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner. The global navigation planner plans a series of local waypoints to reach the desired goal locations while respecting locomotion stability constraints. Then, a local navigation planner is used to generate a sequence of dynamically feasible footsteps to reach local waypoints. We develop a novel trajectory evaluation metric to minimize motion deviation and maximize information gain of the terrain elevation map. We evaluate the efficacy of our planning framework on Digit bipedal robot <b>simulation</b> in MuJoCo.</p></p class="citation"></blockquote><h3 id=1327--253340-se3-linear-parameter-varying-dynamical-systems-for-globally-asymptotically-stable-end-effector-control-sunan-sun-et-al-2024>(13/27 | 253/340) SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control (Sunan Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunan Sun, Nadia Figueroa. (2024)<br><strong>SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control</strong><br><button class=copy-to-clipboard title="SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16366v1.pdf filename=2403.16366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear Parameter Varying Dynamical Systems (LPV-DS) encode trajectories into an autonomous first-order DS that enables reactive responses to perturbations, while ensuring globally asymptotic stability at the target. However, the current LPV-DS framework is established on Euclidean data only and has not been applicable to broader robotic applications requiring pose control. In this paper we present an extension to the current LPV-DS framework, named Quaternion-DS, which efficiently learns a DS-based motion policy for orientation. Leveraging techniques from differential <b>geometry</b> and Riemannian statistics, our approach properly handles the non-Euclidean orientation data in quaternion space, enabling the integration with positional control, namely SE(3) LPV-DS, so that the synergistic behaviour within the full SE(3) pose is preserved. Through <b>simulation</b> and real robot experiments, we validate our method, demonstrating its ability to efficiently and accurately reproduce the original SE(3) trajectory while exhibiting strong robustness to perturbations in task space.</p></p class="citation"></blockquote><h3 id=1427--254340-impact-aware-bimanual-catching-of-large-momentum-objects-lei-yan-et-al-2024>(14/27 | 254/340) Impact-Aware Bimanual Catching of Large-Momentum Objects (Lei Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Yan, Theodoros Stouraitis, João Moura, Wenfu Xu, Michael Gienger, Sethu Vijayakumar. (2024)<br><strong>Impact-Aware Bimanual Catching of Large-Momentum Objects</strong><br><button class=copy-to-clipboard title="Impact-Aware Bimanual Catching of Large-Momentum Objects" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17249v1.pdf filename=2403.17249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates one of the most challenging tasks in dynamic manipulation &ndash; catching large-momentum moving objects. Beyond the realm of quasi-static manipulation, dealing with highly dynamic objects can significantly improve the robot&rsquo;s capability of interacting with its surrounding environment. Yet, the inevitable motion mismatch between the fast moving object and the approaching robot will result in large impulsive forces, which lead to the unstable contacts and irreversible damage to both the object and the robot. To address the above problems, we propose an online optimization framework to: 1) estimate and predict the linear and angular motion of the object; 2) search and select the optimal contact locations across every surface of the object to mitigate impact through sequential quadratic programming (SQP); 3) simultaneously optimize the end-effector motion, stiffness, and contact force for both robots using multi-mode trajectory optimization (MMTO); and 4) realise the impact-aware catching motion on the compliant robotic system based on indirect force controller. We validate the impulse distribution, contact selection, and impact-aware MMTO algorithms in <b>simulation</b> and demonstrate the benefits of the proposed framework in real-world experiments including catching large-momentum moving objects with well-defined motion, constrained motion and free-flying motion.</p></p class="citation"></blockquote><h3 id=1527--255340-a-comparative-analysis-of-visual-odometry-in-virtual-and-real-world-railways-environments-gianluca-damico-et-al-2024>(15/27 | 255/340) A Comparative Analysis of Visual Odometry in Virtual and Real-World Railways Environments (Gianluca D&rsquo;Amico et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gianluca D&rsquo;Amico, Mauro Marinoni, Giorgio Buttazzo. (2024)<br><strong>A Comparative Analysis of Visual Odometry in Virtual and Real-World Railways Environments</strong><br><button class=copy-to-clipboard title="A Comparative Analysis of Visual Odometry in Virtual and Real-World Railways Environments" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17084v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17084v1.pdf filename=2403.17084v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perception tasks play a crucial role in the development of automated operations and systems across multiple application fields. In the railway transportation domain, these tasks can improve the safety, reliability, and efficiency of various perations, including train localization, signal recognition, and track discrimination. However, collecting considerable and precisely labeled datasets for testing such novel algorithms poses extreme challenges in the railway environment due to the severe restrictions in accessing the infrastructures and the practical difficulties associated with properly equipping trains with the required sensors, such as cameras and LiDARs. The remarkable innovations of graphic engine tools offer new solutions to craft realistic synthetic datasets. To illustrate the advantages of employing graphic <b>simulation</b> for early-stage testing of perception tasks in the railway domain, this paper presents a comparative analysis of the performance of a SLAM algorithm applied both in a virtual synthetic environment and a real-world scenario. The analysis leverages virtual railway environments created with the latest version of Unreal Engine, facilitating data collection and allowing the examination of challenging scenarios, including low-visibility, dangerous operational modes, and complex environments. The results highlight the feasibility and potentiality of graphic <b>simulation</b> to advance perception tasks in the railway domain.</p></p class="citation"></blockquote><h3 id=1627--256340-trajectory-optimization-with-global-yaw-parameterization-for-field-of-view-constrained-autonomous-flight-yuwei-wu-et-al-2024>(16/27 | 256/340) Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight (Yuwei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwei Wu, Yuezhan Tao, Igor Spasojevic, Vijay Kumar. (2024)<br><strong>Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight</strong><br><button class=copy-to-clipboard title="Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17067v1.pdf filename=2403.17067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory generation for quadrotors with limited field-of-view sensors has numerous applications such as aerial exploration, coverage, inspection, videography, and target tracking. Most previous works simplify the task of optimizing yaw trajectories by either aligning the heading of the robot with its velocity, or potentially restricting the feasible space of candidate trajectories by using a limited yaw domain to circumvent angular singularities. In this paper, we propose a novel \textit{global} yaw parameterization method for trajectory optimization that allows a 360-degree yaw variation as demanded by the underlying algorithm. This approach effectively bypasses inherent singularities by including supplementary quadratic constraints and transforming the final decision variables into the desired state representation. This method significantly reduces the needed control effort, and improves optimization feasibility. Furthermore, we apply the method to several examples of different applications that require jointly optimizing over both the yaw and position trajectories. Ultimately, we present a comprehensive numerical analysis and evaluation of our proposed method in both <b>simulation</b> and real-world experiments.</p></p class="citation"></blockquote><h3 id=1727--257340-visual-whole-body-control-for-legged-loco-manipulation-minghuan-liu-et-al-2024>(17/27 | 257/340) Visual Whole-Body Control for Legged Loco-Manipulation (Minghuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang. (2024)<br><strong>Visual Whole-Body Control for Legged Loco-Manipulation</strong><br><button class=copy-to-clipboard title="Visual Whole-Body Control for Legged Loco-Manipulation" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16967v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16967v2.pdf filename=2403.16967v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely Visual Whole-Body Control(VBC), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in <b>simulation</b> and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights, locations, orientations) and environments. Project page: <a href=https://wholebody-b1.github.io>https://wholebody-b1.github.io</a></p></p class="citation"></blockquote><h3 id=1827--258340-arm-constrained-curriculum-learning-for-loco-manipulation-of-the-wheel-legged-robot-zifan-wang-et-al-2024>(18/27 | 258/340) Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-Legged Robot (Zifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zifan Wang, Yufei Jia, Lu Shi, Haoyu Wang, Haizhou Zhao, Xueyang Li, Jinni Zhou, Jun Ma, Guyue Zhou. (2024)<br><strong>Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-Legged Robot</strong><br><button class=copy-to-clipboard title="Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-Legged Robot" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Curriculum Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16535v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16535v2.pdf filename=2403.16535v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating a robotic manipulator into a wheel-legged robot enhances its agility and expands its potential for practical applications. However, the presence of potential instability and uncertainties presents additional challenges for control objectives. In this paper, we introduce an arm-constrained <b>curriculum</b> <b>learning</b> architecture to tackle the issues introduced by adding the manipulator. Firstly, we develop an arm-constrained <b>reinforcement</b> <b>learning</b> algorithm to ensure safety and stability in control performance. Additionally, to address discrepancies in reward settings between the arm and the base, we propose a reward-aware <b>curriculum</b> <b>learning</b> method. The policy is first trained in Isaac gym and transferred to the physical robot to do dynamic grasping tasks, including the door-opening task, fan-twitching task and the relay-baton-picking and following task. The results demonstrate that our proposed approach effectively controls the arm-equipped wheel-legged robot to master dynamic grasping skills, allowing it to chase and catch a moving object while in motion. Please refer to our website (<a href=https://acodedog.github.io/wheel-legged-loco-manipulation>https://acodedog.github.io/wheel-legged-loco-manipulation</a>) for the code and supplemental videos.</p></p class="citation"></blockquote><h3 id=1927--259340-real-time-model-predictive-control-with-zonotope-based-neural-networks-for-bipedal-social-navigation-abdulaziz-shamsah-et-al-2024>(19/27 | 259/340) Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation (Abdulaziz Shamsah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdulaziz Shamsah, Krishanu Agarwal, Shreyas Kousik, Ye Zhao. (2024)<br><strong>Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation</strong><br><button class=copy-to-clipboard title="Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16485v1.pdf filename=2403.16485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians&rsquo; future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order <b>simulation</b> of Digit. The overall proposed framework is validated with extensive <b>simulations</b> on randomly generated initial settings with varying human crowd densities.</p></p class="citation"></blockquote><h3 id=2027--260340-learning-symbolic-and-subsymbolic-temporal-task-constraints-from-bimanual-human-demonstrations-christian-dreher-et-al-2024>(20/27 | 260/340) Learning Symbolic and Subsymbolic Temporal Task Constraints from Bimanual Human Demonstrations (Christian Dreher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Dreher, Tamim Asfour. (2024)<br><strong>Learning Symbolic and Subsymbolic Temporal Task Constraints from Bimanual Human Demonstrations</strong><br><button class=copy-to-clipboard title="Learning Symbolic and Subsymbolic Temporal Task Constraints from Bimanual Human Demonstrations" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16953v1.pdf filename=2403.16953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning task models of bimanual manipulation from human demonstration and their execution on a robot should take temporal constraints between actions into account. This includes constraints on (i) the symbolic level such as precedence relations or temporal overlap in the execution, and (ii) the subsymbolic level such as the duration of different actions, or their starting and end points in time. Such temporal constraints are crucial for temporal planning, <b>reasoning,</b> and the exact timing for the execution of bimanual actions on a bimanual robot. In our previous work, we addressed the learning of temporal task constraints on the symbolic level and demonstrated how a robot can leverage this knowledge to respond to failures during execution. In this work, we propose a novel model-driven approach for the combined learning of symbolic and subsymbolic temporal task constraints from multiple bimanual human demonstrations. Our main contributions are a subsymbolic foundation of a temporal task model that describes temporal nexuses of actions in the task based on distributions of temporal differences between semantic action keypoints, as well as a method based on fuzzy logic to derive symbolic temporal task constraints from this representation. This complements our previous work on learning comprehensive temporal task models by integrating symbolic and subsymbolic information based on a subsymbolic foundation, while still maintaining the symbolic expressiveness of our previous approach. We compare our proposed approach with our previous pure-symbolic approach and show that we can reproduce and even outperform it. Additionally, we show how the subsymbolic temporal task constraints can synchronize otherwise unimanual movement primitives for bimanual behavior on a humanoid robot.</p></p class="citation"></blockquote><h3 id=2127--261340-synapse-learning-preferential-concepts-from-visual-demonstrations-sadanand-modak-et-al-2024>(21/27 | 261/340) Synapse: Learning Preferential Concepts from Visual Demonstrations (Sadanand Modak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sadanand Modak, Noah Patton, Isil Dillig, Joydeep Biswas. (2024)<br><strong>Synapse: Learning Preferential Concepts from Visual Demonstrations</strong><br><button class=copy-to-clipboard title="Synapse: Learning Preferential Concepts from Visual Demonstrations" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-LG, cs-PL, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16689v1.pdf filename=2403.16689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., &ldquo;good parking spot&rdquo;, &ldquo;convenient drop-off location&rdquo;) from visual input. Despite its similarity to learning factual concepts (e.g., &ldquo;red cube&rdquo;), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, <b>large</b> <b>language</b> <b>models,</b> and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related concepts in mobile robotics and autonomous driving. Our evaluation demonstrates that Synapse significantly outperforms existing baselines as well as its own ablations. The code and other details can be found on the project website <a href=https://amrl.cs.utexas.edu/synapse>https://amrl.cs.utexas.edu/synapse</a> .</p></p class="citation"></blockquote><h3 id=2227--262340-trajectory-planning-of-robotic-manipulator-in-dynamic-environment-exploiting-drl-osama-ahmad-et-al-2024>(22/27 | 262/340) Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL (Osama Ahmad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Osama Ahmad, Zawar Hussain, Hammad Naeem. (2024)<br><strong>Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL</strong><br><button class=copy-to-clipboard title="Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16652v1.pdf filename=2403.16652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study is about the implementation of a <b>reinforcement</b> <b>learning</b> algorithm in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick and place the randomly placed block at a random target point in an unknown environment. The obstacle is randomly moving which creates a hurdle in picking the object. The objective of the robot is to avoid the obstacle and pick the block with constraints to a fixed timestamp. In this literature, we have applied a deep deterministic policy gradient (DDPG) algorithm and compared the model&rsquo;s efficiency with dense and sparse rewards.</p></p class="citation"></blockquote><h3 id=2327--263340-spatially-temporally-distributed-informative-path-planning-for-multi-robot-systems-binh-nguyen-et-al-2024>(23/27 | 263/340) Spatially temporally distributed informative path planning for multi-robot systems (Binh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen. (2024)<br><strong>Spatially temporally distributed informative path planning for multi-robot systems</strong><br><button class=copy-to-clipboard title="Spatially temporally distributed informative path planning for multi-robot systems" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16489v1.pdf filename=2403.16489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the problem of informative path planning for a mobile robotic sensor network in spatially temporally distributed mapping. The robots are able to gather noisy measurements from an area of interest during their movements to build a <b>Gaussian</b> <b>Process</b> (GP) model of a spatio-temporal field. The model is then utilized to predict the spatio-temporal phenomenon at different points of interest. To spatially and temporally navigate the group of robots so that they can optimally acquire maximal information gains while their connectivity is preserved, we propose a novel multistep prediction informative path planning optimization strategy employing our newly defined local cost functions. By using the dual decomposition method, it is feasible and practical to effectively solve the optimization problem in a distributed manner. The proposed method was validated through synthetic experiments utilizing real-world data sets.</p></p class="citation"></blockquote><h3 id=2427--264340-human-stress-response-and-perceived-safety-during-encounters-with-quadruped-robots-ryan-gupta-et-al-2024>(24/27 | 264/340) Human Stress Response and Perceived Safety during Encounters with Quadruped Robots (Ryan Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Gupta, Hyonyoung Shin, Emily Norman, Keri K. Stephens, Nanshu Lu, Luis Sentis. (2024)<br><strong>Human Stress Response and Perceived Safety during Encounters with Quadruped Robots</strong><br><button class=copy-to-clipboard title="Human Stress Response and Perceived Safety during Encounters with Quadruped Robots" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17270v1.pdf filename=2403.17270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature. To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response. Stress is a key component of perceived safety and is strongly associated with human physiological response. In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing <b>multimodal</b> physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA). The encounters are varied through several trials and participants self-rate their stress levels after each encounter. The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters. To this end, acute stress responses were decoded from the human participants&rsquo; ECG and EDA and compared across different human-robot encounter conditions. Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior.</p></p class="citation"></blockquote><h3 id=2527--265340-tail-a-terrain-aware-multi-modal-slam-dataset-for-robot-locomotion-in-deformable-granular-environments-chen-yao-et-al-2024>(25/27 | 265/340) TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments (Chen Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Yao, Yangtao Ge, Guowei Shi, Zirui Wang, Ningbo Yang, Zheng Zhu, Hexiang Wei, Yuntian Zhao, Jing Wu, Zhenzhong Jia. (2024)<br><strong>TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments</strong><br><button class=copy-to-clipboard title="TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16875v1.pdf filename=2403.16875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Terrain-aware perception holds the potential to improve the robustness and accuracy of autonomous robot navigation in the wilds, thereby facilitating effective off-road traversals. However, the lack of <b>multi-modal</b> perception across various motion patterns hinders the solutions of Simultaneous Localization And Mapping (SLAM), especially when confronting non-geometric hazards in demanding landscapes. In this paper, we first propose a Terrain-Aware <b>multI-modaL</b> (TAIL) dataset tailored to deformable and sandy terrains. It incorporates various types of robotic proprioception and distinct ground interactions for the unique challenges and <b>benchmark</b> of multi-sensor fusion SLAM. The versatile sensor suite comprises stereo frame cameras, multiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK device. This ensemble is hardware-synchronized, well-calibrated, and self-contained. Utilizing both wheeled and quadrupedal locomotion, we efficiently collect comprehensive sequences to capture rich unstructured scenarios. It spans the spectrum of scope, terrain interactions, scene changes, ground-level properties, and dynamic robot characteristics. We <b>benchmark</b> several state-of-the-art SLAM methods against ground truth and provide performance validations. Corresponding challenges and limitations are also reported. All associated resources are accessible upon request at \url{https://tailrobot.github.io/}.</p></p class="citation"></blockquote><h3 id=2627--266340-hearing-the-shape-of-an-arena-with-spectral-swarm-robotics-leo-cazenille-et-al-2024>(26/27 | 266/340) Hearing the shape of an arena with spectral swarm robotics (Leo Cazenille et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leo Cazenille, Nicolas Lobato-Dauzier, Alessia Loi, Mika Ito, Olivier Marchal, Nathanael Aubert-Kato, Nicolas Bredeche, Anthony J. Genot. (2024)<br><strong>Hearing the shape of an arena with spectral swarm robotics</strong><br><button class=copy-to-clipboard title="Hearing the shape of an arena with spectral swarm robotics" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CG, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17147v1.pdf filename=2403.17147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Swarm robotics promises adaptability to unknown situations and robustness against failures. However, it still struggles with global tasks that require understanding the broader context in which the robots operate, such as identifying the shape of the arena in which the robots are embedded. Biological swarms, such as shoals of fish, flocks of birds, and colonies of insects, routinely solve global geometrical problems through the diffusion of local cues. This paradigm can be explicitly described by mathematical models that could be directly computed and exploited by a robotic swarm. Diffusion over a domain is mathematically encapsulated by the Laplacian, a linear operator that measures the local curvature of a function. Crucially the <b>geometry</b> of a domain can generally be reconstructed from the eigenspectrum of its Laplacian. Here we introduce spectral swarm robotics where robots diffuse information to their neighbors to emulate the Laplacian operator - enabling them to &ldquo;hear&rdquo; the spectrum of their arena. We reveal a universal scaling that links the optimal number of robots (a global parameter) with their optimal radius of interaction (a local parameter). We validate experimentally spectral swarm robotics under challenging conditions with the one-shot classification of arena shapes using a sparse swarm of Kilobots. Spectral methods can assist with challenging tasks where robots need to build an emergent consensus on their environment, such as adaptation to unknown terrains, division of labor, or quorum sensing. Spectral methods may extend beyond robotics to analyze and coordinate swarms of agents of various natures, such as traffic or crowds, and to better understand the long-range dynamics of natural systems emerging from short-range interactions.</p></p class="citation"></blockquote><h3 id=2727--267340-dhp-mapping-a-dense-panoptic-mapping-system-with-hierarchical-world-representation-and-label-optimization-techniques-tianshuai-hu-et-al-2024>(27/27 | 267/340) DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques (Tianshuai Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianshuai Hu, Jianhao Jiao, Yucheng Xu, Hongji Liu, Sheng Wang, Ming Liu. (2024)<br><strong>DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques</strong><br><button class=copy-to-clipboard title="DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16880v1.pdf filename=2403.16880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Maps provide robots with crucial environmental knowledge, thereby enabling them to perform interactive tasks effectively. Easily accessing accurate abstract-to-detailed geometric and semantic concepts from maps is crucial for robots to make informed and efficient decisions. To comprehensively model the environment and effectively manage the map data structure, we propose DHP-Mapping, a dense mapping system that utilizes multiple Truncated Signed Distance Field (TSDF) submaps and panoptic labels to hierarchically model the environment. The output map is able to maintain both voxel- and submap-level metric and semantic information. Two modules are presented to enhance the mapping efficiency and label consistency: (1) an inter-submaps label fusion strategy to eliminate duplicate points across submaps and (2) a conditional random field (CRF) based approach to enhance panoptic labels through object label comprehension and contextual information. We conducted experiments with two public datasets including indoor and outdoor scenarios. Our system performs comparably to state-of-the-art (SOTA) methods across <b>geometry</b> and label accuracy evaluation metrics. The experiment results highlight the effectiveness and scalability of our system, as it is capable of constructing precise <b>geometry</b> and maintaining consistent panoptic labels. Our code is publicly available at <a href=https://github.com/hutslib/DHP-Mapping>https://github.com/hutslib/DHP-Mapping</a>.</p></p class="citation"></blockquote><h2 id=csir-4>cs.IR (4)</h2><h3 id=14--268340-reinforcement-learning-based-recommender-systems-with-large-language-models-for-state-reward-and-action-modeling-jie-wang-et-al-2024>(1/4 | 268/340) Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling (Jie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose. (2024)<br><strong>Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling</strong><br><button class=copy-to-clipboard title="Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Recommendation, Recommender System, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16948v1.pdf filename=2403.16948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL)-based <b>recommender</b> <b>systems</b> have demonstrated promising performance in meeting user expectations by learning to make accurate next-item <b>recommendations</b> from historical user-item interactions. However, existing offline RL-based sequential <b>recommendation</b> methods face the challenge of obtaining effective user feedback from the environment. Effectively modeling the user state and shaping an appropriate reward for <b>recommendation</b> remains a challenge. In this paper, we leverage language understanding capabilities and adapt <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as an environment (LE) to enhance RL-based <b>recommenders.</b> <b>The</b> LE is learned from a subset of user-item interaction data, thus reducing the need for <b>large</b> <b>training</b> <b>data,</b> and can synthesise user feedback for offline data by: (i) acting as a state model that produces high quality states that enrich the user representation, and (ii) functioning as a reward model to accurately capture nuanced user preferences on actions. Moreover, the LE allows to generate positive actions that augment the limited offline training data. We propose a LE Augmentation (LEA) method to further improve <b>recommendation</b> performance by optimising jointly the <b>supervised</b> component and the RL policy, using the augmented actions and historical user signals. We use LEA, the state and reward models in conjunction with state-of-the-art RL <b>recommenders</b> <b>and</b> report experimental results on two publicly available datasets.</p></p class="citation"></blockquote><h3 id=24--269340-coarse-tuning-for-ad-hoc-document-retrieval-using-pre-trained-language-models-atsushi-keyaki-et-al-2024>(2/4 | 269/340) Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models (Atsushi Keyaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atsushi Keyaki, Ribeka Keyaki. (2024)<br><strong>Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Information Retrieval, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16915v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16915v3.pdf filename=2403.16915v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> in <b>information</b> <b>retrieval</b> systems using <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLM-based</b> IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and <b>fine-tuning.</b> By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of <b>fine-tuning</b> and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.</p></p class="citation"></blockquote><h3 id=34--270340-play-to-your-strengths-collaborative-intelligence-of-conventional-recommender-models-and-large-language-models-yunjia-xi-et-al-2024>(3/4 | 270/340) Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models (Yunjia Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunjia Xi, Weiwen Liu, Jianghao Lin, Chuhan Wu, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu. (2024)<br><strong>Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models</strong><br><button class=copy-to-clipboard title="Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Recommender System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16378v1.pdf filename=2403.16378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has opened new opportunities in <b>Recommender</b> <b>Systems</b> (RSs) by enhancing user behavior modeling and content understanding. However, current approaches that integrate <b>LLMs</b> into RSs solely utilize either <b>LLM</b> or conventional <b>recommender</b> <b>model</b> (CRM) to generate final <b>recommendations,</b> without considering which data segments <b>LLM</b> or CRM excel in. To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books datasets, and compare the performance of a representative CRM (DCNv2) and an <b>LLM</b> (LLaMA2-7B) on various groups of data samples. Our findings reveal that <b>LLMs</b> excel in data segments where CRMs exhibit lower confidence and precision, while samples where CRM excels are relatively challenging for <b>LLM,</b> requiring substantial training data and a long training time for comparable performance. This suggests potential synergies in the combination between <b>LLM</b> and CRM. Motivated by these insights, we propose Collaborative <b>Recommendation</b> with conventional <b>Recommender</b> <b>and</b> <b>Large</b> <b>Language</b> <b>Model</b> (dubbed \textit{CoReLLa}). In this framework, we first jointly train <b>LLM</b> and CRM and address the issue of decision boundary shifts through alignment loss. Then, the resource-efficient CRM, with a shorter inference time, handles simple and moderate samples, while <b>LLM</b> processes the small subset of challenging samples for CRM. Our experimental results demonstrate that CoReLLa outperforms state-of-the-art CRM and <b>LLM</b> methods significantly, underscoring its effectiveness in <b>recommendation</b> tasks.</p></p class="citation"></blockquote><h3 id=44--271340-uncovering-selective-state-space-models-capabilities-in-lifelong-sequential-recommendation-jiyuan-yang-et-al-2024>(4/4 | 271/340) Uncovering Selective State Space Model&rsquo;s Capabilities in Lifelong Sequential Recommendation (Jiyuan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyuan Yang, Yuanzi Li, Jingyu Zhao, Hanbing Wang, Muyang Ma, Jun Ma, Zhaochun Ren, Mengqi Zhang, Xin Xin, Zhumin Chen, Pengjie Ren. (2024)<br><strong>Uncovering Selective State Space Model&rsquo;s Capabilities in Lifelong Sequential Recommendation</strong><br><button class=copy-to-clipboard title="Uncovering Selective State Space Model's Capabilities in Lifelong Sequential Recommendation" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16371v1.pdf filename=2403.16371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sequential Recommenders have been widely applied in various online services, aiming to model users&rsquo; dynamic interests from their sequential interactions. With users increasingly engaging with online platforms, vast amounts of lifelong user behavioral sequences have been generated. However, existing sequential recommender models often struggle to handle such lifelong sequences. The primary challenges stem from computational complexity and the ability to capture long-range dependencies within the sequence. Recently, a state space model featuring a selective mechanism (i.e., Mamba) has emerged. In this work, we investigate the performance of Mamba for lifelong sequential <b>recommendation</b> (i.e., length>=2k). More specifically, we leverage the Mamba block to model lifelong user sequences selectively. We conduct extensive experiments to evaluate the performance of representative sequential <b>recommendation</b> models in the setting of lifelong sequences. Experiments on two real-world datasets demonstrate the superiority of Mamba. We found that RecMamba achieves performance comparable to the representative model while significantly reducing training duration by approximately 70% and memory costs by 80%. Codes and data are available at \url{https://github.com/nancheng58/RecMamba}.</p></p class="citation"></blockquote><h2 id=eessiv-13>eess.IV (13)</h2><h3 id=113--272340-3d-effivitcaps-3d-efficient-vision-transformer-with-capsule-for-medical-image-segmentation-dongwei-gan-et-al-2024>(1/13 | 272/340) 3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation (Dongwei Gan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongwei Gan, Ming Chang, Juan Chen. (2024)<br><strong>3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16350v1.pdf filename=2403.16350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation (MIS) aims to finely segment various organs. It requires grasping global information from both parts and the entire image for better segmenting, and clinically there are often certain requirements for segmentation efficiency. <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have made considerable achievements in MIS. However, they are difficult to fully collect global context information and their pooling layer may cause information loss. Capsule networks, which combine the benefits of <b>CNNs</b> while taking into account additional information such as relative location that <b>CNNs</b> do not, have lately demonstrated some advantages in MIS. <b>Vision</b> <b>Transformer</b> (ViT) employs <b>transformers</b> in visual tasks. <b>Transformer</b> based on attention mechanism has excellent global inductive modeling capabilities and is expected to capture longrange information. Moreover, there have been resent studies on making ViT more lightweight to minimize model complexity and increase efficiency. In this paper, we propose a U-shaped 3D encoder-decoder network named 3D-EffiViTCaps, which combines 3D capsule blocks with 3D EfficientViT blocks for MIS. Our encoder uses capsule blocks and EfficientViT blocks to jointly capture local and global semantic information more effectively and efficiently with less information loss, while the decoder employs <b>CNN</b> blocks and EfficientViT blocks to catch ffner details for segmentation. We conduct experiments on various datasets, including iSeg-2017, Hippocampus and Cardiac to verify the performance and efficiency of 3D-EffiViTCaps, which performs better than previous 3D <b>CNN-based,</b> 3D Capsule-based and 3D <b>Transformer-based</b> models. We further implement a series of ablation experiments on the main blocks. Our code is available at: <a href=https://github.com/HidNeuron/3D-EffiViTCaps>https://github.com/HidNeuron/3D-EffiViTCaps</a>.</p></p class="citation"></blockquote><h3 id=213--273340-brain-stroke-segmentation-using-deep-learning-models-a-comparative-study-ahmed-soliman-et-al-2024>(2/13 | 273/340) Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study (Ahmed Soliman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed Soliman, Yousif Yousif, Ahmed Ibrahim, Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok. (2024)<br><strong>Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study</strong><br><button class=copy-to-clipboard title="Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17177v1.pdf filename=2403.17177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the <b>vision</b> <b>Transformers,</b> several models have been introduced based on them, while others have aimed to design better modules based on traditional <b>convolutional</b> layers to extract long-range dependencies like <b>Transformers.</b> The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep models that were recently proposed and evaluated their performance for stroke segmentation: a pure <b>Transformer-based</b> architecture (DAE-Former), two advanced <b>CNN-based</b> models (LKA and DLKA) with attention mechanisms in their design, an advanced hybrid model that incorporates <b>CNNs</b> with <b>Transformers</b> (FCT), and the well-known self-adaptive nnUNet framework with its configuration based on given data. We examined their performance on two publicly available datasets, and found that the nnUNet achieved the best results with the simplest design among all. Revealing the robustness issue of <b>Transformers</b> to such variabilities serves as a potential reason for their weaker performance. Furthermore, nnUNet&rsquo;s success underscores the significant impact of preprocessing and postprocessing techniques in enhancing segmentation results, surpassing the focus solely on architectural designs</p></p class="citation"></blockquote><h3 id=313--274340-self-storm-deep-unrolled-self-supervised-learning-for-super-resolution-microscopy-yair-ben-sahel-et-al-2024>(3/13 | 274/340) Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy (Yair Ben Sahel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yair Ben Sahel, Yonina C. Eldar. (2024)<br><strong>Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy</strong><br><button class=copy-to-clipboard title="Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16974v1.pdf filename=2403.16974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of fluorescent molecules to create long sequences of low-density, diffraction-limited images enables highly-precise molecule localization. However, this methodology requires lengthy imaging times, which limits the ability to view dynamic interactions of live cells on short time scales. Many techniques have been developed to reduce the number of frames needed for localization, from classic iterative optimization to deep neural networks. Particularly, deep algorithm unrolling utilizes both the structure of iterative sparse recovery algorithms and the performance gains of <b>supervised</b> deep learning. However, the robustness of this approach is highly dependant on having sufficient training data. In this paper we introduce deep unrolled <b>self-supervised</b> <b>learning,</b> which alleviates the need for such data by training a sequence-specific, model-based <b>autoencoder</b> that learns only from given measurements. Our proposed method exceeds the performance of its <b>supervised</b> counterparts, thus allowing for robust, dynamic imaging well below the diffraction limit without any labeled training samples. Furthermore, the suggested model-based <b>autoencoder</b> scheme can be utilized to enhance generalization in any sparse recovery framework, without the need for external training data.</p></p class="citation"></blockquote><h3 id=413--275340-multi-scale-texture-loss-for-ct-denoising-with-gans-francesco-di-feola-et-al-2024>(4/13 | 275/340) Multi-Scale Texture Loss for CT denoising with GANs (Francesco Di Feola et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Di Feola, Lorenzo Tronchin, Valerio Guarrasi, Paolo Soda. (2024)<br><strong>Multi-Scale Texture Loss for CT denoising with GANs</strong><br><button class=copy-to-clipboard title="Multi-Scale Texture Loss for CT denoising with GANs" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16640v1.pdf filename=2403.16640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> have proved as a powerful framework for denoising applications in medical imaging. However, <b>GAN-based</b> denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into <b>GANs&rsquo;</b> training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimization. Our approach also introduces a <b>self-attention</b> layer that dynamically aggregates the multi-scale texture information extracted from the images. We validate our approach by carrying out extensive experiments in the context of low-dose CT denoising, a challenging application that aims to enhance the quality of noisy CT scans. We utilize three publicly available datasets, including one simulated and two real datasets. The results are promising as compared to other well-established loss functions, being also consistent across three different <b>GAN</b> architectures. The code is available at: <a href=https://github.com/FrancescoDiFeola/DenoTextureLoss>https://github.com/FrancescoDiFeola/DenoTextureLoss</a></p></p class="citation"></blockquote><h3 id=513--276340-deepgleason-a-system-for-automated-gleason-grading-of-prostate-cancer-using-deep-neural-networks-dominik-müller-et-al-2024>(5/13 | 276/340) DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks (Dominik Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Müller, Philip Meyer, Lukas Rentschler, Robin Manz, Jonas Bäcker, Samantha Cramer, Christoph Wengenmayr, Bruno Märkl, Ralf Huss, Iñaki Soto-Rey, Johannes Raffler. (2024)<br><strong>DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks</strong><br><button class=copy-to-clipboard title="DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV, q-bio-TO<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16678v1.pdf filename=2403.16678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in digital pathology and artificial intelligence (AI) offer promising opportunities for clinical decision support and enhancing diagnostic workflows. Previous studies already demonstrated AI&rsquo;s potential for automated Gleason grading, but lack state-of-the-art methodology and model reusability. To address this issue, we propose DeepGleason: an open-source deep neural network based image classification system for automated Gleason grading using whole-slide histopathology images from prostate tissue sections. Implemented with the standardized AUCMEDI framework, our tool employs a tile-wise classification approach utilizing <b>fine-tuned</b> image preprocessing techniques in combination with a ConvNeXt architecture which was compared to various state-of-the-art architectures. The neural network model was trained and validated on an in-house dataset of 34,264 annotated tiles from 369 prostate carcinoma slides. We demonstrated that DeepGleason is capable of highly accurate and reliable Gleason grading with a macro-averaged F1-score of 0.806, AUC of 0.991, and Accuracy of 0.974. The internal architecture comparison revealed that the ConvNeXt model was superior performance-wise on our dataset to established and other modern architectures like <b>transformers.</b> Furthermore, we were able to outperform the current state-of-the-art in tile-wise fine-classification with a sensitivity and specificity of 0.94 and 0.98 for benign vs malignant detection as well as of 0.91 and 0.75 for Gleason 3 vs Gleason 4 & 5 classification, respectively. Our tool contributes to the wider adoption of AI-based Gleason grading within the research community and paves the way for broader clinical application of deep learning models in digital pathology. DeepGleason is open-source and publicly available for research application in the following Git repository: <a href=https://github.com/frankkramer-lab/DeepGleason>https://github.com/frankkramer-lab/DeepGleason</a>.</p></p class="citation"></blockquote><h3 id=613--277340-meddap-medical-dataset-enhancement-via-diversified-augmentation-pipeline-yasamin-medghalchi-et-al-2024>(6/13 | 277/340) MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline (Yasamin Medghalchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker Hacihaliloglu. (2024)<br><strong>MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline</strong><br><button class=copy-to-clipboard title="MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16335v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16335v2.pdf filename=2403.16335v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The effectiveness of Deep Neural Networks (DNNs) heavily relies on the abundance and accuracy of available training data. However, collecting and annotating data on a large scale is often both costly and time-intensive, particularly in medical cases where practitioners are already occupied with their duties. Moreover, ensuring that the model remains robust across various scenarios of image capture is crucial in medical domains, especially when dealing with ultrasound images that vary based on the settings of different devices and the manual operation of the transducer. To address this challenge, we introduce a novel pipeline called MEDDAP, which leverages Stable Diffusion (SD) models to augment existing small datasets by automatically generating new informative labeled samples. Pretrained checkpoints for SD are typically based on natural images, and training them for medical images requires significant GPU resources due to their heavy parameters. To overcome this challenge, we introduce USLoRA (Ultrasound Low-Rank Adaptation), a novel <b>fine-tuning</b> method tailored specifically for ultrasound applications. USLoRA allows for selective <b>fine-tuning</b> of weights within SD, requiring fewer than 0.1% of parameters compared to fully <b>fine-tuning</b> only the UNet portion of SD. To enhance dataset diversity, we incorporate different adjectives into the generation process <b>prompts,</b> thereby desensitizing the classifiers to intensity changes across different images. This approach is inspired by clinicians&rsquo; decision-making processes regarding breast tumors, where tumor shape often plays a more crucial role than intensity. In conclusion, our pipeline not only outperforms classifiers trained on the original dataset but also demonstrates superior performance when encountering unseen datasets. The source code is available at <a href=https://github.com/yasamin-med/MEDDAP>https://github.com/yasamin-med/MEDDAP</a>.</p></p class="citation"></blockquote><h3 id=713--278340-decoding-the-visual-attention-of-pathologists-to-reveal-their-level-of-expertise-souradeep-chakraborty-et-al-2024>(7/13 | 278/340) Decoding the visual attention of pathologists to reveal their level of expertise (Souradeep Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souradeep Chakraborty, Dana Perez, Paul Friedman, Natallia Sheuka, Constantin Friedman, Oksana Yaskiv, Rajarsi Gupta, Gregory J. Zelinsky, Joel H. Saltz, Dimitris Samaras. (2024)<br><strong>Decoding the visual attention of pathologists to reveal their level of expertise</strong><br><button class=copy-to-clipboard title="Decoding the visual attention of pathologists to reveal their level of expertise" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17255v1.pdf filename=2403.17255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a method for classifying the expertise of a pathologist based on how they allocated their attention during a cancer reading. We engage this decoding task by developing a novel method for predicting the attention of pathologists as they read whole-slide Images (WSIs) of prostate and make cancer grade classifications. Our ground truth measure of a pathologists&rsquo; attention is the x, y and z (magnification) movement of their viewport as they navigated through WSIs during readings, and to date we have the attention behavior of 43 pathologists reading 123 WSIs. These data revealed that specialists have higher agreement in both their attention and cancer grades compared to general pathologists and residents, suggesting that sufficient information may exist in their attention behavior to classify their expertise level. To attempt this, we trained a <b>transformer-based</b> model to predict the visual attention heatmaps of resident, general, and specialist (GU) pathologists during Gleason grading. Based solely on a pathologist&rsquo;s attention during a reading, our model was able to predict their level of expertise with 75.3%, 56.1%, and 77.2% accuracy, respectively, better than chance and baseline models. Our model therefore enables a pathologist&rsquo;s expertise level to be easily and objectively evaluated, important for pathology training and competency assessment. Tools developed from our model could also be used to help pathology trainees learn how to read WSIs like an expert.</p></p class="citation"></blockquote><h3 id=813--279340-a-study-in-dataset-pruning-for-image-super-resolution-brian-b-moser-et-al-2024>(8/13 | 279/340) A Study in Dataset Pruning for Image Super-Resolution (Brian B. Moser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brian B. Moser, Federico Raue, Andreas Dengel. (2024)<br><strong>A Study in Dataset Pruning for Image Super-Resolution</strong><br><button class=copy-to-clipboard title="A Study in Dataset Pruning for Image Super-Resolution" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17083v1.pdf filename=2403.17083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset <b>pruning</b> as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new perspectives to the untapped potential of dataset <b>pruning</b> in image SR. It suggests that careful selection of training data based on loss-value metrics can lead to better SR models, challenging the conventional wisdom that more data inevitably leads to better performance.</p></p class="citation"></blockquote><h3 id=913--280340-joint-chest-x-ray-diagnosis-and-clinical-visual-attention-prediction-with-multi-stage-cooperative-learning-enhancing-interpretability-zirui-qiu-et-al-2024>(9/13 | 280/340) Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability (Zirui Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Qiu, Hassan Rivaz, Yiming Xiao. (2024)<br><strong>Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability</strong><br><button class=copy-to-clipboard title="Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16970v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16970v2.pdf filename=2403.16970v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks in multi-task learning, we proposed a multi-stage cooperative learning strategy, with <b>contrastive</b> <b>learning</b> for feature encoder pretraining to boost performance. Experiments show that our proposed method outperformed existing techniques for chest X-ray diagnosis and the quality of visual saliency map prediction.</p></p class="citation"></blockquote><h3 id=1013--281340-provably-robust-score-based-diffusion-posterior-sampling-for-plug-and-play-image-reconstruction-xingyu-xu-et-al-2024>(10/13 | 281/340) Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction (Xingyu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Xu, Yuejie Chi. (2024)<br><strong>Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction</strong><br><button class=copy-to-clipboard title="Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess-SP, eess.IV, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17042v1.pdf filename=2403.17042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality. Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space. Score-based <b>diffusion</b> <b>models,</b> due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models. This work develops an algorithmic framework for employing score-based <b>diffusion</b> <b>models</b> as an expressive data prior in general nonlinear inverse problems. Motivated by the plug-and-play framework in the imaging community, we introduce a <b>diffusion</b> <b>plug-and-play</b> method (\textsf{DPnP}) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising <b>diffusion</b> <b>sampler</b> based solely on the score functions of the image prior. The key insight is that denoising under white Gaussian noise can be solved {\em rigorously} via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the unconditional score functions. We establish both asymptotic and non-asymptotic performance guarantees of \textsf{DPnP}, and provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks. To the best of our knowledge, \textsf{DPnP} is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional <b>diffusion</b> <b>priors.</b></p></p class="citation"></blockquote><h3 id=1113--282340-diff-def-diffusion-generated-deformation-fields-for-conditional-atlases-sophie-starck-et-al-2024>(11/13 | 282/340) Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases (Sophie Starck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Starck, Vasiliki Sideri-Lampretsa, Bernhard Kainz, Martin Menten, Tamara Mueller, Daniel Rueckert. (2024)<br><strong>Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases</strong><br><button class=copy-to-clipboard title="Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16776v1.pdf filename=2403.16776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent <b>diffusion</b> <b>models</b> to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art atlas generation methods in experiments using 5000 brain as well as whole-body MR images from UK Biobank. Our method generates highly realistic atlases with smooth transformations and high anatomical fidelity, outperforming the baselines.</p></p class="citation"></blockquote><h3 id=1213--283340-residual-dense-swin-transformer-for-continuous-depth-independent-ultrasound-imaging-jintong-hu-et-al-2024>(12/13 | 283/340) Residual Dense Swin Transformer for Continuous Depth-Independent Ultrasound Imaging (Jintong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jintong Hu, Hui Che, Zishuo Li, Wenming Yang. (2024)<br><strong>Residual Dense Swin Transformer for Continuous Depth-Independent Ultrasound Imaging</strong><br><button class=copy-to-clipboard title="Residual Dense Swin Transformer for Continuous Depth-Independent Ultrasound Imaging" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16384v1.pdf filename=2403.16384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ultrasound imaging is crucial for evaluating organ morphology and function, yet depth adjustment can degrade image quality and field-of-view, presenting a depth-dependent dilemma. Traditional interpolation-based zoom-in techniques often sacrifice detail and introduce artifacts. Motivated by the potential of arbitrary-scale super-resolution to naturally address these inherent challenges, we present the Residual Dense Swin <b>Transformer</b> Network (RDSTN), designed to capture the non-local characteristics and long-range dependencies intrinsic to ultrasound images. It comprises a linear embedding module for feature enhancement, an encoder with shifted-window attention for modeling non-locality, and an MLP decoder for continuous detail reconstruction. This strategy streamlines balancing image quality and field-of-view, which offers superior textures over traditional methods. Experimentally, RDSTN outperforms existing approaches while requiring fewer parameters. In conclusion, RDSTN shows promising potential for ultrasound image enhancement by overcoming the limitations of conventional interpolation-based methods and achieving depth-independent imaging.</p></p class="citation"></blockquote><h3 id=1313--284340-rstar-rotational-streak-artifact-reduction-in-4d-cbct-using-separable-and-circular-convolutions-ziheng-deng-et-al-2024>(13/13 | 284/340) RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable and Circular Convolutions (Ziheng Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziheng Deng, Hua Chen, Haibo Hu, Zhiyong Xu, Tianling Lyu, Yan Xi, Yang Chen, Jun Zhao. (2024)<br><strong>RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable and Circular Convolutions</strong><br><button class=copy-to-clipboard title="RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable and Circular Convolutions" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16361v1.pdf filename=2403.16361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Four-dimensional cone-beam computed tomography (4D CBCT) provides respiration-resolved images and can be used for image-guided radiation therapy. However, the ability to reveal respiratory motion comes at the cost of image artifacts. As raw projection data are sorted into multiple respiratory phases, there is a limited number of cone-beam projections available for image reconstruction. Consequently, the 4D CBCT images are covered by severe streak artifacts. Although several deep learning-based methods have been proposed to address this issue, most algorithms employ ordinary network models, neglecting the intrinsic structural prior within 4D CBCT images. In this paper, we first explore the origin and appearance of streak artifacts in 4D CBCT images.Specifically, we find that streak artifacts exhibit a periodic rotational motion along with the patient&rsquo;s respiration. This unique motion pattern inspires us to distinguish the artifacts from the desired anatomical structures in the spatiotemporal domain. Thereafter, we propose a spatiotemporal neural network named RSTAR-Net with separable and circular <b>convolutions</b> for Rotational Streak Artifact Reduction. The specially designed model effectively encodes dynamic image features, facilitating the recovery of 4D CBCT images. Moreover, RSTAR-Net is also lightweight and computationally efficient. Extensive experiments substantiate the effectiveness of our proposed method, and RSTAR-Net shows superior performance to comparison methods.</p></p class="citation"></blockquote><h2 id=csma-3>cs.MA (3)</h2><h3 id=13--285340-norm-violation-detection-in-multi-agent-systems-using-large-language-models-a-pilot-study-shawn-he-et-al-2024>(1/3 | 285/340) Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study (Shawn He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shawn He, Surangika Ranathunga, Stephen Cranefield, Bastin Tony Roy Savarimuthu. (2024)<br><strong>Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study</strong><br><button class=copy-to-clipboard title="Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 50<br>Keywords: ChatGPT, LLaMA, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16517v1.pdf filename=2403.16517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Norms are an important component of the social fabric of society by prescribing expected behaviour. In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as <b>reasoning</b> about norms and trust. Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic <b>reasoning</b> mechanisms generally used may suffer from a lack of extensibility and robustness. In contrast, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> offer opportunities to discover and reason about norms across a <b>large</b> <b>range</b> <b>of</b> social situations. This paper evaluates the capability of <b>LLMs</b> to detecting norm violations. Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated. For our evaluations we first obtained the ground truth from three human evaluators for each story. Then, the majority result was compared against the results from three well-known <b>LLM</b> models <b>(Llama</b> 2 7B, Mixtral 7B and <b>ChatGPT-4).</b> Our results show the promise of <b>ChatGPT-4</b> for detecting norm violations, with Mixtral some distance behind. Also, we identify areas where these models perform poorly and discuss implications for future work.</p></p class="citation"></blockquote><h3 id=23--286340-conformal-off-policy-prediction-for-multi-agent-systems-tom-kuipers-et-al-2024>(2/3 | 286/340) Conformal Off-Policy Prediction for Multi-Agent Systems (Tom Kuipers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Kuipers, Renukanandan Tumu, Shuo Yang, Milad Kazemi, Rahul Mangharam, Nicola Paoletti. (2024)<br><strong>Conformal Off-Policy Prediction for Multi-Agent Systems</strong><br><button class=copy-to-clipboard title="Conformal Off-Policy Prediction for Multi-Agent Systems" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs.MA, stat-ML<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16871v1.pdf filename=2403.16871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the <b>distribution</b> <b>shifts</b> induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents&rsquo; trajectories when one or more &ldquo;ego&rdquo; agents change their policies. Unlike the single-agent scenario, this setting introduces higher complexity as the <b>distribution</b> <b>shifts</b> affect predictions for all agents, not just the ego agents, and the prediction task involves full multi-dimensional trajectories, not just reward values. A key contribution of MA-COPP is to avoid enumeration or exhaustive search of the output space of agent trajectories, which is instead required by existing COPP methods to construct the prediction region. We achieve this by showing that an over-approximation of the true JPR can be constructed, without enumeration, from the maximum density ratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in multi-agent systems from the PettingZoo library and the F1TENTH autonomous racing environment, achieving nominal coverage in higher dimensions and various shift settings.</p></p class="citation"></blockquote><h3 id=33--287340-towards-a-formalisation-of-value-based-actions-and-consequentialist-ethics-adam-wyner-et-al-2024>(3/3 | 287/340) Towards a Formalisation of Value-based Actions and Consequentialist Ethics (Adam Wyner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Wyner, Tomasz Zurek, DOrota Stachura-Zurek. (2024)<br><strong>Towards a Formalisation of Value-based Actions and Consequentialist Ethics</strong><br><button class=copy-to-clipboard title="Towards a Formalisation of Value-based Actions and Consequentialist Ethics" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16719v1.pdf filename=2403.16719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Agents act to bring about a state of the world that is more compatible with their personal or institutional values. To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation. Technically, the contribution expresses actions in terms of Value-based Formal <b>Reasoning</b> (VFR), which provides a set of propositions derived from an Agent&rsquo;s value profile and the Agent&rsquo;s assessment of propositions with respect to the profile. Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential.</p></p class="citation"></blockquote><h2 id=csit-6>cs.IT (6)</h2><h3 id=16--288340-latency-aware-generative-semantic-communications-with-pre-trained-diffusion-models-li-qiao-et-al-2024>(1/6 | 288/340) Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models (Li Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Qiao, Mahdi Boloursaz Mashhadi, Zhen Gao, Chuan Heng Foh, Pei Xiao, Mehdi Bennis. (2024)<br><strong>Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models</strong><br><button class=copy-to-clipboard title="Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 43<br>Keywords: Diffusion Model, Multi-modal, Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17256v1.pdf filename=2403.17256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative foundation AI models have recently shown great success in synthesizing natural signals with high perceptual quality using only textual <b>prompts</b> and conditioning signals to guide the generation process. This enables semantic communications at extremely low data rates in future wireless networks. In this paper, we develop a latency-aware semantic communications framework with pre-trained generative models. The transmitter performs <b>multi-modal</b> semantic decomposition on the input signal and transmits each semantic stream with the appropriate coding and communication schemes based on the intent. For the <b>prompt,</b> we adopt a re-transmission-based scheme to ensure reliable transmission, and for the other semantic modalities we use an adaptive modulation/coding scheme to achieve robustness to the changing wireless channel. Furthermore, we design a semantic and latency-aware scheme to allocate transmission power to different semantic modalities based on their importance subjected to semantic quality constraints. At the receiver, a pre-trained generative model synthesizes a high fidelity signal using the received multi-stream semantics. <b>Simulation</b> results demonstrate ultra-low-rate, low-latency, and channel-adaptive semantic communications.</p></p class="citation"></blockquote><h3 id=26--289340-6d-movable-antenna-enhanced-wireless-network-via-discrete-position-and-rotation-optimization-xiaodan-shao-et-al-2024>(2/6 | 289/340) 6D Movable Antenna Enhanced Wireless Network Via Discrete Position and Rotation Optimization (Xiaodan Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaodan Shao, Rui Zhang, Qijun Jiang, Robert Schober. (2024)<br><strong>6D Movable Antenna Enhanced Wireless Network Via Discrete Position and Rotation Optimization</strong><br><button class=copy-to-clipboard title="6D Movable Antenna Enhanced Wireless Network Via Discrete Position and Rotation Optimization" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17122v1.pdf filename=2403.17122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Six-dimensional movable antenna (6DMA) is an effective approach to improve wireless network capacity by adjusting the 3D positions and 3D rotations of distributed antenna surfaces based on the users&rsquo; spatial distribution and statistical channel information. Although continuously positioning/rotating 6DMA surfaces can achieve the greatest flexibility and thus the highest capacity improvement, it is difficult to implement due to the discrete movement constraints of practical stepper motors. Thus, in this paper, we consider a 6DMA-aided base station (BS) with only a finite number of possible discrete positions and rotations for the 6DMA surfaces. We aim to maximize the average network capacity for random numbers of users at random locations by jointly optimizing the 3D positions and 3D rotations of multiple 6DMA surfaces at the BS subject to discrete movement constraints. In particular, we consider the practical cases with and without statistical channel knowledge of the users, and propose corresponding offline and online optimization algorithms, by leveraging the Monte Carlo and conditional sample mean (CSM) methods, respectively. <b>Simulation</b> results verify the effectiveness of our proposed offline and online algorithms for discrete position/rotation optimization of 6DMA surfaces as compared to various <b>benchmark</b> schemes with fixed-position antennas (FPAs) and 6DMAs with limited movability. It is shown that 6DMA-BS can significantly enhance wireless network capacity, even under discrete position/rotation constraints, by exploiting the spatial distribution characteristics of the users.</p></p class="citation"></blockquote><h3 id=36--290340-a-progressive-codebook-optimization-scheme-for-sparse-code-multiple-access-in-downlink-channels-tuofeng-lei-et-al-2024>(3/6 | 290/340) A Progressive Codebook Optimization Scheme for Sparse Code Multiple Access in Downlink Channels (Tuofeng Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuofeng Lei, Qu Luo, Shimiao Chen, Xin Song, Pei Xiao. (2024)<br><strong>A Progressive Codebook Optimization Scheme for Sparse Code Multiple Access in Downlink Channels</strong><br><button class=copy-to-clipboard title="A Progressive Codebook Optimization Scheme for Sparse Code Multiple Access in Downlink Channels" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16826v1.pdf filename=2403.16826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse code multiple access (SCMA) is a promising technique for enabling massive connectivity and high spectrum efficiency in future machine-type communication networks. However, its performance crucially depends on well-designed multi-dimensional codebooks. In this paper, we propose a novel progressive codebook optimization scheme that can achieve near-optimal performance over downlink fading channels. By examining the pair-wise error probability (PEP), we first derive the symbol error rate (SER) performance of the sparse codebook in downlink channels, which is considered as the design criterion for codebook optimization. Then, the <b>benchmark</b> constellation group at a single resource element is optimized with a sequential quadratic programming approach. Next, we propose a constellation group reconstruction process to assign the sub-constellations in each resource element (RE) progressively. For the current RE, the assignment of the sub-constellations is designed by minimizing the error performance of the product distance of the superimposed codewords in previous REs. The design process involves both permutation and labeling of the sub-constellations in the <b>benchmark</b> constellation group. <b>Simulation</b> results show that the proposed codebooks exhibit significant performance gains over state-of-the-art codebooks in the low signal-to-noise ratio (SNR) region over various downlink fading channels.</p></p class="citation"></blockquote><h3 id=46--291340-design-and-performance-of-resonant-beam-communications----part-ii-mobile-scenario-dongxu-li-et-al-2024>(4/6 | 291/340) Design and Performance of Resonant Beam Communications &ndash; Part II: Mobile Scenario (Dongxu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongxu Li, Yuanming Tian, Chuan Huang, Qingwen Liu, Shengli Zhou. (2024)<br><strong>Design and Performance of Resonant Beam Communications &ndash; Part II: Mobile Scenario</strong><br><button class=copy-to-clipboard title="Design and Performance of Resonant Beam Communications -- Part II: Mobile Scenario" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16694v1.pdf filename=2403.16694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This two-part paper focuses on the system design and performance analysis for a point-to-point resonant beam communication (RBCom) system under both the quasi-static and mobile scenarios. Part I of this paper proposes a synchronization-based information transmission scheme and derives the capacity upper and lower bounds for the quasi-static channel case. In Part II, we address the mobile scenario, where the receiver is in relative motion to the transmitter, and derive a mobile RBCom channel model that jointly considers the Doppler effect, channel variation, and echo interference. With the obtained channel model, we prove that the channel gain of the mobile RBCom decreases as the number of transmitted frames increases, and thus show that the considered mobile RBCom terminates after the transmitter sends a certain number of frames without frequency compensation. By deriving an upper bound on the number of successfully transmitted frames, we formulate the throughput maximization problem for the considered mobile RBCom system, and solve it via a sequential parametric convex approximation (SPCA) method. Finally, <b>simulation</b> results validate the analysis of our proposed method in some typical scenarios.</p></p class="citation"></blockquote><h3 id=56--292340-single-carrier-delay-doppler-domain-equalization-yuto-hama-et-al-2024>(5/6 | 292/340) Single-Carrier Delay-Doppler Domain Equalization (Yuto Hama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuto Hama, Hideki Ochiai. (2024)<br><strong>Single-Carrier Delay-Doppler Domain Equalization</strong><br><button class=copy-to-clipboard title="Single-Carrier Delay-Doppler Domain Equalization" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16453v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16453v1.pdf filename=2403.16453v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For doubly-selective channels, delay-Doppler (DD) modulation, mostly known as orthogonal time frequency space (OTFS) modulation, enables simultaneous compensation of delay and Doppler shifts. However, OTFS modulated signal has high peak-to-average power ratio (PAPR) because of its precoding operation performed over the DD domain. In order to deal with this problem, we propose a single-carrier transmission with delay-Doppler domain equalization (SC-DDE). In this system, the discretized time-domain SC signal is converted to the DD domain by discrete Zak transform (DZT) at the receiver side, followed by delay-Doppler domain equalization (DDE). Since equalization is performed in the DD domain, the SC-DDE receiver should acquire the channel delay-Doppler response. To this end, we introduce an embedded pilot-aided channel estimation scheme designed for SC-DDE, which does not affect the peak power property of transmitted signals. Through computer <b>simulation,</b> distribution of PAPR and bit error rate (BER) performance of the proposed system are compared with those of the conventional OTFS and SC with frequency-domain equalization (SC-FDE). As a result, our proposed SC-DDE significantly outperforms SC-FDE in terms of BER at the expense of additional computational complexity at the receiver. Furthermore, SC-DDE shows much lower PAPR than OTFS even though they achieve comparable coded BER performance.</p></p class="citation"></blockquote><h3 id=66--293340-movable-antenna-position-optimization-a-graph-based-approach-weidong-mei-et-al-2024>(6/6 | 293/340) Movable-Antenna Position Optimization: A Graph-based Approach (Weidong Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weidong Mei, Xin Wei, Boyu Ning, Zhi Chen, Rui Zhang. (2024)<br><strong>Movable-Antenna Position Optimization: A Graph-based Approach</strong><br><button class=copy-to-clipboard title="Movable-Antenna Position Optimization: A Graph-based Approach" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16886v1.pdf filename=2403.16886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fluid antennas (FAs) and movable antennas (MAs) have emerged as promising technologies in wireless communications, which offer the flexibility to improve channel conditions by adjusting transmit/receive antenna positions within a spatial region. In this letter, we focus on an MA-enhanced multiple-input single-output (MISO) communication system, aiming to optimize the positions of multiple transmit MAs to maximize the received signal power. Unlike the prior works on continuously searching for the optimal MA positions, we propose to sample the transmit region into discrete points, such that the continuous antenna position optimization problem is transformed to a discrete sampling point selection problem based on the point-wise channel information. However, such a point selection problem is combinatory and challenging to be optimally solved. To tackle this challenge, we ingeniously recast it as an equivalent fixed-hop shortest path problem in <b>graph</b> theory and propose a customized algorithm to solve it optimally in polynomial time. To further reduce the complexity, a linear-time sequential update algorithm is also proposed to obtain a high-quality suboptimal solution. Numerical results demonstrate that the proposed algorithms can yield considerable performance gains over the conventional fixed-position antennas with/without antenna selection.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--294340-leveraging-large-language-model-to-generate-a-novel-metaheuristic-algorithm-with-crispe-framework-rui-zhong-et-al-2024>(1/2 | 294/340) Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework (Rui Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhong, Yuefeng Xu, Chao Zhang, Jun Yu. (2024)<br><strong>Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 43<br>Keywords: Benchmarking, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16417v1.pdf filename=2403.16417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we borrow the <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> <b>ChatGPT-3.5</b> to automatically and quickly design a new metaheuristic algorithm (MA) with only a small amount of input. The novel animal-inspired MA named zoological search optimization (ZSO) draws inspiration from the collective behaviors of animals for solving continuous optimization problems. Specifically, the basic ZSO algorithm involves two search operators: the prey-predator interaction operator and the social flocking operator to balance exploration and exploitation well. Besides, the standard <b>prompt</b> engineering framework CRISPE (i.e., Capacity and Role, Insight, Statement, Personality, and Experiment) is responsible for the specific <b>prompt</b> design. Furthermore, we designed four variants of the ZSO algorithm with slight human-interacted adjustment. In numerical experiments, we comprehensively investigate the performance of ZSO-derived algorithms on CEC2014 <b>benchmark</b> functions, CEC2022 <b>benchmark</b> functions, and six engineering optimization problems. 20 popular and state-of-the-art MAs are employed as competitors. The experimental results and statistical analysis confirm the efficiency and effectiveness of ZSO-derived algorithms. At the end of this paper, we explore the prospects for the development of the metaheuristics community under the <b>LLM</b> era.</p></p class="citation"></blockquote><h3 id=22--295340-qkformer-hierarchical-spiking-transformer-using-q-k-attention-chenlin-zhou-et-al-2024>(2/2 | 295/340) QKFormer: Hierarchical Spiking Transformer using Q-K Attention (Chenlin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian. (2024)<br><strong>QKFormer: Hierarchical Spiking Transformer using Q-K Attention</strong><br><button class=copy-to-clipboard title="QKFormer: Hierarchical Spiking Transformer using Q-K Attention" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-CV, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16552v1.pdf filename=2403.16552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spiking <b>Transformers,</b> which integrate Spiking Neural Networks (SNNs) with <b>Transformer</b> architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking <b>transformers</b> to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking <b>transformers.</b> Together, we develop QKFormer, a hierarchical spiking <b>transformer</b> based on Q-K attention with direct training. QKFormer shows significantly superior performance over existing state-of-the-art SNN models on various mainstream datasets. Notably, with comparable size to Spikformer (66.34 M, 74.81%), QKFormer (64.96 M) achieves a groundbreaking top-1 accuracy of 85.65% on ImageNet-1k, substantially outperforming Spikformer by 10.84%. To our best knowledge, this is the first time that directly training SNNs have exceeded 85% accuracy on ImageNet-1K. The code and models are publicly available at <a href=https://github.com/zhouchenlin2096/QKFormer>https://github.com/zhouchenlin2096/QKFormer</a></p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--296340-accuracy-aware-cooperative-sensing-and-computing-for-connected-autonomous-vehicles-xuehan-ye-et-al-2024>(1/2 | 296/340) Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous Vehicles (Xuehan Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuehan Ye, Kaige Qu, Weihua Zhuang, Xuemin Shen. (2024)<br><strong>Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous Vehicles</strong><br><button class=copy-to-clipboard title="Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous Vehicles" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 43<br>Keywords: Benchmarking, Simulation, Simulator, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16408v1.pdf filename=2403.16408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To maintain high perception performance among connected and autonomous vehicles (CAVs), in this paper, we propose an accuracy-aware and resource-efficient raw-level cooperative sensing and computing scheme among CAVs and road-side infrastructure. The scheme enables fined-grained partial raw sensing data selection, transmission, fusion, and processing in per-object granularity, by exploiting the parallelism among object classification subtasks associated with each object. A <b>supervised</b> <b>learning</b> model is trained to capture the relationship between the object classification accuracy and the data quality of selected object sensing data, facilitating accuracy-aware sensing data selection. We formulate an optimization problem for joint sensing data selection, subtask placement and resource allocation among multiple object classification subtasks, to minimize the total resource cost while satisfying the delay and accuracy requirements. A genetic algorithm based iterative solution is proposed for the optimization problem. <b>Simulation</b> results demonstrate the accuracy awareness and resource efficiency achieved by the proposed cooperative sensing and computing scheme, in comparison with <b>benchmark</b> solutions.</p></p class="citation"></blockquote><h3 id=22--297340-relational-network-verification-xieyang-xu-et-al-2024>(2/2 | 297/340) Relational Network Verification (Xieyang Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xieyang Xu, Yifei Yuan, Zachary Kincaid, Arvind Krishnamurthy, Ratul Mahajan, David Walker, Ennan Zhai. (2024)<br><strong>Relational Network Verification</strong><br><button class=copy-to-clipboard title="Relational Network Verification" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17277v1.pdf filename=2403.17277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relational network verification is a new approach to validating network changes. In contrast to traditional network verification, which analyzes specifications for a single network snapshot, relational network verification analyzes specifications concerning two network snapshots (e.g., pre- and post-change snapshots) and captures their similarities and differences. Relational change specifications are compact and precise because they specify the flows or paths that change between snapshots and then simply mandate that other behaviors of the network &ldquo;stay the same&rdquo;, without enumerating them. To achieve similar guarantees, single-snapshot specifications need to enumerate all flow and path behaviors that are not expected to change, so we can check that nothing has accidentally changed. Thus, precise single-snapshot specifications are proportional to network size, which makes them impractical to generate for many real-world networks. To demonstrate the value of relational <b>reasoning,</b> we develop a high-level relational specification language and a tool called Rela to validate network changes. Rela first compiles input specifications and network snapshot representations to finite state transducers. It then checks compliance using decision procedures for automaton equivalence. Our experiments using data on complex changes to a global backbone (with over 10^3 routers) find that Rela specifications need fewer than 10 terms for 93% of them and it validates 80% of them within 20 minutes.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--298340-radiogat-a-joint-model-based-and-data-driven-framework-for-multi-band-radiomap-reconstruction-via-graph-attention-networks-xiaojie-li-et-al-2024>(1/2 | 298/340) RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks (Xiaojie Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaojie Li, Songyang Zhang, Hang Li, Xiaoyang Li, Lexi Xu, Haigao Xu, Hui Mei, Guangxu Zhu, Nan Qi, Ming Xiao. (2024)<br><strong>RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks</strong><br><button class=copy-to-clipboard title="RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, eess-SP, eess.SP<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Graph Attention Networks, Graph, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16397v1.pdf filename=2403.16397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on <b>Graph</b> <b>Attention</b> <b>Network</b> <b>(GAT)</b> tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a <b>graph</b> <b>structure</b> <b>through</b> an innovative encoding strategy that leverages radio propagation models to capture the spatial-spectral correlation inherent in the data. This <b>graph-based</b> <b>representation</b> <b>not</b> only simplifies data handling but also enables tailored label sampling during training, significantly enhancing the framework&rsquo;s adaptability for deployment. Subsequently, The <b>GAT</b> is employed to generalize the radiomap information across various frequency bands. Extensive experiments using raytracing datasets based on real-world environments have demonstrated RadioGAT&rsquo;s enhanced accuracy in <b>supervised</b> <b>learning</b> settings and its robustness in semi-supervised scenarios. These results underscore RadioGAT&rsquo;s effectiveness and practicality for MB-RMR in environments with limited data availability.</p></p class="citation"></blockquote><h3 id=22--299340-resolution-limit-of-single-photon-lidar-stanley-h-chan-et-al-2024>(2/2 | 299/340) Resolution Limit of Single-Photon LiDAR (Stanley H. Chan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stanley H. Chan, Hashan K. Weerasooriya, Weijian Zhang, Pamela Abshire, Istvan Gyongy, Robert K. Henderson. (2024)<br><strong>Resolution Limit of Single-Photon LiDAR</strong><br><button class=copy-to-clipboard title="Resolution Limit of Single-Photon LiDAR" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-CV, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17719v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17719v1.pdf filename=2403.17719v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single-photon Light Detection and Ranging (LiDAR) systems are often equipped with an array of detectors for improved spatial resolution and sensing speed. However, given a fixed amount of flux produced by the laser transmitter across the scene, the per-pixel Signal-to-Noise Ratio (SNR) will decrease when more pixels are packed in a unit space. This presents a fundamental trade-off between the spatial resolution of the sensor array and the SNR received at each pixel. Theoretical characterization of this fundamental limit is explored. By deriving the photon arrival statistics and introducing a series of new approximation techniques, the Mean Squared Error (MSE) of the maximum-likelihood estimator of the time delay is derived. The theoretical predictions align well with <b>simulations</b> and real data.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--300340-voicecraft-zero-shot-speech-editing-and-text-to-speech-in-the-wild-puyuan-peng-et-al-2024>(1/3 | 300/340) VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild (Puyuan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Puyuan Peng, Po-Yao Huang, Daniel Li, Abdelrahman Mohamed, David Harwath. (2024)<br><strong>VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild</strong><br><button class=copy-to-clipboard title="VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Zero-shot, Transformer, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16973v1.pdf filename=2403.16973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and <b>zero-shot</b> <b>text-to-speech</b> <b>(TTS)</b> on audiobooks, internet videos, and podcasts. VoiceCraft employs a <b>Transformer</b> decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for <b>zero-shot</b> <b>TTS,</b> our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models and real recordings. In particular, for speech editing evaluation, we introduce a high quality, challenging, and realistic dataset named RealEdit. We encourage readers to listen to the demos at <a href=https://jasonppy.github.io/VoiceCraft_web>https://jasonppy.github.io/VoiceCraft_web</a>.</p></p class="citation"></blockquote><h3 id=23--301340-hierarchical-recurrent-adapters-for-efficient-multi-task-adaptation-of-large-speech-models-tsendsuren-munkhdalai-et-al-2024>(2/3 | 301/340) Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models (Tsendsuren Munkhdalai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsendsuren Munkhdalai, Youzheng Chen, Khe Chai Sim, Fadi Biadsy, Tara Sainath, Pedro Moreno Mengibar. (2024)<br><strong>Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models</strong><br><button class=copy-to-clipboard title="Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, cs-NE, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.19709v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.19709v1.pdf filename=2403.19709v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model <b>fine-tuning</b> baseline in both single and multi-task adaptation settings when evaluated on <b>automatic</b> <b>speech</b> <b>recognition</b> tasks.</p></p class="citation"></blockquote><h3 id=33--302340-distributed-collaborative-anomalous-sound-detection-by-embedding-sharing-kota-dohi-et-al-2024>(3/3 | 302/340) Distributed collaborative anomalous sound detection by embedding sharing (Kota Dohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kota Dohi, Yohei Kawaguchi. (2024)<br><strong>Distributed collaborative anomalous sound detection by embedding sharing</strong><br><button class=copy-to-clipboard title="Distributed collaborative anomalous sound detection by embedding sharing" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CR, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16610v1.pdf filename=2403.16610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed. In this paper, we explore a method for multiple clients to collaboratively learn an anomalous sound detection model while keeping their raw data private from each other. In the context of industrial machine anomalous sound detection, each client possesses data from different machines or different operational states, making it challenging to learn through <b>federated</b> <b>learning</b> or split learning. In our proposed method, each client calculates embeddings using a common pre-trained model developed for sound data classification, and these calculated embeddings are aggregated on the server to perform anomalous sound detection through outlier exposure. Experiments showed that our proposed method improves the AUC of anomalous sound detection by an average of 6.8%.</p></p class="citation"></blockquote><h2 id=mathoc-4>math.OC (4)</h2><h3 id=14--303340-robust-finite-time-stabilization-of-linear-systems-with-limited-state-quantization-yu-zhou-et-al-2024>(1/4 | 303/340) Robust Finite-time Stabilization of Linear Systems with Limited State Quantization (Yu Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhou, Andrey Polyakov, Gang Zheng. (2024)<br><strong>Robust Finite-time Stabilization of Linear Systems with Limited State Quantization</strong><br><button class=copy-to-clipboard title="Robust Finite-time Stabilization of Linear Systems with Limited State Quantization" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17184v1.pdf filename=2403.17184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the robust asymptotic stabilization of a linear time-invariant (LTI) system by a static feedback with a static state <b>quantization.</b> It is shown that the controllable LTI system can be stabilized to zero in a finite time by means of a nonlinear feedback with a quantizer having a limited (finite) number of values <b>(quantization</b> seeds) even when all parameters of the controller and the quantizer are time-invariant. The control design is based on generalized homogeneity. A homogeneous spherical quantizer is introduced. The static homogeneous feedback is shown to be local (or global) finite-time stabilizer for the linear system (dependently of the system matrix). The tuning rules for both the quantizer and the feedback law are obtained in the form of Linear Matrix Inequalities (LMIs). The closed-loop system is proven to be robust with respect to some bounded matched and vanishing mismatched perturbations. Theoretical results are supported by numerical <b>simulations.</b> \</p></p class="citation"></blockquote><h3 id=24--304340-data-driven-extrusion-force-control-tuning-for-3d-printing-xavier-guidetti-et-al-2024>(2/4 | 304/340) Data-Driven Extrusion Force Control Tuning for 3D Printing (Xavier Guidetti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xavier Guidetti, Ankita Mukne, Marvin Rueppel, Yannick Nagel, Efe C. Balta, John Lygeros. (2024)<br><strong>Data-Driven Extrusion Force Control Tuning for 3D Printing</strong><br><button class=copy-to-clipboard title="Data-Driven Extrusion Force Control Tuning for 3D Printing" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 15<br>Keywords: Geometry, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16470v1.pdf filename=2403.16470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of 3D prints often varies due to different conditions inherent to each print, such as filament type, print speed, and nozzle size. Closed-loop process control methods improve the accuracy and repeatability of 3D prints. However, optimal tuning of controllers for given process parameters and design <b>geometry</b> is often a challenge with manually tuned controllers resulting in inconsistent and suboptimal results. This work employs Bayesian optimization to identify the optimal controller parameters. Additionally, we explore <b>transfer</b> <b>learning</b> in the context of 3D printing by leveraging prior information from past trials. By integrating optimized extrusion force control and <b>transfer</b> <b>learning,</b> we provide a novel framework for closed-loop 3D printing and propose an automated calibration routine that produces high-quality prints for a desired combination of print settings, material, and shape.</p></p class="citation"></blockquote><h3 id=34--305340-approximation-with-random-shallow-relu-networks-with-applications-to-model-reference-adaptive-control-andrew-lamperski-et-al-2024>(3/4 | 305/340) Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control (Andrew Lamperski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Lamperski, Tyler Lekang. (2024)<br><strong>Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control</strong><br><button class=copy-to-clipboard title="Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17142v1.pdf filename=2403.17142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o <b>reinforcement</b> <b>learning.</b> A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the weights uniformly over a sphere and the biases uniformly over an interval. We show how the result can be used to get approximations of required accuracy in a model reference adaptive control application.</p></p class="citation"></blockquote><h3 id=44--306340-output-feedback-synthesis-orbit-geometry-quotient-manifolds-and-lqg-direct-policy-optimization-spencer-kraisler-et-al-2024>(4/4 | 306/340) Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization (Spencer Kraisler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spencer Kraisler, Mehran Mesbahi. (2024)<br><strong>Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization</strong><br><button class=copy-to-clipboard title="Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17157v1.pdf filename=2403.17157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider direct policy optimization for the linear-quadratic Gaussian (LQG) setting. Over the past few years, it has been recognized that the landscape of stabilizing output-feedback controllers of relevance to LQG has an intricate <b>geometry,</b> particularly as it pertains to the existence of spurious stationary points. In order to address such challenges, in this paper, we first adopt a Riemannian metric for the space of stabilizing full-order minimal output-feedback controllers. We then proceed to prove that the orbit of such controllers modulo coordinate transformation admits a Riemannian quotient manifold structure. This geometric structure is then used to develop a Riemannian gradient descent for the direct LQG policy optimization. We prove a local convergence guarantee with linear rate and show the proposed approach exhibits significantly faster and more robust numerical performance as compared with ordinary gradient descent for LQG. Subsequently, we provide reasons for this observed behavior; in particular, we argue that optimizing over the orbit space of controllers is the right theoretical and computational setup for direct LQG policy optimization.</p></p class="citation"></blockquote><h2 id=csdl-2>cs.DL (2)</h2><h3 id=12--307340-chatgpt-contamination-estimating-the-prevalence-of-llms-in-the-scholarly-literature-andrew-gray-2024>(1/2 | 307/340) ChatGPT &lsquo;contamination&rsquo;: estimating the prevalence of LLMs in the scholarly literature (Andrew Gray, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Gray. (2024)<br><strong>ChatGPT &lsquo;contamination&rsquo;: estimating the prevalence of LLMs in the scholarly literature</strong><br><button class=copy-to-clipboard title="ChatGPT 'contamination': estimating the prevalence of LLMs in the scholarly literature" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16887v1.pdf filename=2403.16887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of <b>ChatGPT</b> and similar <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> tools in scholarly communication and academic publishing has been widely discussed since they became easily accessible to a general audience in late 2022. This study uses keywords known to be disproportionately present in <b>LLM-generated</b> text to provide an overall estimate for the prevalence of <b>LLM-assisted</b> writing in the scholarly literature. For the publishing year 2023, it is found that several of those keywords show a distinctive and disproportionate increase in their prevalence, individually and in combination. It is estimated that at least 60,000 papers (slightly over 1% of all articles) were <b>LLM-assisted,</b> though this number could be extended and refined by analysis of other characteristics of the papers or by identification of further indicative keywords.</p></p class="citation"></blockquote><h3 id=22--308340-can-chatgpt-predict-article-retraction-based-on-twitter-mentions-er-te-zheng-et-al-2024>(2/2 | 308/340) Can ChatGPT predict article retraction based on Twitter mentions? (Er-Te Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Er-Te Zheng, Hui-Zhen Fu, Zhichao Fang. (2024)<br><strong>Can ChatGPT predict article retraction based on Twitter mentions?</strong><br><button class=copy-to-clipboard title="Can ChatGPT predict article retraction based on Twitter mentions?" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-CL, cs-DL, cs-LG, cs.DL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16851v1.pdf filename=2403.16851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and <b>ChatGPT.</b> Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with Twitter mention data (approximately 16%). Using the manual labelling results as the baseline, <b>ChatGPT</b> demonstrates superior performance compared to other methods, implying its potential in assisting human judgment for predicting article retraction. This study uncovers both the potential and limitation of social media events as an early warning system for article retraction, shedding light on a potential application of generative artificial intelligence in promoting research integrity.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--309340-investigation-of-the-effectiveness-of-applying-chatgpt-in-dialogic-teaching-using-electroencephalography-jiayue-zhang-et-al-2024>(1/1 | 309/340) Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography (Jiayue Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Senqing Qi, Taotao Long, Bao Ge. (2024)<br><strong>Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography</strong><br><button class=copy-to-clipboard title="Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY, physics-ed-ph<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16687v1.pdf filename=2403.16687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, the rapid development of artificial intelligence technology, especially the emergence of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT,</b> has presented significant prospects for application in the field of education. <b>LLMs</b> possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of <b>LLMs</b> to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using <b>ChatGPT,</b> while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course &ldquo;Digital Image Processing&rdquo;. The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with <b>ChatGPT</b> exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with <b>ChatGPT</b> exhibited higher levels of cognitive activity, suggesting that <b>ChatGPT</b> could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that <b>ChatGPT</b> cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining <b>ChatGPT</b> with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.</p></p class="citation"></blockquote><h2 id=csar-3>cs.AR (3)</h2><h3 id=13--310340-partially-precise-computing-paradigm-for-efficient-hardware-implementation-of-application-specific-embedded-systems-mohsen-faryabi-et-al-2024>(1/3 | 310/340) Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems (Mohsen Faryabi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohsen Faryabi, Amir Hossein Moradi. (2024)<br><strong>Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems</strong><br><button class=copy-to-clipboard title="Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR<br>Keyword Score: 30<br>Keywords: Face Recognition, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16577v1.pdf filename=2403.16577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, the number of emerging embedded systems rapidly grows in many application domains, due to recent advances in artificial intelligence and internet of things. The main inherent specification of these application-specific systems is that they have not a general nature and are basically developed to only perform a particular task and therefore, deal only with a limited and predefined range of custom input values. Despite this significant feature, these emerging applications are still conventionally implemented using general-purpose and precise digital computational blocks, which are essentially developed to provide the correct result for all possible input values. This highly degrades the physical properties of these applications while does not improve their functionality. To resolve this conflict, a novel computational paradigm named as partially-precise computing is introduced in this paper, based on an inspiration from the brain information reduction hypothesis as a tenet of neuroscience. The main specification of a Partially-Precise Computational (PPC) block is that it provides the precise result only for a desired, limited, and predefined set of input values. This relaxes its internal structure which results in improved physical properties with respect to a conventional precise block. The PPC blocks improve the implementation costs of the embedded applications, with a negligible or even without any output quality degradation with respect to the conventional implementation. The applicability and efficiency of the first instances of PPC adders and multipliers in a Gaussian denoising filter, an image blending and a <b>face</b> <b>recognition</b> neural network are demonstrated by means of a wide range of <b>simulation</b> and synthesis results.</p></p class="citation"></blockquote><h3 id=23--311340-electron-tunnelling-noise-programmable-random-variate-accelerator-for-monte-carlo-sampling-james-t-meech-et-al-2024>(2/3 | 311/340) Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling (James T. Meech et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James T. Meech, Vasileios Tsoutsouras, Phillip Stanley-Marbell. (2024)<br><strong>Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling</strong><br><button class=copy-to-clipboard title="Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR, physics-comp-ph, stat-AP, stat-CO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16421v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16421v2.pdf filename=2403.16421v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents an electron tunneling noise programmable random variate accelerator for accelerating the sampling stage of Monte Carlo <b>simulations.</b> We used the LiteX framework to generate a FemtoRV imfc RISC-V instruction set soft processor and deploy it on a Digilent Arty-100T FPGA development board. The RISC-V soft processor augmented with our programmable random variate accelerator achieves an average speedup of 8.70 times and a median speedup of 8.68 times for a suite of twelve different <b>benchmark</b> applications when compared to GNU Scientific Library software random number generation. These speedups are achievable because the <b>benchmarks</b> spend an average of 90.0 % of their execution time generating random samples. The results of the Monte Carlo <b>benchmark</b> programs run over the programmable random variate accelerator have an average Wasserstein distance of 1.48 times and a median Wasserstein distance of 1.41 times$that of the results produced by the GNU Scientific Library random number generators. The soft processor samples the electron tunneling noise source using the hardened XADC block in the FPGA. The flexibility of the LiteX framework allows for the deployment of any LiteX-supported soft processor with an electron tunneling noise programmable random variate accelerator on any LiteX-supported development board that contains an FPGA with an XADC.</p></p class="citation"></blockquote><h3 id=33--312340-sip-autotuning-gpu-native-schedules-via-stochastic-instruction-perturbation-guoliang-he-et-al-2024>(3/3 | 312/340) SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation (Guoliang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoliang He, Eiko Yoneki. (2024)<br><strong>SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation</strong><br><button class=copy-to-clipboard title="SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16863v1.pdf filename=2403.16863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become a significant workload since their appearance. However, they are also computationally expensive as they have billions of parameters and are trained with massive amounts of data. Thus, recent works have developed dedicated CUDA kernels for <b>LLM</b> training and inference instead of relying on compilergenerated ones, so that hardware resources are as fully utilized as possible. In this work, we explore the possibility of GPU native instruction optimization to further push the CUDA kernels to extreme performance. Contrary to prior works, we adopt an automatic optimization approach by defining a search space of possible GPU native instruction schedules, and then we apply stochastic search to perform optimization. Experiments show that SIP can further improve CUDA kernel throughput by automatically discovering better GPU native instruction schedules and the optimized schedules are tested by 10 million test samples.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--313340-training-generative-adversarial-network-based-vocoder-with-limited-data-using-augmentation-conditional-discriminator-takuhiro-kaneko-et-al-2024>(1/1 | 313/340) Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator (Takuhiro Kaneko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka. (2024)<br><strong>Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator</strong><br><button class=copy-to-clipboard title="Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Data Augmentation, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16464v1.pdf filename=2403.16464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)-based</b> vocoder trained with an adversarial discriminator is commonly used for speech synthesis because of its fast, lightweight, and high-quality characteristics. However, this <b>data-driven</b> <b>model</b> requires a large amount of training <b>data</b> <b>incurring</b> high <b>data-collection</b> <b>costs.</b> This fact motivates us to train a <b>GAN-based</b> vocoder on limited <b>data.</b> <b>A</b> promising solution is to augment the training <b>data</b> <b>to</b> avoid overfitting. However, a standard discriminator is unconditional and insensitive to distributional changes caused by <b>data</b> <b>augmentation.</b> Thus, augmented speech (which can be extraordinary) may be considered real speech. To address this issue, we propose an augmentation-conditional discriminator (AugCondD) that receives the augmentation state as input in addition to speech, thereby assessing the input speech according to the augmentation state, without inhibiting the learning of the original non-augmented distribution. Experimental results indicate that AugCondD improves speech quality under limited <b>data</b> <b>conditions</b> while achieving comparable speech quality under sufficient <b>data</b> <b>conditions.</b> Audio samples are available at <a href=https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/>https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/augcondd/</a>.</p></p class="citation"></blockquote><h2 id=mathna-5>math.NA (5)</h2><h3 id=15--314340-high-order-transient-multidimensional-simulation-of-a-thermo-electro-chemo-mechanical-model-for-lithium-ion-batteries-jaime-mora-paz-2024>(1/5 | 314/340) High-order transient multidimensional simulation of a thermo-electro-chemo-mechanical model for Lithium-ion batteries (Jaime Mora-Paz, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaime Mora-Paz. (2024)<br><strong>High-order transient multidimensional simulation of a thermo-electro-chemo-mechanical model for Lithium-ion batteries</strong><br><button class=copy-to-clipboard title="High-order transient multidimensional simulation of a thermo-electro-chemo-mechanical model for Lithium-ion batteries" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M60 (Primary), 35Q99, J-2, cs-NA, math-MP, math-NA, math-ph, math.NA<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16928v1.pdf filename=2403.16928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We build a transient multidimensional multiphysical model based on continuum theories, involving the coupled mechanical, thermal and electrochemical phenomena occurring simultaneously in the discharge or charge of lithium-ion batteries. The process delivers a system of coupled nonlinear partial differential equations. Besides initial and boundary conditions, we highlight the treatment of the electrode-electrolyte interface condition, which corresponds to a Butler-Volmer reaction kinetics equation. We present the derivation of the strong and weak forms of the model, as well as the discretization procedure in space and in time. The discretized model is computationally solved in two dimensions by means of a finite element method that employs $hp$ layered meshes, along with staggered second order semi-implicit time integration. The expected error estimate is of higher order than any other similar work, both in space and in time. A representative battery cell <b>geometry,</b> under distinct operating scenarios, is simulated. The numerical results show that the full model allows for important additional insights to be drawn than when caring only for the electrochemical coupling. Considering the multiphysics becomes more important as the applied current is increased, whether for discharge or for charge. Our full model provides battery design professionals with a valuable tool to optimize designs and advance the energy storage industry.</p></p class="citation"></blockquote><h3 id=25--315340-nonlinearsolvejl-high-performance-and-robust-solvers-for-systems-of-nonlinear-equations-in-julia-avik-pal-et-al-2024>(2/5 | 315/340) NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia (Avik Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avik Pal, Flemming Holtorf, Axel Larsson, Torkel Loman, Utkarsh, Frank Schäefer, Qingyu Qu, Alan Edelman, Chris Rackauckas. (2024)<br><strong>NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia</strong><br><button class=copy-to-clipboard title="NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16341v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16341v2.pdf filename=2403.16341v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving <b>benchmark</b> problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and <b>simulation</b> across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.</p></p class="citation"></blockquote><h3 id=35--316340-stochastic-active-discretizations-for-accelerating-temporal-uncertainty-management-of-gas-pipeline-loads-jake-j-harmon-et-al-2024>(3/5 | 316/340) Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads (Jake J. Harmon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jake J. Harmon, Svetlana Tokareva, Anatoly Zlotnik. (2024)<br><strong>Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads</strong><br><button class=copy-to-clipboard title="Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16929v1.pdf filename=2403.16929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a predictor-corrector adaptive method for the <b>simulation</b> of hyperbolic partial differential equations (PDEs) on networks under general uncertainty in parameters, initial conditions, or boundary conditions. The approach is based on the stochastic finite volume (SFV) framework that circumvents sampling schemes or <b>simulation</b> ensembles while also preserving fundamental properties, in particular hyperbolicity of the resulting systems and conservation of the discrete solutions. The initial boundary value problem (IBVP) on a set of network-connected one-dimensional domains that represent a pipeline is represented using active discretization of the physical and stochastic spaces, and we evaluate the propagation of uncertainty through network nodes by solving a junction Riemann problem. The adaptivity of our method in refining discretization based on error metrics enables computationally tractable evaluation of intertemporal uncertainty in order to support decisions about timing and quantity of pipeline operations to maximize delivery under transient and uncertain conditions. We illustrate our computational method using <b>simulations</b> for a representative network.</p></p class="citation"></blockquote><h3 id=45--317340-the-cubic-nonlinear-schrödinger-equation-with-rough-potential-norbert-j-mauser-et-al-2024>(4/5 | 317/340) The cubic nonlinear Schrödinger equation with rough potential (Norbert J. Mauser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norbert J. Mauser, Yifei Wu, Xiaofei Zhao. (2024)<br><strong>The cubic nonlinear Schrödinger equation with rough potential</strong><br><button class=copy-to-clipboard title="The cubic nonlinear Schrödinger equation with rough potential" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M12, 65M15, 35Q55, cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16772v1.pdf filename=2403.16772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the cubic nonlinear Schr"odinger equation with a spatially rough potential, a key equation in the mathematical setup for nonlinear Anderson localization. Our study comprises two main parts: new optimal results on the well-posedness analysis on the PDE level, and subsequently a new efficient numerical method, its convergence analysis and <b>simulations</b> that illustrate our analytical results. In the analysis part, our results focus on understanding how the regularity of the solution is influenced by the regularity of the potential, where we provide quantitative and explicit characterizations. Ill-posedness results are also established to demonstrate the sharpness of the obtained regularity characterizations and to indicate the minimum regularity required from the potential for the NLS to be solvable. Building upon the obtained regularity results, we design an appropriate numerical discretization for the model and establish its convergence with an optimal error bound. The numerical experiments in the end not only verify the theoretical regularity results, but also confirm the established convergence rate of the proposed scheme. Additionally, a comparison with other existing schemes is conducted to demonstrate the better accuracy of our new scheme in the case of a rough potential.</p></p class="citation"></blockquote><h3 id=55--318340-on-solution-of-tropical-discrete-best-approximation-problems-nikolai-krivulin-2024>(5/5 | 318/340) On solution of tropical discrete best approximation problems (Nikolai Krivulin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolai Krivulin. (2024)<br><strong>On solution of tropical discrete best approximation problems</strong><br><button class=copy-to-clipboard title="On solution of tropical discrete best approximation problems" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 15A80 (Primary), 90C24, 41A50, 41A65, 65D15 (Secondary), cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16337v1.pdf filename=2403.16337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a discrete best approximation problem formulated in the framework of tropical algebra, which deals with the theory and applications of algebraic systems with idempotent operations. Given a set of samples of input and output of an unknown function, the problem is to construct a generalized tropical Puiseux polynomial that best approximates the function in the sense of a tropical distance function. The construction of an approximate polynomial involves the evaluation of both unknown coefficient and exponent of each monomial in the polynomial. To solve the approximation problem, we first reduce the problem to an equation in unknown vector of coefficients, which is given by a matrix with entries parameterized by unknown exponents. We derive a best approximate solution of the equation, which yields both vector of coefficients and approximation error parameterized by the exponents. Optimal values of exponents are found by minimization of the approximation error, which is reduced to a minimization of a function of exponents over all partitions of a finite set. We solve this minimization problem in terms of max-plus algebra (where addition is defined as maximum and multiplication as arithmetic addition) by using a computational procedure based on the agglomerative <b>clustering</b> technique. This solution is extended to the minimization problem of finding optimal exponents in the polynomial in terms of max-algebra (where addition is defined as maximum). The results obtained are applied to develop new solutions for conventional problems of discrete best approximation of real functions by piecewise linear functions and piecewise Puiseux polynomials. We discuss computational complexity of the proposed solution and estimate upper bounds on the computational time. We demonstrate examples of approximation problems solved in terms of max-plus and max-algebra, and give graphical illustrations.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--319340-distributed-simulation-of-large-multi-body-systems-manas-kale-et-al-2024>(1/1 | 319/340) Distributed Simulation of Large Multi-body Systems (Manas Kale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manas Kale, Paul G. Kry. (2024)<br><strong>Distributed Simulation of Large Multi-body Systems</strong><br><button class=copy-to-clipboard title="Distributed Simulation of Large Multi-body Systems" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: I-3, cs-DC, cs-GR, cs.GR<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17261v1.pdf filename=2403.17261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a technique designed for parallelizing large rigid body <b>simulations,</b> capable of exploiting multiple CPU cores within a computer and across a network. Our approach can be applied to simulate both unilateral and bilateral constraints, requiring straightforward modifications to the underlying physics engine. Starting from an approximate partitioning, we identify interface bodies and add them to overlapping sets such that they are simulated by multiple workers. At each timestep, we blend the states of overlap bodies using weights based on <b>graph</b> geodesic distances within the constraint <b>graph.</b> The use of overlap <b>simulation</b> also allows us to perform load balancing using efficient local evaluations of the constraint <b>graph.</b> We demonstrate our technique&rsquo;s scalability and load-balancing capabilities using several large-scale scenes.</p></p class="citation"></blockquote><h2 id=csdc-4>cs.DC (4)</h2><h3 id=14--320340-a-unified-cpu-gpu-protocol-for-gnn-training-yi-chien-lin-et-al-2024>(1/4 | 320/340) A Unified CPU-GPU Protocol for GNN Training (Yi-Chien Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi-Chien Lin, Gangda Deng, Viktor Prasanna. (2024)<br><strong>A Unified CPU-GPU Protocol for GNN Training</strong><br><button class=copy-to-clipboard title="A Unified CPU-GPU Protocol for GNN Training" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17092v1.pdf filename=2403.17092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training a <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> model on large-scale <b>graphs</b> <b>involves</b> <b>a</b> high volume of data communication and computations. While state-of-the-art CPUs and GPUs feature high computing power, the Standard <b>GNN</b> training protocol adopted in existing <b>GNN</b> frameworks cannot efficiently utilize the platform resources. To this end, we propose a novel Unified CPU-GPU protocol that can improve the resource utilization of <b>GNN</b> training on a CPU-GPU platform. The Unified CPU-GPU protocol instantiates multiple <b>GNN</b> training processes in parallel on both the CPU and the GPU. By allocating training processes on the CPU to perform <b>GNN</b> training collaboratively with the GPU, the proposed protocol improves the platform resource utilization and reduces the CPU-GPU data transfer overhead. Since the performance of a CPU and a GPU varies, we develop a novel load balancer that balances the workload dynamically between CPUs and GPUs during runtime. We evaluate our protocol using two representative <b>GNN</b> sampling algorithms, with two widely-used <b>GNN</b> models, on three datasets. Compared with the standard training protocol adopted in the state-of-the-art <b>GNN</b> frameworks, our protocol effectively improves resource utilization and overall training time. On a platform where the GPU moderately outperforms the CPU, our protocol speeds up <b>GNN</b> training by up to 1.41x. On a platform where the GPU significantly outperforms the CPU, our protocol speeds up <b>GNN</b> training by up to 1.26x. Our protocol is open-sourced and can be seamlessly integrated into state-of-the-art <b>GNN</b> frameworks and accelerate <b>GNN</b> training. Our protocol particularly benefits those with limited GPU access due to its high demand.</p></p class="citation"></blockquote><h3 id=24--321340-lessons-learned-from-building-edge-software-system-testbeds-tobias-pfandzelter-et-al-2024>(2/4 | 321/340) Lessons Learned from Building Edge Software System Testbeds (Tobias Pfandzelter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Pfandzelter, David Bermbach. (2024)<br><strong>Lessons Learned from Building Edge Software System Testbeds</strong><br><button class=copy-to-clipboard title="Lessons Learned from Building Edge Software System Testbeds" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16869v1.pdf filename=2403.16869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Edge computing requires the complex software interaction of geo-distributed, heterogeneous components. The growing research and industry interest in edge computing software systems has necessitated exploring ways of testing and evaluating edge software at scale without relying on physical infrastructure. Beyond <b>simulation,</b> virtual testbeds that emulate edge infrastructure can provide a cost-efficient yet realistic environment to evaluate edge software. In this experience paper, we share lessons learned from building a total of five edge software testbeds. We describe pitfalls in architecture and development as well as experiences from having students use our testbed tooling in distributed systems prototyping classes. While we remain confident that building custom testbed tooling is the right approach for edge computing researchers and practitioners alike, we hope this paper allows others to avoid common mistakes and benefit from our experience.</p></p class="citation"></blockquote><h3 id=34--322340-union-an-automatic-workload-manager-for-accelerating-network-simulation-xin-wang-et-al-2024>(3/4 | 322/340) Union: An Automatic Workload Manager for Accelerating Network Simulation (Xin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Wang, Misbah Mubarak, Yao Kang, Robert B. Ross, Zhiling Lan. (2024)<br><strong>Union: An Automatic Workload Manager for Accelerating Network Simulation</strong><br><button class=copy-to-clipboard title="Union: An Automatic Workload Manager for Accelerating Network Simulation" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17036v1.pdf filename=2403.17036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid growth of the machine learning applications, the workloads of future HPC systems are anticipated to be a mix of scientific <b>simulation,</b> big data analytics, and machine learning applications. <b>Simulation</b> is a great research vehicle to understand the performance implications of co-running scientific applications with big data and machine learning workloads on large-scale systems. In this paper, we present Union, a workload manager that provides an automatic framework to facilitate hybrid workload <b>simulation</b> in CODES. Furthermore, we use Union, along with CODES, to investigate various hybrid workloads composed of traditional <b>simulation</b> applications and emerging learning applications on two dragonfly systems. The experiment results show that both message latency and communication time are important performance metrics to evaluate network interference. Network interference on HPC applications is more reflected by the message latency variation, whereas ML application performance depends more on the communication time.</p></p class="citation"></blockquote><h3 id=44--323340-colonyos----a-meta-operating-system-for-distributed-computing-across-heterogeneous-platform-johan-kristiansson-2024>(4/4 | 323/340) ColonyOS &ndash; A Meta-Operating System for Distributed Computing Across Heterogeneous Platform (Johan Kristiansson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johan Kristiansson. (2024)<br><strong>ColonyOS &ndash; A Meta-Operating System for Distributed Computing Across Heterogeneous Platform</strong><br><button class=copy-to-clipboard title="ColonyOS -- A Meta-Operating System for Distributed Computing Across Heterogeneous Platform" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16486v1.pdf filename=2403.16486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents ColonyOS, an open-source meta-operating system designed to improve integration and utilization of diverse computing platforms, including IoT, edge, cloud, and HPC. Operating as an overlay, ColonyOS can interface with a wide range of computing environments, fostering creation of so-called compute continuums. This makes it possible to develop AI workflows and applications that can operate across platforms. At its core, ColonyOS consists of distributed executors that integrate with various underlying platforms based on a distributed microservice architecture. These executors collectively form a colony, serving as a unified computing unit. To enable secure integration of various platforms, each colony is provisioned with precisely the resources needed, and all communication is confined within the colony governed by a strict <b>zero-trust</b> <b>security</b> protocol. Interaction with ColonyOS is done by submitting functional meta-descriptions of computational tasks, called function specifications. These are sent to a Colonies server, which acts as intermediary between applications and the executors. Upon assignment, an executor interprets the meta-description and translates it into an executable format, e.g. a Kubernetes deployment description, a Slurm script, or a direct function call within the executor. Furthermore, a built-in meta-file system enables data synchronization directives to be included in meta-descriptions, enabling seamless data management across platforms. Ultimately, ColonyOS paves the way for development of hyper-distributed applications and workflows, which can seamlessly operate in a computing continuum. The paper describes design principles and implementation details of ColonyOS.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--324340-graph-bayesian-optimization-for-multiplex-influence-maximization-zirui-yuan-et-al-2024>(1/2 | 324/340) Graph Bayesian Optimization for Multiplex Influence Maximization (Zirui Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Yuan, Minglai Shao, Zhiqian Chen. (2024)<br><strong>Graph Bayesian Optimization for Multiplex Influence Maximization</strong><br><button class=copy-to-clipboard title="Graph Bayesian Optimization for Multiplex Influence Maximization" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI<br>Keyword Score: 23<br>Keywords: Diffusion Model, Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18866v1.pdf filename=2403.18866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Influence maximization (IM) is the problem of identifying a limited number of initial influential users within a social network to maximize the number of influenced users. However, previous research has mostly focused on individual information propagation, neglecting the simultaneous and interactive dissemination of multiple information items. In reality, when users encounter a piece of information, such as a smartphone product, they often associate it with related products in their minds, such as earphones or computers from the same brand. Additionally, information platforms frequently recommend related content to users, amplifying this cascading effect and leading to multiplex influence <b>diffusion.</b> <b>This</b> paper first formulates the Multiplex Influence Maximization (Multi-IM) problem using multiplex <b>diffusion</b> <b>models</b> with an information association mechanism. In this problem, the seed set is a combination of influential users and information. To effectively manage the combinatorial complexity, we propose <b>Graph</b> Bayesian Optimization for Multi-IM (GBIM). The multiplex <b>diffusion</b> <b>process</b> is thoroughly investigated using a highly effective global kernelized attention <b>message-passing</b> module. This module, in conjunction with Bayesian linear regression (BLR), produces a scalable surrogate model. A data acquisition module incorporating the exploration-exploitation trade-off is developed to optimize the seed set further. Extensive experiments on synthetic and real-world datasets have proven our proposed framework effective. The code is available at <a href=https://github.com/zirui-yuan/GBIM>https://github.com/zirui-yuan/GBIM</a>.</p></p class="citation"></blockquote><h3 id=22--325340-a-recommender-network-perspective-on-the-informational-value-of-critics-and-crowds-pantelis-p-analytis-et-al-2024>(2/2 | 325/340) A recommender network perspective on the informational value of critics and crowds (Pantelis P. Analytis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pantelis P. Analytis, Karthikeya Kaushik, Stefan Herzog, Bahador Bahrami, Ophelia Deroy. (2024)<br><strong>A recommender network perspective on the informational value of critics and crowds</strong><br><button class=copy-to-clipboard title="A recommender network perspective on the informational value of critics and crowds" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.18868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.18868v1.pdf filename=2403.18868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How do the ratings of critics and amateurs compare and how should they be combined? Previous research has produced mixed results about the first question, while the second remains unanswered. We have created a new, unique dataset, with wine ratings from critics and amateurs, and simulated a <b>recommender</b> <b>system</b> using the k-nearest-neighbor algorithm. We then formalized the advice seeking network spanned by that algorithm and studied people&rsquo;s relative influence. We find that critics are more consistent than amateurs, and thus their advice is more predictive than advice from amateurs. Getting advice from both groups can further boost performance. Our network theoretic approach allows us to identify influential critics, talented amateurs, and the information flow between groups. Our results provide evidence about the informational function of critics, while our framework is broadly applicable and can be leveraged to devise good decision strategies and more transparent <b>recommender</b> <b>systems.</b></p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--326340-antigen-specific-antibody-design-via-direct-energy-based-preference-optimization-xiangxin-zhou-et-al-2024>(1/1 | 326/340) Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization (Xiangxin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangxin Zhou, Dongyu Xue, Ruizhe Chen, Zaixiang Zheng, Liang Wang, Quanquan Gu. (2024)<br><strong>Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization</strong><br><button class=copy-to-clipboard title="Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 23<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16576v1.pdf filename=2403.16576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional <b>diffusion</b> <b>model</b> that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves <b>fine-tuning</b> the pre-trained <b>diffusion</b> <b>model</b> using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD <b>benchmark</b> show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity, demonstrating the superiority of our approach.</p></p class="citation"></blockquote><h2 id=csos-1>cs.OS (1)</h2><h3 id=11--327340-aios-llm-agent-operating-system-kai-mei-et-al-2024>(1/1 | 327/340) AIOS: LLM Agent Operating System (Kai Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang. (2024)<br><strong>AIOS: LLM Agent Operating System</strong><br><button class=copy-to-clipboard title="AIOS: LLM Agent Operating System" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.OS<br>Categories: cs-AI, cs-CL, cs-OS, cs.OS<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16971v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16971v2.pdf filename=2403.16971v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration and deployment of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the <b>LLM,</b> the difficulties in maintaining context during interactions between agent and <b>LLM,</b> and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an <b>LLM</b> agent operating system, which embeds <b>large</b> <b>language</b> <b>model</b> into operating systems (OS) as the brain of the OS, enabling an operating system &ldquo;with soul&rdquo; &ndash; an important step towards AGI. Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, and maintain access control for agents. We present the architecture of such an operating system, outline the core challenges it aims to resolve, and provide the basic design and implementation of the AIOS. Our experiments on concurrent execution of multiple agents demonstrate the reliability and efficiency of our AIOS modules. Through this, we aim to not only improve the performance and efficiency of <b>LLM</b> agents but also to pioneer for better development and deployment of the AIOS ecosystem in the future. The project is open-source at <a href=https://github.com/agiresearch/AIOS>https://github.com/agiresearch/AIOS</a>.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--328340-backpropagation-through-space-time-and-the-brain-benjamin-ellenberger-et-al-2024>(1/1 | 328/340) Backpropagation through space, time, and the brain (Benjamin Ellenberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Ellenberger, Paul Haider, Jakob Jordan, Kevin Max, Ismael Jaras, Laura Kriener, Federico Benitez, Mihai A. Petrovici. (2024)<br><strong>Backpropagation through space, time, and the brain</strong><br><button class=copy-to-clipboard title="Backpropagation through space, time, and the brain" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-AI, cs-LG, cs-NE, eess-SP, q-bio-NC, q-bio.NC<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16933v1.pdf filename=2403.16933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems &ndash; whether biological or artificial &ndash; are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of BPTT in deep cortical networks with <b>continuous-time</b> <b>neuronal</b> dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, performing an effective spatiotemporal <b>convolution.</b> For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint states necessary for useful parameter updates.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-2>physics.flu-dyn (2)</h2><h3 id=12--329340-entropy-conservative-high-order-methods-for-high-enthalpy-gas-flows-georgii-oblapenko-et-al-2024>(1/2 | 329/340) Entropy-conservative high-order methods for high-enthalpy gas flows (Georgii Oblapenko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgii Oblapenko, Manuel Torrilhon. (2024)<br><strong>Entropy-conservative high-order methods for high-enthalpy gas flows</strong><br><button class=copy-to-clipboard title="Entropy-conservative high-order methods for high-enthalpy gas flows" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16882v1.pdf filename=2403.16882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A framework for numerical evaluation of entropy-conservative volume fluxes in gas flows with internal energies is developed, for use with high-order discretization methods. The novelty of the approach lies in the ability to use arbitrary expressions for the internal degrees of freedom of the constituent gas species. The developed approach is implemented in an open-source discontinuous Galerkin code for solving hyperbolic equations. Numerical <b>simulations</b> are carried out for several model 2-D flows and the results are compared to those obtained with the finite volume-based solver DLR TAU.</p></p class="citation"></blockquote><h3 id=22--330340-instantaneous-visual-analysis-of-blood-flow-in-stenoses-using-morphological-similarity-pepe-eulzer-et-al-2024>(2/2 | 330/340) Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity (Pepe Eulzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pepe Eulzer, Kevin Richter, Anna Hundertmark, Ralf Wickenhöfer, Carsten M. Klingner, Kai Lawonn. (2024)<br><strong>Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity</strong><br><button class=copy-to-clipboard title="Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity" index=330>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-330 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-GR, cs-HC, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16653v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16653v1.pdf filename=2403.16653v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of computational fluid dynamics (CFD) enabled the <b>simulation</b> of intricate transport processes, including flow in physiological structures, such as blood vessels. While these so-called hemodynamic <b>simulations</b> offer groundbreaking opportunities to solve problems at the clinical forefront, a successful translation of CFD to clinical decision-making is challenging. Hemodynamic <b>simulations</b> are intrinsically complex, time-consuming, and resource-intensive, which conflicts with the time-sensitive nature of clinical workflows and the fact that hospitals usually do not have the necessary resources or infrastructure to support CFD <b>simulations.</b> To address these transfer challenges, we propose a novel visualization system which enables instant flow exploration without performing on-site <b>simulation.</b> To gain insights into the viability of the approach, we focus on hemodynamic <b>simulations</b> of the carotid bifurcation, which is a highly relevant arterial subtree in stroke diagnostics and prevention. We created an initial database of 120 high-resolution carotid bifurcation flow models and developed a set of similarity metrics used to place a new carotid surface model into a neighborhood of simulated cases with the highest geometric similarity. The neighborhood can be immediately explored and the flow fields analyzed. We found that if the artery models are similar enough in the regions of interest, a new <b>simulation</b> leads to coinciding results, allowing the user to circumvent individual flow <b>simulations.</b> We conclude that similarity-based visual analysis is a promising approach toward the usability of CFD in medical practice.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=11--331340-multilevel-modeling-as-a-methodology-for-the-simulation-of-human-mobility-luca-serena-et-al-2024>(1/1 | 331/340) Multilevel Modeling as a Methodology for the Simulation of Human Mobility (Luca Serena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Serena, Moreno Marzolla, Gabriele D&rsquo;Angelo, Stefano Ferretti. (2024)<br><strong>Multilevel Modeling as a Methodology for the Simulation of Human Mobility</strong><br><button class=copy-to-clipboard title="Multilevel Modeling as a Methodology for the Simulation of Human Mobility" index=331>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-331 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: 68U99, I-6-5, cs-PF, cs.PF, physics-soc-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16745v1.pdf filename=2403.16745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multilevel modeling is increasingly relevant in the context of modelling and <b>simulation</b> since it leads to several potential benefits, such as software reuse and integration, the split of semantically separated levels into sub-models, the possibility to employ different levels of detail, and the potential for parallel execution. The coupling that inevitably exists between the sub-models, however, implies the need for maintaining consistency between the various components, more so when different <b>simulation</b> paradigms are employed (e.g., sequential vs parallel, discrete vs continuous). In this paper we argue that multilevel modelling is well suited for the <b>simulation</b> of human mobility, since it naturally leads to the decomposition of the model into two layers, the &ldquo;micro&rdquo; and &ldquo;macro&rdquo; layer, where individual entities (micro) and long-range interactions (macro) are described. In this paper we investigate the challenges of multilevel modeling, and describe some preliminary results using prototype implementations of multilayer simulators in the context of epidemic diffusion and vehicle pollution.</p></p class="citation"></blockquote><h2 id=mathag-1>math.AG (1)</h2><h3 id=11--332340-machine-learning-for-moduli-space-of-genus-two-curves-and-an-application-to-post-quantum-cryptography-elira-shaska-et-al-2024>(1/1 | 332/340) Machine learning for moduli space of genus two curves and an application to post-quantum cryptography (Elira Shaska et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elira Shaska, Tony Shaska. (2024)<br><strong>Machine learning for moduli space of genus two curves and an application to post-quantum cryptography</strong><br><button class=copy-to-clipboard title="Machine learning for moduli space of genus two curves and an application to post-quantum cryptography" index=332>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-332 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AG<br>Categories: 94A60, 68T20, cs-CR, math-AG, math.AG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17250v1.pdf filename=2403.17250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We use machine learning to study the locus ${\mathcal L}_n$ of genus two curves with $(n, n)$-split Jacobian. More precisely we design a <b>transformer</b> model which given values for the Igusa invariants determines if the corresponding genus two curve is in the locus ${\mathcal L}_n$, for $n=2, 3, 5, 7$. Such curves are important in isogeny based cryptography. During this study we discover that there are no rational points ${\mathfrak p} \in {\mathcal L}_n$ with weighted moduli height $\leq 2$ in any of ${\mathcal L}_2$, ${\mathcal L}_3$, and ${\mathcal L}_5$. This extends on previous work of the authors to use machine learning methods to study the moduli space of genus 2 algebraic curves.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--333340-an-equilibrium-analysis-of-the-arad-rubinstein-game-christian-ewerhart-et-al-2024>(1/1 | 333/340) An Equilibrium Analysis of the Arad-Rubinstein Game (Christian Ewerhart et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Ewerhart, Stanisław Kaźmierowski. (2024)<br><strong>An Equilibrium Analysis of the Arad-Rubinstein Game</strong><br><button class=copy-to-clipboard title="An Equilibrium Analysis of the Arad-Rubinstein Game" index=333>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-333 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: 91A05, J-4, cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17139v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17139v2.pdf filename=2403.17139v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Colonel Blotto games with discrete strategy spaces effectively illustrate the intricate nature of multidimensional strategic <b>reasoning.</b> This paper studies the equilibrium set of such games where, in line with prior experimental work, the tie-breaking rule is allowed to be flexible. We begin by pointing out that equilibrium constructions known from the literature extend to our class of games. However, we also note that irrespective of the tie-breaking rule, the equilibrium set is excessively large. Specifically, any pure strategy that allocates at most twice the fair share to each battlefield is used with positive probability in some equilibrium. Furthermore, refinements based on the elimination of weakly dominated strategies prove ineffective. To derive specific predictions amid this multiplicity, we compute strategies resulting from long-run adaptive learning.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--334340-constant-selection-evolutionary-dynamics-on-weighted-networks-jnanajyoti-bhaumik-et-al-2024>(1/1 | 334/340) Constant-selection evolutionary dynamics on weighted networks (Jnanajyoti Bhaumik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jnanajyoti Bhaumik, Naoki Masuda. (2024)<br><strong>Constant-selection evolutionary dynamics on weighted networks</strong><br><button class=copy-to-clipboard title="Constant-selection evolutionary dynamics on weighted networks" index=334>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-334 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-SI, math-PR, physics-soc-ph, physics.soc-ph, q-bio-PE<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17208v1.pdf filename=2403.17208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The population structure often impacts evolutionary dynamics. In constant-selection evolutionary dynamics between two types, amplifiers of selection are networks that promote the fitter mutant to take over the entire population, and suppressors of selection do the opposite. It has been shown that most undirected and unweighted networks are amplifiers of selection under a common updating rule and initial condition. Here, we extensively investigate how edge weights influence selection on undirected networks. We show that random edge weights make small networks less amplifying than the corresponding unweighted networks in a majority of cases and also make them suppressors of selection (i.e., less amplifying than the complete <b>graph,</b> or equivalently, the Moran process) in many cases. Qualitatively, the same result holds true for larger empirical networks. These results suggest that amplifiers of selection are not as common for weighted networks as for unweighted counterparts.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--335340-free-sets-in-planar-graphs-history-and-applications-vida-dujmovic-et-al-2024>(1/1 | 335/340) Free Sets in Planar Graphs: History and Applications (Vida Dujmovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vida Dujmovic, Pat Morin. (2024)<br><strong>Free Sets in Planar Graphs: History and Applications</strong><br><button class=copy-to-clipboard title="Free Sets in Planar Graphs: History and Applications" index=335>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-335 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DM, cs.CG, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17090v1.pdf filename=2403.17090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A subset $S$ of vertices in a planar <b>graph</b> $G$ is a free set if, for every set $P$ of $|S|$ points in the plane, there exists a straight-line crossing-free drawing of $G$ in which vertices of $S$ are mapped to distinct points in $P$. In this survey, we review - several equivalent definitions of free sets, - results on the existence of large free sets in planar <b>graphs</b> and subclasses of planar <b>graphs,</b> - and applications of free sets in <b>graph</b> drawing. The survey concludes with a list of open problems in this still very active research area.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--336340-ggdminer----discovery-of-graph-generating-dependencies-for-graph-data-profiling-larissa-c-shimomura-et-al-2024>(1/1 | 336/340) GGDMiner &ndash; Discovery of Graph Generating Dependencies for Graph Data Profiling (Larissa C. Shimomura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Larissa C. Shimomura, Nikolay Yakovets, George Fletcher. (2024)<br><strong>GGDMiner &ndash; Discovery of Graph Generating Dependencies for Graph Data Profiling</strong><br><button class=copy-to-clipboard title="GGDMiner -- Discovery of Graph Generating Dependencies for Graph Data Profiling" index=336>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-336 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.17082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.17082v1.pdf filename=2403.17082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing use of <b>graph-structured</b> data, there is also increasing interest in investigating <b>graph</b> data dependencies and their applications, e.g., in <b>graph</b> data profiling. <b>Graph</b> Generating Dependencies (GGDs) are a class of dependencies for property <b>graphs</b> that can express the relation between different <b>graph</b> patterns and constraints based on their attribute similarities. Rich syntax and semantics of GGDs make them a good candidate for <b>graph</b> data profiling. Nonetheless, GGDs are difficult to define manually, especially when there are no data experts available. In this paper, we propose GGDMiner, a framework for discovering approximate GGDs from <b>graph</b> data automatically, with the intention of profiling <b>graph</b> data through GGDs for the user. GGDMiner has three main steps: (1) pre-processing, (2) candidate generation, and, (3) GGD extraction. To optimize memory consumption and execution time, GGDMiner uses a factorized representation of each discovered <b>graph</b> pattern, called Answer <b>Graph.</b> Our results show that the discovered set of GGDs can give an overview about the input <b>graph,</b> both schema level information and also correlations between the <b>graph</b> patterns and attributes.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--337340-robust-filter-design-for-graph-signals-lucia-testa-et-al-2024>(1/1 | 337/340) Robust Filter Design for Graph Signals (Lucia Testa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucia Testa, Stefania Sardellitti, Sergio Barbarossa. (2024)<br><strong>Robust Filter Design for Graph Signals</strong><br><button class=copy-to-clipboard title="Robust Filter Design for Graph Signals" index=337>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-337 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16983v1.pdf filename=2403.16983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our goal in this paper is the robust design of filters acting on signals observed over <b>graphs</b> subject to small perturbations of their edges. The focus is on developing a method to identify spectral and polynomial <b>graph</b> filters that can adapt to the perturbations in the underlying <b>graph</b> structure while ensuring the filters adhere to the desired spectral mask. To address this, we propose a novel approach that leverages approximate closed-form expressions for the perturbed eigendecomposition of the Laplacian matrix associated with the nominal topology. Furthermore, when dealing with noisy input signals for <b>graph</b> filters, we propose a strategy for designing FIR filters that jointly minimize the approximation error with respect to the ideal filter and the estimation error of the output, ensuring robustness against both <b>graph</b> perturbations and noise. Numerical results validate the effectiveness of our proposed strategies, highlighting their capability to efficiently manage perturbations and noise.</p></p class="citation"></blockquote><h2 id=mathmg-1>math.MG (1)</h2><h3 id=11--338340-optimality-of-spherical-codes-via-exact-semidefinite-programming-bounds-henry-cohn-et-al-2024>(1/1 | 338/340) Optimality of spherical codes via exact semidefinite programming bounds (Henry Cohn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Henry Cohn, David de Laat, Nando Leijenhorst. (2024)<br><strong>Optimality of spherical codes via exact semidefinite programming bounds</strong><br><button class=copy-to-clipboard title="Optimality of spherical codes via exact semidefinite programming bounds" index=338>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-338 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.MG<br>Categories: cs-IT, math-IT, math-MG, math-OC, math.MG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16874v1.pdf filename=2403.16874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that the spectral embeddings of all known triangle-free strongly regular <b>graphs</b> are optimal spherical codes (the new cases are $56$ points in $20$ dimensions, $50$ points in $21$ dimensions, and $77$ points in $21$ dimensions), as are certain mutually unbiased basis arrangements constructed using Kerdock codes in up to $1024$ dimensions (namely, $2^{4k} + 2^{2k+1}$ points in $2^{2k}$ dimensions for $2 \le k \le 5$). As a consequence of the latter, we obtain optimality of the Kerdock binary codes of block length $64$, $256$, and $1024$, as well as uniqueness for block length $64$. We also prove universal optimality for $288$ points on a sphere in $16$ dimensions. To prove these results, we use three-point semidefinite programming bounds, for which only a few sharp cases were known previously. To obtain rigorous results, we develop improved techniques for rounding approximate solutions of semidefinite programs to produce exact optimal solutions.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--339340-high-temperature-gibbs-states-are-unentangled-and-efficiently-preparable-ainesh-bakshi-et-al-2024>(1/1 | 339/340) High-Temperature Gibbs States are Unentangled and Efficiently Preparable (Ainesh Bakshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ainesh Bakshi, Allen Liu, Ankur Moitra, Ewin Tang. (2024)<br><strong>High-Temperature Gibbs States are Unentangled and Efficiently Preparable</strong><br><button class=copy-to-clipboard title="High-Temperature Gibbs States are Unentangled and Efficiently Preparable" index=339>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-339 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, math-MP, math-ph, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16850v1.pdf filename=2403.16850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that thermal states of local Hamiltonians are separable above a constant temperature. Specifically, for a local Hamiltonian $H$ on a <b>graph</b> with degree $\mathfrak{d}$, its Gibbs state at inverse temperature $\beta$, denoted by $\rho =e^{-\beta H}/ \textrm{tr}(e^{-\beta H})$, is a classical distribution over product states for all $\beta &lt; 1/(c\mathfrak{d})$, where $c$ is a constant. This sudden death of thermal entanglement upends conventional wisdom about the presence of short-range quantum correlations in Gibbs states. Moreover, we show that we can efficiently sample from the distribution over product states. In particular, for any $\beta &lt; 1/( c \mathfrak{d}^3)$, we can prepare a state $\epsilon$-close to $\rho$ in trace distance with a depth-one quantum circuit and $\textrm{poly}(n) \log(1/\epsilon)$ classical overhead. A priori the task of preparing a Gibbs state is a natural candidate for achieving super-polynomial quantum speedups, but our results rule out this possibility above a fixed constant temperature.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--340340-presenting-interval-pomsets-with-interfaces-amazigh-amrane-et-al-2024>(1/1 | 340/340) Presenting Interval Pomsets with Interfaces (Amazigh Amrane et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amazigh Amrane, Hugo Bazille, Emily Clement, Uli Fahrenberg, Krzysztof Ziemianski. (2024)<br><strong>Presenting Interval Pomsets with Interfaces</strong><br><button class=copy-to-clipboard title="Presenting Interval Pomsets with Interfaces" index=340>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-340 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs.FL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.16626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.16626v1.pdf filename=2403.16626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interval-order partially ordered multisets with interfaces (ipomsets) have shown to be a versatile model for executions of concurrent systems in which both precedence and concurrency need to be taken into account. In this paper, we develop a presentation of ipomsets as generated by a <b>graph</b> of certain discrete ipomsets (starters and terminators) under the relation which composes subsequent starters and subsequent terminators. Using this presentation, we show that also subsumptions are generated by elementary relations. We develop a similar correspondence on the automata side, relating higher-dimensional automata, which generate ipomsets, and ST-automata, which generate step sequences, and their respective languages.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.26</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.28</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-56>cs.CL (56)</a><ul><li><a href=#156--1340-textitlinkprompt-natural-and-universal-adversarial-attacks-on-prompt-based-language-models-yue-xu-et-al-2024>(1/56 | 1/340) $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models (Yue Xu et al., 2024)</a></li><li><a href=#256--2340-the-strong-pull-of-prior-knowledge-in-large-language-models-and-its-impact-on-emotion-recognition-georgios-chochlakis-et-al-2024>(2/56 | 2/340) The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition (Georgios Chochlakis et al., 2024)</a></li><li><a href=#356--3340-iterative-refinement-of-project-level-code-context-for-precise-code-generation-with-compiler-feedback-zhangqian-bi-et-al-2024>(3/56 | 3/340) Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback (Zhangqian Bi et al., 2024)</a></li><li><a href=#456--4340-llms-are-few-shot-in-context-low-resource-language-learners-samuel-cahyawijaya-et-al-2024>(4/56 | 4/340) LLMs Are Few-Shot In-Context Low-Resource Language Learners (Samuel Cahyawijaya et al., 2024)</a></li><li><a href=#556--5340-a-hybrid-approach-to-aspect-based-sentiment-analysis-using-transfer-learning-gaurav-negi-et-al-2024>(5/56 | 5/340) A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning (Gaurav Negi et al., 2024)</a></li><li><a href=#656--6340-outcome-constrained-large-language-models-for-countering-hate-speech-lingzi-hong-et-al-2024>(6/56 | 6/340) Outcome-Constrained Large Language Models for Countering Hate Speech (Lingzi Hong et al., 2024)</a></li><li><a href=#756--7340-attribute-first-then-generate-locally-attributable-grounded-text-generation-aviv-slobodkin-et-al-2024>(7/56 | 7/340) Attribute First, then Generate: Locally-attributable Grounded Text Generation (Aviv Slobodkin et al., 2024)</a></li><li><a href=#856--8340-grammatical-vs-spelling-error-correction-an-investigation-into-the-responsiveness-of-transformer-based-language-models-using-bart-and-marianmt-rohit-raju-et-al-2024>(8/56 | 8/340) Grammatical vs Spelling Error Correction: An Investigation into the Responsiveness of Transformer-based Language Models using BART and MarianMT (Rohit Raju et al., 2024)</a></li><li><a href=#956--9340-lara-linguistic-adaptive-retrieval-augmented-llms-for-multi-turn-intent-classification-liu-junhua-et-al-2024>(9/56 | 9/340) LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification (Liu Junhua et al., 2024)</a></li><li><a href=#1056--10340-efficient-information-extraction-in-few-shot-relation-classification-through-contrastive-representation-learning-philipp-borchert-et-al-2024>(10/56 | 10/340) Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning (Philipp Borchert et al., 2024)</a></li><li><a href=#1156--11340-instupr--instruction-based-unsupervised-passage-reranking-with-large-language-models-chao-wei-huang-et-al-2024>(11/56 | 11/340) InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models (Chao-Wei Huang et al., 2024)</a></li><li><a href=#1256--12340-a-comparison-of-human-gpt-35-and-gpt-4-performance-in-a-university-level-coding-course-will-yeadon-et-al-2024>(12/56 | 12/340) A comparison of Human, GPT-3.5, and GPT-4 Performance in a University-Level Coding Course (Will Yeadon et al., 2024)</a></li><li><a href=#1356--13340-a-comparative-analysis-of-embedding-models-for-patent-similarity-grazia-sveva-ascione-et-al-2024>(13/56 | 13/340) A comparative analysis of embedding models for patent similarity (Grazia Sveva Ascione et al., 2024)</a></li><li><a href=#1456--14340-visually-guided-generative-text-layout-pre-training-for-document-intelligence-zhiming-mao-et-al-2024>(14/56 | 14/340) Visually Guided Generative Text-Layout Pre-training for Document Intelligence (Zhiming Mao et al., 2024)</a></li><li><a href=#1556--15340-metaaligner-conditional-weak-to-strong-correction-for-generalizable-multi-objective-alignment-of-language-models-kailai-yang-et-al-2024>(15/56 | 15/340) MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models (Kailai Yang et al., 2024)</a></li><li><a href=#1656--16340-cross-lingual-contextualized-phrase-retrieval-huayang-li-et-al-2024>(16/56 | 16/340) Cross-lingual Contextualized Phrase Retrieval (Huayang Li et al., 2024)</a></li><li><a href=#1756--17340-few-shot-named-entity-recognition-via-superposition-concept-discrimination-jiawei-chen-et-al-2024>(17/56 | 17/340) Few-shot Named Entity Recognition via Superposition Concept Discrimination (Jiawei Chen et al., 2024)</a></li><li><a href=#1856--18340-kit-19-a-comprehensive-korean-instruction-toolkit-on-19-tasks-for-fine-tuning-korean-large-language-models-dongjun-jang-et-al-2024>(18/56 | 18/340) KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for Fine-Tuning Korean Large Language Models (Dongjun Jang et al., 2024)</a></li><li><a href=#1956--19340-ontology-completion-with-natural-language-inference-and-concept-embeddings-an-analysis-na-li-et-al-2024>(19/56 | 19/340) Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis (Na Li et al., 2024)</a></li><li><a href=#2056--20340-evaluating-shortest-edit-script-methods-for-contextual-lemmatization-olia-toporkov-et-al-2024>(20/56 | 20/340) Evaluating Shortest Edit Script Methods for Contextual Lemmatization (Olia Toporkov et al., 2024)</a></li><li><a href=#2156--21340-synthetic-data-generation-and-joint-learning-for-robust-code-mixed-translation-kartik-et-al-2024>(21/56 | 21/340) Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation (Kartik et al., 2024)</a></li><li><a href=#2256--22340-can-large-language-models-or-humans-distill-text-nicolas-audinet-de-pieuchon-et-al-2024>(22/56 | 22/340) Can Large Language Models (or Humans) Distill Text? (Nicolas Audinet de Pieuchon et al., 2024)</a></li><li><a href=#2356--23340-is-there-a-one-model-fits-all-approach-to-information-extraction-revisiting-task-definition-biases-wenhao-huang-et-al-2024>(23/56 | 23/340) Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases (Wenhao Huang et al., 2024)</a></li><li><a href=#2456--24340-splice-a-singleton-enhanced-pipeline-for-coreference-resolution-yilun-zhu-et-al-2024>(24/56 | 24/340) SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution (Yilun Zhu et al., 2024)</a></li><li><a href=#2556--25340-nsina-a-news-corpus-for-sinhala-hansi-hettiarachchi-et-al-2024>(25/56 | 25/340) NSINA: A News Corpus for Sinhala (Hansi Hettiarachchi et al., 2024)</a></li><li><a href=#2656--26340-can-machine-translation-bridge-multilingual-pretraining-and-cross-lingual-transfer-learning-shaoxiong-ji-et-al-2024>(26/56 | 26/340) Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning? (Shaoxiong Ji et al., 2024)</a></li><li><a href=#2756--27340-semantically-enriched-cross-lingual-sentence-embeddings-for-crisis-related-social-media-texts-rabindra-lamsal-et-al-2024>(27/56 | 27/340) Semantically Enriched Cross-Lingual Sentence Embeddings for Crisis-related Social Media Texts (Rabindra Lamsal et al., 2024)</a></li><li><a href=#2856--28340-a-study-on-how-attention-scores-in-the-bert-model-are-aware-of-lexical-categories-in-syntactic-and-semantic-tasks-on-the-glue-benchmark-dongjun-jang-et-al-2024>(28/56 | 28/340) A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark (Dongjun Jang et al., 2024)</a></li><li><a href=#2956--29340-towards-automatic-evaluation-for-llms-clinical-capabilities-metric-data-and-algorithm-lei-liu-et-al-2024>(29/56 | 29/340) Towards Automatic Evaluation for LLMs&rsquo; Clinical Capabilities: Metric, Data, and Algorithm (Lei Liu et al., 2024)</a></li><li><a href=#3056--30340-language-rectified-flow-advancing-diffusion-language-generation-with-probabilistic-flows-shujian-zhang-et-al-2024>(30/56 | 30/340) Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows (Shujian Zhang et al., 2024)</a></li><li><a href=#3156--31340-aligning-with-human-judgement-the-role-of-pairwise-preference-in-large-language-model-evaluators-yinhong-liu-et-al-2024>(31/56 | 31/340) Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators (Yinhong Liu et al., 2024)</a></li><li><a href=#3256--32340-toxcl-a-unified-framework-for-toxic-speech-detection-and-explanation-nhat-m-hoang-et-al-2024>(32/56 | 32/340) ToXCL: A Unified Framework for Toxic Speech Detection and Explanation (Nhat M. Hoang et al., 2024)</a></li><li><a href=#3356--33340-ru22fact-optimizing-evidence-for-multilingual-explainable-fact-checking-on-russia-ukraine-conflict-yirong-zeng-et-al-2024>(33/56 | 33/340) RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict (Yirong Zeng et al., 2024)</a></li><li><a href=#3456--34340-if-clip-could-talk-understanding-vision-language-model-representations-through-their-preferred-concept-descriptions-reza-esfandiarpoor-et-al-2024>(34/56 | 34/340) If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions (Reza Esfandiarpoor et al., 2024)</a></li><li><a href=#3556--35340-enhanced-facet-generation-with-llm-editing-joosung-lee-et-al-2024>(35/56 | 35/340) Enhanced Facet Generation with LLM Editing (Joosung Lee et al., 2024)</a></li><li><a href=#3656--36340-codes-natural-language-to-code-repository-via-multi-layer-sketch-daoguang-zan-et-al-2024>(36/56 | 36/340) CodeS: Natural Language to Code Repository via Multi-Layer Sketch (Daoguang Zan et al., 2024)</a></li><li><a href=#3756--37340-procqa-a-large-scale-community-based-programming-question-answering-dataset-for-code-search-zehan-li-et-al-2024>(37/56 | 37/340) ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search (Zehan Li et al., 2024)</a></li><li><a href=#3856--38340-the-role-of-n-gram-smoothing-in-the-age-of-neural-networks-luca-malagutti-et-al-2024>(38/56 | 38/340) The Role of $n$-gram Smoothing in the Age of Neural Networks (Luca Malagutti et al., 2024)</a></li><li><a href=#3956--39340-extracting-social-support-and-social-isolation-information-from-clinical-psychiatry-notes-comparing-a-rule-based-nlp-system-and-a-large-language-model-braja-gopal-patra-et-al-2024>(39/56 | 39/340) Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model (Braja Gopal Patra et al., 2024)</a></li><li><a href=#4056--40340-gpt-4-understands-discourse-at-least-as-well-as-humans-do-thomas-shultz-et-al-2024>(40/56 | 40/340) GPT-4 Understands Discourse at Least as Well as Humans Do (Thomas Shultz et al., 2024)</a></li><li><a href=#4156--41340-task-agnostic-detector-for-insertion-based-backdoor-attacks-weimin-lyu-et-al-2024>(41/56 | 41/340) Task-Agnostic Detector for Insertion-Based Backdoor Attacks (Weimin Lyu et al., 2024)</a></li><li><a href=#4256--42340-guided-distant-supervision-for-multilingual-relation-extraction-data-adapting-to-a-new-language-alistair-plum-et-al-2024>(42/56 | 42/340) Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language (Alistair Plum et al., 2024)</a></li><li><a href=#4356--43340-exploring-the-generalization-of-cancer-clinical-trial-eligibility-classifiers-across-diseases-yumeng-yang-et-al-2024>(43/56 | 43/340) Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases (Yumeng Yang et al., 2024)</a></li><li><a href=#4456--44340-strum-llm-attributed-and-structured-contrastive-summarization-beliz-gunel-et-al-2024>(44/56 | 44/340) STRUM-LLM: Attributed and Structured Contrastive Summarization (Beliz Gunel et al., 2024)</a></li><li><a href=#4556--45340-data-mixing-laws-optimizing-data-mixtures-by-predicting-language-modeling-performance-jiasheng-ye-et-al-2024>(45/56 | 45/340) Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance (Jiasheng Ye et al., 2024)</a></li><li><a href=#4656--46340-conversational-grounding-annotation-and-analysis-of-grounding-acts-and-grounding-units-biswesh-mohapatra-et-al-2024>(46/56 | 46/340) Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units (Biswesh Mohapatra et al., 2024)</a></li><li><a href=#4756--47340-trustai-at-semeval-2024-task-8-a-comprehensive-analysis-of-multi-domain-machine-generated-text-detection-techniques-ashok-urlana-et-al-2024>(47/56 | 47/340) TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain Machine Generated Text Detection Techniques (Ashok Urlana et al., 2024)</a></li><li><a href=#4856--48340-skews-in-the-phenomenon-space-hinder-generalization-in-text-to-image-generation-yingshan-chang-et-al-2024>(48/56 | 48/340) Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation (Yingshan Chang et al., 2024)</a></li><li><a href=#4956--49340-pe-a-poincare-explanation-method-for-fast-text-hierarchy-generation-qian-chen-et-al-2024>(49/56 | 49/340) PE: A Poincare Explanation Method for Fast Text Hierarchy Generation (Qian Chen et al., 2024)</a></li><li><a href=#5056--50340-numtemp-a-real-world-benchmark-to-verify-claims-with-statistical-and-temporal-expressions-venktesh-v-et-al-2024>(50/56 | 50/340) NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions (Venktesh V et al., 2024)</a></li><li><a href=#5156--51340-new-intent-discovery-with-attracting-and-dispersing-prototype-shun-zhang-et-al-2024>(51/56 | 51/340) New Intent Discovery with Attracting and Dispersing Prototype (Shun Zhang et al., 2024)</a></li><li><a href=#5256--52340-an-expert-is-worth-one-token-synergizing-multiple-expert-llms-as-generalist-via-expert-token-routing-ziwei-chai-et-al-2024>(52/56 | 52/340) An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing (Ziwei Chai et al., 2024)</a></li><li><a href=#5356--53340-making-sentence-embeddings-robust-to-user-generated-content-lydia-nishimwe-et-al-2024>(53/56 | 53/340) Making Sentence Embeddings Robust to User-Generated Content (Lydia Nishimwe et al., 2024)</a></li><li><a href=#5456--54340-reflecting-the-male-gaze-quantifying-female-objectification-in-19th-and-20th-century-novels-kexin-luo-et-al-2024>(54/56 | 54/340) Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels (Kexin Luo et al., 2024)</a></li><li><a href=#5556--55340-encoding-of-lexical-tone-in-self-supervised-models-of-spoken-language-gaofei-shen-et-al-2024>(55/56 | 55/340) Encoding of lexical tone in self-supervised models of spoken language (Gaofei Shen et al., 2024)</a></li><li><a href=#5656--56340-towards-explainability-in-legal-outcome-prediction-models-josef-valvoda-et-al-2024>(56/56 | 56/340) Towards Explainability in Legal Outcome Prediction Models (Josef Valvoda et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#13--57340-cygent-a-cybersecurity-conversational-agent-with-log-summarization-powered-by-gpt-3-prasasthy-balasubramanian-et-al-2024>(1/3 | 57/340) CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3 (Prasasthy Balasubramanian et al., 2024)</a></li><li><a href=#23--58340-semantic-ranking-for-automated-adversarial-technique-annotation-in-security-text-udesh-kumarasinghe-et-al-2024>(2/3 | 58/340) Semantic Ranking for Automated Adversarial Technique Annotation in Security Text (Udesh Kumarasinghe et al., 2024)</a></li><li><a href=#33--59340-cipherformer-efficient-transformer-private-inference-with-low-round-complexity-weize-wang-et-al-2024>(3/3 | 59/340) CipherFormer: Efficient Transformer Private Inference with Low Round Complexity (Weize Wang et al., 2024)</a></li></ul></li><li><a href=#cslg-42>cs.LG (42)</a><ul><li><a href=#142--60340-graph-augmentation-for-recommendation-qianru-zhang-et-al-2024>(1/42 | 60/340) Graph Augmentation for Recommendation (Qianru Zhang et al., 2024)</a></li><li><a href=#242--61340-do-llm-agents-have-regret-a-case-study-in-online-learning-and-games-chanwoo-park-et-al-2024>(2/42 | 61/340) Do LLM Agents Have Regret? A Case Study in Online Learning and Games (Chanwoo Park et al., 2024)</a></li><li><a href=#342--62340-learning-from-reduced-labels-for-long-tailed-data-meng-wei-et-al-2024>(3/42 | 62/340) Learning from Reduced Labels for Long-Tailed Data (Meng Wei et al., 2024)</a></li><li><a href=#442--63340-the-anatomy-of-adversarial-attacks-concept-based-xai-dissection-georgii-mikriukov-et-al-2024>(4/42 | 63/340) The Anatomy of Adversarial Attacks: Concept-based XAI Dissection (Georgii Mikriukov et al., 2024)</a></li><li><a href=#542--64340-learning-action-based-representations-using-invariance-max-rudolph-et-al-2024>(5/42 | 64/340) Learning Action-based Representations Using Invariance (Max Rudolph et al., 2024)</a></li><li><a href=#642--65340-deepknowledge-generalisation-driven-deep-learning-testing-sondess-missaoui-et-al-2024>(6/42 | 65/340) DeepKnowledge: Generalisation-Driven Deep Learning Testing (Sondess Missaoui et al., 2024)</a></li><li><a href=#742--66340-less-is-more----on-the-importance-of-sparsification-for-transformers-and-graph-neural-networks-for-tsp-attila-lischka-et-al-2024>(7/42 | 66/340) Less Is More &ndash; On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP (Attila Lischka et al., 2024)</a></li><li><a href=#842--67340-stochastic-gradient-langevin-unlearning-eli-chien-et-al-2024>(8/42 | 67/340) Stochastic Gradient Langevin Unlearning (Eli Chien et al., 2024)</a></li><li><a href=#942--68340-multiple-source-localization-from-a-single-snapshot-observation-using-graph-bayesian-optimization-zonghan-zhang-et-al-2024>(9/42 | 68/340) Multiple-Source Localization from a Single-Snapshot Observation Using Graph Bayesian Optimization (Zonghan Zhang et al., 2024)</a></li><li><a href=#1042--69340-lsttn-a-long-short-term-transformer-based-spatio-temporal-neural-network-for-traffic-flow-forecasting-qinyao-luo-et-al-2024>(10/42 | 69/340) LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting (Qinyao Luo et al., 2024)</a></li><li><a href=#1142--70340-rethinking-the-representation-in-federated-unsupervised-learning-with-non-iid-data-xinting-liao-et-al-2024>(11/42 | 70/340) Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data (Xinting Liao et al., 2024)</a></li><li><a href=#1242--71340-fligan-enhancing-federated-learning-with-incomplete-data-using-gan-paul-joe-maliakel-et-al-2024>(12/42 | 71/340) FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN (Paul Joe Maliakel et al., 2024)</a></li><li><a href=#1342--72340-convergence-of-a-model-free-entropy-regularized-inverse-reinforcement-learning-algorithm-titouan-renard-et-al-2024>(13/42 | 72/340) Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm (Titouan Renard et al., 2024)</a></li><li><a href=#1442--73340-symmetric-basis-convolutions-for-learning-lagrangian-fluid-mechanics-rene-winchenbach-et-al-2024>(14/42 | 73/340) Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics (Rene Winchenbach et al., 2024)</a></li><li><a href=#1542--74340-revealing-vulnerabilities-of-neural-networks-in-parameter-learning-and-defense-against-explanation-aware-backdoors-md-abdul-kadir-et-al-2024>(15/42 | 74/340) Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors (Md Abdul Kadir et al., 2024)</a></li><li><a href=#1642--75340-fedac-an-adaptive-clustered-federated-learning-framework-for-heterogeneous-data-yuxin-zhang-et-al-2024>(16/42 | 75/340) FedAC: An Adaptive Clustered Federated Learning Framework for Heterogeneous Data (Yuxin Zhang et al., 2024)</a></li><li><a href=#1742--76340-on-the-rates-of-convergence-for-learning-with-convolutional-neural-networks-yunfei-yang-et-al-2024>(17/42 | 76/340) On the rates of convergence for learning with convolutional neural networks (Yunfei Yang et al., 2024)</a></li><li><a href=#1842--77340-diffusion-based-negative-sampling-on-graphs-for-link-prediction-trung-kien-nguyen-et-al-2024>(18/42 | 77/340) Diffusion-based Negative Sampling on Graphs for Link Prediction (Trung-Kien Nguyen et al., 2024)</a></li><li><a href=#1942--78340-neural-image-compression-with-quantization-rectifier-wei-luo-et-al-2024>(19/42 | 78/340) Neural Image Compression with Quantization Rectifier (Wei Luo et al., 2024)</a></li><li><a href=#2042--79340-greedy-and-cody-counterfactual-explainers-for-dynamic-graphs-zhan-qu-et-al-2024>(20/42 | 79/340) GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs (Zhan Qu et al., 2024)</a></li><li><a href=#2142--80340-cluster-based-normalization-layer-for-neural-networks-bilal-faye-et-al-2024>(21/42 | 80/340) Cluster-Based Normalization Layer for Neural Networks (Bilal Faye et al., 2024)</a></li><li><a href=#2242--81340-graphs-generalization-under-distribution-shifts-qin-tian-et-al-2024>(22/42 | 81/340) Graphs Generalization under Distribution Shifts (Qin Tian et al., 2024)</a></li><li><a href=#2342--82340-exploring-the-potential-of-prototype-based-soft-labels-data-distillation-for-imbalanced-data-classification-radu-andrei-rosu-et-al-2024>(23/42 | 82/340) Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification (Radu-Andrei Rosu et al., 2024)</a></li><li><a href=#2442--83340-enhancing-uav-security-through-zero-trust-architecture-an-advanced-deep-learning-and-explainable-ai-analysis-ekramul-haque-et-al-2024>(24/42 | 83/340) Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis (Ekramul Haque et al., 2024)</a></li><li><a href=#2542--84340-offline-reinforcement-learning-role-of-state-aggregation-and-trajectory-data-zeyu-jia-et-al-2024>(25/42 | 84/340) Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data (Zeyu Jia et al., 2024)</a></li><li><a href=#2642--85340-iso-diffusion-improving-diffusion-probabilistic-models-using-the-isotropy-of-the-additive-gaussian-noise-dilum-fernando-et-al-2024>(26/42 | 85/340) Iso-Diffusion: Improving Diffusion Probabilistic Models Using the Isotropy of the Additive Gaussian Noise (Dilum Fernando et al., 2024)</a></li><li><a href=#2742--86340-enhancing-industrial-transfer-learning-with-style-filter-cost-reduction-and-defect-focus-chen-li-et-al-2024>(27/42 | 86/340) Enhancing Industrial Transfer Learning with Style Filter: Cost Reduction and Defect-Focus (Chen Li et al., 2024)</a></li><li><a href=#2842--87340-differentially-private-online-federated-learning-with-correlated-noise-jiaojiao-zhang-et-al-2024>(28/42 | 87/340) Differentially Private Online Federated Learning with Correlated Noise (Jiaojiao Zhang et al., 2024)</a></li><li><a href=#2942--88340-determined-multi-label-learning-via-similarity-based-prompt-meng-wei-et-al-2024>(29/42 | 88/340) Determined Multi-Label Learning via Similarity-Based Prompt (Meng Wei et al., 2024)</a></li><li><a href=#3042--89340-proin-learning-to-predict-trajectory-based-on-progressive-interactions-for-autonomous-driving-yinke-dong-et-al-2024>(30/42 | 89/340) ProIn: Learning to Predict Trajectory Based on Progressive Interactions for Autonomous Driving (Yinke Dong et al., 2024)</a></li><li><a href=#3142--90340-manufacturing-service-capability-prediction-with-graph-neural-networks-yunqing-li-et-al-2024>(31/42 | 90/340) Manufacturing Service Capability Prediction with Graph Neural Networks (Yunqing Li et al., 2024)</a></li><li><a href=#3242--91340-cadgl-context-aware-deep-graph-learning-for-predicting-drug-drug-interactions-azmine-toushik-wasi-et-al-2024>(32/42 | 91/340) CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions (Azmine Toushik Wasi et al., 2024)</a></li><li><a href=#3342--92340-fedfixer-mitigating-heterogeneous-label-noise-in-federated-learning-xinyuan-ji-et-al-2024>(33/42 | 92/340) FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning (Xinyuan Ji et al., 2024)</a></li><li><a href=#3442--93340-scod-from-heuristics-to-theory-vojtech-franc-et-al-2024>(34/42 | 93/340) SCOD: From Heuristics to Theory (Vojtech Franc et al., 2024)</a></li><li><a href=#3542--94340-deciphering-the-interplay-between-local-differential-privacy-average-bayesian-privacy-and-maximum-bayesian-privacy-xiaojin-zhang-et-al-2024>(35/42 | 94/340) Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy (Xiaojin Zhang et al., 2024)</a></li><li><a href=#3642--95340-in-the-search-for-optimal-multi-view-learning-models-for-crop-classification-with-global-remote-sensing-data-francisco-mena-et-al-2024>(36/42 | 95/340) In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data (Francisco Mena et al., 2024)</a></li><li><a href=#3742--96340-accelerating-federated-learning-by-selecting-beneficial-herd-of-local-gradients-ping-luo-et-al-2024>(37/42 | 96/340) Accelerating Federated Learning by Selecting Beneficial Herd of Local Gradients (Ping Luo et al., 2024)</a></li><li><a href=#3842--97340-human-understanding-ai-paper-challenge-2024----dataset-design-se-won-oh-et-al-2024>(38/42 | 97/340) Human Understanding AI Paper Challenge 2024 &ndash; Dataset Design (Se Won Oh et al., 2024)</a></li><li><a href=#3942--98340-deepmachining-online-prediction-of-machining-errors-of-lathe-machines-xiang-li-lu-et-al-2024>(39/42 | 98/340) DeepMachining: Online Prediction of Machining Errors of Lathe Machines (Xiang-Li Lu et al., 2024)</a></li><li><a href=#4042--99340-ensemble-adversarial-defense-via-integration-of-multiple-dispersed-low-curvature-models-kaikang-zhao-et-al-2024>(40/42 | 99/340) Ensemble Adversarial Defense via Integration of Multiple Dispersed Low Curvature Models (Kaikang Zhao et al., 2024)</a></li><li><a href=#4142--100340-signsgd-with-federated-voting-chanho-park-et-al-2024>(41/42 | 100/340) SignSGD with Federated Voting (Chanho Park et al., 2024)</a></li><li><a href=#4242--101340-discrete-latent-graph-generative-modeling-with-diffusion-bridges-van-khoa-nguyen-et-al-2024>(42/42 | 101/340) Discrete Latent Graph Generative Modeling with Diffusion Bridges (Van Khoa Nguyen et al., 2024)</a></li></ul></li><li><a href=#csai-20>cs.AI (20)</a><ul><li><a href=#120--102340-clha-a-simple-yet-effective-contrastive-learning-framework-for-human-alignment-feiteng-fang-et-al-2024>(1/20 | 102/340) CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment (Feiteng Fang et al., 2024)</a></li><li><a href=#220--103340-re2llm-reflective-reinforcement-large-language-model-for-session-based-recommendation-ziyan-wang-et-al-2024>(2/20 | 103/340) Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation (Ziyan Wang et al., 2024)</a></li><li><a href=#320--104340-hallucination-detection-in-foundation-models-for-decision-making-a-flexible-definition-and-review-of-the-state-of-the-art-neeloy-chakraborty-et-al-2024>(3/20 | 104/340) Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art (Neeloy Chakraborty et al., 2024)</a></li><li><a href=#420--105340-twostep-multi-agent-task-planning-using-classical-planners-and-large-language-models-ishika-singh-et-al-2024>(4/20 | 105/340) TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models (Ishika Singh et al., 2024)</a></li><li><a href=#520--106340-generation-of-asset-administration-shell-with-large-language-model-agents-interoperability-in-digital-twins-with-semantic-node-yuchen-xia-et-al-2024>(5/20 | 106/340) Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node (Yuchen Xia et al., 2024)</a></li><li><a href=#620--107340-how-reliable-is-your-simulator-analysis-on-the-limitations-of-current-llm-based-user-simulators-for-conversational-recommendation-lixi-zhu-et-al-2024>(6/20 | 107/340) How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation (Lixi Zhu et al., 2024)</a></li><li><a href=#720--108340-concurrent-linguistic-error-detection-cled-for-large-language-models-jinhua-zhu-et-al-2024>(7/20 | 108/340) Concurrent Linguistic Error Detection (CLED) for Large Language Models (Jinhua Zhu et al., 2024)</a></li><li><a href=#820--109340-towards-algorithmic-fidelity-mental-health-representation-across-demographics-in-synthetic-vs-human-generated-data-shinka-mori-et-al-2024>(8/20 | 109/340) Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data (Shinka Mori et al., 2024)</a></li><li><a href=#920--110340-harnessing-the-power-of-llms-for-normative-reasoning-in-mass-bastin-tony-roy-savarimuthu-et-al-2024>(9/20 | 110/340) Harnessing the power of LLMs for normative reasoning in MASs (Bastin Tony Roy Savarimuthu et al., 2024)</a></li><li><a href=#1020--111340-an-experiment-with-the-use-of-chatgpt-for-lcsh-subject-assignment-on-electronic-theses-and-dissertations-eric-h-c-chow-et-al-2024>(10/20 | 111/340) An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations (Eric H. C. Chow et al., 2024)</a></li><li><a href=#1120--112340-all-artificial-less-intelligence-genai-through-the-lens-of-formal-verification-deepak-narayan-gadde-et-al-2024>(11/20 | 112/340) All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification (Deepak Narayan Gadde et al., 2024)</a></li><li><a href=#1220--113340-graph-protection-under-multiple-simultaneous-attacks-a-heuristic-approach-marko-djukanovic-et-al-2024>(12/20 | 113/340) Graph Protection under Multiple Simultaneous Attacks: A Heuristic Approach (Marko Djukanovic et al., 2024)</a></li><li><a href=#1320--114340-enhancing-graph-representation-learning-with-attention-driven-spiking-neural-networks-huifeng-yin-et-al-2024>(13/20 | 114/340) Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks (Huifeng Yin et al., 2024)</a></li><li><a href=#1420--115340-speeding-up-path-planning-via-reinforcement-learning-in-mcts-for-automated-parking-xinlong-zheng-et-al-2024>(14/20 | 115/340) Speeding Up Path Planning via Reinforcement Learning in MCTS for Automated Parking (Xinlong Zheng et al., 2024)</a></li><li><a href=#1520--116340-xaiport-a-service-framework-for-the-early-adoption-of-xai-in-ai-model-development-zerui-wang-et-al-2024>(15/20 | 116/340) XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development (Zerui Wang et al., 2024)</a></li><li><a href=#1620--117340-improving-diffusion-modelss-data-corruption-resistance-using-scheduled-pseudo-huber-loss-artem-khrapov-et-al-2024>(16/20 | 117/340) Improving Diffusion Models&rsquo;s Data-Corruption Resistance using Scheduled Pseudo-Huber Loss (Artem Khrapov et al., 2024)</a></li><li><a href=#1720--118340-deep-reinforcement-learning-and-mean-variance-strategies-for-responsible-portfolio-optimization-fernando-acero-et-al-2024>(17/20 | 118/340) Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization (Fernando Acero et al., 2024)</a></li><li><a href=#1820--119340-learning-to-guide-human-decision-makers-with-vision-language-models-debodeep-banerjee-et-al-2024>(18/20 | 119/340) Learning To Guide Human Decision Makers With Vision-Language Models (Debodeep Banerjee et al., 2024)</a></li><li><a href=#1920--120340-towards-trustworthy-automated-driving-through-qualitative-scene-understanding-and-explanations-nassim-belmecheri-et-al-2024>(19/20 | 120/340) Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations (Nassim Belmecheri et al., 2024)</a></li><li><a href=#2020--121340-return-to-tradition-learning-reliable-heuristics-with-classical-machine-learning-dillon-z-chen-et-al-2024>(20/20 | 121/340) Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning (Dillon Z. Chen et al., 2024)</a></li></ul></li><li><a href=#cscv-76>cs.CV (76)</a><ul><li><a href=#176--122340-synthesize-step-by-step-tools-templates-and-llms-as-data-generators-for-reasoning-based-chart-vqa-zhuowan-li-et-al-2024>(1/76 | 122/340) Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA (Zhuowan Li et al., 2024)</a></li><li><a href=#276--123340-proptest-automatic-property-testing-for-improved-visual-programming-jaywon-koo-et-al-2024>(2/76 | 123/340) PropTest: Automatic Property Testing for Improved Visual Programming (Jaywon Koo et al., 2024)</a></li><li><a href=#376--124340-vp3d-unleashing-2d-visual-prompt-for-text-to-3d-generation-yang-chen-et-al-2024>(3/76 | 124/340) VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation (Yang Chen et al., 2024)</a></li><li><a href=#476--125340-comp4d-llm-guided-compositional-4d-scene-generation-dejia-xu-et-al-2024>(4/76 | 125/340) Comp4D: LLM-Guided Compositional 4D Scene Generation (Dejia Xu et al., 2024)</a></li><li><a href=#576--126340-satsynth-augmenting-image-mask-pairs-through-diffusion-models-for-aerial-semantic-segmentation-aysim-toker-et-al-2024>(5/76 | 126/340) SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation (Aysim Toker et al., 2024)</a></li><li><a href=#676--127340-segicl-a-universal-in-context-learning-framework-for-enhanced-segmentation-in-medical-imaging-lingdong-shen-et-al-2024>(6/76 | 127/340) SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging (Lingdong Shen et al., 2024)</a></li><li><a href=#776--128340-refining-text-to-image-generation-towards-accurate-training-free-glyph-enhanced-image-generation-sanyam-lakhanpal-et-al-2024>(7/76 | 128/340) Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation (Sanyam Lakhanpal et al., 2024)</a></li><li><a href=#876--129340-diffusionact-controllable-diffusion-autoencoder-for-one-shot-face-reenactment-stella-bounareli-et-al-2024>(8/76 | 129/340) DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment (Stella Bounareli et al., 2024)</a></li><li><a href=#976--130340-isolated-diffusion-optimizing-multi-concept-text-to-image-generation-training-freely-with-isolated-diffusion-guidance-jingyuan-zhu-et-al-2024>(9/76 | 130/340) Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance (Jingyuan Zhu et al., 2024)</a></li><li><a href=#1076--131340-dora-3d-visual-grounding-with-order-aware-referring-tung-yu-wu-et-al-2024>(10/76 | 131/340) DOrA: 3D Visual Grounding with Order-Aware Referring (Tung-Yu Wu et al., 2024)</a></li><li><a href=#1176--132340-dia-llama-towards-large-language-model-driven-ct-report-generation-zhixuan-chen-et-al-2024>(11/76 | 132/340) Dia-LLaMA: Towards Large Language Model-driven CT Report Generation (Zhixuan Chen et al., 2024)</a></li><li><a href=#1276--133340-flasheval-towards-fast-and-accurate-evaluation-of-text-to-image-diffusion-generative-models-lin-zhao-et-al-2024>(12/76 | 133/340) FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models (Lin Zhao et al., 2024)</a></li><li><a href=#1376--134340-chebmixer-efficient-graph-representation-learning-with-mlp-mixer-xiaoyan-kui-et-al-2024>(13/76 | 134/340) ChebMixer: Efficient Graph Representation Learning with MLP Mixer (Xiaoyan Kui et al., 2024)</a></li><li><a href=#1476--135340-an-intermediate-fusion-vit-enables-efficient-text-image-alignment-in-diffusion-models-zizhao-hu-et-al-2024>(14/76 | 135/340) An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models (Zizhao Hu et al., 2024)</a></li><li><a href=#1576--136340-make-it-vivid-dressing-your-animatable-biped-cartoon-characters-from-text-junshu-tang-et-al-2024>(15/76 | 136/340) Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text (Junshu Tang et al., 2024)</a></li><li><a href=#1676--137340-engagement-measurement-based-on-facial-landmarks-and-spatial-temporal-graph-convolutional-networks-ali-abedi-et-al-2024>(16/76 | 137/340) Engagement Measurement Based on Facial Landmarks and Spatial-Temporal Graph Convolutional Networks (Ali Abedi et al., 2024)</a></li><li><a href=#1776--138340-continuous-subject-specific-attribute-control-in-t2i-models-by-identifying-semantic-directions-stefan-andreas-baumann-et-al-2024>(17/76 | 138/340) Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions (Stefan Andreas Baumann et al., 2024)</a></li><li><a href=#1876--139340-dreamlip-language-image-pre-training-with-long-captions-kecheng-zheng-et-al-2024>(18/76 | 139/340) DreamLIP: Language-Image Pre-training with Long Captions (Kecheng Zheng et al., 2024)</a></li><li><a href=#1976--140340-sd-dit-unleashing-the-power-of-self-supervised-discrimination-in-diffusion-transformer-rui-zhu-et-al-2024>(19/76 | 140/340) SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer (Rui Zhu et al., 2024)</a></li><li><a href=#2076--141340-multiple-object-tracking-as-id-prediction-ruopeng-gao-et-al-2024>(20/76 | 141/340) Multiple Object Tracking as ID Prediction (Ruopeng Gao et al., 2024)</a></li><li><a href=#2176--142340-from-two-stream-to-one-stream-efficient-rgb-t-tracking-via-mutual-prompt-learning-and-knowledge-distillation-yang-luo-et-al-2024>(21/76 | 142/340) From Two Stream to One Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation (Yang Luo et al., 2024)</a></li><li><a href=#2276--143340-let-real-images-be-as-a-judger-spotting-fake-images-synthesized-with-generative-models-ziyou-liang-et-al-2024>(22/76 | 143/340) Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models (Ziyou Liang et al., 2024)</a></li><li><a href=#2376--144340-cmvim-contrastive-masked-vim-autoencoder-for-3d-multi-modal-representation-learning-for-ad-classification-guangqian-yang-et-al-2024>(23/76 | 144/340) CMViM: Contrastive Masked Vim Autoencoder for 3D Multi-modal Representation Learning for AD classification (Guangqian Yang et al., 2024)</a></li><li><a href=#2476--145340-self-supervised-learning-for-medical-image-data-with-anatomy-oriented-imaging-planes-tianwei-zhang-et-al-2024>(24/76 | 145/340) Self-Supervised Learning for Medical Image Data with Anatomy-Oriented Imaging Planes (Tianwei Zhang et al., 2024)</a></li><li><a href=#2576--146340-animateme-4d-facial-expressions-via-diffusion-models-dimitrios-gerogiannis-et-al-2024>(25/76 | 146/340) AnimateMe: 4D Facial Expressions via Diffusion Models (Dimitrios Gerogiannis et al., 2024)</a></li><li><a href=#2676--147340-dpstyler-dynamic-promptstyler-for-source-free-domain-generalization-yunlong-tang-et-al-2024>(26/76 | 147/340) DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization (Yunlong Tang et al., 2024)</a></li><li><a href=#2776--148340-ai-generated-video-detection-via-spatio-temporal-anomaly-learning-jianfa-bai-et-al-2024>(27/76 | 148/340) AI-Generated Video Detection via Spatio-Temporal Anomaly Learning (Jianfa Bai et al., 2024)</a></li><li><a href=#2876--149340-goodsam-bridging-domain-and-capacity-gaps-via-segment-anything-model-for-distortion-aware-panoramic-semantic-segmentation-weiming-zhang-et-al-2024>(28/76 | 149/340) GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation (Weiming Zhang et al., 2024)</a></li><li><a href=#2976--150340-flashface-human-image-personalization-with-high-fidelity-identity-preservation-shilong-zhang-et-al-2024>(29/76 | 150/340) FlashFace: Human Image Personalization with High-fidelity Identity Preservation (Shilong Zhang et al., 2024)</a></li><li><a href=#3076--151340-invertible-diffusion-models-for-compressed-sensing-bin-chen-et-al-2024>(30/76 | 151/340) Invertible Diffusion Models for Compressed Sensing (Bin Chen et al., 2024)</a></li><li><a href=#3176--152340-be-yourself-bounded-attention-for-multi-subject-text-to-image-generation-omer-dahary-et-al-2024>(31/76 | 152/340) Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation (Omer Dahary et al., 2024)</a></li><li><a href=#3276--153340-urbanvlp-a-multi-granularity-vision-language-pre-trained-foundation-model-for-urban-indicator-prediction-xixuan-hao-et-al-2024>(32/76 | 153/340) UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction (Xixuan Hao et al., 2024)</a></li><li><a href=#3376--154340-hpl-ess-hybrid-pseudo-labeling-for-unsupervised-event-based-semantic-segmentation-linglin-jing-et-al-2024>(33/76 | 154/340) HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation (Linglin Jing et al., 2024)</a></li><li><a href=#3476--155340-sdxs-real-time-one-step-latent-diffusion-models-with-image-conditions-yuda-song-et-al-2024>(34/76 | 155/340) SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions (Yuda Song et al., 2024)</a></li><li><a href=#3576--156340-vmrnn-integrating-vision-mamba-and-lstm-for-efficient-and-accurate-spatiotemporal-forecasting-yujin-tang-et-al-2024>(35/76 | 156/340) VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting (Yujin Tang et al., 2024)</a></li><li><a href=#3676--157340-ct-bound-fast-boundary-estimation-from-noisy-images-via-hybrid-convolution-and-transformer-neural-networks-wei-xu-et-al-2024>(36/76 | 157/340) CT-Bound: Fast Boundary Estimation From Noisy Images Via Hybrid Convolution and Transformer Neural Networks (Wei Xu et al., 2024)</a></li><li><a href=#3776--158340-distilling-semantic-priors-from-sam-to-efficient-image-restoration-models-quan-zhang-et-al-2024>(37/76 | 158/340) Distilling Semantic Priors from SAM to Efficient Image Restoration Models (Quan Zhang et al., 2024)</a></li><li><a href=#3876--159340-understanding-long-videos-in-one-multimodal-language-model-pass-kanchana-ranasinghe-et-al-2024>(38/76 | 159/340) Understanding Long Videos in One Multimodal Language Model Pass (Kanchana Ranasinghe et al., 2024)</a></li><li><a href=#3976--160340-visual-cot-unleashing-chain-of-thought-reasoning-in-multi-modal-language-models-hao-shao-et-al-2024>(39/76 | 160/340) Visual CoT: Unleashing Chain-of-Thought Reasoning in Multi-Modal Language Models (Hao Shao et al., 2024)</a></li><li><a href=#4076--161340-rcbevdet-radar-camera-fusion-in-birds-eye-view-for-3d-object-detection-zhiwei-lin-et-al-2024>(40/76 | 161/340) RCBEVDet: Radar-camera Fusion in Bird&rsquo;s Eye View for 3D Object Detection (Zhiwei Lin et al., 2024)</a></li><li><a href=#4176--162340-task2box-box-embeddings-for-modeling-asymmetric-task-relationships-rangel-daroya-et-al-2024>(41/76 | 162/340) Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships (Rangel Daroya et al., 2024)</a></li><li><a href=#4276--163340-optimizing-lidar-placements-for-robust-driving-perception-in-adverse-conditions-ye-li-et-al-2024>(42/76 | 163/340) Optimizing LiDAR Placements for Robust Driving Perception in Adverse Conditions (Ye Li et al., 2024)</a></li><li><a href=#4376--164340-elysium-exploring-object-level-perception-in-videos-via-mllm-han-wang-et-al-2024>(43/76 | 164/340) Elysium: Exploring Object-level Perception in Videos via MLLM (Han Wang et al., 2024)</a></li><li><a href=#4476--165340-pathotune-adapting-visual-foundation-model-to-pathological-specialists-jiaxuan-lu-et-al-2024>(44/76 | 165/340) PathoTune: Adapting Visual Foundation Model to Pathological Specialists (Jiaxuan Lu et al., 2024)</a></li><li><a href=#4576--166340-dreampolisher-towards-high-quality-text-to-3d-generation-via-geometric-diffusion-yuanze-lin-et-al-2024>(45/76 | 166/340) DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion (Yuanze Lin et al., 2024)</a></li><li><a href=#4676--167340-synfog-a-photo-realistic-synthetic-fog-dataset-based-on-end-to-end-imaging-simulation-for-advancing-real-world-defogging-in-autonomous-driving-yiming-xie-et-al-2024>(46/76 | 167/340) SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving (Yiming Xie et al., 2024)</a></li><li><a href=#4776--168340-trip-temporal-residual-learning-with-image-noise-prior-for-image-to-video-diffusion-models-zhongwei-zhang-et-al-2024>(47/76 | 168/340) TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models (Zhongwei Zhang et al., 2024)</a></li><li><a href=#4876--169340-learning-spatial-adaptation-and-temporal-coherence-in-diffusion-models-for-video-super-resolution-zhikai-chen-et-al-2024>(48/76 | 169/340) Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution (Zhikai Chen et al., 2024)</a></li><li><a href=#4976--170340-make-your-anchor-a-diffusion-based-2d-avatar-generation-framework-ziyao-huang-et-al-2024>(49/76 | 170/340) Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework (Ziyao Huang et al., 2024)</a></li><li><a href=#5076--171340-elite360d-towards-efficient-360-depth-estimation-via-semantic--and-distance-aware-bi-projection-fusion-hao-ai-et-al-2024>(50/76 | 171/340) Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion (Hao Ai et al., 2024)</a></li><li><a href=#5176--172340-co-occurring-of-object-detection-and-identification-towards-unlabeled-object-discovery-binay-kumar-singh-et-al-2024>(51/76 | 172/340) Co-Occurring of Object Detection and Identification towards unlabeled object discovery (Binay Kumar Singh et al., 2024)</a></li><li><a href=#5276--173340-histogram-layers-for-neural-engineered-features-joshua-peeples-et-al-2024>(52/76 | 173/340) Histogram Layers for Neural Engineered Features (Joshua Peeples et al., 2024)</a></li><li><a href=#5376--174340-calib3d-calibrating-model-preferences-for-reliable-3d-scene-understanding-lingdong-kong-et-al-2024>(53/76 | 174/340) Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding (Lingdong Kong et al., 2024)</a></li><li><a href=#5476--175340-drivecot-integrating-chain-of-thought-reasoning-with-end-to-end-driving-tianqi-wang-et-al-2024>(54/76 | 175/340) DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving (Tianqi Wang et al., 2024)</a></li><li><a href=#5576--176340-v2x-pc-vehicle-to-everything-collaborative-perception-via-point-cluster-si-liu-et-al-2024>(55/76 | 176/340) V2X-PC: Vehicle-to-everything Collaborative Perception via Point Cluster (Si Liu et al., 2024)</a></li><li><a href=#5676--177340-open-set-recognition-in-the-age-of-vision-language-models-dimity-miller-et-al-2024>(56/76 | 177/340) Open-Set Recognition in the Age of Vision-Language Models (Dimity Miller et al., 2024)</a></li><li><a href=#5776--178340-camera-aware-label-refinement-for-unsupervised-person-re-identification-pengna-li-et-al-2024>(57/76 | 178/340) Camera-aware Label Refinement for Unsupervised Person Re-identification (Pengna Li et al., 2024)</a></li><li><a href=#5876--179340-benchmarks-and-challenges-in-pose-estimation-for-egocentric-hand-interactions-with-objects-zicong-fan-et-al-2024>(58/76 | 179/340) Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects (Zicong Fan et al., 2024)</a></li><li><a href=#5976--180340-composed-video-retrieval-via-enriched-context-and-discriminative-embeddings-omkar-thawakar-et-al-2024>(59/76 | 180/340) Composed Video Retrieval via Enriched Context and Discriminative Embeddings (Omkar Thawakar et al., 2024)</a></li><li><a href=#6076--181340-twinlitenetplus-a-stronger-model-for-real-time-drivable-area-and-lane-segmentation-quang-huy-che-et-al-2024>(60/76 | 181/340) TwinLiteNetPlus: A Stronger Model for Real-time Drivable Area and Lane Segmentation (Quang-Huy Che et al., 2024)</a></li><li><a href=#6176--182340-cvt-xrf-contrastive-in-voxel-transformer-for-3d-consistent-radiance-fields-from-sparse-inputs-yingji-zhong-et-al-2024>(61/76 | 182/340) CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs (Yingji Zhong et al., 2024)</a></li><li><a href=#6276--183340-modetv2-gpu-accelerated-motion-decomposition-transformer-for-pairwise-optimization-in-medical-image-registration-haiqiao-wang-et-al-2024>(62/76 | 183/340) ModeTv2: GPU-accelerated Motion Decomposition Transformer for Pairwise Optimization in Medical Image Registration (Haiqiao Wang et al., 2024)</a></li><li><a href=#6376--184340-medical-image-registration-and-its-application-in-retinal-images-a-review-qiushi-nie-et-al-2024>(63/76 | 184/340) Medical Image Registration and Its Application in Retinal Images: A Review (Qiushi Nie et al., 2024)</a></li><li><a href=#6476--185340-doctr-disentangled-object-centric-transformer-for-point-scene-understanding-xiaoxuan-yu-et-al-2024>(64/76 | 185/340) DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding (Xiaoxuan Yu et al., 2024)</a></li><li><a href=#6576--186340-unsupervised-template-assisted-point-cloud-shape-correspondence-network-jiacheng-deng-et-al-2024>(65/76 | 186/340) Unsupervised Template-assisted Point Cloud Shape Correspondence Network (Jiacheng Deng et al., 2024)</a></li><li><a href=#6676--187340-asdf-assembly-state-detection-utilizing-late-fusion-by-integrating-6d-pose-estimation-hannah-schieber-et-al-2024>(66/76 | 187/340) ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation (Hannah Schieber et al., 2024)</a></li><li><a href=#6776--188340-impact-of-video-compression-artifacts-on-fisheye-camera-visual-perception-tasks-madhumitha-sakthi-et-al-2024>(67/76 | 188/340) Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks (Madhumitha Sakthi et al., 2024)</a></li><li><a href=#6876--189340-benchmarking-video-frame-interpolation-simon-kiefhaber-et-al-2024>(68/76 | 189/340) Benchmarking Video Frame Interpolation (Simon Kiefhaber et al., 2024)</a></li><li><a href=#6976--190340-clustering-propagation-for-universal-medical-image-segmentation-yuhang-ding-et-al-2024>(69/76 | 190/340) Clustering Propagation for Universal Medical Image Segmentation (Yuhang Ding et al., 2024)</a></li><li><a href=#7076--191340-gsdf-3dgs-meets-sdf-for-improved-rendering-and-reconstruction-mulin-yu-et-al-2024>(70/76 | 191/340) GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction (Mulin Yu et al., 2024)</a></li><li><a href=#7176--192340-towards-balanced-rgb-tsdf-fusion-for-consistent-semantic-scene-completion-by-3d-rgb-feature-completion-and-a-classwise-entropy-loss-function-laiyan-ding-et-al-2024>(71/76 | 192/340) Towards Balanced RGB-TSDF Fusion for Consistent Semantic Scene Completion by 3D RGB Feature Completion and a Classwise Entropy Loss Function (Laiyan Ding et al., 2024)</a></li><li><a href=#7276--193340-creating-a-digital-twin-of-spinal-surgery-a-proof-of-concept-jonas-hein-et-al-2024>(72/76 | 193/340) Creating a Digital Twin of Spinal Surgery: A Proof of Concept (Jonas Hein et al., 2024)</a></li><li><a href=#7376--194340-inpc-implicit-neural-point-clouds-for-radiance-field-rendering-florian-hahlbohm-et-al-2024>(73/76 | 194/340) INPC: Implicit Neural Point Clouds for Radiance Field Rendering (Florian Hahlbohm et al., 2024)</a></li><li><a href=#7476--195340-curbnet-curb-detection-framework-based-on-lidar-point-cloud-segmentation-guoyang-zhao-et-al-2024>(74/76 | 195/340) CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation (Guoyang Zhao et al., 2024)</a></li><li><a href=#7576--196340-multi-attention-associate-prediction-network-for-visual-tracking-xinglong-sun-et-al-2024>(75/76 | 196/340) Multi-attention Associate Prediction Network for Visual Tracking (Xinglong Sun et al., 2024)</a></li><li><a href=#7676--197340-text-if-leveraging-semantic-text-guidance-for-degradation-aware-and-interactive-image-fusion-xunpeng-yi-et-al-2024>(76/76 | 197/340) Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion (Xunpeng Yi et al., 2024)</a></li></ul></li><li><a href=#cshc-11>cs.HC (11)</a><ul><li><a href=#111--198340-golf-goal-oriented-long-term-life-tasks-supported-by-human-ai-collaboration-ben-wang-2024>(1/11 | 198/340) GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI collaboration (Ben Wang, 2024)</a></li><li><a href=#211--199340-sesame-a-framework-to-simulate-self-reported-ground-truth-for-mental-health-sensing-studies-akshat-choube-et-al-2024>(2/11 | 199/340) SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies (Akshat Choube et al., 2024)</a></li><li><a href=#311--200340-virtual-co-pilot-multimodal-large-language-model-enabled-quick-access-procedures-for-single-pilot-operations-fan-li-et-al-2024>(3/11 | 200/340) Virtual Co-Pilot: Multimodal Large Language Model-enabled Quick-access Procedures for Single Pilot Operations (Fan Li et al., 2024)</a></li><li><a href=#411--201340-towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making-shuai-ma-et-al-2024>(4/11 | 201/340) Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (Shuai Ma et al., 2024)</a></li><li><a href=#511--202340-compressing-and-interpreting-word-embeddings-with-latent-space-regularization-and-interactive-semantics-probing-haoyu-li-et-al-2024>(5/11 | 202/340) Compressing and Interpreting Word Embeddings with Latent Space Regularization and Interactive Semantics Probing (Haoyu Li et al., 2024)</a></li><li><a href=#611--203340-enhancing-cross-dataset-eeg-emotion-recognition-a-novel-approach-with-emotional-eeg-style-transfer-network-yijin-zhou-et-al-2024>(6/11 | 203/340) Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network (Yijin Zhou et al., 2024)</a></li><li><a href=#711--204340-as-good-as-a-coin-toss-human-detection-of-ai-generated-images-videos-audio-and-audiovisual-stimuli-di-cooke-et-al-2024>(7/11 | 204/340) As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli (Di Cooke et al., 2024)</a></li><li><a href=#811--205340-behind-the-counter-exploring-the-motivations-and-barriers-of-online-counterspeech-writing-kaike-ping-et-al-2024>(8/11 | 205/340) Behind the Counter: Exploring the Motivations and Barriers of Online Counterspeech Writing (Kaike Ping et al., 2024)</a></li><li><a href=#911--206340-it-is-there-and-you-need-it-so-why-do-you-not-use-it-achieving-better-adoption-of-ai-systems-by-domain-experts-in-the-case-study-of-natural-science-research-auste-simkute-et-al-2024>(9/11 | 206/340) &lsquo;It is there, and you need it, so why do you not use it?&rsquo; Achieving better adoption of AI systems by domain experts, in the case study of natural science research (Auste Simkute et al., 2024)</a></li><li><a href=#1011--207340-we-have-no-idea-how-models-will-behave-in-production-until-production-how-engineers-operationalize-machine-learning-shreya-shankar-et-al-2024>(10/11 | 207/340) &lsquo;We Have No Idea How Models will Behave in Production until Production&rsquo;: How Engineers Operationalize Machine Learning (Shreya Shankar et al., 2024)</a></li><li><a href=#1111--208340-explora-a-teacher-apprentice-methodology-for-eliciting-natural-child-computer-interactions-vanessa-figueiredo-et-al-2024>(11/11 | 208/340) EXPLORA: A teacher-apprentice methodology for eliciting natural child-computer interactions (Vanessa Figueiredo et al., 2024)</a></li></ul></li><li><a href=#csse-15>cs.SE (15)</a><ul><li><a href=#115--209340-a-comprehensive-study-of-the-capabilities-of-large-language-models-for-vulnerability-detection-benjamin-steenhoek-et-al-2024>(1/15 | 209/340) A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection (Benjamin Steenhoek et al., 2024)</a></li><li><a href=#215--210340-repairagent-an-autonomous-llm-based-agent-for-program-repair-islem-bouzenia-et-al-2024>(2/15 | 210/340) RepairAgent: An Autonomous, LLM-Based Agent for Program Repair (Islem Bouzenia et al., 2024)</a></li><li><a href=#315--211340-chatgpt-incorrectness-detection-in-software-reviews-minaoar-hossain-tanzil-et-al-2024>(3/15 | 211/340) ChatGPT Incorrectness Detection in Software Reviews (Minaoar Hossain Tanzil et al., 2024)</a></li><li><a href=#415--212340-exploring-the-impact-of-the-output-format-on-the-evaluation-of-large-language-models-for-code-translation-marcos-macedo-et-al-2024>(4/15 | 212/340) Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation (Marcos Macedo et al., 2024)</a></li><li><a href=#515--213340-evaluating-large-language-models-with-runtime-behavior-of-program-execution-junkai-chen-et-al-2024>(5/15 | 213/340) Evaluating Large Language Models with Runtime Behavior of Program Execution (Junkai Chen et al., 2024)</a></li><li><a href=#615--214340-agentfl-scaling-llm-based-fault-localization-to-project-level-context-yihao-qin-et-al-2024>(6/15 | 214/340) AgentFL: Scaling LLM-based Fault Localization to Project-Level Context (Yihao Qin et al., 2024)</a></li><li><a href=#715--215340-on-the-impact-of-black-box-deployment-strategies-for-edge-ai-on-latency-and-model-performance-jaskirat-singh-et-al-2024>(7/15 | 215/340) On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance (Jaskirat Singh et al., 2024)</a></li><li><a href=#815--216340-design-patterns-for-multilevel-modeling-and-simulation-luca-serena-et-al-2024>(8/15 | 216/340) Design Patterns for Multilevel Modeling and Simulation (Luca Serena et al., 2024)</a></li><li><a href=#915--217340-a-mixed-method-study-of-devops-challenges-minaoar-hossain-tanzil-et-al-2024>(9/15 | 217/340) A Mixed Method Study of DevOps Challenges (Minaoar Hossain Tanzil et al., 2024)</a></li><li><a href=#1015--218340-chatdbg-an-ai-powered-debugging-assistant-kyla-levin-et-al-2024>(10/15 | 218/340) ChatDBG: An AI-Powered Debugging Assistant (Kyla Levin et al., 2024)</a></li><li><a href=#1115--219340-model-less-is-the-best-model-generating-pure-code-implementations-to-replace-on-device-dl-models-mingyi-zhou-et-al-2024>(11/15 | 219/340) Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models (Mingyi Zhou et al., 2024)</a></li><li><a href=#1215--220340-concerned-with-data-contamination-assessing-countermeasures-in-code-language-model-jialun-cao-et-al-2024>(12/15 | 220/340) Concerned with Data Contamination? Assessing Countermeasures in Code Language Model (Jialun Cao et al., 2024)</a></li><li><a href=#1315--221340-seeking-enlightenment-incorporating-evidence-based-practice-techniques-in-a-research-software-engineering-team-reed-milewicz-et-al-2024>(13/15 | 221/340) Seeking Enlightenment: Incorporating Evidence-Based Practice Techniques in a Research Software Engineering Team (Reed Milewicz et al., 2024)</a></li><li><a href=#1415--222340-enhancing-software-effort-estimation-through-reinforcement-learning-based-project-management-oriented-feature-selection-haoyang-chen-et-al-2024>(14/15 | 222/340) Enhancing Software Effort Estimation through Reinforcement Learning-based Project Management-Oriented Feature Selection (Haoyang Chen et al., 2024)</a></li><li><a href=#1515--223340-disl-fueling-research-with-a-large-dataset-of-solidity-smart-contracts-gabriele-morello-et-al-2024>(15/15 | 223/340) DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts (Gabriele Morello et al., 2024)</a></li></ul></li><li><a href=#eesssy-17>eess.SY (17)</a><ul><li><a href=#117--224340-an-llm-based-digital-twin-for-optimizing-human-in-the-loop-systems-hanqing-yang-et-al-2024>(1/17 | 224/340) An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems (Hanqing Yang et al., 2024)</a></li><li><a href=#217--225340-state-space-models-as-foundation-models-a-control-theoretic-overview-carmen-amo-alonso-et-al-2024>(2/17 | 225/340) State Space Models as Foundation Models: A Control Theoretic Overview (Carmen Amo Alonso et al., 2024)</a></li><li><a href=#317--226340-a-discrete-time-least-squares-adaptive-state-tracking-control-scheme-with-a-mobile-robot-system-study-qianhong-zhao-et-al-2024>(3/17 | 226/340) A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme with A Mobile-Robot System Study (Qianhong Zhao et al., 2024)</a></li><li><a href=#417--227340-physics-informed-rl-for-maximal-safety-probability-estimation-hikaru-hoshino-et-al-2024>(4/17 | 227/340) Physics-informed RL for Maximal Safety Probability Estimation (Hikaru Hoshino et al., 2024)</a></li><li><a href=#517--228340-optimal-operation-of-reconfigurable-active-distribution-networks-aiming-at-resiliency-improvement-saeed-behzadi-et-al-2024>(5/17 | 228/340) Optimal Operation of Reconfigurable Active Distribution Networks Aiming at Resiliency Improvement (Saeed Behzadi et al., 2024)</a></li><li><a href=#617--229340-spline-trajectory-tracking-and-obstacle-avoidance-for-mobile-agents-via-convex-optimization-akua-dickson-et-al-2024>(6/17 | 229/340) Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via Convex Optimization (Akua Dickson et al., 2024)</a></li><li><a href=#717--230340-semantic-aware-remote-estimation-of-multiple-markov-sources-under-constraints-jiping-luo-et-al-2024>(7/17 | 230/340) Semantic-Aware Remote Estimation of Multiple Markov Sources Under Constraints (Jiping Luo et al., 2024)</a></li><li><a href=#817--231340-energy-efficiency-optimization-method-of-wdm-visible-light-communication-system-for-indoor-broadcasting-networks-dayu-shi-et-al-2024>(8/17 | 231/340) Energy Efficiency Optimization Method of WDM Visible Light Communication System for Indoor Broadcasting Networks (Dayu Shi et al., 2024)</a></li><li><a href=#917--232340-policy-gradient-based-model-free-optimal-lqg-control-with-a-probabilistic-risk-constraint-arunava-naha-et-al-2024>(9/17 | 232/340) Policy Gradient-based Model Free Optimal LQG Control with a Probabilistic Risk Constraint (Arunava Naha et al., 2024)</a></li><li><a href=#1017--233340-a-branch-and-bound-method-for-the-exact-parameter-identification-of-the-pkpd-model-for-anesthetic-drugs-giulia-di-credico-et-al-2024>(10/17 | 233/340) A Branch and Bound method for the exact parameter identification of the PK/PD model for anesthetic drugs (Giulia Di Credico et al., 2024)</a></li><li><a href=#1117--234340-guided-bayesian-optimization-data-efficient-controller-tuning-with-digital-twin-mahdi-nobar-et-al-2024>(11/17 | 234/340) Guided Bayesian Optimization: Data-Efficient Controller Tuning with Digital Twin (Mahdi Nobar et al., 2024)</a></li><li><a href=#1217--235340-sparsity-constrained-linear-quadratic-regulation-problem-greedy-approach-with-performance-guarantee-shumpei-nishida-et-al-2024>(12/17 | 235/340) Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy Approach with Performance Guarantee (Shumpei Nishida et al., 2024)</a></li><li><a href=#1317--236340-ensuring-disturbance-rejection-performance-by-synthesizing-grid-following-and-grid-forming-inverters-in-power-systems-fuyilong-ma-et-al-2024>(13/17 | 236/340) Ensuring Disturbance Rejection Performance by Synthesizing Grid-Following and Grid-Forming Inverters in Power Systems (Fuyilong Ma et al., 2024)</a></li><li><a href=#1417--237340-active-learning-of-dynamics-using-prior-domain-knowledge-in-the-sampling-process-kevin-s-miller-et-al-2024>(14/17 | 237/340) Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process (Kevin S. Miller et al., 2024)</a></li><li><a href=#1517--238340-sis-epidemics-on-open-networks-a-replacement-based-approximation-renato-vizuete-et-al-2024>(15/17 | 238/340) SIS epidemics on open networks: A replacement-based approximation (Renato Vizuete et al., 2024)</a></li><li><a href=#1617--239340-predictable-interval-mdps-through-entropy-regularization-menno-van-zutphen-et-al-2024>(16/17 | 239/340) Predictable Interval MDPs through Entropy Regularization (Menno van Zutphen et al., 2024)</a></li><li><a href=#1717--240340-a-geometric-perspective-on-fusing-gaussian-distributions-on-lie-groups-yixiao-ge-et-al-2024>(17/17 | 240/340) A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups (Yixiao Ge et al., 2024)</a></li></ul></li><li><a href=#csro-27>cs.RO (27)</a><ul><li><a href=#127--241340-grounding-language-plans-in-demonstrations-through-counterfactual-perturbations-yanwei-wang-et-al-2024>(1/27 | 241/340) Grounding Language Plans in Demonstrations Through Counterfactual Perturbations (Yanwei Wang et al., 2024)</a></li><li><a href=#227--242340-exploring-causalworld-enhancing-robotic-manipulation-via-knowledge-transfer-and-curriculum-learning-xinrui-wang-et-al-2024>(2/27 | 242/340) Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning (Xinrui Wang et al., 2024)</a></li><li><a href=#327--243340-temporal-and-semantic-evaluation-metrics-for-foundation-models-in-post-hoc-analysis-of-robotic-sub-tasks-jonathan-salfity-et-al-2024>(3/27 | 243/340) Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks (Jonathan Salfity et al., 2024)</a></li><li><a href=#427--244340-a-robotic-skill-learning-system-built-upon-diffusion-policies-and-foundation-models-nils-ingelhag-et-al-2024>(4/27 | 244/340) A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models (Nils Ingelhag et al., 2024)</a></li><li><a href=#527--245340-domain-adaptive-detection-of-mavs-a-benchmark-and-noise-suppression-network-yin-zhang-et-al-2024>(5/27 | 245/340) Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network (Yin Zhang et al., 2024)</a></li><li><a href=#627--246340-towards-cooperative-maneuver-planning-in-mixed-traffic-at-urban-intersections-marvin-klimke-et-al-2024>(6/27 | 246/340) Towards Cooperative Maneuver Planning in Mixed Traffic at Urban Intersections (Marvin Klimke et al., 2024)</a></li><li><a href=#727--247340-dyna-lflh-learning-agile-navigation-in-dynamic-environments-from-learned-hallucination-saad-abdul-ghani-et-al-2024>(7/27 | 247/340) Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination (Saad Abdul Ghani et al., 2024)</a></li><li><a href=#827--248340-adaptive-step-duration-for-precise-foot-placement-achieving-robust-bipedal-locomotion-on-terrains-with-restricted-footholds-zhaoyang-xiang-et-al-2024>(8/27 | 248/340) Adaptive Step Duration for Precise Foot Placement: Achieving Robust Bipedal Locomotion on Terrains with Restricted Footholds (Zhaoyang Xiang et al., 2024)</a></li><li><a href=#927--249340-proprioception-is-all-you-need-terrain-classification-for-boreal-forests-damien-larocque-et-al-2024>(9/27 | 249/340) Proprioception Is All You Need: Terrain Classification for Boreal Forests (Damien LaRocque et al., 2024)</a></li><li><a href=#1027--250340-exploiting-priors-from-3d-diffusion-models-for-rgb-based-one-shot-view-planning-sicong-pan-et-al-2024>(10/27 | 250/340) Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View Planning (Sicong Pan et al., 2024)</a></li><li><a href=#1127--251340-skill-q-network-learning-adaptive-skill-ensemble-for-mapless-navigation-in-unknown-environments-hyunki-seong-et-al-2024>(11/27 | 251/340) Skill Q-Network: Learning Adaptive Skill Ensemble for Mapless Navigation in Unknown Environments (Hyunki Seong et al., 2024)</a></li><li><a href=#1227--252340-bipedal-safe-navigation-over-uncertain-rough-terrain-unifying-terrain-mapping-and-locomotion-stability-kasidit-muenprasitivej-et-al-2024>(12/27 | 252/340) Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability (Kasidit Muenprasitivej et al., 2024)</a></li><li><a href=#1327--253340-se3-linear-parameter-varying-dynamical-systems-for-globally-asymptotically-stable-end-effector-control-sunan-sun-et-al-2024>(13/27 | 253/340) SE(3) Linear Parameter Varying Dynamical Systems for Globally Asymptotically Stable End-Effector Control (Sunan Sun et al., 2024)</a></li><li><a href=#1427--254340-impact-aware-bimanual-catching-of-large-momentum-objects-lei-yan-et-al-2024>(14/27 | 254/340) Impact-Aware Bimanual Catching of Large-Momentum Objects (Lei Yan et al., 2024)</a></li><li><a href=#1527--255340-a-comparative-analysis-of-visual-odometry-in-virtual-and-real-world-railways-environments-gianluca-damico-et-al-2024>(15/27 | 255/340) A Comparative Analysis of Visual Odometry in Virtual and Real-World Railways Environments (Gianluca D&rsquo;Amico et al., 2024)</a></li><li><a href=#1627--256340-trajectory-optimization-with-global-yaw-parameterization-for-field-of-view-constrained-autonomous-flight-yuwei-wu-et-al-2024>(16/27 | 256/340) Trajectory Optimization with Global Yaw Parameterization for Field-of-View Constrained Autonomous Flight (Yuwei Wu et al., 2024)</a></li><li><a href=#1727--257340-visual-whole-body-control-for-legged-loco-manipulation-minghuan-liu-et-al-2024>(17/27 | 257/340) Visual Whole-Body Control for Legged Loco-Manipulation (Minghuan Liu et al., 2024)</a></li><li><a href=#1827--258340-arm-constrained-curriculum-learning-for-loco-manipulation-of-the-wheel-legged-robot-zifan-wang-et-al-2024>(18/27 | 258/340) Arm-Constrained Curriculum Learning for Loco-Manipulation of the Wheel-Legged Robot (Zifan Wang et al., 2024)</a></li><li><a href=#1927--259340-real-time-model-predictive-control-with-zonotope-based-neural-networks-for-bipedal-social-navigation-abdulaziz-shamsah-et-al-2024>(19/27 | 259/340) Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation (Abdulaziz Shamsah et al., 2024)</a></li><li><a href=#2027--260340-learning-symbolic-and-subsymbolic-temporal-task-constraints-from-bimanual-human-demonstrations-christian-dreher-et-al-2024>(20/27 | 260/340) Learning Symbolic and Subsymbolic Temporal Task Constraints from Bimanual Human Demonstrations (Christian Dreher et al., 2024)</a></li><li><a href=#2127--261340-synapse-learning-preferential-concepts-from-visual-demonstrations-sadanand-modak-et-al-2024>(21/27 | 261/340) Synapse: Learning Preferential Concepts from Visual Demonstrations (Sadanand Modak et al., 2024)</a></li><li><a href=#2227--262340-trajectory-planning-of-robotic-manipulator-in-dynamic-environment-exploiting-drl-osama-ahmad-et-al-2024>(22/27 | 262/340) Trajectory Planning of Robotic Manipulator in Dynamic Environment Exploiting DRL (Osama Ahmad et al., 2024)</a></li><li><a href=#2327--263340-spatially-temporally-distributed-informative-path-planning-for-multi-robot-systems-binh-nguyen-et-al-2024>(23/27 | 263/340) Spatially temporally distributed informative path planning for multi-robot systems (Binh Nguyen et al., 2024)</a></li><li><a href=#2427--264340-human-stress-response-and-perceived-safety-during-encounters-with-quadruped-robots-ryan-gupta-et-al-2024>(24/27 | 264/340) Human Stress Response and Perceived Safety during Encounters with Quadruped Robots (Ryan Gupta et al., 2024)</a></li><li><a href=#2527--265340-tail-a-terrain-aware-multi-modal-slam-dataset-for-robot-locomotion-in-deformable-granular-environments-chen-yao-et-al-2024>(25/27 | 265/340) TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments (Chen Yao et al., 2024)</a></li><li><a href=#2627--266340-hearing-the-shape-of-an-arena-with-spectral-swarm-robotics-leo-cazenille-et-al-2024>(26/27 | 266/340) Hearing the shape of an arena with spectral swarm robotics (Leo Cazenille et al., 2024)</a></li><li><a href=#2727--267340-dhp-mapping-a-dense-panoptic-mapping-system-with-hierarchical-world-representation-and-label-optimization-techniques-tianshuai-hu-et-al-2024>(27/27 | 267/340) DHP-Mapping: A Dense Panoptic Mapping System with Hierarchical World Representation and Label Optimization Techniques (Tianshuai Hu et al., 2024)</a></li></ul></li><li><a href=#csir-4>cs.IR (4)</a><ul><li><a href=#14--268340-reinforcement-learning-based-recommender-systems-with-large-language-models-for-state-reward-and-action-modeling-jie-wang-et-al-2024>(1/4 | 268/340) Reinforcement Learning-based Recommender Systems with Large Language Models for State Reward and Action Modeling (Jie Wang et al., 2024)</a></li><li><a href=#24--269340-coarse-tuning-for-ad-hoc-document-retrieval-using-pre-trained-language-models-atsushi-keyaki-et-al-2024>(2/4 | 269/340) Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models (Atsushi Keyaki et al., 2024)</a></li><li><a href=#34--270340-play-to-your-strengths-collaborative-intelligence-of-conventional-recommender-models-and-large-language-models-yunjia-xi-et-al-2024>(3/4 | 270/340) Play to Your Strengths: Collaborative Intelligence of Conventional Recommender Models and Large Language Models (Yunjia Xi et al., 2024)</a></li><li><a href=#44--271340-uncovering-selective-state-space-models-capabilities-in-lifelong-sequential-recommendation-jiyuan-yang-et-al-2024>(4/4 | 271/340) Uncovering Selective State Space Model&rsquo;s Capabilities in Lifelong Sequential Recommendation (Jiyuan Yang et al., 2024)</a></li></ul></li><li><a href=#eessiv-13>eess.IV (13)</a><ul><li><a href=#113--272340-3d-effivitcaps-3d-efficient-vision-transformer-with-capsule-for-medical-image-segmentation-dongwei-gan-et-al-2024>(1/13 | 272/340) 3D-EffiViTCaps: 3D Efficient Vision Transformer with Capsule for Medical Image Segmentation (Dongwei Gan et al., 2024)</a></li><li><a href=#213--273340-brain-stroke-segmentation-using-deep-learning-models-a-comparative-study-ahmed-soliman-et-al-2024>(2/13 | 273/340) Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study (Ahmed Soliman et al., 2024)</a></li><li><a href=#313--274340-self-storm-deep-unrolled-self-supervised-learning-for-super-resolution-microscopy-yair-ben-sahel-et-al-2024>(3/13 | 274/340) Self-STORM: Deep Unrolled Self-Supervised Learning for Super-Resolution Microscopy (Yair Ben Sahel et al., 2024)</a></li><li><a href=#413--275340-multi-scale-texture-loss-for-ct-denoising-with-gans-francesco-di-feola-et-al-2024>(4/13 | 275/340) Multi-Scale Texture Loss for CT denoising with GANs (Francesco Di Feola et al., 2024)</a></li><li><a href=#513--276340-deepgleason-a-system-for-automated-gleason-grading-of-prostate-cancer-using-deep-neural-networks-dominik-müller-et-al-2024>(5/13 | 276/340) DeepGleason: a System for Automated Gleason Grading of Prostate Cancer using Deep Neural Networks (Dominik Müller et al., 2024)</a></li><li><a href=#613--277340-meddap-medical-dataset-enhancement-via-diversified-augmentation-pipeline-yasamin-medghalchi-et-al-2024>(6/13 | 277/340) MEDDAP: Medical Dataset Enhancement via Diversified Augmentation Pipeline (Yasamin Medghalchi et al., 2024)</a></li><li><a href=#713--278340-decoding-the-visual-attention-of-pathologists-to-reveal-their-level-of-expertise-souradeep-chakraborty-et-al-2024>(7/13 | 278/340) Decoding the visual attention of pathologists to reveal their level of expertise (Souradeep Chakraborty et al., 2024)</a></li><li><a href=#813--279340-a-study-in-dataset-pruning-for-image-super-resolution-brian-b-moser-et-al-2024>(8/13 | 279/340) A Study in Dataset Pruning for Image Super-Resolution (Brian B. Moser et al., 2024)</a></li><li><a href=#913--280340-joint-chest-x-ray-diagnosis-and-clinical-visual-attention-prediction-with-multi-stage-cooperative-learning-enhancing-interpretability-zirui-qiu-et-al-2024>(9/13 | 280/340) Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability (Zirui Qiu et al., 2024)</a></li><li><a href=#1013--281340-provably-robust-score-based-diffusion-posterior-sampling-for-plug-and-play-image-reconstruction-xingyu-xu-et-al-2024>(10/13 | 281/340) Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction (Xingyu Xu et al., 2024)</a></li><li><a href=#1113--282340-diff-def-diffusion-generated-deformation-fields-for-conditional-atlases-sophie-starck-et-al-2024>(11/13 | 282/340) Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases (Sophie Starck et al., 2024)</a></li><li><a href=#1213--283340-residual-dense-swin-transformer-for-continuous-depth-independent-ultrasound-imaging-jintong-hu-et-al-2024>(12/13 | 283/340) Residual Dense Swin Transformer for Continuous Depth-Independent Ultrasound Imaging (Jintong Hu et al., 2024)</a></li><li><a href=#1313--284340-rstar-rotational-streak-artifact-reduction-in-4d-cbct-using-separable-and-circular-convolutions-ziheng-deng-et-al-2024>(13/13 | 284/340) RSTAR: Rotational Streak Artifact Reduction in 4D CBCT using Separable and Circular Convolutions (Ziheng Deng et al., 2024)</a></li></ul></li><li><a href=#csma-3>cs.MA (3)</a><ul><li><a href=#13--285340-norm-violation-detection-in-multi-agent-systems-using-large-language-models-a-pilot-study-shawn-he-et-al-2024>(1/3 | 285/340) Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study (Shawn He et al., 2024)</a></li><li><a href=#23--286340-conformal-off-policy-prediction-for-multi-agent-systems-tom-kuipers-et-al-2024>(2/3 | 286/340) Conformal Off-Policy Prediction for Multi-Agent Systems (Tom Kuipers et al., 2024)</a></li><li><a href=#33--287340-towards-a-formalisation-of-value-based-actions-and-consequentialist-ethics-adam-wyner-et-al-2024>(3/3 | 287/340) Towards a Formalisation of Value-based Actions and Consequentialist Ethics (Adam Wyner et al., 2024)</a></li></ul></li><li><a href=#csit-6>cs.IT (6)</a><ul><li><a href=#16--288340-latency-aware-generative-semantic-communications-with-pre-trained-diffusion-models-li-qiao-et-al-2024>(1/6 | 288/340) Latency-Aware Generative Semantic Communications with Pre-Trained Diffusion Models (Li Qiao et al., 2024)</a></li><li><a href=#26--289340-6d-movable-antenna-enhanced-wireless-network-via-discrete-position-and-rotation-optimization-xiaodan-shao-et-al-2024>(2/6 | 289/340) 6D Movable Antenna Enhanced Wireless Network Via Discrete Position and Rotation Optimization (Xiaodan Shao et al., 2024)</a></li><li><a href=#36--290340-a-progressive-codebook-optimization-scheme-for-sparse-code-multiple-access-in-downlink-channels-tuofeng-lei-et-al-2024>(3/6 | 290/340) A Progressive Codebook Optimization Scheme for Sparse Code Multiple Access in Downlink Channels (Tuofeng Lei et al., 2024)</a></li><li><a href=#46--291340-design-and-performance-of-resonant-beam-communications----part-ii-mobile-scenario-dongxu-li-et-al-2024>(4/6 | 291/340) Design and Performance of Resonant Beam Communications &ndash; Part II: Mobile Scenario (Dongxu Li et al., 2024)</a></li><li><a href=#56--292340-single-carrier-delay-doppler-domain-equalization-yuto-hama-et-al-2024>(5/6 | 292/340) Single-Carrier Delay-Doppler Domain Equalization (Yuto Hama et al., 2024)</a></li><li><a href=#66--293340-movable-antenna-position-optimization-a-graph-based-approach-weidong-mei-et-al-2024>(6/6 | 293/340) Movable-Antenna Position Optimization: A Graph-based Approach (Weidong Mei et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--294340-leveraging-large-language-model-to-generate-a-novel-metaheuristic-algorithm-with-crispe-framework-rui-zhong-et-al-2024>(1/2 | 294/340) Leveraging Large Language Model to Generate a Novel Metaheuristic Algorithm with CRISPE Framework (Rui Zhong et al., 2024)</a></li><li><a href=#22--295340-qkformer-hierarchical-spiking-transformer-using-q-k-attention-chenlin-zhou-et-al-2024>(2/2 | 295/340) QKFormer: Hierarchical Spiking Transformer using Q-K Attention (Chenlin Zhou et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--296340-accuracy-aware-cooperative-sensing-and-computing-for-connected-autonomous-vehicles-xuehan-ye-et-al-2024>(1/2 | 296/340) Accuracy-Aware Cooperative Sensing and Computing for Connected Autonomous Vehicles (Xuehan Ye et al., 2024)</a></li><li><a href=#22--297340-relational-network-verification-xieyang-xu-et-al-2024>(2/2 | 297/340) Relational Network Verification (Xieyang Xu et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--298340-radiogat-a-joint-model-based-and-data-driven-framework-for-multi-band-radiomap-reconstruction-via-graph-attention-networks-xiaojie-li-et-al-2024>(1/2 | 298/340) RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks (Xiaojie Li et al., 2024)</a></li><li><a href=#22--299340-resolution-limit-of-single-photon-lidar-stanley-h-chan-et-al-2024>(2/2 | 299/340) Resolution Limit of Single-Photon LiDAR (Stanley H. Chan et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--300340-voicecraft-zero-shot-speech-editing-and-text-to-speech-in-the-wild-puyuan-peng-et-al-2024>(1/3 | 300/340) VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild (Puyuan Peng et al., 2024)</a></li><li><a href=#23--301340-hierarchical-recurrent-adapters-for-efficient-multi-task-adaptation-of-large-speech-models-tsendsuren-munkhdalai-et-al-2024>(2/3 | 301/340) Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models (Tsendsuren Munkhdalai et al., 2024)</a></li><li><a href=#33--302340-distributed-collaborative-anomalous-sound-detection-by-embedding-sharing-kota-dohi-et-al-2024>(3/3 | 302/340) Distributed collaborative anomalous sound detection by embedding sharing (Kota Dohi et al., 2024)</a></li></ul></li><li><a href=#mathoc-4>math.OC (4)</a><ul><li><a href=#14--303340-robust-finite-time-stabilization-of-linear-systems-with-limited-state-quantization-yu-zhou-et-al-2024>(1/4 | 303/340) Robust Finite-time Stabilization of Linear Systems with Limited State Quantization (Yu Zhou et al., 2024)</a></li><li><a href=#24--304340-data-driven-extrusion-force-control-tuning-for-3d-printing-xavier-guidetti-et-al-2024>(2/4 | 304/340) Data-Driven Extrusion Force Control Tuning for 3D Printing (Xavier Guidetti et al., 2024)</a></li><li><a href=#34--305340-approximation-with-random-shallow-relu-networks-with-applications-to-model-reference-adaptive-control-andrew-lamperski-et-al-2024>(3/4 | 305/340) Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control (Andrew Lamperski et al., 2024)</a></li><li><a href=#44--306340-output-feedback-synthesis-orbit-geometry-quotient-manifolds-and-lqg-direct-policy-optimization-spencer-kraisler-et-al-2024>(4/4 | 306/340) Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG Direct Policy Optimization (Spencer Kraisler et al., 2024)</a></li></ul></li><li><a href=#csdl-2>cs.DL (2)</a><ul><li><a href=#12--307340-chatgpt-contamination-estimating-the-prevalence-of-llms-in-the-scholarly-literature-andrew-gray-2024>(1/2 | 307/340) ChatGPT &lsquo;contamination&rsquo;: estimating the prevalence of LLMs in the scholarly literature (Andrew Gray, 2024)</a></li><li><a href=#22--308340-can-chatgpt-predict-article-retraction-based-on-twitter-mentions-er-te-zheng-et-al-2024>(2/2 | 308/340) Can ChatGPT predict article retraction based on Twitter mentions? (Er-Te Zheng et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--309340-investigation-of-the-effectiveness-of-applying-chatgpt-in-dialogic-teaching-using-electroencephalography-jiayue-zhang-et-al-2024>(1/1 | 309/340) Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography (Jiayue Zhang et al., 2024)</a></li></ul></li><li><a href=#csar-3>cs.AR (3)</a><ul><li><a href=#13--310340-partially-precise-computing-paradigm-for-efficient-hardware-implementation-of-application-specific-embedded-systems-mohsen-faryabi-et-al-2024>(1/3 | 310/340) Partially-Precise Computing Paradigm for Efficient Hardware Implementation of Application-Specific Embedded Systems (Mohsen Faryabi et al., 2024)</a></li><li><a href=#23--311340-electron-tunnelling-noise-programmable-random-variate-accelerator-for-monte-carlo-sampling-james-t-meech-et-al-2024>(2/3 | 311/340) Electron-Tunnelling-Noise Programmable Random Variate Accelerator for Monte Carlo Sampling (James T. Meech et al., 2024)</a></li><li><a href=#33--312340-sip-autotuning-gpu-native-schedules-via-stochastic-instruction-perturbation-guoliang-he-et-al-2024>(3/3 | 312/340) SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation (Guoliang He et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--313340-training-generative-adversarial-network-based-vocoder-with-limited-data-using-augmentation-conditional-discriminator-takuhiro-kaneko-et-al-2024>(1/1 | 313/340) Training Generative Adversarial Network-Based Vocoder with Limited Data Using Augmentation-Conditional Discriminator (Takuhiro Kaneko et al., 2024)</a></li></ul></li><li><a href=#mathna-5>math.NA (5)</a><ul><li><a href=#15--314340-high-order-transient-multidimensional-simulation-of-a-thermo-electro-chemo-mechanical-model-for-lithium-ion-batteries-jaime-mora-paz-2024>(1/5 | 314/340) High-order transient multidimensional simulation of a thermo-electro-chemo-mechanical model for Lithium-ion batteries (Jaime Mora-Paz, 2024)</a></li><li><a href=#25--315340-nonlinearsolvejl-high-performance-and-robust-solvers-for-systems-of-nonlinear-equations-in-julia-avik-pal-et-al-2024>(2/5 | 315/340) NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia (Avik Pal et al., 2024)</a></li><li><a href=#35--316340-stochastic-active-discretizations-for-accelerating-temporal-uncertainty-management-of-gas-pipeline-loads-jake-j-harmon-et-al-2024>(3/5 | 316/340) Stochastic Active Discretizations for Accelerating Temporal Uncertainty Management of Gas Pipeline Loads (Jake J. Harmon et al., 2024)</a></li><li><a href=#45--317340-the-cubic-nonlinear-schrödinger-equation-with-rough-potential-norbert-j-mauser-et-al-2024>(4/5 | 317/340) The cubic nonlinear Schrödinger equation with rough potential (Norbert J. Mauser et al., 2024)</a></li><li><a href=#55--318340-on-solution-of-tropical-discrete-best-approximation-problems-nikolai-krivulin-2024>(5/5 | 318/340) On solution of tropical discrete best approximation problems (Nikolai Krivulin, 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--319340-distributed-simulation-of-large-multi-body-systems-manas-kale-et-al-2024>(1/1 | 319/340) Distributed Simulation of Large Multi-body Systems (Manas Kale et al., 2024)</a></li></ul></li><li><a href=#csdc-4>cs.DC (4)</a><ul><li><a href=#14--320340-a-unified-cpu-gpu-protocol-for-gnn-training-yi-chien-lin-et-al-2024>(1/4 | 320/340) A Unified CPU-GPU Protocol for GNN Training (Yi-Chien Lin et al., 2024)</a></li><li><a href=#24--321340-lessons-learned-from-building-edge-software-system-testbeds-tobias-pfandzelter-et-al-2024>(2/4 | 321/340) Lessons Learned from Building Edge Software System Testbeds (Tobias Pfandzelter et al., 2024)</a></li><li><a href=#34--322340-union-an-automatic-workload-manager-for-accelerating-network-simulation-xin-wang-et-al-2024>(3/4 | 322/340) Union: An Automatic Workload Manager for Accelerating Network Simulation (Xin Wang et al., 2024)</a></li><li><a href=#44--323340-colonyos----a-meta-operating-system-for-distributed-computing-across-heterogeneous-platform-johan-kristiansson-2024>(4/4 | 323/340) ColonyOS &ndash; A Meta-Operating System for Distributed Computing Across Heterogeneous Platform (Johan Kristiansson, 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--324340-graph-bayesian-optimization-for-multiplex-influence-maximization-zirui-yuan-et-al-2024>(1/2 | 324/340) Graph Bayesian Optimization for Multiplex Influence Maximization (Zirui Yuan et al., 2024)</a></li><li><a href=#22--325340-a-recommender-network-perspective-on-the-informational-value-of-critics-and-crowds-pantelis-p-analytis-et-al-2024>(2/2 | 325/340) A recommender network perspective on the informational value of critics and crowds (Pantelis P. Analytis et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--326340-antigen-specific-antibody-design-via-direct-energy-based-preference-optimization-xiangxin-zhou-et-al-2024>(1/1 | 326/340) Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization (Xiangxin Zhou et al., 2024)</a></li></ul></li><li><a href=#csos-1>cs.OS (1)</a><ul><li><a href=#11--327340-aios-llm-agent-operating-system-kai-mei-et-al-2024>(1/1 | 327/340) AIOS: LLM Agent Operating System (Kai Mei et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--328340-backpropagation-through-space-time-and-the-brain-benjamin-ellenberger-et-al-2024>(1/1 | 328/340) Backpropagation through space, time, and the brain (Benjamin Ellenberger et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-2>physics.flu-dyn (2)</a><ul><li><a href=#12--329340-entropy-conservative-high-order-methods-for-high-enthalpy-gas-flows-georgii-oblapenko-et-al-2024>(1/2 | 329/340) Entropy-conservative high-order methods for high-enthalpy gas flows (Georgii Oblapenko et al., 2024)</a></li><li><a href=#22--330340-instantaneous-visual-analysis-of-blood-flow-in-stenoses-using-morphological-similarity-pepe-eulzer-et-al-2024>(2/2 | 330/340) Instantaneous Visual Analysis of Blood Flow in Stenoses Using Morphological Similarity (Pepe Eulzer et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#11--331340-multilevel-modeling-as-a-methodology-for-the-simulation-of-human-mobility-luca-serena-et-al-2024>(1/1 | 331/340) Multilevel Modeling as a Methodology for the Simulation of Human Mobility (Luca Serena et al., 2024)</a></li></ul></li><li><a href=#mathag-1>math.AG (1)</a><ul><li><a href=#11--332340-machine-learning-for-moduli-space-of-genus-two-curves-and-an-application-to-post-quantum-cryptography-elira-shaska-et-al-2024>(1/1 | 332/340) Machine learning for moduli space of genus two curves and an application to post-quantum cryptography (Elira Shaska et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--333340-an-equilibrium-analysis-of-the-arad-rubinstein-game-christian-ewerhart-et-al-2024>(1/1 | 333/340) An Equilibrium Analysis of the Arad-Rubinstein Game (Christian Ewerhart et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--334340-constant-selection-evolutionary-dynamics-on-weighted-networks-jnanajyoti-bhaumik-et-al-2024>(1/1 | 334/340) Constant-selection evolutionary dynamics on weighted networks (Jnanajyoti Bhaumik et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--335340-free-sets-in-planar-graphs-history-and-applications-vida-dujmovic-et-al-2024>(1/1 | 335/340) Free Sets in Planar Graphs: History and Applications (Vida Dujmovic et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--336340-ggdminer----discovery-of-graph-generating-dependencies-for-graph-data-profiling-larissa-c-shimomura-et-al-2024>(1/1 | 336/340) GGDMiner &ndash; Discovery of Graph Generating Dependencies for Graph Data Profiling (Larissa C. Shimomura et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--337340-robust-filter-design-for-graph-signals-lucia-testa-et-al-2024>(1/1 | 337/340) Robust Filter Design for Graph Signals (Lucia Testa et al., 2024)</a></li></ul></li><li><a href=#mathmg-1>math.MG (1)</a><ul><li><a href=#11--338340-optimality-of-spherical-codes-via-exact-semidefinite-programming-bounds-henry-cohn-et-al-2024>(1/1 | 338/340) Optimality of spherical codes via exact semidefinite programming bounds (Henry Cohn et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--339340-high-temperature-gibbs-states-are-unentangled-and-efficiently-preparable-ainesh-bakshi-et-al-2024>(1/1 | 339/340) High-Temperature Gibbs States are Unentangled and Efficiently Preparable (Ainesh Bakshi et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--340340-presenting-interval-pomsets-with-interfaces-amazigh-amrane-et-al-2024>(1/1 | 340/340) Presenting Interval Pomsets with Interfaces (Amazigh Amrane et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>