<!doctype html><html><head><title>arXiv @ 2024.03.12</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.03.12"><meta property="og:description" content="Primary Categories cs.AI (3) cs.CC (1) cs.CE (3) cs.CG (1) cs.CL (15) cs.CR (4) cs.CV (41) cs.CY (1) cs.DC (1) cs.DS (2) cs.GR (1) cs.HC (4) cs.IT (4) cs.LG (15) cs.MA (2) cs.NI (1) cs.RO (4) cs.SE (2) eess.IV (6) eess.SY (3) math.NA (1) math.OC (3) physics.flu-dyn (1) q-bio.NC (1) quant-ph (1) stat.ML (3) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Anomaly Detection 2 Automatic Speech Recognition 3 BERTScore 1 Bandit Algorithm 1 Benchmarking 4 6 1 Black Box 2 ChatGPT 1 1 Clustering 1 Contrastive Learning 3 Convolution 3 1 Convolutional Neural Network 3 1 Data Augmentation 2 1 1 Dialogue System 1 Diffusion Model 7 1 Essay Scoring 1 Fairness 1 Federated Learning 2 Few-shot 3 Fine-tuning 3 6 Foundation Model 4 1 Generative AI 1 Generative Adversarial Network 2 Geometry 1 Graph 1 1 6 Graph Classification 1 Graph Convolutional Network 2 Graph Neural Network 8 High-Resource 1 Image2text 1 In-context Learning 2 Knowledge Distillation 8 2 LLaMA 1 Large Language Model 13 3 Low-Resource 1 MNIST 1 Markov Decision Process 2 Model Compression 1 1 Multi-modal 9 Mutual Information 1 Named Entity Recognition 1 Node Classification 2 Object Detection 6 Out-of-distribution 1 1 Pre-trained Language Model 4 1 Probabilistic Model 1 Prompt 4 7 Prompt Learning 2 Pruning 1 1 Quantization 2 Question Answering 1 Reasoning 3 Recommendation 1 Reinforcement Learning 3 Representation Learning 2 Self-Attention 1 Self-Distillation 1 Self-supervised Learning 2 2 Semi-Supervised Learning 1 Sentiment Analysis 2 Simulation 2 Simulator 2 Sora 2 Style Transfer 2 Summarization 1 Supervised Learning 1 4 Text Understanding 1 Text2image 2 Tokenization 1 Transfer Learning 1 1 Transformer 2 7 2 Unsupervised Learning 4 Vision Transformer 2 Vision-and-Language 4 Weakly-supervised Learning 1 Word Embedding 1 Zero-shot 1 3 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202403/20240312000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-12T00:00:00+00:00"><meta name=description content="arXiv @ 2024.03.12"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202403/20240312000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Mar 12, 2024</p></div><div class=title><h1>arXiv @ 2024.03.12</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csai-3>cs.AI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csce-3>cs.CE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscl-15>cs.CL (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscv-41>cs.CV (41)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#cslg-15>cs.LG (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csro-4>cs.RO (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#eessiv-6>eess.IV (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td>2</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td>3</td><td></td><td></td></tr><tr><td>BERTScore</td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>4</td><td>6</td><td>1</td></tr><tr><td>Black Box</td><td></td><td>2</td><td></td></tr><tr><td>ChatGPT</td><td>1</td><td>1</td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td>3</td><td></td></tr><tr><td>Convolution</td><td></td><td>3</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>3</td><td>1</td></tr><tr><td>Data Augmentation</td><td>2</td><td>1</td><td>1</td></tr><tr><td>Dialogue System</td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>7</td><td>1</td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>2</td></tr><tr><td>Few-shot</td><td></td><td>3</td><td></td></tr><tr><td>Fine-tuning</td><td>3</td><td>6</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>4</td><td>1</td></tr><tr><td>Generative AI</td><td></td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>1</td><td>1</td><td>6</td></tr><tr><td>Graph Classification</td><td></td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>2</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>8</td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td></tr><tr><td>Image2text</td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>8</td><td>2</td></tr><tr><td>LLaMA</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>13</td><td>3</td><td></td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>MNIST</td><td></td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>2</td></tr><tr><td>Model Compression</td><td></td><td>1</td><td>1</td></tr><tr><td>Multi-modal</td><td></td><td>9</td><td></td></tr><tr><td>Mutual Information</td><td>1</td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>2</td></tr><tr><td>Object Detection</td><td></td><td>6</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>4</td><td>1</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>4</td><td>7</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td>2</td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td>1</td></tr><tr><td>Quantization</td><td></td><td></td><td>2</td></tr><tr><td>Question Answering</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td></td><td>3</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td>3</td></tr><tr><td>Representation Learning</td><td></td><td>2</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td>2</td><td>2</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>2</td><td></td></tr><tr><td>Simulator</td><td></td><td>2</td><td></td></tr><tr><td>Sora</td><td></td><td>2</td><td></td></tr><tr><td>Style Transfer</td><td></td><td>2</td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>1</td><td>4</td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>2</td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Transformer</td><td>2</td><td>7</td><td>2</td></tr><tr><td>Unsupervised Learning</td><td></td><td>4</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>4</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>3</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-15>cs.CL (15)</h2><h3 id=115--1124-score-self-supervised-correspondence-fine-tuning-for-improved-content-representations-amit-meghanani-et-al-2024>(1/15 | 1/124) SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations (Amit Meghanani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amit Meghanani, Thomas Hain. (2024)<br><strong>SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations</strong><br><button class=copy-to-clipboard title="SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 83<br>Keywords: Benchmarking, Data Augmentation, Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Learning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06260v1.pdf filename=2403.06260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing interest in cost-effective <b>self-supervised</b> <b>fine-tuning</b> (SSFT) of <b>self-supervised</b> <b>learning</b> (SSL)-based <b>speech</b> <b>models</b> to obtain task-specific representations. These task-specific representations are used for robust performance on various downstream tasks by <b>fine-tuning</b> on the labelled <b>data.</b> <b>This</b> work presents a cost-effective SSFT method named <b>Self-supervised</b> <b>Correspondence</b> (SCORE) <b>fine-tuning</b> to adapt the SSL <b>speech</b> <b>representations</b> for content-related tasks. The proposed method uses a correspondence training strategy, aiming to learn similar representations from perturbed <b>speech</b> <b>and</b> original <b>speech.</b> <b>Commonly</b> used <b>data</b> <b>augmentation</b> techniques for content-related tasks <b>(ASR)</b> are applied to obtain perturbed <b>speech.</b> <b>SCORE</b> <b>fine-tuned</b> HuBERT outperforms the vanilla HuBERT on SUPERB <b>benchmark</b> with only a few hours of <b>fine-tuning</b> (&lt; 5 hrs) on a single GPU for <b>automatic</b> <b>speech</b> <b>recognition,</b> phoneme recognition, and query-by-example tasks, with relative improvements of 1.09%, 3.58%, and 12.65%, respectively. SCORE provides competitive results with the recently proposed SSFT method SPIN, using only 1/3 of the processed <b>speech</b> <b>compared</b> to SPIN.</p></p class="citation"></blockquote><h3 id=215--2124-can-large-language-models-automatically-score-proficiency-of-written-essays-watheq-mansour-et-al-2024>(2/15 | 2/124) Can Large Language Models Automatically Score Proficiency of Written Essays? (Watheq Mansour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Watheq Mansour, Salam Albatarni, Sohaila Eltanbouly, Tamer Elsayed. (2024)<br><strong>Can Large Language Models Automatically Score Proficiency of Written Essays?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Automatically Score Proficiency of Written Essays?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: ChatGPT, LLaMA, Transformer, Essay Scoring, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06149v1.pdf filename=2403.06149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although several methods were proposed to address the problem of automated <b>essay</b> <b>scoring</b> (AES) in the last 50 years, there is still much to desire in terms of effectiveness. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are <b>transformer-based</b> models that demonstrate extraordinary capabilities on various tasks. In this paper, we test the ability of <b>LLMs,</b> given their powerful linguistic knowledge, to analyze and effectively score written <b>essays.</b> <b>We</b> experimented with two popular <b>LLMs,</b> namely <b>ChatGPT</b> and <b>Llama.</b> We aim to check if these models can do this task and, if so, how their performance is positioned among the state-of-the-art (SOTA) models across two levels, holistically and per individual writing trait. We utilized <b>prompt-engineering</b> tactics in designing four different <b>prompts</b> to bring their maximum potential to this task. Our experiments conducted on the ASAP dataset revealed several interesting observations. First, choosing the right <b>prompt</b> depends highly on the model and nature of the task. Second, the two <b>LLMs</b> exhibited comparable average performance in AES, with a slight advantage for <b>ChatGPT.</b> Finally, despite the performance gap between the two <b>LLMs</b> and SOTA models in terms of predictions, they provide feedback to enhance the quality of the <b>essays,</b> <b>which</b> can potentially help both teachers and students.</p></p class="citation"></blockquote><h3 id=315--3124-personalized-lora-for-human-centered-text-understanding-you-zhang-et-al-2024>(3/15 | 3/124) Personalized LoRA for Human-Centered Text Understanding (You Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>You Zhang, Jin Wang, Liang-Chih Yu, Dan Xu, Xuejie Zhang. (2024)<br><strong>Personalized LoRA for Human-Centered Text Understanding</strong><br><button class=copy-to-clipboard title="Personalized LoRA for Human-Centered Text Understanding" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Mutual Information, Pre-trained Language Model, Pre-trained Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06208v1.pdf filename=2403.06208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effectively and efficiently adapting a <b>pre-trained</b> <b>language</b> <b>model</b> <b>(PLM)</b> for human-centered <b>text</b> <b>understanding</b> (HCTU) is challenging since user tokens are million-level in most personalized applications and do not have concrete explicit semantics. A standard and parameter-efficient approach (e.g., LoRA) necessitates memorizing numerous suits of adapters for each user. In this work, we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically deploying in <b>PLMs.</b> Moreover, a personalized dropout and a <b>mutual</b> <b>information</b> maximizing strategies are adopted and hence the proposed PLoRA can be well adapted to few/zero-shot learning scenarios for the cold-start issue. Experiments conducted on four <b>benchmark</b> datasets show that the proposed method outperforms existing methods in full/few/zero-shot learning scenarios for the HCTU task, even though it has fewer trainable parameters. For reproducibility, the code for this paper is available at: <a href=https://github.com/yoyo-yun/PLoRA>https://github.com/yoyo-yun/PLoRA</a>.</p></p class="citation"></blockquote><h3 id=415--4124-are-you-being-tracked-discover-the-power-of-zero-shot-trajectory-tracing-with-llms-huanqi-yang-et-al-2024>(4/15 | 4/124) Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs! (Huanqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huanqi Yang, Sijie Ji, Rucheng Wu, Weitao Xu. (2024)<br><strong>Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!</strong><br><button class=copy-to-clipboard title="Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs!" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06201v1.pdf filename=2403.06201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a burgeoning discussion around the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in acting as fundamental components that can be seamlessly incorporated into Artificial Intelligence of Things (AIoT) to interpret complex trajectories. This study introduces LLMTrack, a model that illustrates how <b>LLMs</b> can be leveraged for <b>Zero-Shot</b> Trajectory Recognition by employing a novel single-prompt technique that combines role-play and think step-by-step methodologies with unprocessed Inertial Measurement Unit (IMU) data. We evaluate the model using real-world datasets designed to challenge it with distinct trajectories characterized by indoor and outdoor scenarios. In both test scenarios, LLMTrack not only meets but exceeds the performance <b>benchmarks</b> set by traditional machine learning approaches and even contemporary state-of-the-art deep learning models, all without the requirement of training on specialized datasets. The results of our research suggest that, with strategically designed <b>prompts,</b> <b>LLMs</b> can tap into their extensive knowledge base and are well-equipped to analyze raw sensor data with remarkable effectiveness.</p></p class="citation"></blockquote><h3 id=515--5124-target-constrained-bidirectional-planning-for-generation-of-target-oriented-proactive-dialogue-jian-wang-et-al-2024>(5/15 | 5/124) Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue (Jian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Wang, Dongding Lin, Wenjie Li. (2024)<br><strong>Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue</strong><br><button class=copy-to-clipboard title="Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Recommendation, Transformer, Dialogue System, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06063v1.pdf filename=2403.06063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Target-oriented proactive <b>dialogue</b> <b>systems</b> aim to lead conversations from a <b>dialogue</b> <b>context</b> toward a pre-determined target, such as making <b>recommendations</b> on designated items or introducing new specific topics. To this end, it is critical for such <b>dialogue</b> <b>systems</b> to plan reasonable actions to drive the conversation proactively, and meanwhile, to plan appropriate topics to move the conversation forward to the target topic smoothly. In this work, we mainly focus on effective <b>dialogue</b> <b>planning</b> for target-oriented <b>dialogue</b> <b>generation.</b> Inspired by decision-making theories in cognitive science, we propose a novel target-constrained bidirectional planning (TRIP) approach, which plans an appropriate <b>dialogue</b> <b>path</b> by looking ahead and looking back. By formulating the planning as a generation task, our TRIP bidirectionally generates a <b>dialogue</b> <b>path</b> consisting of a sequence of &lt;action, topic> pairs using two <b>Transformer</b> decoders. They are expected to supervise each other and converge on consistent actions and topics by minimizing the decision gap and contrastive generation of targets. Moreover, we propose a target-constrained decoding algorithm with a bidirectional agreement to better control the planning process. Subsequently, we adopt the planned <b>dialogue</b> <b>paths</b> to guide <b>dialogue</b> <b>generation</b> in a pipeline manner, where we explore two variants: <b>prompt-based</b> generation and plan-controlled generation. Extensive experiments are conducted on two challenging <b>dialogue</b> <b>datasets,</b> which are re-purposed for exploring target-oriented <b>dialogue.</b> <b>Our</b> automatic and human evaluations demonstrate that the proposed methods significantly outperform various baseline models.</p></p class="citation"></blockquote><h3 id=615--6124-fine-grainedly-synthesize-streaming-data-based-on-large-language-models-with-graph-structure-understanding-for-data-sparsity-xin-zhang-et-al-2024>(6/15 | 6/124) Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity (Xin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu. (2024)<br><strong>Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity</strong><br><button class=copy-to-clipboard title="Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, Sentiment Analysis, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06139v1.pdf filename=2403.06139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the sparsity of user data, <b>sentiment</b> <b>analysis</b> on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of <b>LLMs</b> has introduced new solutions to such problems by leveraging <b>graph</b> structures to generate supplementary user profiles. However, previous approaches have not fully utilized the <b>graph</b> understanding capabilities of <b>LLMs</b> and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design <b>LLMs</b> to comprehensively understand three key <b>graph</b> elements in streaming data, including Local-global <b>Graph</b> Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality synthetic data to effectively address sparsity across different categories. Experimental results on three real datasets demonstrate significant performance improvements, with synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%, respectively.</p></p class="citation"></blockquote><h3 id=715--7124-from-instructions-to-constraints-language-model-alignment-with-automatic-constraint-verification-fei-wang-et-al-2024>(7/15 | 7/124) From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification (Fei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Wang, Chao Shang, Sarthak Jain, Shuai Wang, Qiang Ning, Bonan Min, Vittorio Castelli, Yassine Benajiba, Dan Roth. (2024)<br><strong>From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification</strong><br><button class=copy-to-clipboard title="From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06326v1.pdf filename=2403.06326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each <b>prompt</b> and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive <b>summarization,</b> and temporal <b>question</b> <b>answering</b> show that ACT is able to enhance LMs&rsquo; capability to adhere to different classes of constraints, thereby improving task performance. Further experiments show that the constraint-following capabilities are transferable.</p></p class="citation"></blockquote><h3 id=815--8124-unpacking-tokenization-evaluating-text-compression-and-its-correlation-with-model-performance-omer-goldman-et-al-2024>(8/15 | 8/124) Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance (Omer Goldman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao, Idan Szpektor, Reut Tsarfaty. (2024)<br><strong>Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance</strong><br><button class=copy-to-clipboard title="Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06265v1.pdf filename=2403.06265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite it being the cornerstone of BPE, the most common <b>tokenization</b> algorithm, the importance of compression in the <b>tokenization</b> process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of <b>pre-trained</b> <b>language</b> <b>models.</b> We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and <b>fine-tune</b> them over several tasks. We show that there is a correlation between tokenizers&rsquo; compression and models&rsquo; downstream performance, suggesting that compression is a reliable intrinsic indicator of <b>tokenization</b> quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance.</p></p class="citation"></blockquote><h3 id=915--9124-identifying-and-interpreting-non-aligned-human-conceptual-representations-using-language-modeling-wanqian-bao-et-al-2024>(9/15 | 9/124) Identifying and interpreting non-aligned human conceptual representations using language modeling (Wanqian Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanqian Bao, Uri Hasson. (2024)<br><strong>Identifying and interpreting non-aligned human conceptual representations using language modeling</strong><br><button class=copy-to-clipboard title="Identifying and interpreting non-aligned human conceptual representations using language modeling" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Pruning, Supervised Learning, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06204v1.pdf filename=2403.06204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The question of whether people&rsquo;s experience in the world shapes conceptual representation and lexical semantics is longstanding. <b>Word-association,</b> <b>feature-listing</b> and similarity rating tasks aim to address this question but require a subjective interpretation of the latent dimensions identified. In this study, we introduce a <b>supervised</b> representational-alignment method that (i) determines whether two groups of individuals share the same basis of a certain category, and (ii) explains in what respects they differ. In applying this method, we show that congenital blindness induces conceptual reorganization in both a-modal and sensory-related verbal domains, and we identify the associated semantic shifts. We first apply <b>supervised</b> feature-pruning to a language model (GloVe) to optimize prediction accuracy of human similarity judgments from <b>word</b> <b>embeddings.</b> <b>Pruning</b> identifies one subset of retained GloVe features that optimizes prediction of judgments made by sighted individuals and another subset that optimizes judgments made by blind. A linear probing analysis then interprets the latent semantics of these feature-subsets by learning a mapping from the retained GloVe features to 65 interpretable semantic dimensions. We applied this approach to seven semantic domains, including verbs related to motion, sight, touch, and amodal verbs related to knowledge acquisition. We find that blind individuals more strongly associate social and cognitive meanings to verbs related to motion or those communicating non-speech vocal utterances (e.g., whimper, moan). Conversely, for amodal verbs, they demonstrate much sparser information. Finally, for some verbs, representations of blind and sighted are highly similar. The study presents a formal approach for studying interindividual differences in <b>word</b> <b>meaning,</b> and the first demonstration of how blindness impacts conceptual representation of everyday verbs.</p></p class="citation"></blockquote><h3 id=1015--10124-large-language-models-on-fine-grained-emotion-detection-dataset-with-data-augmentation-and-transfer-learning-kaipeng-wang-et-al-2024>(10/15 | 10/124) Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning (Kaipeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaipeng Wang, Zhi Jing, Yongye Su, Yikun Han. (2024)<br><strong>Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning</strong><br><button class=copy-to-clipboard title="Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Data Augmentation, Transfer Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06108v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06108v1.pdf filename=2403.06108v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into enhancing the classification performance on the GoEmotions dataset, a <b>large,</b> <b>manually</b> <b>annotated</b> dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.</p></p class="citation"></blockquote><h3 id=1115--11124-ensemble-language-models-for-multilingual-sentiment-analysis-md-arid-hasan-2024>(11/15 | 11/124) Ensemble Language Models for Multilingual Sentiment Analysis (Md Arid Hasan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Arid Hasan. (2024)<br><strong>Ensemble Language Models for Multilingual Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Ensemble Language Models for Multilingual Sentiment Analysis" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Sentiment Analysis, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06060v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06060v1.pdf filename=2403.06060v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of social media enables us to analyze user opinions. In recent times, <b>sentiment</b> <b>analysis</b> has shown a prominent research gap in understanding human <b>sentiment</b> <b>based</b> on the content shared on social media. Although <b>sentiment</b> <b>analysis</b> for commonly spoken languages has advanced significantly, <b>low-resource</b> languages like Arabic continue to get little research due to resource limitations. In this study, we explore <b>sentiment</b> <b>analysis</b> on tweet texts from SemEval-17 and the Arabic <b>Sentiment</b> <b>Tweet</b> dataset. Moreover, We investigated four <b>pretrained</b> <b>language</b> <b>models</b> and proposed two ensemble language models. Our findings include monolingual models exhibiting superior performance and ensemble models outperforming the baseline while the majority voting ensemble outperforms the English language.</p></p class="citation"></blockquote><h3 id=1215--12124-editing-conceptual-knowledge-for-large-language-models-xiaohan-wang-et-al-2024>(12/15 | 12/124) Editing Conceptual Knowledge for Large Language Models (Xiaohan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen. (2024)<br><strong>Editing Conceptual Knowledge for Large Language Models</strong><br><button class=copy-to-clipboard title="Editing Conceptual Knowledge for Large Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs-IR, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06259v1.pdf filename=2403.06259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a growing interest in knowledge editing for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Current approaches and evaluations merely explore the instance-level editing, while whether <b>LLMs</b> possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for <b>LLMs,</b> by constructing a novel <b>benchmark</b> dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in <b>LLMs,</b> leading to poor performance. We anticipate this can inspire further progress in better understanding <b>LLMs.</b> Our project homepage is available at <a href=https://zjunlp.github.io/project/ConceptEdit>https://zjunlp.github.io/project/ConceptEdit</a>.</p></p class="citation"></blockquote><h3 id=1315--13124-fmpaf-how-do-fed-chairs-affect-the-financial-market-a-fine-grained-monetary-policy-analysis-framework-on-their-language-yayue-deng-et-al-2024>(13/15 | 13/124) FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language (Yayue Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yayue Deng, Mohan Xu, Yao Tang. (2024)<br><strong>FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language</strong><br><button class=copy-to-clipboard title="FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CE, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06115v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06115v1.pdf filename=2403.06115v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The effectiveness of central bank communication is a crucial aspect of monetary policy transmission. While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis. In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets. We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios. Based on our preferred specification, a one-unit increase in the sentiment score is associated with an increase of the price of S&amp;P 500 Exchange-Traded Fund by approximately 500 basis points, a 15-basis-point decrease in the policy interest rate, while not leading to a significant response in exchange rates.</p></p class="citation"></blockquote><h3 id=1415--14124-can-llm-substitute-human-labeling-a-case-study-of-fine-grained-chinese-address-entity-recognition-dataset-for-uav-delivery-yuxuan-yao-et-al-2024>(14/15 | 14/124) Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery (Yuxuan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song. (2024)<br><strong>Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery</strong><br><button class=copy-to-clipboard title="Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 20<br>Keywords: Named Entity Recognition, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06097v1.pdf filename=2403.06097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame \textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle delivery systems. The dataset encompasses a diverse range of five categories, enabling comprehensive training and evaluation of <b>NER</b> models. To construct this dataset, we sourced the data from a real-world UAV delivery system and conducted a rigorous data cleaning and desensitization process to ensure privacy and data integrity. The resulting dataset, consisting of around 12,000 annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage \textbf{M}odel annotation. We evaluated classical <b>NER</b> models on our dataset and provided in-depth analysis. The dataset and models are publicly available at \url{https://github.com/zhhvvv/CNER-UAV}.</p></p class="citation"></blockquote><h3 id=1515--15124-lieder-linguistically-informed-evaluation-for-discourse-entity-recognition-xiaomeng-zhu-et-al-2024>(15/15 | 15/124) LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition (Xiaomeng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaomeng Zhu, Robert Frank. (2024)<br><strong>LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition</strong><br><button class=copy-to-clipboard title="LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06301v1.pdf filename=2403.06301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that <b>large</b> <b>language</b> <b>models</b> have basic, if imperfect, DE recognition abilities (Schuster and Linzen, 2022), it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models&rsquo; knowledge of four crucial semantic properties: existence, uniqueness, plurality, and novelty. We find evidence that state-of-the-art <b>large</b> <b>language</b> <b>models</b> exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities.</p></p class="citation"></blockquote><h2 id=cscv-41>cs.CV (41)</h2><h3 id=141--16124-knowledge-distillation-of-convolutional-neural-networks-through-feature-map-transformation-using-decision-trees-maddimsetti-srinivas-et-al-2024>(1/41 | 16/124) Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees (Maddimsetti Srinivas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maddimsetti Srinivas, Debdoot Sheet. (2024)<br><strong>Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees</strong><br><button class=copy-to-clipboard title="Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-SP<br>Keyword Score: 75<br>Keywords: MNIST, Black Box, Convolution, Convolutional Neural Network, Convolutional Neural Network, Knowledge Distillation, Knowledge Distillation, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06089v1.pdf filename=2403.06089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The interpretation of <b>reasoning</b> by Deep Neural Networks (DNN) is still challenging due to their perceived <b>black-box</b> <b>nature.</b> Therefore, deploying DNNs in several real-world tasks is restricted by the lack of transparency of these models. We propose a <b>distillation</b> approach by extracting features from the final layer of the <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> to address insights to its <b>reasoning.</b> The feature maps in the final layer of a <b>CNN</b> are transformed into a one-dimensional feature vector using a fully connected layer. Subsequently, the extracted features are used to train a decision tree to achieve the best accuracy under constraints of depth and nodes. We use the medical images of dermaMNIST, octMNIST, and pneumoniaMNIST from the medical <b>MNIST</b> datasets to demonstrate our proposed work. We observed that performance of the decision tree is as good as a <b>CNN</b> with minimum complexity. The results encourage interpreting decisions made by the <b>CNNs</b> using decision trees.</p></p class="citation"></blockquote><h3 id=241--17124-in-context-prompt-learning-for-test-time-vision-recognition-with-frozen-vision-language-model-junhui-yin-et-al-2024>(2/41 | 17/124) In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model (Junhui Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhui Yin, Xinyu Zhang, Lin Wu, Xianghua Xie, Xiaojie Wang. (2024)<br><strong>In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model</strong><br><button class=copy-to-clipboard title="In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Unsupervised Learning, Zero-shot, In-context Learning, In-context Learning, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06126v1.pdf filename=2403.06126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing pre-trained <b>vision-language</b> models, e.g., CLIP, have demonstrated impressive <b>zero-shot</b> generalization capabilities in various downstream tasks. However, the performance of these models will degrade significantly when test inputs present different distributions. To this end, we explore the concept of test-time <b>prompt</b> <b>tuning</b> (TTPT), which enables the adaptation of the CLIP model to novel downstream tasks through only one step of optimization on an <b>unsupervised</b> objective that involves the test sample. Motivated by <b>in-context</b> <b>learning</b> within field of natural language processing (NLP), we propose <b>In-Context</b> <b>Prompt</b> <b>Learning</b> (InCPL) for test-time visual recognition task. InCPL involves associating a new test sample with very few or even just one labeled example as its <b>in-context</b> <b>prompt.</b> <b>As</b> a result, it can reliably estimate a label for the test sample, thereby facilitating the model adaptation process. InCPL first employs a token net to represent language descriptions as visual <b>prompts</b> <b>that</b> the vision encoder of a CLIP model can comprehend. Paired with <b>in-context</b> <b>examples,</b> we further propose a context-aware <b>unsupervised</b> loss to optimize test sample-aware visual <b>prompts.</b> <b>This</b> optimization allows a pre-trained, frozen CLIP model to be adapted to a test sample from any task using its learned adaptive <b>prompt.</b> <b>Our</b> method has demonstrated superior performance and achieved state-of-the-art results across various downstream datasets.</p></p class="citation"></blockquote><h3 id=341--18124-towards-in-vehicle-multi-task-facial-attribute-recognition-investigating-synthetic-data-and-vision-foundation-models-esmaeil-seraj-et-al-2024>(3/41 | 18/124) Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models (Esmaeil Seraj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esmaeil Seraj, Walter Talamonti. (2024)<br><strong>Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models</strong><br><button class=copy-to-clipboard title="Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Foundation Model, Out-of-distribution, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06088v1.pdf filename=2403.06088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art <b>vision</b> <b>foundation</b> <b>models</b> in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing <b>transfer</b> <b>learning</b> techniques with both pre-trained <b>Vision</b> <b>Transformer</b> (ViT) and Residual Network (ResNet) models, we explore various training and adaptation methods to optimize performance, particularly when data availability is limited. We provide extensive post-evaluation analysis, investigating the effects of synthetic data distributions on model performance in in-distribution data and <b>out-of-distribution</b> inference. Our study unveils counter-intuitive findings, notably the superior performance of ResNet over ViTs in our specific multi-task context, which is attributed to the mismatch in model complexity relative to task complexity. Our results highlight the challenges and opportunities for enhancing the use of synthetic data and <b>vision</b> <b>foundation</b> <b>models</b> in practical applications.</p></p class="citation"></blockquote><h3 id=441--19124-a-streamlined-approach-to-multimodal-few-shot-class-incremental-learning-for-fine-grained-datasets-thang-doan-et-al-2024>(4/41 | 19/124) A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets (Thang Doan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thang Doan, Sima Behpour, Xin Li, Wenbin He, Liang Gou, Liu Ren. (2024)<br><strong>A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets</strong><br><button class=copy-to-clipboard title="A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Few-shot, Fine-tuning, Multi-modal, Multi-modal, Image2text, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06295v1.pdf filename=2403.06295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> Class-Incremental Learning (FSCIL) poses the challenge of retaining prior knowledge while learning from limited new data streams, all without overfitting. The rise of <b>Vision-Language</b> models (VLMs) has unlocked numerous applications, leveraging their existing knowledge to <b>fine-tune</b> on custom data. However, training the whole model is computationally prohibitive, and VLMs while being versatile in general domains still struggle with fine-grained datasets crucial for many applications. We tackle these challenges with two proposed simple modules. The first, Session-Specific <b>Prompts</b> (SSP), enhances the separability of <b>image-text</b> embeddings across sessions. The second, Hyperbolic distance, compresses representations of <b>image-text</b> pairs within the same class while expanding those from different classes, leading to better representations. Experimental results demonstrate an average 10-point increase compared to baselines while requiring at least 8 times fewer trainable parameters. This improvement is further underscored on our three newly introduced fine-grained datasets.</p></p class="citation"></blockquote><h3 id=541--20124-worldgpt-a-sora-inspired-video-ai-agent-as-rich-world-models-from-text-and-image-inputs-deshun-yang-et-al-2024>(5/41 | 20/124) WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs (Deshun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang, Cindy Yang, Yuexian Zou. (2024)<br><strong>WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs</strong><br><button class=copy-to-clipboard title="WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Diffusion Model, Sora, Knowledge Distillation, Multi-modal, Multi-modal, ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07944v1.pdf filename=2403.07944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several text-to-video <b>diffusion</b> <b>models</b> have demonstrated commendable capabilities in synthesizing high-quality video content. However, it remains a formidable challenge pertaining to maintaining temporal consistency and ensuring action smoothness throughout the generated sequences. In this paper, we present an innovative video generation AI agent that harnesses the power of <b>Sora-inspired</b> <b>multimodal</b> learning to build skilled world models framework based on textual <b>prompts</b> and accompanying images. The framework includes two parts: <b>prompt</b> enhancer and full video translation. The first part employs the capabilities of <b>ChatGPT</b> to meticulously <b>distill</b> and proactively construct precise <b>prompts</b> for each subsequent step, thereby guaranteeing the utmost accuracy in <b>prompt</b> communication and accurate execution in following model operations. The second part employ compatible with existing advanced <b>diffusion</b> <b>techniques</b> to expansively generate and refine the key frame at the conclusion of a video. Then we can expertly harness the power of leading and trailing key frames to craft videos with enhanced temporal consistency and action smoothness. The experimental results confirm that our method has strong effectiveness and novelty in constructing world models from text and image inputs over the other methods.</p></p class="citation"></blockquote><h3 id=641--21124-on-depth-prediction-for-autonomous-driving-using-self-supervised-learning-houssem-boulahbal-2024>(6/41 | 21/124) On depth prediction for autonomous driving using self-supervised learning (Houssem Boulahbal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Houssem Boulahbal. (2024)<br><strong>On depth prediction for autonomous driving using self-supervised learning</strong><br><button class=copy-to-clipboard title="On depth prediction for autonomous driving using self-supervised learning" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 55<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Geometry, Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06194v1.pdf filename=2403.06194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Perception of the environment is a critical component for enabling autonomous driving. It provides the vehicle with the ability to comprehend its surroundings and make informed decisions. Depth prediction plays a pivotal role in this process, as it helps the understanding of the <b>geometry</b> and motion of the environment. This thesis focuses on the challenge of depth prediction using monocular <b>self-supervised</b> <b>learning</b> techniques. The problem is approached from a broader perspective first, exploring conditional <b>generative</b> <b>adversarial</b> <b>networks</b> (cGANs) as a potential technique to achieve better generalization was performed. In doing so, a fundamental contribution to the conditional <b>GANs,</b> the acontrario cGAN was proposed. The second contribution entails a single image-to-depth <b>self-supervised</b> <b>method,</b> proposing a solution for the rigid-scene assumption using a novel <b>transformer-based</b> method that outputs a pose for each dynamic object. The third significant aspect involves the introduction of a video-to-depth map forecasting approach. This method serves as an extension of <b>self-supervised</b> <b>techniques</b> to predict future depths. This involves the creation of a novel <b>transformer</b> model capable of predicting the future depth of a given scene. Moreover, the various limitations of the aforementioned methods were addressed and a video-to-video depth maps model was proposed. This model leverages the spatio-temporal consistency of the input and output sequence to predict a more accurate depth sequence output. These methods have significant applications in autonomous driving (AD) and advanced driver assistance systems (ADAS).</p></p class="citation"></blockquote><h3 id=741--22124-restore-towards-feature-shift-for-vision-language-prompt-learning-yuncheng-yang-et-al-2024>(7/41 | 22/124) RESTORE: Towards Feature Shift for Vision-Language Prompt Learning (Yuncheng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuncheng Yang, Chuyan Zhang, Zuopeng Yang, Yuting Gao, Yulei Qin, Ke Li, Xing Sun, Jie Yang, Yun Gu. (2024)<br><strong>RESTORE: Towards Feature Shift for Vision-Language Prompt Learning</strong><br><button class=copy-to-clipboard title="RESTORE: Towards Feature Shift for Vision-Language Prompt Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06136v1.pdf filename=2403.06136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> <b>learning</b> is effective for <b>fine-tuning</b> <b>foundation</b> <b>models</b> to improve their generalization across a variety of downstream tasks. However, the <b>prompts</b> <b>that</b> are independently optimized along a single modality path, may sacrifice the <b>vision-language</b> alignment of pre-trained models in return for improved performance on specific tasks and classes, leading to poorer generalization. In this paper, we first demonstrate that <b>prompt</b> <b>tuning</b> along only one single branch of CLIP (e.g., language or vision) is the reason why the misalignment occurs. Without proper regularization across the learnable parameters in different modalities, <b>prompt</b> <b>learning</b> violates the original pre-training constraints inherent in the two-tower architecture. To address such misalignment, we first propose feature shift, which is defined as the variation of embeddings after introducing the learned <b>prompts,</b> <b>to</b> serve as an explanatory tool. We dive into its relation with generalizability and thereafter propose RESTORE, a <b>multi-modal</b> <b>prompt</b> <b>learning</b> method that exerts explicit constraints on cross-modal consistency. To be more specific, to prevent feature misalignment, a feature shift consistency is introduced to synchronize inter-modal feature shifts by measuring and regularizing the magnitude of discrepancy during <b>prompt</b> <b>tuning.</b> In addition, we propose a &ldquo;surgery&rdquo; block to avoid short-cut hacking, where cross-modal misalignment can still be severe if the feature shift of each modality varies drastically at the same rate. It is implemented as feed-forward adapters upon both modalities to alleviate the misalignment problem. Extensive experiments on 15 datasets demonstrate that our method outperforms the state-of-the-art <b>prompt</b> <b>tuning</b> methods without compromising feature alignment.</p></p class="citation"></blockquote><h3 id=841--23124-decoupled-contrastive-learning-for-long-tailed-recognition-shiyu-xuan-et-al-2024>(8/41 | 23/124) Decoupled Contrastive Learning for Long-Tailed Recognition (Shiyu Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyu Xuan, Shiliang Zhang. (2024)<br><strong>Decoupled Contrastive Learning for Long-Tailed Recognition</strong><br><button class=copy-to-clipboard title="Decoupled Contrastive Learning for Long-Tailed Recognition" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 48<br>Keywords: Benchmarking, Contrastive Learning, Knowledge Distillation, Representation Learning, Self-Distillation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06151v1.pdf filename=2403.06151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>Contrastive</b> <b>Loss</b> (SCL) is popular in visual <b>representation</b> <b>learning.</b> Given an anchor image, SCL pulls two types of positive samples, i.e., its augmentation and other images from the same class together, while pushes negative images apart to optimize the learned embedding. In the scenario of long-tailed recognition, where the number of samples in each class is imbalanced, treating two types of positive samples equally leads to the biased optimization for intra-category distance. In addition, similarity relationship among negative samples, that are ignored by SCL, also presents meaningful semantic cues. To improve the performance on long-tailed recognition, this paper addresses those two issues of SCL by decoupling the training objective. Specifically, it decouples two types of positives in SCL and optimizes their relations toward different objectives to alleviate the influence of the imbalanced dataset. We further propose a patch-based self <b>distillation</b> to transfer knowledge from head to tail classes to relieve the under-representation of tail classes. It uses patch-based features to mine shared visual patterns among different instances and leverages a self <b>distillation</b> procedure to transfer such knowledge. Experiments on different long-tailed classification <b>benchmarks</b> demonstrate the superiority of our method. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT dataset. Combined with the ensemble-based method, the performance can be further boosted to 59.7%, which substantially outperforms many recent works. The code is available at <a href=https://github.com/SY-Xuan/DSCL>https://github.com/SY-Xuan/DSCL</a>.</p></p class="citation"></blockquote><h3 id=941--24124-fastvideoedit-leveraging-consistency-models-for-efficient-text-to-video-editing-youyuan-zhang-et-al-2024>(9/41 | 24/124) FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing (Youyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youyuan Zhang, Xuan Ju, James J. Clark. (2024)<br><strong>FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing</strong><br><button class=copy-to-clipboard title="FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Zero-shot, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06269v1.pdf filename=2403.06269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated remarkable capabilities in <b>text-to-image</b> and text-to-video generation, opening up possibilities for video editing based on textual input. However, the computational cost associated with sequential sampling in <b>diffusion</b> <b>models</b> poses challenges for efficient video editing. Existing approaches relying on image generation models for video editing suffer from time-consuming one-shot <b>fine-tuning,</b> additional condition extraction, or DDIM inversion, making real-time applications impractical. In this work, we propose FastVideoEdit, an efficient <b>zero-shot</b> video editing approach inspired by Consistency Models (CMs). By leveraging the self-consistency property of CMs, we eliminate the need for time-consuming inversion or additional condition extraction, reducing editing time. Our method enables direct mapping from source video to target video with strong preservation ability utilizing a special variance schedule. This results in improved speed advantages, as fewer sampling steps can be used while maintaining comparable generation quality. Experimental results validate the state-of-the-art performance and speed advantages of FastVideoEdit across evaluation metrics encompassing editing speed, temporal consistency, and text-video alignment.</p></p class="citation"></blockquote><h3 id=1041--25124-v_kd-improving-knowledge-distillation-using-orthogonal-projections-roy-miles-et-al-2024>(10/41 | 25/124) $V_kD:$ Improving Knowledge Distillation using Orthogonal Projections (Roy Miles et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Miles, Ismail Elezi, Jiankang Deng. (2024)<br><strong>$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections</strong><br><button class=copy-to-clipboard title="$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Knowledge Distillation, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06213v1.pdf filename=2403.06213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> is an effective method for training small and efficient deep learning models. However, the efficacy of a single method can degenerate when transferring to other tasks, modalities, or even other architectures. To address this limitation, we propose a novel constrained feature <b>distillation</b> method. This method is derived from a small set of core principles, which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components, our <b>transformer</b> models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method, we apply it to <b>object</b> <b>detection</b> and image generation, whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available: <a href=https://github.com/roymiles/vkd>https://github.com/roymiles/vkd</a></p></p class="citation"></blockquote><h3 id=1141--26124-glancevad-exploring-glance-supervision-for-label-efficient-video-anomaly-detection-huaxin-zhang-et-al-2024>(11/41 | 26/124) GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection (Huaxin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huaxin Zhang, Xiang Wang, Xiaohao Xu, Xiaonan Huang, Chuchu Han, Yuehuan Wang, Changxin Gao, Shanjun Zhang, Nong Sang. (2024)<br><strong>GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection</strong><br><button class=copy-to-clipboard title="GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Anomaly Detection, Supervised Learning, Unsupervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06154v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06154v2.pdf filename=2403.06154v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, video <b>anomaly</b> <b>detection</b> has been extensively investigated in both <b>unsupervised</b> and weakly <b>supervised</b> settings to alleviate costly temporal labeling. Despite significant progress, these methods still suffer from unsatisfactory results such as numerous false alarms, primarily due to the absence of precise temporal <b>anomaly</b> <b>annotation.</b> In this paper, we present a novel labeling paradigm, termed &ldquo;glance annotation&rdquo;, to achieve a better balance between <b>anomaly</b> <b>detection</b> accuracy and annotation cost. Specifically, glance annotation is a random frame within each abnormal event, which can be easily accessed and is cost-effective. To assess its effectiveness, we manually annotate the glance annotations for two standard video <b>anomaly</b> <b>detection</b> datasets: UCF-Crime and XD-Violence. Additionally, we propose a customized GlanceVAD method, that leverages gaussian kernels as the basic unit to compose the temporal <b>anomaly</b> <b>distribution,</b> enabling the learning of diverse and robust <b>anomaly</b> <b>representations</b> from the glance annotations. Through comprehensive analysis and experiments, we verify that the proposed labeling paradigm can achieve an excellent trade-off between annotation cost and model performance. Extensive experimental results also demonstrate the effectiveness of our GlanceVAD approach, which significantly outperforms existing advanced <b>unsupervised</b> and weakly <b>supervised</b> methods. Code and annotations will be publicly available at <a href=https://github.com/pipixin321/GlanceVAD>https://github.com/pipixin321/GlanceVAD</a>.</p></p class="citation"></blockquote><h3 id=1241--27124-mace-mass-concept-erasure-in-diffusion-models-shilin-lu-et-al-2024>(12/41 | 27/124) MACE: Mass Concept Erasure in Diffusion Models (Shilin Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, Adams Wai-Kin Kong. (2024)<br><strong>MACE: Mass Concept Erasure in Diffusion Models</strong><br><button class=copy-to-clipboard title="MACE: Mass Concept Erasure in Diffusion Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06135v1.pdf filename=2403.06135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid expansion of large-scale <b>text-to-image</b> <b>diffusion</b> <b>models</b> has raised growing concerns regarding their potential misuse in creating harmful or misleading content. In this paper, we introduce MACE, a <b>finetuning</b> framework for the task of mass concept erasure. This task aims to prevent models from generating images that embody unwanted concepts when <b>prompted.</b> Existing concept erasure methods are typically restricted to handling fewer than five concepts simultaneously and struggle to find a balance between erasing concept synonyms (generality) and maintaining unrelated concepts (specificity). In contrast, MACE differs by successfully scaling the erasure scope up to 100 concepts and by achieving an effective balance between generality and specificity. This is achieved by leveraging closed-form cross-attention refinement along with LoRA <b>finetuning,</b> collectively eliminating the information of undesirable concepts. Furthermore, MACE integrates multiple LoRAs without mutual interference. We conduct extensive evaluations of MACE against prior methods across four different tasks: object erasure, celebrity erasure, explicit content erasure, and artistic style erasure. Our results reveal that MACE surpasses prior methods in all evaluated tasks. Code is available at <a href=https://github.com/Shilin-LU/MACE>https://github.com/Shilin-LU/MACE</a>.</p></p class="citation"></blockquote><h3 id=1341--28124-bit-mask-robust-contrastive-knowledge-distillation-for-unsupervised-semantic-hashing-liyang-he-et-al-2024>(13/41 | 28/124) Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing (Liyang He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyang He, Zhenya Huang, Jiayu Liu, Enhong Chen, Fei Wang, Jing Sha, Shijin Wang. (2024)<br><strong>Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing</strong><br><button class=copy-to-clipboard title="Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-IR, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Model Compression, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06071v1.pdf filename=2403.06071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in <b>unsupervised</b> semantic hashing <b>models</b> <b>can</b> yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. <b>Knowledge</b> <b>distillation</b> provides a means for practical <b>model</b> <b>compression</b> to alleviate this delay. Nevertheless, the prevailing <b>knowledge</b> <b>distillation</b> approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the <b>distillation</b> process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive <b>knowledge</b> <b>Distillation</b> (BRCD) method, specifically devised for the <b>distillation</b> of semantic hashing <b>models.</b> <b>To</b> ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student <b>models</b> <b>through</b> a contrastive <b>knowledge</b> <b>distillation</b> objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the <b>knowledge</b> <b>distillation</b> process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our <b>knowledge</b> <b>distillation</b> objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other <b>knowledge</b> <b>distillation</b> methods but also substantiate the generality of our methods across diverse semantic hashing <b>models</b> <b>and</b> backbones. The code for BRCD is available at <a href=https://github.com/hly1998/BRCD>https://github.com/hly1998/BRCD</a>.</p></p class="citation"></blockquote><h3 id=1441--29124-reframe-anything-llm-agent-for-open-world-video-reframing-jiawang-cao-et-al-2024>(14/41 | 29/124) Reframe Anything: LLM Agent for Open World Video Reframing (Jiawang Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawang Cao, Yongliang Wu, Weiheng Chi, Wenbo Zhu, Ziyue Su, Jay Wu. (2024)<br><strong>Reframe Anything: LLM Agent for Open World Video Reframing</strong><br><button class=copy-to-clipboard title="Reframe Anything: LLM Agent for Open World Video Reframing" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Foundation Model, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06070v1.pdf filename=2403.06070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of mobile devices and social media has revolutionized content dissemination, with short-form video becoming increasingly prevalent. This shift has introduced the challenge of video reframing to fit various screen aspect ratios, a process that highlights the most compelling parts of a video. Traditionally, video reframing is a manual, time-consuming task requiring professional expertise, which incurs high production costs. A potential solution is to adopt some machine learning models, such as video salient <b>object</b> <b>detection,</b> to automate the process. However, these methods often lack generalizability due to their reliance on specific training data. The advent of powerful <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> open new avenues for AI capabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a <b>LLM-based</b> agent that leverages visual <b>foundation</b> <b>models</b> and human instructions to restructure visual content for video reframing. RAVA operates in three stages: perception, where it interprets user instructions and video content; planning, where it determines aspect ratios and reframing strategies; and execution, where it invokes the editing tools to produce the final video. Our experiments validate the effectiveness of RAVA in video salient <b>object</b> <b>detection</b> and real-world reframing tasks, demonstrating its potential as a tool for AI-powered video editing.</p></p class="citation"></blockquote><h3 id=1541--30124-test-time-distribution-learning-adapter-for-cross-modal-visual-reasoning-yi-zhang-et-al-2024>(15/41 | 30/124) Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning (Yi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zhang, Ce Zhang. (2024)<br><strong>Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning</strong><br><button class=copy-to-clipboard title="Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Few-shot, Fine-tuning, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06059v1.pdf filename=2403.06059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Pre-Trained (VLP) models, such as CLIP, have demonstrated remarkable effectiveness in learning generic visual representations. Several approaches aim to efficiently adapt VLP models to downstream tasks with limited supervision, aiming to leverage the acquired knowledge from VLP models. However, these methods suffer from either introducing biased representations or requiring high computational complexity, which hinders their effectiveness in <b>fine-tuning</b> the CLIP model. Moreover, when a model is trained on data specific to a particular domain, its ability to generalize to uncharted domains diminishes. In this work, we propose Test-Time Distribution LearNing Adapter (TT-DNA) which directly works during the testing period. Specifically, we estimate Gaussian distributions to model visual features of the <b>few-shot</b> support images to capture the knowledge from the support set. The cosine similarity between query image and the feature distribution of support images is used as the prediction of visual adapter. Subsequently, the visual adapter&rsquo;s prediction merges with the original CLIP prediction via a residual connection, resulting in the final prediction. Our extensive experimental results on visual <b>reasoning</b> for human object interaction demonstrate that our proposed TT-DNA outperforms existing state-of-the-art methods by large margins.</p></p class="citation"></blockquote><h3 id=1641--31124-understanding-and-mitigating-human-labelling-errors-in-supervised-contrastive-learning-zijun-long-et-al-2024>(16/41 | 31/124) Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning (Zijun Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijun Long, Lipeng Zhuang, George Killick, Richard McCreadie, Gerardo Aragon Camarasa, Paul Henderson. (2024)<br><strong>Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning</strong><br><button class=copy-to-clipboard title="Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 38<br>Keywords: Benchmarking, Contrastive Learning, Representation Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06289v1.pdf filename=2403.06289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human-annotated vision datasets inevitably contain a fraction of human mislabelled examples. While the detrimental effects of such mislabelling on <b>supervised</b> <b>learning</b> are well-researched, their influence on <b>Supervised</b> <b>Contrastive</b> <b>Learning</b> (SCL) remains largely unexplored. In this paper, we show that human-labelling errors not only differ significantly from synthetic label errors, but also pose unique challenges in SCL, different to those in traditional <b>supervised</b> <b>learning</b> methods. Specifically, our results indicate they adversely impact the learning process in the ~99% of cases when they occur as false positive samples. Existing noise-mitigating methods primarily focus on synthetic label errors and tackle the unrealistic setting of very high synthetic noise rates (40-80%), but they often underperform on common image datasets due to overfitting. To address this issue, we introduce a novel SCL objective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is designed to mitigate the effects of real-world mislabelled examples, typically characterized by much lower noise rates (&lt;5%). We demonstrate that SCL-RHE consistently outperforms state-of-the-art <b>representation</b> <b>learning</b> and noise-mitigating methods across various vision <b>benchmarks,</b> by offering improved resilience against human-labelling errors.</p></p class="citation"></blockquote><h3 id=1741--32124-transformer-based-multitask-learning-for-image-captioning-and-object-detection-debolena-basak-et-al-2024>(17/41 | 32/124) Transformer based Multitask Learning for Image Captioning and Object Detection (Debolena Basak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debolena Basak, P. K. Srijith, Maunendra Sankar Desarkar. (2024)<br><strong>Transformer based Multitask Learning for Image Captioning and Object Detection</strong><br><button class=copy-to-clipboard title="Transformer based Multitask Learning for Image Captioning and Object Detection" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Transformer, BERTScore<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06292v1.pdf filename=2403.06292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In several real-world scenarios like autonomous navigation and mobility, to obtain a better visual understanding of the surroundings, image captioning and <b>object</b> <b>detection</b> play a crucial role. This work introduces a novel multitask learning framework that combines image captioning and <b>object</b> <b>detection</b> into a joint model. We propose TICOD, <b>Transformer-based</b> Image Captioning and <b>Object</b> <b>detection</b> model for jointly training both tasks by combining the losses obtained from image captioning and <b>object</b> <b>detection</b> networks. By leveraging joint training, the model benefits from the complementary information shared between the two tasks, leading to improved performance for image captioning. Our approach utilizes a <b>transformer-based</b> architecture that enables end-to-end network integration for image captioning and <b>object</b> <b>detection</b> and performs both tasks jointly. We evaluate the effectiveness of our approach through comprehensive experiments on the MS-COCO dataset. Our model outperforms the baselines from image captioning literature by achieving a 3.65% improvement in <b>BERTScore.</b></p></p class="citation"></blockquote><h3 id=1841--33124-vidprom-a-million-scale-real-prompt-gallery-dataset-for-text-to-video-diffusion-models-wenhao-wang-et-al-2024>(18/41 | 33/124) VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models (Wenhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhao Wang, Yi Yang. (2024)<br><strong>VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models</strong><br><button class=copy-to-clipboard title="VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Sora, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06098v1.pdf filename=2403.06098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The arrival of <b>Sora</b> marks a new era for text-to-video <b>diffusion</b> <b>models,</b> bringing significant advancements in video generation and potential applications. However, <b>Sora,</b> as well as other text-to-video <b>diffusion</b> <b>models,</b> highly relies on the <b>prompts,</b> and there is no publicly available dataset featuring a study of text-to-video <b>prompts.</b> In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video <b>prompts</b> from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art <b>diffusion</b> <b>models</b> and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale <b>prompt-gallery</b> dataset for image generation. Based on the analysis of these <b>prompts,</b> we identify the necessity for a new <b>prompt</b> dataset specifically designed for text-to-video generation and gain insights into the preferences of real users when creating videos. Our large-scale and diverse dataset also inspires many exciting new research areas. For instance, to develop better, more efficient, and safer text-to-video <b>diffusion</b> <b>models,</b> we suggest exploring text-to-video <b>prompt</b> engineering, efficient video generation, and video copy detection for <b>diffusion</b> <b>models.</b> We make the collected dataset VidProM publicly available at GitHub and Hugging Face under the CC-BY- NC 4.0 License.</p></p class="citation"></blockquote><h3 id=1941--34124-diffusion-models-trained-with-large-data-are-transferable-visual-models-guangkai-xu-et-al-2024>(19/41 | 34/124) Diffusion Models Trained with Large Data Are Transferable Visual Models (Guangkai Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, Chunhua Shen. (2024)<br><strong>Diffusion Models Trained with Large Data Are Transferable Visual Models</strong><br><button class=copy-to-clipboard title="Diffusion Models Trained with Large Data Are Transferable Visual Models" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06090v1.pdf filename=2403.06090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that, simply initializing image understanding models using a pre-trained UNet (or <b>transformer)</b> of <b>diffusion</b> <b>models,</b> it is possible to achieve remarkable transferable performance on fundamental vision perception tasks using a moderate amount of target data (even synthetic data only), including monocular depth, surface normal, image segmentation, matting, human pose estimation, among virtually many others. Previous works have adapted <b>diffusion</b> <b>models</b> for various perception tasks, often reformulating these tasks as generation processes to align with the <b>diffusion</b> <b>process.</b> In sharp contrast, we demonstrate that <b>fine-tuning</b> these models with minimal adjustments can be a more effective alternative, offering the advantages of being embarrassingly simple and significantly faster. As the backbone network of Stable <b>Diffusion</b> <b>models</b> is trained on giant datasets comprising billions of images, we observe very robust generalization capabilities of the <b>diffusion</b> <b>backbone.</b> Experimental results showcase the remarkable transferability of the backbone of <b>diffusion</b> <b>models</b> across diverse tasks and real-world datasets.</p></p class="citation"></blockquote><h3 id=2041--35124-mipha-a-comprehensive-overhaul-of-multimodal-assistant-with-small-language-models-minjie-zhu-et-al-2024>(20/41 | 35/124) Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models (Minjie Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang. (2024)<br><strong>Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models</strong><br><button class=copy-to-clipboard title="Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06199v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06199v2.pdf filename=2403.06199v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have showcased impressive skills in tasks related to visual understanding and <b>reasoning.</b> Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of <b>Multimodal</b> Small Language Models (MSLMs) and propose an efficient <b>multimodal</b> assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art <b>large</b> <b>MLLMs,</b> <b>especially</b> LLaVA-1.5-13B, on multiple <b>benchmarks.</b> Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is available at <a href=https://github.com/zhuyiche/Mipha>https://github.com/zhuyiche/Mipha</a>.</p></p class="citation"></blockquote><h3 id=2141--36124-universal-debiased-editing-for-fair-medical-image-classification-ruinan-jin-et-al-2024>(21/41 | 36/124) Universal Debiased Editing for Fair Medical Image Classification (Ruinan Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruinan Jin, Wenlong Deng, Minghui Chen, Xiaoxiao Li. (2024)<br><strong>Universal Debiased Editing for Fair Medical Image Classification</strong><br><button class=copy-to-clipboard title="Universal Debiased Editing for Fair Medical Image Classification" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Black Box, Fairness, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06104v1.pdf filename=2403.06104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of <b>Foundation</b> <b>Models&rsquo;</b> (FMs) rising prominence in AI, our study addresses the challenge of biases in medical images while using FM API, particularly spurious correlations between pixels and sensitive attributes. Traditional methods for bias mitigation face limitations due to the restricted access to web-hosted FMs and difficulties in addressing the underlying bias encoded within the FM API. We propose an U(niversal) D(ebiased) E(diting) strategy, termed UDE, which generates UDE noise to mask such spurious correlation. UDE is capable of mitigating bias both within the FM API embedding and the images themselves. Furthermore, UDE is suitable for both white-box and <b>black-box</b> <b>FM</b> APIs, where we introduced G(reedy) (Z)eroth-O(rder) (GeZO) optimization for it when the gradient is inaccessible in <b>black-box</b> <b>APIs.</b> Our whole pipeline enables <b>fairness-aware</b> image editing that can be applied across various medical contexts without requiring direct model manipulation or significant computational resources. Our empirical results demonstrate the method&rsquo;s effectiveness in maintaining <b>fairness</b> and utility across different patient groups and diseases. In the era of AI-driven medicine, this work contributes to making healthcare diagnostics more equitable, showcasing a practical solution for bias mitigation in pre-trained image FMs.</p></p class="citation"></blockquote><h3 id=2241--37124-poly-kernel-inception-network-for-remote-sensing-detection-xinhao-cai-et-al-2024>(22/41 | 37/124) Poly Kernel Inception Network for Remote Sensing Detection (Xinhao Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhao Cai, Qiuxia Lai, Yuwei Wang, Wenguan Wang, Zeren Sun, Yazhou Yao. (2024)<br><strong>Poly Kernel Inception Network for Remote Sensing Detection</strong><br><button class=copy-to-clipboard title="Poly Kernel Inception Network for Remote Sensing Detection" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Object Detection, Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06258v1.pdf filename=2403.06258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Object</b> <b>detection</b> in remote sensing images (RSIs) often suffers from several increasing challenges, including the large variation in <b>object</b> <b>scales</b> and the diverse-ranging context. Prior methods tried to address these challenges by expanding the spatial receptive field of the backbone, either through large-kernel <b>convolution</b> or dilated <b>convolution.</b> However, the former typically introduces considerable background noise, while the latter risks generating overly sparse feature representations. In this paper, we introduce the Poly Kernel Inception Network (PKINet) to handle the above challenges. PKINet employs multi-scale <b>convolution</b> kernels without dilation to extract <b>object</b> <b>features</b> of varying scales and capture local context. In addition, a Context Anchor Attention (CAA) module is introduced in parallel to capture long-range contextual information. These two components work jointly to advance the performance of PKINet on four challenging remote sensing detection <b>benchmarks,</b> namely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.</p></p class="citation"></blockquote><h3 id=2341--38124-unicorn-ultrasound-nakagami-imaging-via-score-matching-and-adaptation-kwanyoung-kim-et-al-2024>(23/41 | 38/124) UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation (Kwanyoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kwanyoung Kim, Jaa-Yeon Lee, Jong Chul Ye. (2024)<br><strong>UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation</strong><br><button class=copy-to-clipboard title="UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, physics-med-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06275v1.pdf filename=2403.06275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nakagami imaging holds promise for visualizing and quantifying tissue scattering in ultrasound waves, with potential applications in tumor diagnosis and fat fraction estimation which are challenging to discern by conventional ultrasound B-mode images. Existing methods struggle with optimal window size selection and suffer from estimator instability, leading to degraded resolution images. To address this, here we propose a novel method called UNICORN (Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an accurate, closed-form estimator for Nakagami parameter estimation in terms of the score function of ultrasonic envelope. Extensive experiments using <b>simulation</b> and real ultrasound RF data demonstrate UNICORN&rsquo;s superiority over conventional approaches in accuracy and resolution quality.</p></p class="citation"></blockquote><h3 id=2441--39124-most-motion-style-transformer-between-diverse-action-contents-boeun-kim-et-al-2024>(24/41 | 39/124) MoST: Motion Style Transformer between Diverse Action Contents (Boeun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boeun Kim, Jungho Kim, Hyung Jin Chang, Jin Young Choi. (2024)<br><strong>MoST: Motion Style Transformer between Diverse Action Contents</strong><br><button class=copy-to-clipboard title="MoST: Motion Style Transformer between Diverse Action Contents" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06225v1.pdf filename=2403.06225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While existing motion <b>style</b> <b>transfer</b> methods are effective between two motions with identical content, their performance significantly diminishes when transferring <b>style</b> <b>between</b> motions with different contents. This challenge lies in the lack of clear separation between content and <b>style</b> <b>of</b> a motion. To tackle this challenge, we propose a novel motion <b>style</b> <b>transformer</b> that effectively disentangles <b>style</b> <b>from</b> content and generates a plausible motion with transferred <b>style</b> <b>from</b> a source motion. Our distinctive approach to achieving the goal of disentanglement is twofold: (1) a new architecture for motion <b>style</b> <b>transformer</b> with &lsquo;part-attentive <b>style</b> <b>modulator</b> across body parts&rsquo; and &lsquo;Siamese encoders that encode <b>style</b> <b>and</b> content features separately&rsquo;; (2) <b>style</b> <b>disentanglement</b> loss. Our method outperforms existing methods and demonstrates exceptionally high quality, particularly in motion pairs with different contents, without the need for heuristic post-processing. Codes are available at <a href=https://github.com/Boeun-Kim/MoST>https://github.com/Boeun-Kim/MoST</a>.</p></p class="citation"></blockquote><h3 id=2541--40124-platypose-calibrated-zero-shot-multi-hypothesis-3d-human-motion-estimation-paweł-a-pierzchlewicz-et-al-2024>(25/41 | 40/124) Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation (Paweł A. Pierzchlewicz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paweł A. Pierzchlewicz, Caio da Silva, R. James Cotton, Fabian H. Sinz. (2024)<br><strong>Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation</strong><br><button class=copy-to-clipboard title="Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06164v1.pdf filename=2403.06164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single camera 3D pose estimation is an ill-defined problem due to inherent ambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose estimation accounts for this uncertainty by providing multiple 3D poses consistent with the 2D measurements. Current research has predominantly concentrated on generating multiple hypotheses for single frame static pose estimation. In this study we focus on the new task of multi-hypothesis motion estimation. Motion estimation is not simply pose estimation applied to multiple frames, which would ignore temporal correlation across frames. Instead, it requires distributions which are capable of generating temporally consistent samples, which is significantly more challenging. To this end, we introduce Platypose, a framework that uses a <b>diffusion</b> <b>model</b> pretrained on 3D human motion sequences for <b>zero-shot</b> 3D pose sequence estimation. Platypose outperforms baseline methods on multiple hypotheses for motion estimation. Additionally, Platypose also achieves state-of-the-art calibration and competitive joint error when tested on static poses from Human3.6M, MPI-INF-3DHP and 3DPW. Finally, because it is <b>zero-shot,</b> our method generalizes flexibly to different settings such as multi-camera inference.</p></p class="citation"></blockquote><h3 id=2641--41124-cracking-the-neural-code-for-word-recognition-in-convolutional-neural-networks-aakash-agrawal-et-al-2024>(26/41 | 41/124) Cracking the neural code for word recognition in convolutional neural networks (Aakash Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aakash Agrawal, Stanislas Dehaene. (2024)<br><strong>Cracking the neural code for word recognition in convolutional neural networks</strong><br><button class=copy-to-clipboard title="Cracking the neural code for word recognition in convolutional neural networks" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, q-bio-NC<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06159v1.pdf filename=2403.06159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning to read places a strong challenge on the visual system. Years of expertise lead to a remarkable capacity to separate highly similar letters and encode their relative positions, thus distinguishing words such as FORM and FROM, invariantly over a large range of sizes and absolute positions. How neural circuits achieve invariant word recognition remains unknown. Here, we address this issue by training deep neural network models to recognize written words and then analyzing how reading-specialized units emerge and operate across different layers of the network. With literacy, a small subset of units becomes specialized for word recognition in the learned script, similar to the &ldquo;visual word form area&rdquo; of the human brain. We show that these units are sensitive to specific letter identities and their distance from the blank space at the left or right of a word, thus acting as &ldquo;space bigrams&rdquo;. These units specifically encode ordinal positions and operate by pooling across low and high-frequency detector units from early layers of the network. The proposed neural code provides a mechanistic insight into how information on letter identity and position is extracted and allow for invariant word recognition, and leads to predictions for reading behavior, error patterns, and the neurophysiology of reading.</p></p class="citation"></blockquote><h3 id=2741--42124-pss-ba-lidar-bundle-adjustment-with-progressive-spatial-smoothing-jianping-li-et-al-2024>(27/41 | 42/124) PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing (Jianping Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianping Li, Thien-Minh Nguyen, Shenghai Yuan, Lihua Xie. (2024)<br><strong>PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing</strong><br><button class=copy-to-clipboard title="PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06124v1.pdf filename=2403.06124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate and consistent construction of point clouds from LiDAR scanning data is fundamental for 3D modeling applications. Current solutions, such as multiview point cloud registration and LiDAR bundle adjustment, predominantly depend on the local plane assumption, which may be inadequate in complex environments lacking of planar geometries or substantial initial pose errors. To mitigate this problem, this paper presents a LiDAR bundle adjustment with progressive spatial smoothing, which is suitable for complex environments and exhibits improved convergence capabilities. The proposed method consists of a spatial smoothing module and a pose adjustment module, which combines the benefits of local consistency and global accuracy. With the spatial smoothing module, we can obtain robust and rich surface constraints employing smoothing kernels across various scales. Then the pose adjustment module corrects all poses utilizing the novel surface constraints. Ultimately, the proposed method simultaneously achieves fine poses and parametric surfaces that can be directly employed for high-quality point cloud reconstruction. The effectiveness and robustness of our proposed approach have been validated on both <b>simulation</b> and real-world datasets. The experimental results demonstrate that the proposed method outperforms the existing methods and achieves better accuracy in complex environments with low planar structures.</p></p class="citation"></blockquote><h3 id=2841--43124-enhancing-3d-object-detection-with-2d-detection-guided-query-anchors-haoxuanye-ji-et-al-2024>(28/41 | 43/124) Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors (Haoxuanye Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxuanye Ji, Pengpeng Liang, Erkang Cheng. (2024)<br><strong>Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors</strong><br><button class=copy-to-clipboard title="Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06093v1.pdf filename=2403.06093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-camera-based 3D <b>object</b> <b>detection</b> has made notable progress in the past several years. However, we observe that there are cases (e.g. faraway regions) in which popular 2D <b>object</b> <b>detectors</b> are more reliable than state-of-the-art 3D detectors. In this paper, to improve the performance of query-based 3D <b>object</b> <b>detectors,</b> we present a novel query generating approach termed QAF2D, which infers 3D query anchors from 2D detection results. A 2D bounding box of an <b>object</b> <b>in</b> an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth, yaw angle, and size candidates. Then, the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box, and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of <b>prompt</b> parameters. We integrate QAF2D into three popular query-based 3D <b>object</b> <b>detectors</b> and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is $2.3%$ NDS and $2.7%$ mAP. Code is available at <a href=https://github.com/nullmax-vision/QAF2D>https://github.com/nullmax-vision/QAF2D</a>.</p></p class="citation"></blockquote><h3 id=2941--44124-foaa-flattened-outer-arithmetic-attention-for-multimodal-tumor-classification-omnia-alwazzan-et-al-2024>(29/41 | 44/124) FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor Classification (Omnia Alwazzan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omnia Alwazzan, Ioannis Patras, Gregory Slabaugh. (2024)<br><strong>FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor Classification</strong><br><button class=copy-to-clipboard title="FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor Classification" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06339v1.pdf filename=2403.06339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fusion of <b>multimodal</b> healthcare data holds great promise to provide a holistic view of a patient&rsquo;s health, taking advantage of the complementarity of different modalities while leveraging their correlation. This paper proposes a simple and effective approach, inspired by attention, to fuse discriminative features from different modalities. We propose a novel attention mechanism, called Flattened Outer Arithmetic Attention (FOAA), which relies on outer arithmetic operators (addition, subtraction, product, and division) to compute attention scores from keys, queries and values derived from flattened embeddings of each modality. We demonstrate how FOAA can be implemented for <b>self-attention</b> and cross-attention, providing a reusable component in neural network architectures. We evaluate FOAA on two datasets for <b>multimodal</b> tumor classification and achieve state-of-the-art results, and we demonstrate that features enriched by FOAA are superior to those derived from other fusion approaches. The code is publicly available at \href{https://github.com/omniaalwazzan/FOAA}{https://github.com/omniaalwazzan/FOAA}</p></p class="citation"></blockquote><h3 id=3041--45124-an-end-to-end-deep-learning-generative-framework-for-refinable-shape-matching-and-generation-soodeh-kalaie-et-al-2024>(30/41 | 45/124) An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation (Soodeh Kalaie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soodeh Kalaie, Andy Bulpitt, Alejandro F. Frangi, Ali Gooya. (2024)<br><strong>An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation</strong><br><button class=copy-to-clipboard title="An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Graph, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06317v1.pdf filename=2403.06317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative modelling for shapes is a prerequisite for In-Silico Clinical Trials (ISCTs), which aim to cost-effectively validate medical device interventions using synthetic anatomical shapes, often represented as 3D surface meshes. However, constructing AI models to generate shapes closely resembling the real mesh samples is challenging due to variable vertex counts, connectivities, and the lack of dense vertex-wise correspondences across the training data. Employing <b>graph</b> representations for meshes, we develop a novel <b>unsupervised</b> geometric deep-learning model to establish refinable shape correspondences in a latent space, construct a population-derived atlas and generate realistic synthetic shapes. We additionally extend our proposed base model to a joint shape generative-clustering multi-atlas framework to incorporate further variability and preserve more details in the generated shapes. Experimental results using liver and left-ventricular models demonstrate the approach&rsquo;s applicability to computational medicine, highlighting its suitability for ISCTs through a comparative analysis.</p></p class="citation"></blockquote><h3 id=3141--46124-clear-cross-transformers-with-pre-trained-language-model-is-all-you-need-for-person-attribute-recognition-and-retrieval-doanh-c-bui-et-al-2024>(31/41 | 46/124) CLEAR: Cross-Transformers with Pre-trained Language Model is All you need for Person Attribute Recognition and Retrieval (Doanh C. Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Doanh C. Bui, Thinh V. Le, Hung Ba Ngo, Tae Jong Choi. (2024)<br><strong>CLEAR: Cross-Transformers with Pre-trained Language Model is All you need for Person Attribute Recognition and Retrieval</strong><br><button class=copy-to-clipboard title="CLEAR: Cross-Transformers with Pre-trained Language Model is All you need for Person Attribute Recognition and Retrieval" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06119v1.pdf filename=2403.06119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Person attribute recognition and attribute-based retrieval are two core human-centric tasks. In the recognition task, the challenge is specifying attributes depending on a person&rsquo;s appearance, while the retrieval task involves searching for matching persons based on attribute queries. There is a significant relationship between recognition and retrieval tasks. In this study, we demonstrate that if there is a sufficiently robust network to solve person attribute recognition, it can be adapted to facilitate better performance for the retrieval task. Another issue that needs addressing in the retrieval task is the modality gap between attribute queries and persons&rsquo; images. Therefore, in this paper, we present CLEAR, a unified network designed to address both tasks. We introduce a robust cross-transformers network to handle person attribute recognition. Additionally, leveraging a <b>pre-trained</b> <b>language</b> <b>model,</b> we construct pseudo-descriptions for attribute queries and introduce an effective training strategy to train only a few additional parameters for adapters, facilitating the handling of the retrieval task. Finally, the unified CLEAR model is evaluated on five <b>benchmarks:</b> PETA, PA100K, Market-1501, RAPv2, and UPAR-2024. Without bells and whistles, CLEAR achieves state-of-the-art performance or competitive results for both tasks, significantly outperforming other competitors in terms of person retrieval performance on the widely-used Market-1501 dataset.</p></p class="citation"></blockquote><h3 id=3241--47124-is-vanilla-mlp-in-neural-radiance-field-enough-for-few-shot-view-synthesis-hanxin-zhu-et-al-2024>(32/41 | 47/124) Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis? (Hanxin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxin Zhu, Tianyu He, Xin Li, Bingchen Li, Zhibo Chen. (2024)<br><strong>Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?</strong><br><button class=copy-to-clipboard title="Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06092v1.pdf filename=2403.06092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Field (NeRF) has achieved superior performance for novel view synthesis by modeling the scene with a Multi-Layer Perception (MLP) and a volume rendering procedure, however, when fewer known views are given (i.e., <b>few-shot</b> view synthesis), the model is prone to overfit the given views. To handle this issue, previous efforts have been made towards leveraging learned priors or introducing additional regularizations. In contrast, in this paper, we for the first time provide an orthogonal method from the perspective of network structure. Given the observation that trivially reducing the number of model parameters alleviates the overfitting issue, but at the cost of missing details, we propose the multi-input MLP (mi-MLP) that incorporates the inputs (i.e., location and viewing direction) of the vanilla MLP into each layer to prevent the overfitting issue without harming detailed synthesis. To further reduce the artifacts, we propose to model colors and volume density separately and present two regularization terms. Extensive experiments on multiple datasets demonstrate that: 1) although the proposed mi-MLP is easy to implement, it is surprisingly effective as it boosts the PSNR of the baseline from $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art results on a wide range of <b>benchmarks.</b> We will release the code upon publication.</p></p class="citation"></blockquote><h3 id=3341--48124-text-guided-variational-image-generation-for-industrial-anomaly-detection-and-segmentation-mingyu-lee-et-al-2024>(33/41 | 48/124) Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation (Mingyu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Lee, Jongwon Choi. (2024)<br><strong>Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation</strong><br><button class=copy-to-clipboard title="Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06247v1.pdf filename=2403.06247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a text-guided variational image generation method to address the challenge of getting clean data for <b>anomaly</b> <b>detection</b> in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of <b>anomaly</b> <b>detection</b> models by utilizing the generated images.</p></p class="citation"></blockquote><h3 id=3441--49124-blazebvd-make-scale-time-equalization-great-again-for-blind-video-deflickering-xinmin-qiu-et-al-2024>(34/41 | 49/124) BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering (Xinmin Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinmin Qiu, Congying Han, Zicheng Zhang, Bonan Li, Tiande Guo, Pingyu Wang, Xuecheng Nie. (2024)<br><strong>BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering</strong><br><button class=copy-to-clipboard title="BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: High-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06243v1.pdf filename=2403.06243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing blind video deflickering (BVD) algorithms to enhance video temporal consistency, is gaining importance amid the flourish of image processing and video generation. However, the intricate nature of video data complicates the training of deep learning methods, leading to high resource consumption and instability, notably under severe lighting flicker. This underscores the critical need for a compact representation beyond pixel values to advance BVD research and applications. Inspired by the classic scale-time equalization (STE), our work introduces the histogram-assisted solution, called BlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly corrects pixel values by temporally smoothing color histograms, BlazeBVD leverages smoothed illumination histograms within STE filtering to ease the challenge of learning temporal data using neural networks. In technique, BlazeBVD begins by condensing pixel values into illumination histograms that precisely capture flickering and local exposure variations. These histograms are then smoothed to produce singular frames set, filtered illumination maps, and exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a 2D network to restore faithful and consistent texture impacted by lighting changes or localized exposure issues. BlazeBVD also incorporates a lightweight 3D network to amend slight temporal inconsistencies, avoiding the resource consumption issue. Comprehensive experiments on synthetic, real-world and generated videos, showcase the superior qualitative and quantitative results of BlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts.</p></p class="citation"></blockquote><h3 id=3541--50124-finding-visual-saliency-in-continuous-spike-stream-lin-zhu-et-al-2024>(35/41 | 50/124) Finding Visual Saliency in Continuous Spike Stream (Lin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Zhu, Xianzhang Chen, Xiao Wang, Hua Huang. (2024)<br><strong>Finding Visual Saliency in Continuous Spike Stream</strong><br><button class=copy-to-clipboard title="Finding Visual Saliency in Continuous Spike Stream" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06233v1.pdf filename=2403.06233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a bio-inspired vision sensor, the spike camera emulates the operational principles of the fovea, a compact retinal region, by employing spike discharges to encode the accumulation of per-pixel luminance intensity. Leveraging its high temporal resolution and bio-inspired neuromorphic design, the spike camera holds significant promise for advancing computer vision applications. Saliency detection mimics the behavior of human beings and captures the most salient region from the scenes. In this paper, we investigate the visual saliency in the continuous spike stream for the first time. To effectively process the binary spike stream, we propose a Recurrent Spiking <b>Transformer</b> (RST) framework, which is based on a full spiking neural network. Our framework enables the extraction of spatio-temporal features from the continuous spatio-temporal spike stream while maintaining low power consumption. To facilitate the training and validation of our proposed model, we build a comprehensive real-world spike-based visual saliency dataset, enriched with numerous light conditions. Extensive experiments demonstrate the superior performance of our Recurrent Spiking <b>Transformer</b> framework in comparison to other spike neural network-based methods. Our framework exhibits a substantial margin of improvement in capturing and highlighting visual saliency in the spike stream, which not only provides a new perspective for spike-based saliency segmentation but also shows a new paradigm for full SNN-based <b>transformer</b> models. The code and dataset are available at \url{https://github.com/BIT-Vision/SVS}.</p></p class="citation"></blockquote><h3 id=3641--51124-s-dyrf-reference-based-stylized-radiance-fields-for-dynamic-scenes-xingyi-li-et-al-2024>(36/41 | 51/124) S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes (Xingyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyi Li, Zhiguo Cao, Yizheng Wu, Kewei Wang, Ke Xian, Zhe Wang, Guosheng Lin. (2024)<br><strong>S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes</strong><br><button class=copy-to-clipboard title="S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06205v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06205v2.pdf filename=2403.06205v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current 3D stylization methods often assume static scenes, which violates the dynamic nature of our real world. To address this limitation, we present S-DyRF, a reference-based spatio-temporal stylization method for dynamic neural radiance fields. However, stylizing dynamic 3D scenes is inherently challenging due to the limited availability of stylized reference images along the temporal axis. Our key insight lies in introducing additional temporal cues besides the provided reference. To this end, we generate temporal pseudo-references from the given stylized reference. These pseudo-references facilitate the propagation of <b>style</b> <b>information</b> from the reference to the entire dynamic 3D scene. For coarse <b>style</b> <b>transfer,</b> we enforce novel views and times to mimic the <b>style</b> <b>details</b> present in pseudo-references at the feature level. To preserve high-frequency details, we create a collection of stylized temporal pseudo-rays from temporal pseudo-references. These pseudo-rays serve as detailed and explicit stylization guidance for achieving fine <b>style</b> <b>transfer.</b> Experiments on both synthetic and real-world datasets demonstrate that our method yields plausible stylized results of space-time view synthesis on dynamic 3D scenes.</p></p class="citation"></blockquote><h3 id=3741--52124-diffumatting-synthesizing-arbitrary-objects-with-matting-level-annotation-xiaobin-hu-et-al-2024>(37/41 | 52/124) DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation (Xiaobin Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaobin Hu, Xu Peng, Donghao Luo, Xiaozhong Ji, Jinlong Peng, Zhengkai Jiang, Jiangning Zhang, Taisong Jin, Chengjie Wang, Rongrong Ji. (2024)<br><strong>DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation</strong><br><button class=copy-to-clipboard title="DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06168v1.pdf filename=2403.06168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public. To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of <b>diffusion</b> <b>and</b> endows the power of &ldquo;matting anything&rdquo;. Our DiffuMatting can 1). act as an anything matting factory with high accurate annotations 2). be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation. Specifically, inspired by green-screen-matting, we aim to teach the <b>diffusion</b> <b>model</b> to paint on a fixed green screen canvas. To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting. Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to distinguish the foreground and background. To ensure the synthesized object has more edge details, a detailed-enhancement of transition boundary loss is proposed as a guideline to generate objects with more complicated edge structures. Aiming to simultaneously generate the object and its matting annotation, we build a matting head to make a green color removal in the latent space of the VAE decoder. Our DiffuMatting shows several potential applications (e.g., matting-data generator, community-friendly art design and controllable generation). As a matting-data generator, DiffuMatting synthesizes general object and portrait matting sets, effectively reducing the relative MSE error by 15.4% in General Object Matting and 11.4% in Portrait Matting tasks.</p></p class="citation"></blockquote><h3 id=3841--53124-cross-cluster-shifting-for-efficient-and-effective-3d-object-detection-in-autonomous-driving-zhili-chen-et-al-2024>(38/41 | 53/124) Cross-Cluster Shifting for Efficient and Effective 3D Object Detection in Autonomous Driving (Zhili Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhili Chen, Kien T. Pham, Maosheng Ye, Zhiqiang Shen, Qifeng Chen. (2024)<br><strong>Cross-Cluster Shifting for Efficient and Effective 3D Object Detection in Autonomous Driving</strong><br><button class=copy-to-clipboard title="Cross-Cluster Shifting for Efficient and Effective 3D Object Detection in Autonomous Driving" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06166v1.pdf filename=2403.06166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a new 3D point-based detector model, named Shift-SSD, for precise 3D <b>object</b> <b>detection</b> in autonomous driving. Traditional point-based 3D <b>object</b> <b>detectors</b> often employ architectures that rely on a progressive downsampling of points. While this method effectively reduces computational demands and increases receptive fields, it will compromise the preservation of crucial non-local information for accurate 3D <b>object</b> <b>detection,</b> especially in the complex driving scenarios. To address this, we introduce an intriguing Cross-Cluster Shifting operation to unleash the representation capacity of the point-based detector by efficiently modeling longer-range inter-dependency while including only a negligible overhead. Concretely, the Cross-Cluster Shifting operation enhances the conventional design by shifting partial channels from neighboring clusters, which enables richer interaction with non-local regions and thus enlarges the receptive field of clusters. We conduct extensive experiments on the KITTI, Waymo, and nuScenes datasets, and the results demonstrate the state-of-the-art performance of Shift-SSD in both detection accuracy and runtime efficiency.</p></p class="citation"></blockquote><h3 id=3941--54124-all-in-one-platform-for-ai-rd-in-medical-imaging-encompassing-data-collection-selection-annotation-and-pre-processing-changhee-han-et-al-2024>(39/41 | 54/124) All-in-one platform for AI R&amp;D in medical imaging, encompassing data collection, selection, annotation, and pre-processing (Changhee Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Changhee Han, Kyohei Shibano, Wataru Ozaki, Keishiro Osaki, Takafumi Haraguchi, Daisuke Hirahara, Shumon Kimura, Yasuyuki Kobayashi, Gento Mogi. (2024)<br><strong>All-in-one platform for AI R&amp;D in medical imaging, encompassing data collection, selection, annotation, and pre-processing</strong><br><button class=copy-to-clipboard title="All-in-one platform for AI R&D in medical imaging, encompassing data collection, selection, annotation, and pre-processing" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06145v1.pdf filename=2403.06145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Learning is advancing medical imaging Research and Development (R&amp;D), leading to the frequent clinical use of Artificial Intelligence/Machine Learning (AI/ML)-based medical devices. However, to advance AI R&amp;D, two challenges arise: 1) significant data imbalance, with most data from Europe/America and under 10% from Asia, despite its 60% global population share; and 2) hefty time and investment needed to curate proprietary datasets for commercial use. In response, we established the first commercial medical imaging platform, encompassing steps like: 1) data collection, 2) data selection, 3) annotation, and 4) pre-processing. Moreover, we focus on harnessing under-represented data from Japan and broader Asia, including Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans. Using the collected data, we are preparing/providing ready-to-use datasets for medical AI R&amp;D by 1) offering these datasets to AI firms, biopharma, and medical device makers and 2) using them as training/test data to develop tailored AI solutions for such entities. We also aim to merge Blockchain for data security and plan to synthesize rare disease data via <b>generative</b> <b>AI.</b> DataHub Website: <a href=https://medical-datahub.ai/>https://medical-datahub.ai/</a></p></p class="citation"></blockquote><h3 id=4041--55124-bayesian-random-semantic-data-augmentation-for-medical-image-classification-yaoyao-zhu-et-al-2024>(40/41 | 55/124) Bayesian Random Semantic Data Augmentation for Medical Image Classification (Yaoyao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaoyao Zhu, Xiuding Cai, Xueyao Wang, Yu Yao. (2024)<br><strong>Bayesian Random Semantic Data Augmentation for Medical Image Classification</strong><br><button class=copy-to-clipboard title="Bayesian Random Semantic Data Augmentation for Medical Image Classification" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06138v1.pdf filename=2403.06138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Data</b> <b>augmentation</b> is a critical regularization technique for deep neural networks, particularly in medical image classification. Popular <b>data</b> <b>augmentation</b> approaches include image transformation-based methods, generative <b>data</b> <b>augmentation,</b> and automatic <b>data</b> <b>augmentation.</b> However, these approaches encounter notable limitations: image transformation-based and automated <b>data</b> <b>augmentation</b> techniques cannot implement semantic transformations, leading to a constrained variety of augmented samples, and generative <b>data</b> <b>augmentation</b> methods are computationally expensive. In response to these challenges, we proposed Bayesian Random Semantic <b>Data</b> <b>Augmentation</b> (BRSDA), a novel, efficient, and plug-and-play semantic <b>data</b> <b>augmentation</b> method. BRSDA is motivated by a simple translation in the feature space along specific directions that can effectuate semantic transformations. When given a feature, we define its augmentable semantic magnitude as a random variable and estimate its distribution using variational Bayesian, then sample semantic magnitude and add to the randomly selected semantic direction to achieve semantic <b>data</b> <b>augmentation.</b> We demonstrate the effectiveness of BRSDA on five 2D and six 3D medical image datasets covering nine modalities. We also test BRSDA with mainstream neural network architectures, showcasing its robustness. Furthermore, combining BRSDA with other leading <b>data</b> <b>augmentation</b> methods achieves superior performance. Code is available online at \url{https://github.com/YaoyaoZhu19/BRSDA}.</p></p class="citation"></blockquote><h3 id=4141--56124-style-blind-domain-generalized-semantic-segmentation-via-covariance-alignment-and-semantic-consistence-contrastive-learning-woo-jin-ahn-et-al-2024>(41/41 | 56/124) Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning (Woo-Jin Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Woo-Jin Ahn, Geun-Yeong Yang, Hyun-Duck Choi, Myo-Taeg Lim. (2024)<br><strong>Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning</strong><br><button class=copy-to-clipboard title="Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06122v1.pdf filename=2403.06122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models for semantic segmentation often experience performance degradation when deployed to unseen target domains unidentified during the training phase. This is mainly due to variations in image texture (\ie style) from different data sources. To tackle this challenge, existing domain generalized semantic segmentation (DGSS) methods attempt to remove style variations from the feature. However, these approaches struggle with the entanglement of style and content, which may lead to the unintentional removal of crucial content information, causing performance degradation. This study addresses this limitation by proposing BlindNet, a novel DGSS approach that blinds the style without external modules or datasets. The main idea behind our proposed approach is to alleviate the effect of style in the encoder whilst facilitating robust segmentation in the decoder. To achieve this, BlindNet comprises two key components: covariance alignment and semantic consistency <b>contrastive</b> <b>learning.</b> Specifically, the covariance alignment trains the encoder to uniformly recognize various styles and preserve the content information of the feature, rather than removing the style-sensitive factor. Meanwhile, semantic consistency <b>contrastive</b> <b>learning</b> enables the decoder to construct discriminative class embedding space and disentangles features that are vulnerable to misclassification. Through extensive experiments, our approach outperforms existing DGSS methods, exhibiting robustness and superior performance for semantic segmentation on unseen target domains.</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--57124-llms-still-cant-avoid-instanceof-an-investigation-into-gpt-35-gpt-4-and-bards-capacity-to-handle-object-oriented-programming-assignments-bruno-pereira-cipriano-et-al-2024>(1/2 | 57/124) LLMs Still Can&rsquo;t Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard&rsquo;s Capacity to Handle Object-Oriented Programming Assignments (Bruno Pereira Cipriano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno Pereira Cipriano, Pedro Alves. (2024)<br><strong>LLMs Still Can&rsquo;t Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard&rsquo;s Capacity to Handle Object-Oriented Programming Assignments</strong><br><button class=copy-to-clipboard title="LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-ET, cs-SE, cs.SE<br>Keyword Score: 70<br>Keywords: Bard, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06254v1.pdf filename=2403.06254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of <b>LLMs</b> in OOP contexts. In this study, we experimented with three prominent <b>LLMs</b> - <b>GPT-3.5,</b> <b>GPT-4,</b> and <b>Bard</b> - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. <b>GPT-4</b> stood out as the most proficient, followed by <b>GPT-3.5,</b> with <b>Bard</b> trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing <b>LLMs</b> with AATs in pedagogical settings. In conclusion, while <b>GPT-4</b> showcases promise, the deployment of these models in OOP education still mandates supervision.</p></p class="citation"></blockquote><h3 id=22--58124-repohyper-better-context-retrieval-is-all-you-need-for-repository-level-code-completion-huy-n-phan-et-al-2024>(2/2 | 58/124) RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion (Huy N. Phan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, Nghi D. Q. Bui. (2024)<br><strong>RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion</strong><br><button class=copy-to-clipboard title="RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Graph, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06095v1.pdf filename=2403.06095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code <b>Large</b> <b>Language</b> <b>Models</b> (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic <b>Graph</b> (RSG), a novel semantic <b>graph</b> structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a <b>graph</b> expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--59124-simulating-family-conversations-using-llms-demonstration-of-parenting-styles-frank-tian-fang-ye-et-al-2024>(1/1 | 59/124) Simulating Family Conversations using LLMs: Demonstration of Parenting Styles (Frank Tian-fang Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Tian-fang Ye, Xiaozi Gao. (2024)<br><strong>Simulating Family Conversations using LLMs: Demonstration of Parenting Styles</strong><br><button class=copy-to-clipboard title="Simulating Family Conversations using LLMs: Demonstration of Parenting Styles" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Simulation, Simulator, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06144v1.pdf filename=2403.06144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a framework for conducting psychological and linguistic research through simulated conversations using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> The proposed methodology offers significant advantages, particularly for simulating human interactions involving potential unethical language or behaviors that would be impermissible in traditional experiments with human participants. As a demonstration, we employed <b>LLMs</b> to simulate family conversations across four parenting styles (authoritarian, authoritative, permissive, and uninvolved). In general, we observed that the characteristics of the four parenting styles were portrayed in the simulated conversations. Several strategies could be used to improve the <b>simulation</b> quality, such as including context awareness, employing a <b>few-shot</b> <b>prompting</b> approach or <b>fine-tuning</b> models to cater to specific <b>simulation</b> requirements. Overall, this study introduces a promising methodology for conducting psychological and linguistic research through simulated conversations, while acknowledging the current limitations and proposing potential solutions for future refinement and improvement.</p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=14--60124-fedpit-towards-privacy-preserving-and-few-shot-federated-instruction-tuning-zhuo-zhang-et-al-2024>(1/4 | 60/124) FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning (Zhuo Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang, Zenglin Xu. (2024)<br><strong>FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning</strong><br><button class=copy-to-clipboard title="FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: Federated Learning, Few-shot, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06131v1.pdf filename=2403.06131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> has proven essential for enhancing the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in generating human-aligned responses. However, collecting diverse, high-quality <b>instruction</b> <b>data</b> for tuning poses challenges, particularly in privacy-sensitive domains. <b>Federated</b> <b>instruction</b> <b>tuning</b> (FedIT) has emerged as a solution, leveraging <b>federated</b> <b>learning</b> from multiple data owners while preserving privacy. Yet, it faces challenges due to limited <b>instruction</b> <b>data</b> and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel <b>federated</b> <b>algorithm,</b> FedPIT, which utilizes <b>LLMs&rsquo;</b> <b>in-context</b> <b>learning</b> capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive experiments on real-world medical data demonstrate the effectiveness of FedPIT in improving <b>federated</b> <b>few-shot</b> performance while preserving privacy and robustness against data heterogeneity.</p></p class="citation"></blockquote><h3 id=24--61124-attacking-transformers-with-feature-diversity-adversarial-perturbation-chenxing-gao-et-al-2024>(2/4 | 61/124) Attacking Transformers with Feature Diversity Adversarial Perturbation (Chenxing Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxing Gao, Hang Zhou, Junqing Yu, YuTeng Ye, Jiale Cai, Junle Wang, Wei Yang. (2024)<br><strong>Attacking Transformers with Feature Diversity Adversarial Perturbation</strong><br><button class=copy-to-clipboard title="Attacking Transformers with Feature Diversity Adversarial Perturbation" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CV, cs.CR<br>Keyword Score: 45<br>Keywords: Vision Transformer, Black Box, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07942v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07942v1.pdf filename=2403.07942v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the mechanisms behind <b>Vision</b> <b>Transformer</b> (ViT), particularly its vulnerability to adversarial perturba tions, is crucial for addressing challenges in its real-world applications. Existing ViT adversarial attackers rely on la bels to calculate the gradient for perturbation, and exhibit low transferability to other structures and tasks. In this paper, we present a label-free white-box attack approach for ViT-based models that exhibits strong transferability to various <b>black</b> <b>box</b> models, including most ViT variants, <b>CNNs,</b> and MLPs, even for models developed for other modalities. Our inspira tion comes from the feature collapse phenomenon in ViTs, where the critical attention mechanism overly depends on the low-frequency component of features, causing the features in middle-to-end layers to become increasingly similar and eventually collapse. We propose the feature diversity attacker to naturally accelerate this process and achieve remarkable performance and transferability.</p></p class="citation"></blockquote><h3 id=34--62124-fluent-round-efficient-secure-aggregation-for-private-federated-learning-xincheng-li-et-al-2024>(3/4 | 62/124) Fluent: Round-efficient Secure Aggregation for Private Federated Learning (Xincheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xincheng Li, Jianting Ning, Geong Sen Poh, Leo Yu Zhang, Xinchun Yin, Tianwei Zhang. (2024)<br><strong>Fluent: Round-efficient Secure Aggregation for Private Federated Learning</strong><br><button class=copy-to-clipboard title="Fluent: Round-efficient Secure Aggregation for Private Federated Learning" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06143v1.pdf filename=2403.06143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) facilitates collaborative training of machine learning models among a large number of clients while safeguarding the privacy of their local datasets. However, FL remains susceptible to vulnerabilities such as privacy inference and inversion attacks. Single-server secure aggregation schemes were proposed to address these threats. Nonetheless, they encounter practical constraints due to their round and communication complexities. This work introduces Fluent, a round and communication-efficient secure aggregation scheme for private FL. Fluent has several improvements compared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et al. (SP 2023): (1) it eliminates frequent handshakes and secret sharing operations by efficiently reusing the shares across multiple training iterations without leaking any private information; (2) it accomplishes both the consistency check and gradient unmasking in one logical step, thereby reducing another round of communication. With these innovations, Fluent achieves the fewest communication rounds (i.e., two in the collection phase) in the malicious server setting, in contrast to at least three rounds in existing schemes. This significantly minimizes the latency for geographically distributed clients; (3) Fluent also introduces Fluent-Dynamic with a participant selection algorithm and an alternative secret sharing scheme. This can facilitate dynamic client joining and enhance the system flexibility and scalability. We implemented Fluent and compared it with existing solutions. Experimental results show that Fluent improves the computational cost by at least 75% and communication overhead by at least 25% for normal clients. Fluent also reduces the communication overhead for the server at the expense of a marginal increase in computational cost.</p></p class="citation"></blockquote><h3 id=44--63124-federated-learning-attacks-defenses-opportunities-and-challenges-ghazaleh-shirvani-et-al-2024>(4/4 | 63/124) Federated Learning: Attacks, Defenses, Opportunities, and Challenges (Ghazaleh Shirvani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghazaleh Shirvani, Saeid Ghasemshirazi, Behzad Beigzadeh. (2024)<br><strong>Federated Learning: Attacks, Defenses, Opportunities, and Challenges</strong><br><button class=copy-to-clipboard title="Federated Learning: Attacks, Defenses, Opportunities, and Challenges" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06067v1.pdf filename=2403.06067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using dispersed data and training, <b>federated</b> <b>learning</b> (FL) moves AI capabilities to edge devices or does tasks locally. Many consider FL the start of a new era in AI, yet it is still immature. FL has not garnered the community&rsquo;s trust since its security and privacy implications are controversial. FL&rsquo;s security and privacy concerns must be discovered, analyzed, and recorded before widespread usage and adoption. A solid comprehension of risk variables allows an FL practitioner to construct a secure environment and provide researchers with a clear perspective of potential study fields, making FL the best solution in situations where security and privacy are primary issues. This research aims to deliver a complete overview of FL&rsquo;s security and privacy features to help bridge the gap between current <b>federated</b> <b>AI</b> and broad adoption in the future. In this paper, we present a comprehensive overview of the attack surface to investigate FL&rsquo;s existing challenges and defense measures to evaluate its robustness and reliability. According to our study, security concerns regarding FL are more frequent than privacy issues. Communication bottlenecks, poisoning, and backdoor attacks represent FL&rsquo;s privacy&rsquo;s most significant security threats. In the final part, we detail future research that will assist FL in adapting to real-world settings.</p></p class="citation"></blockquote><h2 id=cslg-15>cs.LG (15)</h2><h3 id=115--64124-cooperative-classification-and-rationalization-for-graph-generalization-linan-yue-et-al-2024>(1/15 | 64/124) Cooperative Classification and Rationalization for Graph Generalization (Linan Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li. (2024)<br><strong>Cooperative Classification and Rationalization for Graph Generalization</strong><br><button class=copy-to-clipboard title="Cooperative Classification and Rationalization for Graph Generalization" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 66<br>Keywords: Graph Classification, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Knowledge Distillation, Knowledge Distillation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06239v1.pdf filename=2403.06239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have achieved impressive results in <b>graph</b> <b>classification</b> <b>tasks,</b> but they struggle to generalize effectively when faced with <b>out-of-distribution</b> (OOD) data. Several approaches have been proposed to address this problem. Among them, one solution is to diversify training distributions in vanilla classification by modifying the data environment, yet accessing the environment information is complex. Besides, another promising approach involves rationalization, extracting invariant rationales for predictions. However, extracting rationales is difficult due to limited learning signals, resulting in less accurate rationales and diminished predictions. To address these challenges, in this paper, we propose a Cooperative Classification and Rationalization (C2R) method, consisting of the classification and the rationalization module. Specifically, we first assume that multiple environments are available in the classification module. Then, we introduce diverse training distributions using an environment-conditional generative network, enabling robust <b>graph</b> <b>representations.</b> <b>Meanwhile,</b> the rationalization module employs a separator to identify relevant rationale subgraphs while the remaining non-rationale subgraphs are de-correlated with labels. Next, we align <b>graph</b> <b>representations</b> <b>from</b> the classification module with rationale subgraph representations using the <b>knowledge</b> <b>distillation</b> methods, enhancing the learning signal for rationales. Finally, we infer multiple environments by gathering non-rationale representations and incorporate them into the classification module for cooperative learning. Extensive experimental results on both <b>benchmarks</b> and synthetic datasets demonstrate the effectiveness of C2R. Code is available at <a href=https://github.com/yuelinan/Codes-of-C2R>https://github.com/yuelinan/Codes-of-C2R</a>.</p></p class="citation"></blockquote><h3 id=215--65124-l2gc-lorentzian-linear-graph-convolutional-networks-for-node-classification-qiuyu-liang-et-al-2024>(2/15 | 65/124) L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification (Qiuyu Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuyu Liang, Weihua Wang, Feilong Bao, Guanglai Gao. (2024)<br><strong>L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification</strong><br><button class=copy-to-clipboard title="L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Node Classification, Graph, Convolution, Convolutional Neural Network, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06064v1.pdf filename=2403.06064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear <b>Graph</b> <b>Convolutional</b> <b>Networks</b> <b>(GCNs)</b> are used to classify the <b>node</b> <b>in</b> the <b>graph</b> <b>data.</b> <b>However,</b> we note that most existing linear <b>GCN</b> models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as <b>graphs.</b> <b>In</b> <b>this</b> paper, we attempt to introduce hyperbolic space into linear <b>GCN</b> and propose a novel framework for Lorentzian linear <b>GCN.</b> Specifically, we map the learned features of <b>graph</b> <b>nodes</b> <b>into</b> hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with <b>semi-supervised</b> <b>learning</b> show that our approach yields new state-of-the-art results of accuracy 74.7$%$ on Citeseer and 81.3$%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitude faster than other nonlinear <b>GCN</b> models on PubMed dataset. Our code is publicly available at <a href=https://github.com/llqy123/LLGC-master>https://github.com/llqy123/LLGC-master</a>.</p></p class="citation"></blockquote><h3 id=315--66124-framequant-flexible-low-bit-quantization-for-transformers-harshavardhan-adepu-et-al-2024>(3/15 | 66/124) FrameQuant: Flexible Low-Bit Quantization for Transformers (Harshavardhan Adepu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas Singh. (2024)<br><strong>FrameQuant: Flexible Low-Bit Quantization for Transformers</strong><br><button class=copy-to-clipboard title="FrameQuant: Flexible Low-Bit Quantization for Transformers" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Foundation Model, Quantization, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06082v1.pdf filename=2403.06082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> are the backbone of powerful <b>foundation</b> <b>models</b> for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training <b>Quantization</b> seeks to modify a pre-trained model and <b>quantize</b> it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully <b>quantized</b> to four bits with some performance loss. In this work, we outline a simple scheme to <b>quantize</b> <b>Transformer-based</b> models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the <b>quantization</b> must take place not in the original weight space, but instead in the Fusion Frame representations. If <b>quantization</b> is interpreted as the addition of noise, our casting of the problem allows invoking an extensive body of known consistent recovery and noise robustness guarantees. Further, if desired, de-noising filters are known in closed form. We show empirically, via a variety of experiments, that (almost) two-bit <b>quantization</b> for <b>Transformer</b> models promises sizable efficiency gains.</p></p class="citation"></blockquote><h3 id=415--67124-revisiting-edge-perturbation-for-graph-neural-network-in-graph-data-augmentation-and-attack-xin-liu-et-al-2024>(4/15 | 67/124) Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack (Xin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Liu, Yuxiang Zhang, Meng Wu, Mingyu Yan, Kun He, Wei Yan, Shirui Pan, Xiaochun Ye, Dongrui Fan. (2024)<br><strong>Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack</strong><br><button class=copy-to-clipboard title="Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07943v1.pdf filename=2403.07943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Edge perturbation is a basic method to modify <b>graph</b> <b>structures.</b> <b>It</b> can be categorized into two veins based on their effects on the performance of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> i.e., <b>graph</b> <b>data</b> <b>augmentation</b> and attack. Surprisingly, both veins of edge perturbation methods employ the same operations, yet yield opposite effects on <b>GNNs&rsquo;</b> accuracy. A distinct boundary between these methods in using edge perturbation has never been clearly defined. Consequently, inappropriate perturbations may lead to undesirable outcomes, necessitating precise adjustments to achieve desired effects. Therefore, questions of <code>why edge perturbation has a two-faced effect?'' and </code>what makes edge perturbation flexible and effective?&rsquo;&rsquo; still remain unanswered. In this paper, we will answer these questions by proposing a unified formulation and establishing a clear boundary between two categories of edge perturbation methods. Specifically, we conduct experiments to elucidate the differences and similarities between these methods and theoretically unify the workflow of these methods by casting it to one optimization problem. Then, we devise Edge Priority Detector (EPD) to generate a novel priority metric, bridging these methods up in the workflow. Experiments show that EPD can make augmentation or attack flexibly and achieve comparable or superior performance to other counterparts with less time overhead.</p></p class="citation"></blockquote><h3 id=515--68124-generalization-of-graph-neural-networks-through-the-lens-of-homomorphism-shouheng-li-et-al-2024>(5/15 | 68/124) Generalization of Graph Neural Networks through the Lens of Homomorphism (Shouheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shouheng Li, Dongwoo Kim, Qing Wang. (2024)<br><strong>Generalization of Graph Neural Networks through the Lens of Homomorphism</strong><br><button class=copy-to-clipboard title="Generalization of Graph Neural Networks through the Lens of Homomorphism" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06079v1.pdf filename=2403.06079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the celebrated popularity of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> across numerous applications, the ability of <b>GNNs</b> to generalize remains less explored. In this work, we propose to study the generalization of <b>GNNs</b> through a novel perspective - analyzing the entropy of <b>graph</b> <b>homomorphism.</b> <b>By</b> linking <b>graph</b> <b>homomorphism</b> <b>with</b> information-theoretic measures, we derive generalization bounds for both <b>graph</b> <b>and</b> <b>node</b> <b>classifications.</b> These bounds are capable of capturing subtleties inherent in various <b>graph</b> <b>structures,</b> <b>including</b> but not limited to paths, cycles and cliques. This enables a data-dependent generalization analysis with robust theoretical guarantees. To shed light on the generality of of our proposed bounds, we present a unifying framework that can characterize a broad spectrum of <b>GNN</b> models through the lens of <b>graph</b> <b>homomorphism.</b> <b>We</b> validate the practical applicability of our theoretical findings by showing the alignment between the proposed bounds and the empirically observed generalization gaps over both real-world and synthetic datasets.</p></p class="citation"></blockquote><h3 id=615--69124-risk-sensitive-rl-with-optimized-certainty-equivalents-via-reduction-to-standard-rl-kaiwen-wang-et-al-2024>(6/15 | 69/124) Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL (Kaiwen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiwen Wang, Dawen Liang, Nathan Kallus, Wen Sun. (2024)<br><strong>Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL</strong><br><button class=copy-to-clipboard title="Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06323v1.pdf filename=2403.06323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study Risk-Sensitive <b>Reinforcement</b> <b>Learning</b> (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz&rsquo;s mean-variance. Using an augmented <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for <b>MDPs</b> with bounded coverability, e.g., exogenous block <b>MDPs.</b> Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive policy while prior algorithms provably fail.</p></p class="citation"></blockquote><h3 id=715--70124-optimal-policy-sparsification-and-low-rank-decomposition-for-deep-reinforcement-learning-vikram-goddla-2024>(7/15 | 70/124) Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning (Vikram Goddla, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vikram Goddla. (2024)<br><strong>Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Model Compression, Pruning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06313v1.pdf filename=2403.06313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning(DRL)</b> has shown significant promise in a wide range of applications including computer games and robotics. Yet, training DRL policies consume extraordinary computing resources resulting in dense policies which are prone to overfitting. Moreover, inference with dense DRL policies limit their practical applications, especially in edge computing. Techniques such as <b>pruning</b> and singular value decomposition have been used with deep learning <b>models</b> <b>to</b> achieve sparsification and <b>model</b> <b>compression</b> to limit overfitting and reduce memory consumption. However, these techniques resulted in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$ regularization techniques have been proposed for neural network sparsification and sparse auto-encoder development, but their implementation in DRL environments has not been apparent. We propose a novel $L_0$-norm-regularization technique using an optimal sparsity map to sparsify DRL policies and promote their decomposition to a lower rank without decay in rewards. We evaluated our $L_0$-norm-regularization technique across five different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2, SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL policy in the SuperMarioBros environment achieved 93% sparsity and gained 70% compression when subjected to low-rank decomposition, while significantly outperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL policy in the Surgical Robot Learning environment achieved a 36% sparsification and gained 46% compression when decomposed to a lower rank, while being performant. The results suggest that our custom $L_0$-norm-regularization technique for sparsification of DRL policies is a promising avenue to reduce computational resources and limit overfitting.</p></p class="citation"></blockquote><h3 id=815--71124-local-vertex-colouring-graph-neural-networks-shouheng-li-et-al-2024>(8/15 | 71/124) Local Vertex Colouring Graph Neural Networks (Shouheng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shouheng Li, Dongwoo Kim, Qing Wang. (2024)<br><strong>Local Vertex Colouring Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Local Vertex Colouring Graph Neural Networks" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06080v1.pdf filename=2403.06080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, there has been a significant amount of research focused on expanding the expressivity of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> beyond the Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded advancements in expressivity, they have frequently come at the expense of decreased efficiency or have been restricted to specific types of <b>graphs.</b> <b>In</b> <b>this</b> study, we investigate the expressivity of <b>GNNs</b> from the perspective of <b>graph</b> <b>search.</b> <b>Specifically,</b> we propose a new vertex colouring scheme and demonstrate that classical search algorithms can efficiently compute <b>graph</b> <b>representations</b> <b>that</b> extend beyond the 1-WL. We show the colouring scheme inherits useful properties from <b>graph</b> <b>search</b> <b>that</b> can help solve problems like <b>graph</b> <b>biconnectivity.</b> <b>Furthermore,</b> we show that under certain conditions, the expressivity of <b>GNNs</b> increases hierarchically with the radius of the search neighbourhood. To further investigate the proposed scheme, we develop a new type of <b>GNN</b> based on two search strategies, breadth-first search and depth-first search, highlighting the <b>graph</b> <b>properties</b> <b>they</b> can capture on top of 1-WL. Our code is available at <a href=https://github.com/seanli3/lvc>https://github.com/seanli3/lvc</a>.</p></p class="citation"></blockquote><h3 id=915--72124-transferable-reinforcement-learning-via-generalized-occupancy-models-chuning-zhu-et-al-2024>(9/15 | 72/124) Transferable Reinforcement Learning via Generalized Occupancy Models (Chuning Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuning Zhu, Xinqi Wang, Tyler Han, Simon S. Du, Abhishek Gupta. (2024)<br><strong>Transferable Reinforcement Learning via Generalized Occupancy Models</strong><br><button class=copy-to-clipboard title="Transferable Reinforcement Learning via Generalized Occupancy Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06328v1.pdf filename=2403.06328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of <b>reinforcement</b> <b>learning</b> (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without having to redo policy optimization. By directly modeling long-term outcomes, GOMs avoid compounding error while retaining generality across arbitrary reward functions. We provide a practical instantiation of GOMs using <b>diffusion</b> <b>models</b> and show its efficacy as a new class of transferable models, both theoretically and empirically across a variety of simulated robotics problems. Videos and code at <a href=https://weirdlabuw.github.io/gom/>https://weirdlabuw.github.io/gom/</a>.</p></p class="citation"></blockquote><h3 id=1015--73124-analysis-of-total-variation-minimization-for-clustered-federated-learning-a-jung-2024>(10/15 | 73/124) Analysis of Total Variation Minimization for Clustered Federated Learning (A. Jung, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. Jung. (2024)<br><strong>Analysis of Total Variation Minimization for Clustered Federated Learning</strong><br><button class=copy-to-clipboard title="Analysis of Total Variation Minimization for Clustered Federated Learning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-11; I-5-3, cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06298v1.pdf filename=2403.06298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A key challenge in <b>federated</b> <b>learning</b> applications is the statistical heterogeneity of local datasets. Clustered <b>federated</b> <b>learning</b> addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered <b>federated</b> <b>learning</b> is generalized total variation minimization (GTVMin). This approach requires a similarity <b>graph</b> which can be obtained by domain expertise or in a data-driven fashion via <b>graph</b> learning techniques. Under a widely applicable <b>clustering</b> assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within <b>federated</b> <b>learning</b> environments.</p></p class="citation"></blockquote><h3 id=1115--74124-fake-or-compromised-making-sense-of-malicious-clients-in-federated-learning-hamid-mozaffari-et-al-2024>(11/15 | 74/124) Fake or Compromised? Making Sense of Malicious Clients in Federated Learning (Hamid Mozaffari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamid Mozaffari, Sunav Choudhary, Amir Houmansadr. (2024)<br><strong>Fake or Compromised? Making Sense of Malicious Clients in Federated Learning</strong><br><button class=copy-to-clipboard title="Fake or Compromised? Making Sense of Malicious Clients in Federated Learning" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06319v1.pdf filename=2403.06319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a distributed machine learning paradigm that enables training models on decentralized data. The field of FL security against poisoning attacks is plagued with confusion due to the proliferation of research that makes different assumptions about the capabilities of adversaries and the adversary models they operate under. Our work aims to clarify this confusion by presenting a comprehensive analysis of the various poisoning attacks and defensive aggregation rules (AGRs) proposed in the literature, and connecting them under a common framework. To connect existing adversary models, we present a hybrid adversary model, which lies in the middle of the spectrum of adversaries, where the adversary compromises a few clients, trains a generative (e.g., DDPM) model with their compromised samples, and generates new synthetic data to solve an optimization for a stronger (e.g., cheaper, more practical) attack against different robust aggregation rules. By presenting the spectrum of FL adversaries, we aim to provide practitioners and researchers with a clear understanding of the different types of threats they need to consider when designing FL systems, and identify areas where further research is needed.</p></p class="citation"></blockquote><h3 id=1215--75124-fwin-transformer-for-dengue-prediction-under-climate-and-ocean-influence-nhat-thanh-tran-et-al-2024>(12/15 | 75/124) FWin transformer for dengue prediction under climate and ocean influence (Nhat Thanh Tran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nhat Thanh Tran, Jack Xin, Guofa Zhou. (2024)<br><strong>FWin transformer for dengue prediction under climate and ocean influence</strong><br><button class=copy-to-clipboard title="FWin transformer for dengue prediction under climate and ocean influence" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07027v1.pdf filename=2403.07027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dengue fever is one of the most deadly mosquito-born tropical infectious diseases. Detailed long range forecast model is vital in controlling the spread of disease and making mitigation efforts. In this study, we examine methods used to forecast dengue cases for long range predictions. The dataset consists of local climate/weather in addition to global climate indicators of Singapore from 2000 to 2019. We utilize newly developed deep neural networks to learn the intricate relationship between the features. The baseline models in this study are in the class of recent <b>transformers</b> for long sequence forecasting tasks. We found that a Fourier mixed window attention (FWin) based <b>transformer</b> performed the best in terms of both the mean square error and the maximum absolute error on the long range dengue forecast up to 60 weeks.</p></p class="citation"></blockquote><h3 id=1315--76124-probabilistic-neural-circuits-pedro-zuidberg-dos-martires-2024>(13/15 | 76/124) Probabilistic Neural Circuits (Pedro Zuidberg Dos Martires, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Zuidberg Dos Martires. (2024)<br><strong>Probabilistic Neural Circuits</strong><br><button class=copy-to-clipboard title="Probabilistic Neural Circuits" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06235v1.pdf filename=2403.06235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Probabilistic</b> <b>circuits</b> (PCs) have gained prominence in recent years as a versatile framework for discussing <b>probabilistic</b> <b>models</b> that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce <b>probabilistic</b> <b>neural</b> circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.</p></p class="citation"></blockquote><h3 id=1415--77124-linearapt-an-adaptive-algorithm-for-the-fixed-budget-thresholding-linear-bandit-problem-yun-ang-wu-et-al-2024>(14/15 | 77/124) LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem (Yun-Ang Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yun-Ang Wu, Yun-Da Tsai, Shou-De Lin. (2024)<br><strong>LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem</strong><br><button class=copy-to-clipboard title="LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06230v1.pdf filename=2403.06230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we delve into the Thresholding Linear <b>Bandit</b> (TLB) problem, a nuanced domain within stochastic Multi-Armed <b>Bandit</b> (MAB) problems, focusing on maximizing decision accuracy against a linearly defined threshold under resource constraints. We present LinearAPT, a novel algorithm designed for the fixed budget setting of TLB, providing an efficient solution to optimize sequential decision-making. This algorithm not only offers a theoretical upper bound for estimated loss but also showcases robust performance on both synthetic and real-world datasets. Our contributions highlight the adaptability, simplicity, and computational efficiency of LinearAPT, making it a valuable addition to the toolkit for addressing complex sequential decision-making challenges.</p></p class="citation"></blockquote><h3 id=1515--78124-domain-adversarial-active-learning-for-domain-generalization-classification-jianting-chen-et-al-2024>(15/15 | 78/124) Domain Adversarial Active Learning for Domain Generalization Classification (Jianting Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianting Chen, Ling Ding, Yunxiao Yang, Zaiyuan Di, Yang Xiang. (2024)<br><strong>Domain Adversarial Active Learning for Domain Generalization Classification</strong><br><button class=copy-to-clipboard title="Domain Adversarial Active Learning for Domain Generalization Classification" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06174v1.pdf filename=2403.06174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain generalization models aim to learn cross-domain knowledge from source domain data, to improve performance on unknown target domains. Recent research has demonstrated that diverse and rich source domain samples can enhance domain generalization capability. This paper argues that the impact of each sample on the model&rsquo;s generalization ability varies. Despite its small scale, a high-quality dataset can still attain a certain level of generalization ability. Motivated by this, we propose a domain-adversarial <b>active</b> <b>learning</b> (DAAL) algorithm for classification tasks in domain generalization. First, we analyze that the objective of tasks is to maximize the inter-class distance within the same domain and minimize the intra-class distance across different domains. To achieve this objective, we design a domain adversarial selection method that prioritizes challenging samples. Second, we posit that even in a converged model, there are subsets of features that lack discriminatory power within each domain. We attempt to identify these feature subsets and optimize them by a constraint loss. We validate and analyze our DAAL algorithm on multiple domain generalization datasets, comparing it with various domain generalization algorithms and <b>active</b> <b>learning</b> algorithms. Our results demonstrate that the DAAL algorithm can achieve strong generalization ability with fewer data resources, thereby reducing data annotation costs in domain generalization tasks.</p></p class="citation"></blockquote><h2 id=csai-3>cs.AI (3)</h2><h3 id=13--79124-trad-enhancing-llm-agents-with-step-wise-thought-retrieval-and-aligned-decision-ruiwen-zhou-et-al-2024>(1/3 | 79/124) TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision (Ruiwen Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, Weinan Zhang. (2024)<br><strong>TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision</strong><br><button class=copy-to-clipboard title="TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-IR, cs.AI<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, In-context Learning, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06221v1.pdf filename=2403.06221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerous <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> agents have been built for different tasks like web navigation and online shopping due to <b>LLM&rsquo;s</b> wide knowledge and <b>text-understanding</b> <b>ability.</b> Among these works, many of them utilize <b>in-context</b> examples to achieve generalization without the need for <b>fine-tuning,</b> while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as <b>in-context</b> examples have been proposed to improve the agent&rsquo;s overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web <b>benchmarks</b> show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.</p></p class="citation"></blockquote><h3 id=23--80124-argmed-agents-explainable-clinical-decision-reasoning-with-large-language-models-via-argumentation-schemes-shengxin-hong-et-al-2024>(2/3 | 80/124) ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes (Shengxin Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengxin Hong, Liang Xiao, Xin Zhang, Jianxia Chen. (2024)<br><strong>ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes</strong><br><button class=copy-to-clipboard title="ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs-SC, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06294v1.pdf filename=2403.06294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are two main barriers to using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in clinical <b>reasoning.</b> Firstly, while <b>LLMs</b> exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex <b>reasoning</b> and planning falls short of expectations. Secondly, <b>LLMs</b> use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician&rsquo;s cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable <b>LLM-based</b> agents to make explainable clinical decision <b>reasoning</b> through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a <b>reasoning</b> mechanism for modeling cognitive processes in clinical <b>reasoning),</b> and then constructs the argumentation process as a directed <b>graph</b> representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision. ArgMed-Agents enables <b>LLMs</b> to mimic the process of clinical argumentative <b>reasoning</b> by generating explanations of <b>reasoning</b> in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision <b>reasoning</b> problems compared to other <b>prompt</b> methods, but more importantly, it provides users with decision explanations that increase their confidence.</p></p class="citation"></blockquote><h3 id=33--81124-towards-generalizable-and-interpretable-motion-prediction-a-deep-variational-bayes-approach-juanwu-lu-et-al-2024>(3/3 | 81/124) Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach (Juanwu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juanwu Lu, Wei Zhan, Masayoshi Tomizuka, Yeping Hu. (2024)<br><strong>Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach</strong><br><button class=copy-to-clipboard title="Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06086v1.pdf filename=2403.06086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the potential behavior of the surrounding human-driven vehicles is crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent state-of-the-art achieved accurate prediction using deep neural networks. However, these end-to-end models are usually black boxes with weak interpretability and generalizability. This paper proposes the Goal-based Neural Variational Agent (GNeVA), an interpretable generative model for motion prediction with robust generalizability to <b>out-of-distribution</b> cases. For interpretability, the model achieves target-driven motion prediction by estimating the spatial distribution of long-term destinations with a variational mixture of Gaussians. We identify a causal structure among maps and agents&rsquo; histories and derive a variational posterior to enhance generalizability. Experiments on motion prediction datasets validate that the fitted model can be interpretable and generalizable and can achieve comparable performance to state-of-the-art results.</p></p class="citation"></blockquote><h2 id=csce-3>cs.CE (3)</h2><h3 id=13--82124-generative-lstm-models-and-asset-hierarchy-creation-in-industrial-facilities-morgen-pronk-2024>(1/3 | 82/124) Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities (Morgen Pronk, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Morgen Pronk. (2024)<br><strong>Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities</strong><br><button class=copy-to-clipboard title="Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 50<br>Keywords: LSTM, LSTM, LSTM, Recurrent Neural Network, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06103v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06103v1.pdf filename=2403.06103v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving field of maintenance and reliability engineering, the organization of equipment into hierarchical structures presents both a challenge and a necessity, directly impacting the operational integrity of industrial facilities. This paper introduces an innovative approach employing machine learning, specifically <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> models, to automate and enhance the creation and management of these hierarchies. By adapting techniques commonly used in natural language processing, the study explores the potential of <b>LSTM</b> models to interpret and predict relationships within equipment tags, offering a novel perspective on understanding facility design. This methodology involved character-wise <b>tokenization</b> of asset tags from approximately 29,000 entries across 50 upstream oil and gas facilities, followed by modeling these sequences using an <b>LSTM-based</b> <b>recurrent</b> <b>neural</b> <b>network.</b> The model&rsquo;s architecture capitalizes on <b>LSTM&rsquo;s</b> ability to learn <b>long-term</b> <b>dependencies,</b> <b>facilitating</b> <b>the</b> prediction of hierarchical relationships and contextual understanding of equipment tags.</p></p class="citation"></blockquote><h3 id=23--83124-no-language-is-an-island-unifying-chinese-and-english-in-financial-large-language-models-instruction-data-and-benchmarks-gang-hu-et-al-2024>(2/3 | 83/124) No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks (Gang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou Wang, Sophia Ananiadou, Wanlong Yu, Jimin Huang, Qianqian Xie. (2024)<br><strong>No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks</strong><br><button class=copy-to-clipboard title="No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-CL, cs.CE<br>Keyword Score: 26<br>Keywords: Benchmarking, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06249v1.pdf filename=2403.06249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the progression of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE <b>benchmark</b> for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and <b>multi-modal</b> instruction data, and an evaluation <b>benchmark</b> with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in translation tasks and utilizing original English data, enhancing both linguistic flexibility and analytical acuity in financial contexts. Notably, ICE-INTENT distinguishes itself by showcasing significant enhancements over conventional <b>LLMs</b> and existing financial <b>LLMs</b> in bilingual milieus, underscoring the profound impact of robust bilingual data on the accuracy and efficacy of financial NLP.</p></p class="citation"></blockquote><h3 id=33--84124-rads--restricted-anisotropic-diffusion-spectrum-model-for-axonal-health-quantification-in-multiple-sclerosis-nand-sharma-2024>(3/3 | 84/124) RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis (Nand Sharma, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nand Sharma. (2024)<br><strong>RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis</strong><br><button class=copy-to-clipboard title="RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06140v1.pdf filename=2403.06140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Axonal damage is the primary pathological correlate of long-term impairment in multiple sclerosis (MS). Our previous work using our method - diffusion basis spectrum imaging (DBSI) - demonstrated a strong, quantitative relationship between axial diffusivity and axonal damage. In the present work, we develop an extension of DBSI which can be used to quantify the fraction of diseased and healthy axons in MS. In this method, we model the MRI signal with the axial diffusion (AD) spectrum for each fiber orientation. We use two component restricted anisotropic diffusion spectrum (RADS) to model the anisotropic component of the diffusion-weighted MRI signal. Diffusion coefficients and signal fractions are computed for the optimal model with the lowest Bayesian information criterion (BIC) score. This gives us the fractions of diseased and healthy axons based on the axial diffusivities of the diseased and healthy axons. We test our method using Monte-Carlo (MC) <b>simulations</b> with the MC <b>simulation</b> package developed as part of this work. First we test and validate our MC <b>simulations</b> for the basic RADS model. It accurately recovers the fiber and cell fractions simulated as well as the simulated diffusivities. For testing and validating RADS to quantify axonal loss, we simulate different fractions of diseased and healthy axons. Our method produces highly accurate quantification of diseased and healthy axons with Pearson&rsquo;s correlation (predicted vs true proportion) of $ r = 0.99 $ (p-value = 0.001); the one Sample t-test for proportion error gives the mean error of 2% (p-value = 0.034). Furthermore, the method finds the axial diffusivities of the diseased and healthy axons very accurately with mean error of 4% (p-value = 0.001). RADS modeling of the diffusion-weighted MRI signal has the potential to be used for Axonal Health quantification in Multiple Sclerosis.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--85124-are-llms-ready-for-visualization-pere-pau-vázquez-2024>(1/4 | 85/124) Are LLMs ready for Visualization? (Pere-Pau Vázquez, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pere-Pau Vázquez. (2024)<br><strong>Are LLMs ready for Visualization?</strong><br><button class=copy-to-clipboard title="Are LLMs ready for Visualization?" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06158v1.pdf filename=2403.06158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models have received a lot of attention in many areas of academia and the industry. Their capabilities span many areas, from the invention of images given a <b>prompt</b> to the generation of concrete code to solve a certain programming issue. These two paradigmatic cases fall within two distinct categories of requirements, ranging from &ldquo;creativity&rdquo; to &ldquo;precision&rdquo;, as characterized by Bing Chat, which employs <b>ChatGPT-4</b> as its backbone. Visualization practitioners and researchers have wondered to what end one of such systems could accomplish our work in a more efficient way. Several works in the literature have utilized them for the creation of visualizations. And some tools such as Lida, incorporate them as part of their pipeline. Nevertheless, to the authors&rsquo; knowledge, no systematic approach for testing their capabilities has been published, which includes both extensive and in-depth evaluation. Our goal is to fill that gap with a systematic approach that analyzes three elements: whether <b>Large</b> <b>Language</b> <b>Models</b> are capable of correctly generating a <b>large</b> <b>variety</b> <b>of</b> charts, what libraries they can deal with effectively, and how far we can go to configure individual charts. To achieve this objective, we initially selected a diverse set of charts, which are commonly utilized in data visualization. We then developed a set of generic <b>prompts</b> that could be used to generate them, and analyzed the performance of different <b>LLMs</b> and libraries. The results include both the set of <b>prompts</b> and the data sources, as well as an analysis of the performance with different configurations.</p></p class="citation"></blockquote><h3 id=24--86124-explaining-code-with-a-purpose-an-integrated-approach-for-developing-code-comprehension-and-prompting-skills-paul-denny-et-al-2024>(2/4 | 86/124) Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills (Paul Denny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Denny, David H. Smith IV, Max Fowler, James Prather, Brett A. Becker, Juho Leinonen. (2024)<br><strong>Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills</strong><br><button class=copy-to-clipboard title="Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs-SE, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06050v1.pdf filename=2403.06050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear <b>prompts</b> that can elicit intended code from an <b>LLM.</b> Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with <b>LLMs.</b> One effective way to develop and assess code comprehension ability is with ``Explain in plain English&rsquo;&rsquo; (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating <b>LLMs</b> to overcome this limitation. We propose using an <b>LLM</b> to generate code based on students&rsquo; responses to EiPE questions &ndash; not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and <b>prompt</b> crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective <b>prompts</b> for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of <b>LLMs</b> for aiding and assessing learning.</p></p class="citation"></blockquote><h3 id=34--87124-developing-an-ai-based-psychometric-system-for-assessing-learning-difficulties-and-adaptive-system-to-overcome-a-qualitative-and-conceptual-framework-aaron-hu-2024>(3/4 | 87/124) Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework (Aaron Hu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Hu. (2024)<br><strong>Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework</strong><br><button class=copy-to-clipboard title="Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-ET, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Autoencoder, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06284v1.pdf filename=2403.06284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning difficulties pose significant challenges for students, impacting their academic performance and overall educational experience. These difficulties could sometimes put students into a downward spiral that lack of educational resources for personalized support consistently led to under-accommodation of students special needs, and the student lose opportunities in the longer term academic and work development. This research aims to propose a conceptual framework for an adaptive AI-based virtual tutor system that incorporates psychometric assessment to support students with learning difficulties. This process involves the careful selection and integration of validated current mature psychometric scales that assess key dimensions of learning, such as cognitive abilities, learning styles, and academic skills. By incorporating scales that specifically assess these difficulties, the psychometric test will provide a comprehensive understanding of each students unique learning profile and inform targeted interventions within the adaptive tutoring system. The paper also proposes using <b>autoencoders</b> to identify the latent patterns to generate the students profile vector for collection of psychometric data, defining state space and action space representing the students desired combination of images, sound and text engagements, employing extended Bayesian knowledge tracing and hierarchical model and Metropolis-Hastings to continuously estimate and monitor the students performance in various psychometric constructs. The proposed system will leverage the capabilities of <b>LLMs,</b> visual generation models, and psychometric assessments to provide personalized instruction and support tailored to each students unique learning characteristics and needs.</p></p class="citation"></blockquote><h3 id=44--88124-understanding-parents-perceptions-and-practices-toward-childrens-security-and-privacy-in-virtual-reality-jiaxun-cao-et-al-2024>(4/4 | 88/124) Understanding Parents&rsquo; Perceptions and Practices Toward Children&rsquo;s Security and Privacy in Virtual Reality (Jiaxun Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxun Cao, Abhinaya S B, Anupam Das, Pardis Emami-Naeini. (2024)<br><strong>Understanding Parents&rsquo; Perceptions and Practices Toward Children&rsquo;s Security and Privacy in Virtual Reality</strong><br><button class=copy-to-clipboard title="Understanding Parents' Perceptions and Practices Toward Children's Security and Privacy in Virtual Reality" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06172v1.pdf filename=2403.06172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have seen a sharp increase in underage users of virtual reality (VR), where security and privacy (S&amp;P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent. Prior work shows children largely rely on parents to mitigate S&amp;P risks in their technology use. Therefore, understanding parents&rsquo; S&amp;P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance children&rsquo;s S&amp;P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR. To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home. Our findings highlight parents generally lack S&amp;P awareness due to the perception that VR is still in its infancy. To protect their children&rsquo;s interaction with VR, parents currently primarily rely on active strategies such as verbal education about S&amp;P. Passive strategies such as parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints. Parents also highlight that a multi-stakeholder ecosystem must be established towards more S&amp;P support for children in VR. Based on the findings, we propose actionable S&amp;P <b>recommendations</b> for critical stakeholders, including parents, educators, VR companies, and governments.</p></p class="citation"></blockquote><h2 id=eessiv-6>eess.IV (6)</h2><h3 id=16--89124-low-dose-ct-denoising-with-language-engaged-dual-space-alignment-zhihao-chen-et-al-2024>(1/6 | 89/124) Low-dose CT Denoising with Language-engaged Dual-space Alignment (Zhihao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Chen, Tao Chen, Chenhui Wang, Chuang Niu, Ge Wang, Hongming Shan. (2024)<br><strong>Low-dose CT Denoising with Language-engaged Dual-space Alignment</strong><br><button class=copy-to-clipboard title="Low-dose CT Denoising with Language-engaged Dual-space Alignment" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Autoencoder, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06128v1.pdf filename=2403.06128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While various deep learning methods were proposed for low-dose computed tomography (CT) denoising, they often suffer from over-smoothing, blurring, and lack of explainability. To alleviate these issues, we propose a plug-and-play Language-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT denoising models. Our idea is to leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to align denoised CT and normal dose CT images in both the continuous perceptual space and discrete semantic space, which is the first <b>LLM-based</b> scheme for low-dose CT denoising. LEDA involves two steps: the first is to pretrain an <b>LLM-guided</b> CT <b>autoencoder,</b> which can encode a CT image into continuous high-level features and <b>quantize</b> them into a token space to produce semantic tokens derived from the <b>LLM&rsquo;s</b> vocabulary; and the second is to minimize the discrepancy between the denoised CT images and normal dose CT in terms of both encoded high-level features and <b>quantized</b> token embeddings derived by the <b>LLM-guided</b> CT <b>autoencoder.</b> Extensive experimental results on two public LDCT denoising datasets demonstrate that our LEDA can enhance existing denoising models in terms of quantitative metrics and qualitative evaluation, and also provide explainability through language-level image understanding. Source code is available at <a href=https://github.com/hao1635/LEDA>https://github.com/hao1635/LEDA</a>.</p></p class="citation"></blockquote><h3 id=26--90124-implicit-image-to-image-schrodinger-bridge-for-ct-super-resolution-and-denoising-yuang-wang-et-al-2024>(2/6 | 90/124) Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising (Yuang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Zhennong Chen, Rui Hu, Li Zhang, Zhiqiang Chen, Quanzheng Li, Dufan Wu. (2024)<br><strong>Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising</strong><br><button class=copy-to-clipboard title="Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06069v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06069v1.pdf filename=2403.06069v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional <b>diffusion</b> <b>models</b> have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional <b>diffusion</b> <b>models.</b> In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising <b>diffusion</b> <b>probabilistic</b> <b>model</b> (cDDPM) and I2SB, in both visual quality and quantitative metrics. These findings underscore the potential of I3SB in improving medical image restoration by providing fast and accurate generative modeling.</p></p class="citation"></blockquote><h3 id=36--91124-causalcellsegmenter-causal-inference-inspired-diversified-aggregation-convolution-for-pathology-image-segmentation-dawei-fan-et-al-2024>(3/6 | 91/124) CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation (Dawei Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dawei Fan, Yifan Gao, Jiaming Yu, Yanping Chen, Wencheng Li, Chuancong Lin, Kaibin Li, Changcai Yang, Riqing Chen, Lifang Wei. (2024)<br><strong>CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation</strong><br><button class=copy-to-clipboard title="CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06066v1.pdf filename=2403.06066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models have shown promising performance for cell nucleus segmentation in the field of pathology image analysis. However, training a robust model from multiple domains remains a great challenge for cell nucleus segmentation. Additionally, the shortcomings of background noise, highly overlapping between cell nucleus, and blurred edges often lead to poor performance. To address these challenges, we propose a novel framework termed CausalCellSegmenter, which combines Causal Inference Module (CIM) with Diversified Aggregation <b>Convolution</b> (DAC) techniques. The DAC module is designed which incorporates diverse downsampling features through a simple, parameter-free attention module (SimAM), aiming to overcome the problems of false-positive identification and edge blurring. Furthermore, we introduce CIM to leverage sample weighting by directly removing the spurious correlations between features for every input sample and concentrating more on the correlation between features and labels. Extensive experiments on the MoNuSeg-2018 dataset achieves promising results, outperforming other state-of-the-art methods, where the mIoU and DSC scores growing by 3.6% and 2.65%.</p></p class="citation"></blockquote><h3 id=46--92124-decoupled-data-consistency-with-diffusion-purification-for-image-restoration-xiang-li-et-al-2024>(4/6 | 92/124) Decoupled Data Consistency with Diffusion Purification for Image Restoration (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing Qu. (2024)<br><strong>Decoupled Data Consistency with Diffusion Purification for Image Restoration</strong><br><button class=copy-to-clipboard title="Decoupled Data Consistency with Diffusion Purification for Image Restoration" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, cs-LG, eess-IV, eess-SP, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06054v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06054v2.pdf filename=2403.06054v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of <b>diffusion</b> <b>models.</b> However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated <b>diffusion</b> <b>model</b> samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel <b>diffusion-based</b> <b>image</b> restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involves alternating between a reconstruction phase to maintain data consistency and a refinement phase that enforces the prior via <b>diffusion</b> <b>purification.</b> Our approach demonstrates versatility, making it highly adaptable for efficient problem-solving in latent space. Additionally, it reduces the necessity for numerous sampling steps through the integration of consistency models. The efficacy of our approach is validated through comprehensive experiments across various image restoration tasks, including image denoising, deblurring, inpainting, and super-resolution.</p></p class="citation"></blockquote><h3 id=56--93124-pepsi-pathology-enhanced-pulse-sequence-invariant-representations-for-brain-mri-peirong-liu-et-al-2024>(5/6 | 93/124) PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI (Peirong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peirong Liu, Oula Puonti, Annabel Sorby-Adams, William T. Kimberly, Juan E. Iglesias. (2024)<br><strong>PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI</strong><br><button class=copy-to-clipboard title="PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06227v1.pdf filename=2403.06227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remarkable progress has been made by data-driven machine-learning methods in the analysis of MRI scans. However, most existing MRI analysis approaches are crafted for specific MR pulse sequences (MR contrasts) and usually require nearly isotropic acquisitions. This limits their applicability to diverse real-world clinical data, where scans commonly exhibit variations in appearances due to being obtained with varying sequence parameters, resolutions, and orientations &ndash; especially in the presence of pathology. In this paper, we propose PEPSI, the first pathology-enhanced, and pulse-sequence-invariant feature <b>representation</b> <b>learning</b> model for brain MRI. PEPSI is trained entirely on synthetic images with a novel pathology encoding strategy, and enables co-training across datasets with diverse pathologies and missing modalities. Despite variations in pathology appearances across different MR pulse sequences or the quality of acquired images (e.g., resolution, orientation, artifacts, etc), PEPSI produces a high-resolution image of reference contrast (MP-RAGE) that captures anatomy, along with an image specifically highlighting the pathology. Our experiments demonstrate PEPSI&rsquo;s remarkable capability for image synthesis compared with the state-of-the-art, contrast-agnostic synthesis models, as it accurately reconstructs anatomical structures while differentiating between pathology and normal tissue. We further illustrate the efficiency and effectiveness of PEPSI features for downstream pathology segmentations on five public datasets covering white matter hyperintensities and stroke lesions. Code is available at <a href=https://github.com/peirong26/PEPSI>https://github.com/peirong26/PEPSI</a>.</p></p class="citation"></blockquote><h3 id=66--94124-drfuse-learning-disentangled-representation-for-clinical-multi-modal-fusion-with-missing-modality-and-modal-inconsistency-wenfang-yao-et-al-2024>(6/6 | 94/124) DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency (Wenfang Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenfang Yao, Kejing Yin, William K. Cheung, Jia Liu, Jing Qin. (2024)<br><strong>DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency</strong><br><button class=copy-to-clipboard title="DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06197v1.pdf filename=2403.06197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The combination of electronic health records (EHR) and medical images is crucial for clinicians in making diagnoses and forecasting prognosis. Strategically fusing these two data modalities has great potential to improve the accuracy of machine learning models in clinical prediction tasks. However, the asynchronous and complementary nature of EHR and medical images presents unique challenges. Missing modalities due to clinical and administrative factors are inevitable in practice, and the significance of each data modality varies depending on the patient and the prediction target, resulting in inconsistent predictions and suboptimal model performance. To address these challenges, we propose DrFuse to achieve effective clinical <b>multi-modal</b> fusion. It tackles the missing modality issue by disentangling the features shared across modalities and those unique within each modality. Furthermore, we address the modal inconsistency issue via a disease-wise attention layer that produces the patient- and disease-wise weighting for each modality to make the final prediction. We validate the proposed method using real-world large-scale datasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed method significantly outperforms the state-of-the-art models. Our implementation is publicly available at <a href=https://github.com/dorothy-yao/drfuse>https://github.com/dorothy-yao/drfuse</a>.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--95124-acm-mmsys-2024-bandwidth-estimation-in-real-time-communications-challenge-sami-khairy-et-al-2024>(1/1 | 95/124) ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge (Sami Khairy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sami Khairy, Gabriel Mittag, Scott Inglis, Vishak Gopal, Mehrsa Golestaneh, Ross Cutler, Francis Yan, Zhixiong Niu. (2024)<br><strong>ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge</strong><br><button class=copy-to-clipboard title="ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-MM, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06324v1.pdf filename=2403.06324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of experience (QoE) delivered by video conferencing systems to end users depends in part on correctly estimating the capacity of the bottleneck link between the sender and the receiver over time. Bandwidth estimation for real-time communications (RTC) remains a significant challenge, primarily due to the continuously evolving heterogeneous network architectures and technologies. From the first bandwidth estimation challenge which was hosted at ACM MMSys 2021, we learnt that bandwidth estimation models trained with <b>reinforcement</b> <b>learning</b> (RL) in <b>simulations</b> to maximize network-based reward functions may not be optimal in reality due to the sim-to-real gap and the difficulty of aligning network-based rewards with user-perceived QoE. This grand challenge aims to advance bandwidth estimation model design by aligning reward maximization with user-perceived QoE optimization using offline RL and a real-world dataset with objective rewards which have high correlations with subjective user-perceived audio/video quality in Microsoft Teams. All models submitted to the grand challenge underwent initial evaluation on our emulation platform. For a comprehensive evaluation under diverse network conditions with temporal fluctuations, top models were further evaluated on our geographically distributed testbed by using each model to conduct 600 calls within a 12-day period. The winning model is shown to deliver comparable performance to the top behavior policy in the released dataset. By leveraging real-world data and integrating objective audio/video quality scores as rewards, offline RL can therefore facilitate the development of competitive bandwidth estimators for RTC.</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=13--96124-fine-tuning-of-diffusion-models-via-stochastic-control-entropy-regularization-and-beyond-wenpin-tang-2024>(1/3 | 96/124) Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond (Wenpin Tang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenpin Tang. (2024)<br><strong>Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond</strong><br><button class=copy-to-clipboard title="Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 30<br>Keywords: Diffusion Model, Continuous Time, Continuous Time, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06279v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06279v2.pdf filename=2403.06279v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper aims to develop and provide a rigorous treatment to the problem of entropy regularized <b>fine-tuning</b> in the context of <b>continuous-time</b> <b>diffusion</b> <b>models,</b> which was recently proposed by Uehara et al. (arXiv:2402.15194, 2024). The idea is to use stochastic control for sample generation, where the entropy regularizer is introduced to mitigate reward collapse. We also show how the analysis can be extended to <b>fine-tuning</b> involving a general $f$-divergence regularizer.</p></p class="citation"></blockquote><h3 id=23--97124-whiteness-based-bilevel-learning-of-regularization-parameters-in-imaging-carlo-santambrogio-et-al-2024>(2/3 | 97/124) Whiteness-based bilevel learning of regularization parameters in imaging (Carlo Santambrogio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlo Santambrogio, Monica Pragliola, Alessandro Lanza, Marco Donatelli, Luca Calatroni. (2024)<br><strong>Whiteness-based bilevel learning of regularization parameters in imaging</strong><br><button class=copy-to-clipboard title="Whiteness-based bilevel learning of regularization parameters in imaging" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, eess-IV, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07026v1.pdf filename=2403.07026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider an <b>unsupervised</b> bilevel optimization strategy for learning regularization parameters in the context of imaging inverse problems in the presence of additive white Gaussian noise. Compared to <b>supervised</b> and semi-supervised metrics relying either on the prior knowledge of reference data and/or on some (partial) knowledge on the noise statistics, the proposed approach optimizes the whiteness of the residual between the observed data and the observation model with no need of ground-truth data.We validate the approach on standard Total Variation-regularized image deconvolution problems which show that the proposed quality metric provides estimates close to the mean-square error oracle and to discrepancy-based principles.</p></p class="citation"></blockquote><h3 id=33--98124-absence-of-spurious-solutions-far-from-ground-truth-a-low-rank-analysis-with-high-order-losses-ziye-ma-et-al-2024>(3/3 | 98/124) Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses (Ziye Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziye Ma, Ying Chen, Javad Lavaei, Somayeh Sojoudi. (2024)<br><strong>Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses</strong><br><button class=copy-to-clipboard title="Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, eess-SP, math-OC, math.OC<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06056v1.pdf filename=2403.06056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Matrix sensing problems exhibit pervasive non-convexity, plaguing optimization with a proliferation of suboptimal spurious solutions. Avoiding convergence to these critical points poses a major challenge. This work provides new theoretical insights that help demystify the intricacies of the non-convex landscape. In this work, we prove that under certain conditions, critical points sufficiently distant from the ground truth matrix exhibit favorable <b>geometry</b> by being strict saddle points rather than troublesome local minima. Moreover, we introduce the notion of higher-order losses for the matrix sensing problem and show that the incorporation of such losses into the objective function amplifies the negative curvature around those distant critical points. This implies that increasing the complexity of the objective function via high-order losses accelerates the escape from such critical points and acts as a desirable alternative to increasing the complexity of the optimization problem via over-parametrization. By elucidating key characteristics of the non-convex optimization landscape, this work makes progress towards a comprehensive framework for tackling broader machine learning objectives plagued by non-convexity.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=14--99124-quantized-constant-envelope-waveform-design-for-massive-mimo-dfrc-systems-zheyu-wu-et-al-2024>(1/4 | 99/124) Quantized Constant-Envelope Waveform Design for Massive MIMO DFRC Systems (Zheyu Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheyu Wu, Ya-Feng Liu, Wei-Kun Chen, Christos Masouros. (2024)<br><strong>Quantized Constant-Envelope Waveform Design for Massive MIMO DFRC Systems</strong><br><button class=copy-to-clipboard title="Quantized Constant-Envelope Waveform Design for Massive MIMO DFRC Systems" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT, math-OC<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06185v1.pdf filename=2403.06185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both dual-functional radar-communication (DFRC) and massive multiple-input multiple-output (MIMO) have been recognized as enabling technologies for 6G wireless networks. This paper considers the advanced waveform design for hardware-efficient massive MIMO DFRC systems. Specifically, the transmit waveform is imposed with the <b>quantized</b> constant-envelope (QCE) constraint, which facilitates the employment of low-resolution digital-to-analog converters (DACs) and power-efficient amplifiers. The waveform design problem is formulated as the minimization of the mean square error (MSE) between the designed and desired beampatterns subject to the constructive interference (CI)-based communication quality of service (QoS) constraints and the QCE constraint. To solve the formulated problem, we first utilize the penalty technique to transform the discrete problem into an equivalent continuous penalty model. Then, we propose an inexact augmented Lagrangian method (ALM) algorithm for solving the penalty model. In particular, the ALM subproblem at each iteration is solved by a custom-built block successive upper-bound minimization (BSUM) algorithm, which admits closed-form updates, making the proposed inexact ALM algorithm computationally efficient. <b>Simulation</b> results demonstrate the superiority of the proposed approach over existing state-of-the-art ones. In addition, extensive <b>simulations</b> are conducted to examine the impact of various system parameters on the trade-off between communication and radar performances.</p></p class="citation"></blockquote><h3 id=24--100124-stochastic-geometry-analysis-for-distributed-riss-assisted-mmwave-communications-yuan-xu-et-al-2024>(2/4 | 100/124) Stochastic Geometry Analysis for Distributed RISs-Assisted mmWave Communications (Yuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xu, Wei Li, Chongwen Huang, Yongxu Zhu, Zhaohui Yang, Jun Yang, Jiguang He, Zhaoyang Zhang, Mérouane Debbah. (2024)<br><strong>Stochastic Geometry Analysis for Distributed RISs-Assisted mmWave Communications</strong><br><button class=copy-to-clipboard title="Stochastic Geometry Analysis for Distributed RISs-Assisted mmWave Communications" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06073v1.pdf filename=2403.06073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Millimeter wave (mmWave) has attracted considerable attention due to its wide bandwidth and high frequency. However, it is highly susceptible to blockages, resulting in significant degradation of the coverage and the sum rate. A promising approach is deploying distributed reconfigurable intelligent surfaces (RISs), which can establish extra communication links. In this paper, we investigate the impact of distributed RISs on the coverage probability and the sum rate in mmWave wireless communication systems. Specifically, we first introduce the system model, which includes the blockage, the RIS and the user distribution models, leveraging the Poisson point process. Then, we define the association criterion and derive the conditional coverage probabilities for the two cases of direct association and reflective association through RISs. Finally, we combine the two cases using Campbell&rsquo;s theorem and the total probability theorem to obtain the closed-form expressions for the ergodic coverage probability and the sum rate. <b>Simulation</b> results validate the effectiveness of the proposed analytical approach, demonstrating that the deployment of distributed RISs significantly improves the ergodic coverage probability by 45.4% and the sum rate by over 1.5 times.</p></p class="citation"></blockquote><h3 id=34--101124-hashing-beam-training-for-near-field-communications-yuan-xu-et-al-2024>(3/4 | 101/124) Hashing Beam Training for Near-Field Communications (Yuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xu, Wei Li, Chongwen Huang, Chen Zhu, Zhaohui Yang, Jun Yang, Jiguang He, Zhaoyang Zhang, Mérouane Debbah. (2024)<br><strong>Hashing Beam Training for Near-Field Communications</strong><br><button class=copy-to-clipboard title="Hashing Beam Training for Near-Field Communications" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06074v1.pdf filename=2403.06074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the millimeter-wave (mmWave) near-field beam training problem to find the correct beam direction. In order to address the high complexity and low identification accuracy of existing beam training techniques, we propose an efficient hashing multi-arm beam (HMB) training scheme for the near-field scenario. Specifically, we first design a set of sparse bases based on the polar domain sparsity of the near-field channel. Then, the random hash functions are chosen to construct the near-field multi-arm beam training codebook. Each multi-arm beam codeword is scanned in a time slot until all the predefined codewords are traversed. Finally, the soft decision and voting methods are applied to distinguish the signal from different base stations and obtain correctly aligned beams. <b>Simulation</b> results show that our proposed near-field HMB training method can reduce the beam training overhead to the logarithmic level, and achieve 96.4% identification accuracy of exhaustive beam training. Moreover, we also verify applicability under the far-field scenario.</p></p class="citation"></blockquote><h3 id=44--102124-channel-estimation-considerate-precoder-design-for-multi-user-massive-mimo-ofdm-systems-the-concept-and-fast-algorithms-liu-junkai-et-al-2024>(4/4 | 102/124) Channel Estimation Considerate Precoder Design for Multi-user Massive MIMO-OFDM Systems: The Concept and Fast Algorithms (Liu Junkai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liu Junkai, Jiang Yi. (2024)<br><strong>Channel Estimation Considerate Precoder Design for Multi-user Massive MIMO-OFDM Systems: The Concept and Fast Algorithms</strong><br><button class=copy-to-clipboard title="Channel Estimation Considerate Precoder Design for Multi-user Massive MIMO-OFDM Systems: The Concept and Fast Algorithms" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06072v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06072v1.pdf filename=2403.06072v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The sixth-generation (6G) communication networks target peak data rates exceeding 1Tbps, necessitating base stations (BS) to support up to 100 simultaneous data streams. However, sparse pilot allocation to accommodate such streams poses challenges for users&rsquo; channel estimation. This paper presents Channel Estimation Considerate Precoding (CECP), where BS precoders prioritize facilitating channel estimation alongside maximizing transmission rate. To address the computational complexity of 6G large-scale multi-input multi-output (MIMO) systems, we propose a computationally-efficient space-time block diagonal channel shortening (ST-BDCS) precoding scheme. By leveraging the sparse Toeplitz property of orthogonal frequency division multiplexing (OFDM) channels, this time-domain precoding design effectively mitigates multi-user interference in the downlink and shortens the effective channel&rsquo;s temporal length. Consequently, users can estimate the channels using sparse pilots. To enable fast implementation, we develop a generalized complex-valued Toeplitz matrix QR decomposition algorithm applicable to various space-time signal processing problems. <b>Simulation</b> results demonstrate that the ST-BDCS precoding method approximates the rate performance of conventional subcarrier-by-subcarrier precoding schemes. However, it offers the advantages of easier channel estimation for users and significantly reduced computational complexity for the BS.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--103124-control-strategies-for-recommendation-systems-in-social-networks-ben-sprenger-et-al-2024>(1/3 | 103/124) Control Strategies for Recommendation Systems in Social Networks (Ben Sprenger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Sprenger, Giulia De Pasquale, Raffaele Soloperto, John Lygeros, Florian Dörfler. (2024)<br><strong>Control Strategies for Recommendation Systems in Social Networks</strong><br><button class=copy-to-clipboard title="Control Strategies for Recommendation Systems in Social Networks" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SI, cs-SY, eess-SY, eess.SY, physics-soc-ph<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06152v1.pdf filename=2403.06152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A closed-loop control model to analyze the impact of <b>recommendation</b> systems on opinion dynamics within social networks is introduced. The core contribution is the development and formalization of model-free and model-based approaches to <b>recommendation</b> system design, integrating the dynamics of social interactions within networks via an extension of the Friedkin-Johnsen (FJ) model. Comparative analysis and numerical <b>simulations</b> demonstrate the effectiveness of the proposed control strategies in maximizing user engagement and their potential for influencing opinion formation processes.</p></p class="citation"></blockquote><h3 id=23--104124-direct-shooting-method-for-numerical-optimal-control-a-modified-transcription-approach-jiawei-tang-et-al-2024>(2/3 | 104/124) Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach (Jiawei Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Tang, Yuxing Zhong, Pengyu Wang, Xingzhou Chen, Shuang Wu, Ling Shi. (2024)<br><strong>Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach</strong><br><button class=copy-to-clipboard title="Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 13<br>Keywords: Benchmarking, Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06167v1.pdf filename=2403.06167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct shooting is an efficient method to solve numerical optimal control. It utilizes the Runge-Kutta scheme to discretize a <b>continuous-time</b> <b>optimal</b> control problem making the problem solvable by nonlinear programming solvers. However, conventional direct shooting raises a contradictory dynamics issue when using an augmented state to handle {high-order} systems. This paper fills the research gap by considering the direct shooting method for {high-order} systems. We derive the modified Euler and Runge-Kutta-4 methods to transcribe the system dynamics constraint directly. Additionally, we provide the global error upper bounds of our proposed methods. A set of <b>benchmark</b> optimal control problems shows that our methods provide more accurate solutions than existing approaches.</p></p class="citation"></blockquote><h3 id=33--105124-multi-gated-perimeter-flow-control-for-monocentric-cities-efficiency-and-equity-ruzanna-mat-jusoh-et-al-2024>(3/3 | 105/124) Multi-gated perimeter flow control for monocentric cities: Efficiency and equity (Ruzanna Mat Jusoh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruzanna Mat Jusoh, Konstantinos Ampountolas. (2024)<br><strong>Multi-gated perimeter flow control for monocentric cities: Efficiency and equity</strong><br><button class=copy-to-clipboard title="Multi-gated perimeter flow control for monocentric cities: Efficiency and equity" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06312v1.pdf filename=2403.06312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A control scheme for the multi-gated perimeter traffic flow control problem of cities is presented. The proposed scheme determines feasible and optimally distributed input flows for the various gates located at the periphery of a protected network. A parsimonious model is employed to describe the traffic dynamics of the protected network. To describe traffic dynamics outside of the protected area, the state-space model is augmented with additional state variables to account for vehicle queues at store-and-forward origin links at the periphery. The perimeter flow control problem is formulated as a convex optimisation problem with finite horizon, and constrained control and state variables. It aims to equalise the relative queues at origin links and to maintain the vehicle accumulation in the protected network around a desired set point, while the system&rsquo;s throughput is maximised. For real-time control, the optimal control problem is embedded in a rolling-horizon scheme using the current state of the system as the initial state as well as predicted demand flows at entrance links. Furthermore, practical flow allocation policies for single-region perimeter control without explicitly considering entrance link dynamics are presented. These policies allocate a global perimeter-ordered flow to candidate gates at the periphery of a protected network by taking into account the different geometric characteristics of origin links. The proposed flow allocation policies are then <b>benchmarked</b> against the multi-gated perimeter flow control. A study is carried out for a 2.5 square mile protected network area of San Francisco, CA, including fifteen gates of different geometric characteristics. The results have showed that the proposed scheme is able to manage excessive queues outside of the protected network and to optimally distribute the input flows, which confirms its efficiency and equity properties.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--106124-disentangling-shared-and-private-latent-factors-in-multimodal-variational-autoencoders-kaspar-märtens-et-al-2024>(1/3 | 106/124) Disentangling shared and private latent factors in multimodal Variational Autoencoders (Kaspar Märtens et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaspar Märtens, Christopher Yau. (2024)<br><strong>Disentangling shared and private latent factors in multimodal Variational Autoencoders</strong><br><button class=copy-to-clipboard title="Disentangling shared and private latent factors in multimodal Variational Autoencoders" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, q-bio-GN, stat-ML, stat.ML<br>Keyword Score: 26<br>Keywords: Autoencoder, Multi-modal, Multi-modal, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06338v1.pdf filename=2403.06338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models for <b>multimodal</b> data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity. Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality. <b>Multimodal</b> <b>Variational</b> <b>Autoencoders,</b> such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private. In this work, we investigate their capability to reliably perform this disentanglement. In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal. Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation. Our findings are supported by experiments on synthetic as well as various real-world multi-omics data sets.</p></p class="citation"></blockquote><h3 id=23--107124-nonparametric-automatic-differentiation-variational-inference-with-spline-approximation-yuda-shao-et-al-2024>(2/3 | 107/124) Nonparametric Automatic Differentiation Variational Inference with Spline Approximation (Yuda Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuda Shao, Shan Yu, Tianshu Feng. (2024)<br><strong>Nonparametric Automatic Differentiation Variational Inference with Spline Approximation</strong><br><button class=copy-to-clipboard title="Nonparametric Automatic Differentiation Variational Inference with Spline Approximation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06302v1.pdf filename=2403.06302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic Differentiation Variational Inference (ADVI) is efficient in learning <b>probabilistic</b> <b>models.</b> Classic ADVI relies on the parametric approach to approximate the posterior. In this paper, we develop a spline-based nonparametric approximation approach that enables flexible posterior approximation for distributions with complicated structures, such as skewness, multimodality, and bounded support. Compared with widely-used nonparametric variational inference methods, the proposed method is easy to implement and adaptive to various data structures. By adopting the spline approximation, we derive a lower bound of the importance weighted <b>autoencoder</b> and establish the asymptotic consistency. Experiments demonstrate the efficiency of the proposed method in approximating complex posterior distributions and improving the performance of generative models with incomplete data.</p></p class="citation"></blockquote><h3 id=33--108124-the-alell_0core-tensor-decomposition-for-sparse-count-data-john-hood-et-al-2024>(3/3 | 108/124) The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data (John Hood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Hood, Aaron Schein. (2024)<br><strong>The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data</strong><br><button class=copy-to-clipboard title="The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06153v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06153v2.pdf filename=2403.06153v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces AL$\ell_0$CORE, a new form of probabilistic non-negative <b>tensor</b> <b>decomposition.</b> AL$\ell_0$CORE is a Tucker decomposition where the number of non-zero elements (i.e., the $\ell_0$-norm) of the core <b>tensor</b> <b>is</b> constrained to a preset value $Q$ much smaller than the size of the core. While the user dictates the total budget $Q$, the locations and values of the non-zero elements are latent variables and allocated across the core <b>tensor</b> <b>during</b> inference. AL$\ell_0$CORE &ndash; i.e., $allo$cated $\ell_0$-$co$nstrained $core$&ndash; thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that AL$\ell_0$CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--109124-vertex-block-descent-anka-he-chen-et-al-2024>(1/1 | 109/124) Vertex Block Descent (Anka He Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anka He Chen, Ziheng Liu, Yin Yang, Cem Yuksel. (2024)<br><strong>Vertex Block Descent</strong><br><button class=copy-to-clipboard title="Vertex Block Descent" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06321v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06321v2.pdf filename=2403.06321v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce vertex block descent, a block coordinate descent solution for the variational form of implicit Euler through vertex-level Gauss-Seidel iterations. It operates with local vertex position updates that achieve reductions in global variational energy with maximized parallelism. This forms a physics solver that can achieve numerical convergence with unconditional stability and exceptional computation performance. It can also fit in a given computation budget by simply limiting the iteration count while maintaining its stability and superior convergence rate. We present and evaluate our method in the context of elastic body dynamics, providing details of all essential components. Then, we discuss how it can be used for other <b>simulation</b> problems, including particle-based <b>simulations</b> and rigid bodies.</p></p class="citation"></blockquote><h2 id=csro-4>cs.RO (4)</h2><h3 id=14--110124-a-study-on-domain-generalization-for-failure-detection-through-human-reactions-in-hri-maria-teresa-parreira-et-al-2024>(1/4 | 110/124) A Study on Domain Generalization for Failure Detection through Human Reactions in HRI (Maria Teresa Parreira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Teresa Parreira, Sukruth Gowdru Lingaraju, Adolfo Ramirez-Aristizabal, Manaswi Saha, Michael Kuniavsky, Wendy Ju. (2024)<br><strong>A Study on Domain Generalization for Failure Detection through Human Reactions in HRI</strong><br><button class=copy-to-clipboard title="A Study on Domain Generalization for Failure Detection through Human Reactions in HRI" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06315v1.pdf filename=2403.06315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models are commonly tested in-distribution (same dataset); performance almost always drops in <b>out-of-distribution</b> settings. For HRI research, the goal is often to develop generalized models. This makes domain generalization - retaining performance in different settings - a critical issue. In this study, we present a concise analysis of domain generalization in failure detection models trained on human facial expressions. Using two distinct datasets of humans reacting to videos where error occurs, one from a controlled lab setting and another collected online, we trained deep learning models on each dataset. When testing these models on the alternate dataset, we observed a significant performance drop. We reflect on the causes for the observed model behavior and leave <b>recommendations.</b> This work emphasizes the need for HRI research focusing on improving model robustness and real-life applicability.</p></p class="citation"></blockquote><h3 id=24--111124-robust-predictive-motion-planning-by-learning-obstacle-uncertainty-jian-zhou-et-al-2024>(2/4 | 111/124) Robust Predictive Motion Planning by Learning Obstacle Uncertainty (Jian Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Zhou, Yulong Gao, Ola Johansson, Björn Olofsson, Erik Frisk. (2024)<br><strong>Robust Predictive Motion Planning by Learning Obstacle Uncertainty</strong><br><button class=copy-to-clipboard title="Robust Predictive Motion Planning by Learning Obstacle Uncertainty" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06222v1.pdf filename=2403.06222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe motion planning for robotic systems in dynamic environments is nontrivial in the presence of uncertain obstacles, where estimation of obstacle uncertainties is crucial in predicting future motions of dynamic obstacles. The worst-case characterization gives a conservative uncertainty prediction and may result in infeasible motion planning for the ego robotic system. In this paper, an efficient, robust, and safe motion-planing algorithm is developed by learning the obstacle uncertainties online. More specifically, the unknown yet intended control set of obstacles is efficiently computed by solving a linear programming problem. The learned control set is used to compute forward reachable sets of obstacles that are less conservative than the worst-case prediction. Based on the forward prediction, a robust model predictive controller is designed to compute a safe reference trajectory for the ego robotic system that remains outside the reachable sets of obstacles over the prediction horizon. The method is applied to a car-like mobile robot in both <b>simulations</b> and hardware experiments to demonstrate its effectiveness.</p></p class="citation"></blockquote><h3 id=34--112124-speeding-up-6-dof-grasp-sampling-with-quality-diversity-johann-huber-et-al-2024>(3/4 | 112/124) Speeding up 6-DoF Grasp Sampling with Quality-Diversity (Johann Huber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johann Huber, François Hélénon, Mathilde Kappel, Elie Chelly, Mahdi Khoramshahi, Faïz Ben Amar, Stéphane Doncieux. (2024)<br><strong>Speeding up 6-DoF Grasp Sampling with Quality-Diversity</strong><br><button class=copy-to-clipboard title="Speeding up 6-DoF Grasp Sampling with Quality-Diversity" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06173v1.pdf filename=2403.06173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in AI have led to significant results in robotic learning, including natural language-conditioned planning and efficient optimization of controllers using generative models. However, the interaction data remains the bottleneck for generalization. Getting data for grasping is a critical challenge, as this skill is required to complete many manipulation tasks. Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse, high-performing solutions to a given problem. This paper investigates how QD can be combined with priors to speed up the generation of diverse grasps poses in <b>simulation</b> compared to standard 6-DoF grasp sampling schemes. Experiments conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD outperforms commonly used methods by a large margin. Further experiments show that QD optimization automatically finds some efficient priors that are usually hard coded. The deployment of generated grasps on a 2-finger gripper and an Allegro hand shows that the diversity produced maintains sim-to-real transferability. We believe these results to be a significant step toward the generation of large datasets that can lead to robust and generalizing robotic grasping policies.</p></p class="citation"></blockquote><h3 id=44--113124-mind-meets-robots-a-review-of-eeg-based-brain-robot-interaction-systems-yuchong-zhang-et-al-2024>(4/4 | 113/124) Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems (Yuchong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchong Zhang, Nona Rajabi, Farzaneh Taleb, Andrii Matviienko, Yong Ma, Mårten Björkman, Danica Kragic. (2024)<br><strong>Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems</strong><br><button class=copy-to-clipboard title="Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06186v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06186v2.pdf filename=2403.06186v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain-robot interaction (BRI) empowers individuals to control (semi-)automated machines through their brain activity, either passively or actively. In the past decade, BRI systems have achieved remarkable success, predominantly harnessing electroencephalogram (EEG) signals as the central component. This paper offers an up-to-date and exhaustive examination of 87 curated studies published during the last five years (2018-2023), focusing on identifying the research landscape of EEG-based BRI systems. This review aims to consolidate and underscore methodologies, interaction modes, application contexts, system evaluation, existing challenges, and potential avenues for future investigations in this domain. Based on our analysis, we present a BRI system model with three entities: Brain, Robot, and Interaction, depicting the internal relationships of a BRI system. We especially investigate the essence and principles on interaction modes between human brains and robots, a domain that has not yet been identified anywhere. We then discuss these entities with different dimensions encompassed. Within this model, we scrutinize and classify current research, reveal insights, specify challenges, and provide <b>recommendations</b> for future research trajectories in this field. Meanwhile, we envision our findings offer a design space for future human-robot interaction (HRI) research, informing the creation of efficient BRI frameworks.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--114124-enhancing-quantum-variational-algorithms-with-zero-noise-extrapolation-via-neural-networks-subhasree-bhattacharjee-et-al-2024>(1/1 | 114/124) Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks (Subhasree Bhattacharjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subhasree Bhattacharjee, Soumyadip Sarkar, Kunal Das, Bikramjit Sarkar. (2024)<br><strong>Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks</strong><br><button class=copy-to-clipboard title="Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.07025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.07025v1.pdf filename=2403.07025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the emergent realm of quantum computing, the Variational Quantum Eigensolver (VQE) stands out as a promising algorithm for solving complex quantum problems, especially in the noisy intermediate-scale quantum (NISQ) era. However, the ubiquitous presence of noise in quantum devices often limits the accuracy and reliability of VQE outcomes. This research introduces a novel approach to ameliorate this challenge by utilizing neural networks for zero noise extrapolation (ZNE) in VQE computations. By employing the Qiskit framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and examined their behavior under varying levels of depolarizing noise. Our investigations spanned from determining the expectation values of a Hamiltonian, defined as a tensor product of Z operators, under different noise intensities to extracting the ground state energy. To bridge the observed outcomes under noise with the ideal noise-free scenario, we trained a Feed Forward Neural Network on the error probabilities and their associated expectation values. Remarkably, our model proficiently predicted the VQE outcome under hypothetical noise-free conditions. By juxtaposing the <b>simulation</b> results with real quantum device executions, we unveiled the discrepancies induced by noise and showcased the efficacy of our neural network-based ZNE technique in rectifying them. This integrative approach not only paves the way for enhanced accuracy in VQE computations on NISQ devices but also underlines the immense potential of hybrid quantum-classical paradigms in circumventing the challenges posed by quantum noise. Through this research, we envision a future where quantum algorithms can be reliably executed on noisy devices, bringing us one step closer to realizing the full potential of quantum computing.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--115124-ideas-information-driven-ev-admission-in-charging-station-considering-user-impatience-to-improve-qos-and-station-utilization-animesh-chattopadhyay-et-al-2024>(1/2 | 115/124) IDEAS: Information-Driven EV Admission in Charging Station Considering User Impatience to Improve QoS and Station Utilization (Animesh Chattopadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Animesh Chattopadhyay, Subrat Kar. (2024)<br><strong>IDEAS: Information-Driven EV Admission in Charging Station Considering User Impatience to Improve QoS and Station Utilization</strong><br><button class=copy-to-clipboard title="IDEAS: Information-Driven EV Admission in Charging Station Considering User Impatience to Improve QoS and Station Utilization" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06223v1.pdf filename=2403.06223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our work delves into user behaviour at Electric Vehicle(EV) charging stations during peak times, particularly focusing on how impatience drives balking (not joining queues) and reneging (leaving queues prematurely). We introduce an Agent-based <b>simulation</b> framework that incorporates user optimism levels (pessimistic, standard, and optimistic) in the queue dynamics. Unlike previous work, this framework highlights the crucial role of human behaviour in shaping station efficiency for peak demand. The <b>simulation</b> reveals a key issue: balking often occurs due to a lack of queue insights, creating user dilemmas. To address this, we propose real-time sharing of wait time metrics with arriving EV users at the station. This ensures better Quality of Service (QoS) with user-informed queue joining and demonstrates significant reductions in reneging (up to 94%) improving the charging operation. Further analysis shows that charging speed decreases significantly beyond 80%, but most users prioritize full charges due to range anxiety, leading to a longer queue. To address this, we propose a two-mode, two-port charger design with power-sharing options. This allows users to fast-charge to 80% and automatically switch to slow charging, enabling fast charging on the second port. Thus, increasing fast charger availability and throughput by up to 5%. As the mobility sector transitions towards intelligent traffic, our modelling framework, which integrates human decision-making within automated planning, provides valuable insights for optimizing charging station efficiency and improving the user experience. This approach is particularly relevant during the introduction phase of new stations, when historical data might be limited.</p></p class="citation"></blockquote><h3 id=22--116124-dynamics-of-polarization-under-normative-institutions-and-opinion-expression-stewarding-atrisha-sarkar-et-al-2024>(2/2 | 116/124) Dynamics of Polarization Under Normative Institutions and Opinion Expression Stewarding (Atrisha Sarkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Atrisha Sarkar, Gillian K. Hadfield. (2024)<br><strong>Dynamics of Polarization Under Normative Institutions and Opinion Expression Stewarding</strong><br><button class=copy-to-clipboard title="Dynamics of Polarization Under Normative Institutions and Opinion Expression Stewarding" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: J-4, cs-CY, cs-MA, cs.MA, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06264v1.pdf filename=2403.06264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although there is mounting empirical evidence for the increase in affective polarization, few mechanistic models can explain its emergence at the population level. The question of how such a phenomenon can emerge from divergent opinions of a population on an ideological issue is still an open issue. In this paper, we establish that human normativity, that is, individual expression of normative opinions based on beliefs about the population, can lead to population-level polarization when ideological institutions distort beliefs in accordance with their objective of moving expressed opinion to one extreme. Using a game-theoretic model, we establish that individuals with more extreme opinions will have more extreme rhetoric and higher misperceptions about their outgroup members. Our model also shows that when social <b>recommendation</b> systems mediate institutional signals, we can observe the formation of different institutional communities, each with its unique community structure and characteristics. Using the model, we identify practical strategies platforms can implement, such as reducing exposure to signals from ideological institutions and a tailored approach to content moderation, both of which can rectify the affective polarization problem within its purview.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--117124-an-approach-using-the-null-space-to-implement-dirichlet-and-constraint-boundary-conditions-into-fem-stefan-schoder-2024>(1/1 | 117/124) An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM (Stefan Schoder, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Schoder. (2024)<br><strong>An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM</strong><br><button class=copy-to-clipboard title="An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06160v1.pdf filename=2403.06160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A handy technique for the Finite Element Method (FEM) is presented that uses the null space for the implementation of Dirichlet and constraint boundary conditions. The focus of this method is to present an illustrative approach to modeling boundary constraints within FEM <b>simulations</b> for teaching. It presents a consistent way of including the boundary terms in the forcing and constructing the field solution after solving the algebraic system of equations.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--118124-fish-inspired-tracking-of-underwater-turbulent-plumes-peter-gunnarson-et-al-2024>(1/1 | 118/124) Fish-inspired tracking of underwater turbulent plumes (Peter Gunnarson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Gunnarson, John O. Dabiri. (2024)<br><strong>Fish-inspired tracking of underwater turbulent plumes</strong><br><button class=copy-to-clipboard title="Fish-inspired tracking of underwater turbulent plumes" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-RO, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, PaLM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06091v1.pdf filename=2403.06091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous ocean-exploring vehicles have begun to take advantage of onboard sensor measurements of water properties such as salinity and temperature to locate oceanic features in real time. Such targeted sampling strategies enable more rapid study of ocean environments by actively steering towards areas of high scientific value. Inspired by the ability of aquatic animals to navigate via flow sensing, this work investigates hydrodynamic cues for accomplishing targeted sampling using a <b>palm-sized</b> robotic swimmer. As proof-of-concept analogy for tracking hydrothermal vent plumes in the ocean, the robot is tasked with locating the center of turbulent jet flows in a 13,000-liter water tank using data from onboard pressure sensors. To learn a navigation strategy, we first implemented <b>Reinforcement</b> <b>Learning</b> (RL) on a simulated version of the robot navigating in proximity to turbulent jets. After training, the RL algorithm discovered an effective strategy for locating the jets by following transverse velocity gradients sensed by pressure sensors located on opposite sides of the robot. When implemented on the physical robot, this gradient following strategy enabled the robot to successfully locate the turbulent plumes at more than twice the rate of random searching. Additionally, we found that navigation performance improved as the distance between the pressure sensors increased, which can inform the design of distributed flow sensors in ocean robots. Our results demonstrate the effectiveness and limits of flow-based navigation for autonomously locating hydrodynamic features of interest.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--119124-solution-hashing-search-based-on-layout-graph-transformation-for-unequal-circle-packing-jianrong-zhou-et-al-2024>(1/1 | 119/124) Solution-Hashing Search Based on Layout-Graph Transformation for Unequal Circle Packing (Jianrong Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianrong Zhou, Jiyao He, Kun He. (2024)<br><strong>Solution-Hashing Search Based on Layout-Graph Transformation for Unequal Circle Packing</strong><br><button class=copy-to-clipboard title="Solution-Hashing Search Based on Layout-Graph Transformation for Unequal Circle Packing" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 11<br>Keywords: Graph, Benchmarking, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06211v1.pdf filename=2403.06211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of packing unequal circles into a circular container stands as a classic and challenging optimization problem in computational <b>geometry.</b> This study introduces a suite of innovative and efficient methods to tackle this problem. Firstly, we present a novel layout-graph transformation method that represents configurations as <b>graphs,</b> together with an inexact hash method facilitating fast comparison of configurations for isomorphism or similarity. Leveraging these advancements, we propose an Iterative Solution-Hashing Search algorithm adept at circumventing redundant exploration through efficient configuration recording. Additionally, we introduce several enhancements to refine the optimization and search processes, including an adaptive adjacency maintenance method, an efficient vacancy detection technique, and a Voronoi-based locating method. Through comprehensive computational experiments across various <b>benchmark</b> instances, our algorithm demonstrates superior performance over existing state-of-the-art methods, showcasing remarkable applicability and versatility. Notably, our algorithm surpasses the best-known results for 56 out of 179 <b>benchmark</b> instances while achieving parity with the remaining instances.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--120124-spectral-lower-bounds-for-local-search-simina-brânzei-et-al-2024>(1/1 | 120/124) Spectral Lower Bounds for Local Search (Simina Brânzei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simina Brânzei, Nicholas J. Recker. (2024)<br><strong>Spectral Lower Bounds for Local Search</strong><br><button class=copy-to-clipboard title="Spectral Lower Bounds for Local Search" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs.CC<br>Keyword Score: 8<br>Keywords: Graph, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06248v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06248v2.pdf filename=2403.06248v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Local search is a powerful heuristic in optimization and computer science, the complexity of which has been studied in the white box and <b>black</b> <b>box</b> models. In the <b>black</b> <b>box</b> model, we are given a <b>graph</b> $G = (V,E)$ and oracle access to a function $f : V \to \mathbb{R}$. The local search problem is to find a vertex $v$ that is a local minimum, i.e. with $f(v) \leq f(u)$ for all $(u,v) \in E$, using as few queries to the oracle as possible. We show that if a <b>graph</b> $G$ admits a lazy, irreducible, and reversible Markov chain with stationary distribution $\pi$, then the randomized query complexity of local search on $G$ is $\Omega\left( \frac{\sqrt{n}}{t_{mix} \cdot \exp(3\sigma)}\right)$, where $t_{mix}$ is the mixing time of the chain and $\sigma = \max_{u,v \in V(G)} \frac{\pi(v)}{\pi(u)}.$ This theorem formally establishes a connection between the query complexity of local search and the mixing time of the fastest mixing Markov chain for the given <b>graph.</b> We also get several corollaries that lower bound the complexity as a function of the spectral gap, one of which slightly improves a result from prior work.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--121124-online-multi-spectral-neuron-tracing-bin-duan-et-al-2024>(1/1 | 121/124) Online Multi-spectral Neuron Tracing (Bin Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bin Duan, Yuzhang Shang, Dawen Cai, Yan Yan. (2024)<br><strong>Online Multi-spectral Neuron Tracing</strong><br><button class=copy-to-clipboard title="Online Multi-spectral Neuron Tracing" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cs-CV, cs-LG, q-bio-NC, q-bio.NC<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06251v1.pdf filename=2403.06251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an online multi-spectral neuron tracing method with uniquely designed modules, where no offline training are required. Our method is trained online to update our enhanced discriminative correlation filter to conglutinate the tracing process. This distinctive offline-training-free schema differentiates us from other training-dependent tracing approaches like deep learning methods since no annotation is needed for our method. Besides, compared to other tracing methods requiring complicated set-up such as for <b>clustering</b> and <b>graph</b> multi-cut, our approach is much easier to be applied to new images. In fact, it only needs a starting bounding box of the tracing neuron, significantly reducing users&rsquo; configuration effort. Our extensive experiments show that our training-free and easy-configured methodology allows fast and accurate neuron reconstructions in multi-spectral images.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--122124-improved-fpt-approximation-scheme-and-approximate-kernel-for-biclique-free-max-k-weight-sat-greedy-strikes-back-pasin-manurangsi-2024>(1/2 | 122/124) Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back (Pasin Manurangsi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pasin Manurangsi. (2024)<br><strong>Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back</strong><br><button class=copy-to-clipboard title="Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06335v1.pdf filename=2403.06335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Max $k$-Weight SAT (aka Max SAT with Cardinality Constraint) problem, we are given a CNF formula with $n$ variables and $m$ clauses together with a positive integer $k$. The goal is to find an assignment where at most $k$ variables are set to one that satisfies as many constraints as possible. Recently, Jain et al. [SODA'23] gave an FPT approximation scheme (FPT-AS) with running time $2^{O\left(\left(dk/\epsilon\right)^d\right)} \cdot (n + m)^{O(1)}$ for Max $k$-Weight SAT when the incidence <b>graph</b> is $K_{d,d}$-free. They asked whether a polynomial-size approximate kernel exists. In this work, we answer this question positively by giving an $(1 - \epsilon)$-approximate kernel with $\left(\frac{d k}{\epsilon}\right)^{O(d)}$ variables. This also implies an improved FPT-AS with running time $(dk/\epsilon)^{O(dk)} \cdot (n + m)^{O(1)}$. Our approximate kernel is based mainly on a couple of greedy strategies together with a sunflower lemma-style reduction rule.</p></p class="citation"></blockquote><h3 id=22--123124-revisiting-path-contraction-and-cycle-contraction-r-krithika-et-al-2024>(2/2 | 123/124) Revisiting Path Contraction and Cycle Contraction (R. Krithika et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>R. Krithika, V. K. Kutty Malu, Prafullkumar Tale. (2024)<br><strong>Revisiting Path Contraction and Cycle Contraction</strong><br><button class=copy-to-clipboard title="Revisiting Path Contraction and Cycle Contraction" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: F-2-2, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06290v1.pdf filename=2403.06290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Path Contraction and Cycle Contraction problems take as input an undirected <b>graph</b> $G$ with $n$ vertices, $m$ edges and an integer $k$ and determine whether one can obtain a path or a cycle, respectively, by performing at most $k$ edge contractions in $G$. We revisit these NP-complete problems and prove the following results. Path Contraction admits an algorithm running in $\mathcal{O}^<em>(2^{k})$ time. This improves over the current algorithm known for the problem [Algorithmica 2014]. Cycle Contraction admits an algorithm running in $\mathcal{O}^</em>((2 + \epsilon_{\ell})^k)$ time where $0 &lt; \epsilon_{\ell} \leq 0.5509$ is inversely proportional to $\ell = n - k$. Central to these results is an algorithm for a general variant of Path Contraction, namely, Path Contraction With Constrained Ends. We also give an $\mathcal{O}^*(2.5191^n)$-time algorithm to solve the optimization version of Cycle Contraction. Next, we turn our attention to restricted <b>graph</b> classes and show the following results. Path Contraction on planar <b>graphs</b> admits a polynomial-time algorithm. Path Contraction on chordal <b>graphs</b> does not admit an algorithm running in time $\mathcal{O}(n^{2-\epsilon} \cdot 2^{o(tw)})$ for any $\epsilon > 0$, unless the Orthogonal Vectors Conjecture fails. Here, $tw$ is the treewidth of the input <b>graph.</b> The second result complements the $\mathcal{O}(nm)$-time, i.e., $\mathcal{O}(n^2 \cdot tw)$-time, algorithm known for the problem [Discret. Appl. Math. 2014].</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--124124-a-two-level-thermal-cycling-aware-task-mapping-technique-for-reliability-management-in-manycore-systems-fatemeh-hossein-khani-et-al-2024>(1/1 | 124/124) A Two-Level Thermal Cycling-aware Task Mapping Technique for Reliability Management in Manycore Systems (Fatemeh Hossein Khani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Hossein Khani, Omid Akbari, Muhammad Shafique. (2024)<br><strong>A Two-Level Thermal Cycling-aware Task Mapping Technique for Reliability Management in Manycore Systems</strong><br><button class=copy-to-clipboard title="A Two-Level Thermal Cycling-aware Task Mapping Technique for Reliability Management in Manycore Systems" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2403.06134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2403.06134v1.pdf filename=2403.06134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reliability management is one of the primary concerns in manycore systems design. Different aging mechanisms such as Negative-Bias Temperature Instability (NBTI), Electromigration (EM), and thermal cycling can reduce the reliability of these systems. However, state-of-the-art works mainly focused on NBTI and EM, whereas a few works have considered the thermal cycling effect. The thermal cycling effect can significantly aggravate the systems lifetime. Moreover, the thermal effects of cores on each other due to their adjacency may also influence the systems Mean Time to Failure (MTTF). This paper introduces a new technique to manage the reliability of manycore systems. The technique considers thermal cycling, adjacency of cores, and process variation-induced diversity of operating frequencies. It uses two levels of task mapping to improve system lifetime. At the first level, cores with close temperatures are packed into the same bin, and then, an arrived task is assigned to a bin with a similar temperature. Afterward in the second level, the task is assigned to a core inside the selected bin in the first level, based on performance requirements and the core frequency. Compared to the conventional TC-aware techniques, the proposed method is performed at a higher level (bins level) to reduce the thermal variations of cores inside a bin, and improves the system MTTFTC, making it a promising solution for manycore systems. The efficacy of our proposed technique is evaluated on 16, 32, 64, and 256 core systems using SPLASH2 and PARSEC <b>benchmark</b> suite applications. The results show up to 20% MTTFTC increment compared to the conventional thermal cycling-aware task mapping techniques.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.03.11</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.03.13</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-15>cs.CL (15)</a><ul><li><a href=#115--1124-score-self-supervised-correspondence-fine-tuning-for-improved-content-representations-amit-meghanani-et-al-2024>(1/15 | 1/124) SCORE: Self-supervised Correspondence Fine-tuning for Improved Content Representations (Amit Meghanani et al., 2024)</a></li><li><a href=#215--2124-can-large-language-models-automatically-score-proficiency-of-written-essays-watheq-mansour-et-al-2024>(2/15 | 2/124) Can Large Language Models Automatically Score Proficiency of Written Essays? (Watheq Mansour et al., 2024)</a></li><li><a href=#315--3124-personalized-lora-for-human-centered-text-understanding-you-zhang-et-al-2024>(3/15 | 3/124) Personalized LoRA for Human-Centered Text Understanding (You Zhang et al., 2024)</a></li><li><a href=#415--4124-are-you-being-tracked-discover-the-power-of-zero-shot-trajectory-tracing-with-llms-huanqi-yang-et-al-2024>(4/15 | 4/124) Are You Being Tracked? Discover the Power of Zero-Shot Trajectory Tracing with LLMs! (Huanqi Yang et al., 2024)</a></li><li><a href=#515--5124-target-constrained-bidirectional-planning-for-generation-of-target-oriented-proactive-dialogue-jian-wang-et-al-2024>(5/15 | 5/124) Target-constrained Bidirectional Planning for Generation of Target-oriented Proactive Dialogue (Jian Wang et al., 2024)</a></li><li><a href=#615--6124-fine-grainedly-synthesize-streaming-data-based-on-large-language-models-with-graph-structure-understanding-for-data-sparsity-xin-zhang-et-al-2024>(6/15 | 6/124) Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity (Xin Zhang et al., 2024)</a></li><li><a href=#715--7124-from-instructions-to-constraints-language-model-alignment-with-automatic-constraint-verification-fei-wang-et-al-2024>(7/15 | 7/124) From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification (Fei Wang et al., 2024)</a></li><li><a href=#815--8124-unpacking-tokenization-evaluating-text-compression-and-its-correlation-with-model-performance-omer-goldman-et-al-2024>(8/15 | 8/124) Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance (Omer Goldman et al., 2024)</a></li><li><a href=#915--9124-identifying-and-interpreting-non-aligned-human-conceptual-representations-using-language-modeling-wanqian-bao-et-al-2024>(9/15 | 9/124) Identifying and interpreting non-aligned human conceptual representations using language modeling (Wanqian Bao et al., 2024)</a></li><li><a href=#1015--10124-large-language-models-on-fine-grained-emotion-detection-dataset-with-data-augmentation-and-transfer-learning-kaipeng-wang-et-al-2024>(10/15 | 10/124) Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning (Kaipeng Wang et al., 2024)</a></li><li><a href=#1115--11124-ensemble-language-models-for-multilingual-sentiment-analysis-md-arid-hasan-2024>(11/15 | 11/124) Ensemble Language Models for Multilingual Sentiment Analysis (Md Arid Hasan, 2024)</a></li><li><a href=#1215--12124-editing-conceptual-knowledge-for-large-language-models-xiaohan-wang-et-al-2024>(12/15 | 12/124) Editing Conceptual Knowledge for Large Language Models (Xiaohan Wang et al., 2024)</a></li><li><a href=#1315--13124-fmpaf-how-do-fed-chairs-affect-the-financial-market-a-fine-grained-monetary-policy-analysis-framework-on-their-language-yayue-deng-et-al-2024>(13/15 | 13/124) FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language (Yayue Deng et al., 2024)</a></li><li><a href=#1415--14124-can-llm-substitute-human-labeling-a-case-study-of-fine-grained-chinese-address-entity-recognition-dataset-for-uav-delivery-yuxuan-yao-et-al-2024>(14/15 | 14/124) Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery (Yuxuan Yao et al., 2024)</a></li><li><a href=#1515--15124-lieder-linguistically-informed-evaluation-for-discourse-entity-recognition-xiaomeng-zhu-et-al-2024>(15/15 | 15/124) LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition (Xiaomeng Zhu et al., 2024)</a></li></ul></li><li><a href=#cscv-41>cs.CV (41)</a><ul><li><a href=#141--16124-knowledge-distillation-of-convolutional-neural-networks-through-feature-map-transformation-using-decision-trees-maddimsetti-srinivas-et-al-2024>(1/41 | 16/124) Knowledge Distillation of Convolutional Neural Networks through Feature Map Transformation using Decision Trees (Maddimsetti Srinivas et al., 2024)</a></li><li><a href=#241--17124-in-context-prompt-learning-for-test-time-vision-recognition-with-frozen-vision-language-model-junhui-yin-et-al-2024>(2/41 | 17/124) In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model (Junhui Yin et al., 2024)</a></li><li><a href=#341--18124-towards-in-vehicle-multi-task-facial-attribute-recognition-investigating-synthetic-data-and-vision-foundation-models-esmaeil-seraj-et-al-2024>(3/41 | 18/124) Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models (Esmaeil Seraj et al., 2024)</a></li><li><a href=#441--19124-a-streamlined-approach-to-multimodal-few-shot-class-incremental-learning-for-fine-grained-datasets-thang-doan-et-al-2024>(4/41 | 19/124) A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets (Thang Doan et al., 2024)</a></li><li><a href=#541--20124-worldgpt-a-sora-inspired-video-ai-agent-as-rich-world-models-from-text-and-image-inputs-deshun-yang-et-al-2024>(5/41 | 20/124) WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs (Deshun Yang et al., 2024)</a></li><li><a href=#641--21124-on-depth-prediction-for-autonomous-driving-using-self-supervised-learning-houssem-boulahbal-2024>(6/41 | 21/124) On depth prediction for autonomous driving using self-supervised learning (Houssem Boulahbal, 2024)</a></li><li><a href=#741--22124-restore-towards-feature-shift-for-vision-language-prompt-learning-yuncheng-yang-et-al-2024>(7/41 | 22/124) RESTORE: Towards Feature Shift for Vision-Language Prompt Learning (Yuncheng Yang et al., 2024)</a></li><li><a href=#841--23124-decoupled-contrastive-learning-for-long-tailed-recognition-shiyu-xuan-et-al-2024>(8/41 | 23/124) Decoupled Contrastive Learning for Long-Tailed Recognition (Shiyu Xuan et al., 2024)</a></li><li><a href=#941--24124-fastvideoedit-leveraging-consistency-models-for-efficient-text-to-video-editing-youyuan-zhang-et-al-2024>(9/41 | 24/124) FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing (Youyuan Zhang et al., 2024)</a></li><li><a href=#1041--25124-v_kd-improving-knowledge-distillation-using-orthogonal-projections-roy-miles-et-al-2024>(10/41 | 25/124) $V_kD:$ Improving Knowledge Distillation using Orthogonal Projections (Roy Miles et al., 2024)</a></li><li><a href=#1141--26124-glancevad-exploring-glance-supervision-for-label-efficient-video-anomaly-detection-huaxin-zhang-et-al-2024>(11/41 | 26/124) GlanceVAD: Exploring Glance Supervision for Label-efficient Video Anomaly Detection (Huaxin Zhang et al., 2024)</a></li><li><a href=#1241--27124-mace-mass-concept-erasure-in-diffusion-models-shilin-lu-et-al-2024>(12/41 | 27/124) MACE: Mass Concept Erasure in Diffusion Models (Shilin Lu et al., 2024)</a></li><li><a href=#1341--28124-bit-mask-robust-contrastive-knowledge-distillation-for-unsupervised-semantic-hashing-liyang-he-et-al-2024>(13/41 | 28/124) Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing (Liyang He et al., 2024)</a></li><li><a href=#1441--29124-reframe-anything-llm-agent-for-open-world-video-reframing-jiawang-cao-et-al-2024>(14/41 | 29/124) Reframe Anything: LLM Agent for Open World Video Reframing (Jiawang Cao et al., 2024)</a></li><li><a href=#1541--30124-test-time-distribution-learning-adapter-for-cross-modal-visual-reasoning-yi-zhang-et-al-2024>(15/41 | 30/124) Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning (Yi Zhang et al., 2024)</a></li><li><a href=#1641--31124-understanding-and-mitigating-human-labelling-errors-in-supervised-contrastive-learning-zijun-long-et-al-2024>(16/41 | 31/124) Understanding and Mitigating Human-Labelling Errors in Supervised Contrastive Learning (Zijun Long et al., 2024)</a></li><li><a href=#1741--32124-transformer-based-multitask-learning-for-image-captioning-and-object-detection-debolena-basak-et-al-2024>(17/41 | 32/124) Transformer based Multitask Learning for Image Captioning and Object Detection (Debolena Basak et al., 2024)</a></li><li><a href=#1841--33124-vidprom-a-million-scale-real-prompt-gallery-dataset-for-text-to-video-diffusion-models-wenhao-wang-et-al-2024>(18/41 | 33/124) VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models (Wenhao Wang et al., 2024)</a></li><li><a href=#1941--34124-diffusion-models-trained-with-large-data-are-transferable-visual-models-guangkai-xu-et-al-2024>(19/41 | 34/124) Diffusion Models Trained with Large Data Are Transferable Visual Models (Guangkai Xu et al., 2024)</a></li><li><a href=#2041--35124-mipha-a-comprehensive-overhaul-of-multimodal-assistant-with-small-language-models-minjie-zhu-et-al-2024>(20/41 | 35/124) Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models (Minjie Zhu et al., 2024)</a></li><li><a href=#2141--36124-universal-debiased-editing-for-fair-medical-image-classification-ruinan-jin-et-al-2024>(21/41 | 36/124) Universal Debiased Editing for Fair Medical Image Classification (Ruinan Jin et al., 2024)</a></li><li><a href=#2241--37124-poly-kernel-inception-network-for-remote-sensing-detection-xinhao-cai-et-al-2024>(22/41 | 37/124) Poly Kernel Inception Network for Remote Sensing Detection (Xinhao Cai et al., 2024)</a></li><li><a href=#2341--38124-unicorn-ultrasound-nakagami-imaging-via-score-matching-and-adaptation-kwanyoung-kim-et-al-2024>(23/41 | 38/124) UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation (Kwanyoung Kim et al., 2024)</a></li><li><a href=#2441--39124-most-motion-style-transformer-between-diverse-action-contents-boeun-kim-et-al-2024>(24/41 | 39/124) MoST: Motion Style Transformer between Diverse Action Contents (Boeun Kim et al., 2024)</a></li><li><a href=#2541--40124-platypose-calibrated-zero-shot-multi-hypothesis-3d-human-motion-estimation-paweł-a-pierzchlewicz-et-al-2024>(25/41 | 40/124) Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion Estimation (Paweł A. Pierzchlewicz et al., 2024)</a></li><li><a href=#2641--41124-cracking-the-neural-code-for-word-recognition-in-convolutional-neural-networks-aakash-agrawal-et-al-2024>(26/41 | 41/124) Cracking the neural code for word recognition in convolutional neural networks (Aakash Agrawal et al., 2024)</a></li><li><a href=#2741--42124-pss-ba-lidar-bundle-adjustment-with-progressive-spatial-smoothing-jianping-li-et-al-2024>(27/41 | 42/124) PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing (Jianping Li et al., 2024)</a></li><li><a href=#2841--43124-enhancing-3d-object-detection-with-2d-detection-guided-query-anchors-haoxuanye-ji-et-al-2024>(28/41 | 43/124) Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors (Haoxuanye Ji et al., 2024)</a></li><li><a href=#2941--44124-foaa-flattened-outer-arithmetic-attention-for-multimodal-tumor-classification-omnia-alwazzan-et-al-2024>(29/41 | 44/124) FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor Classification (Omnia Alwazzan et al., 2024)</a></li><li><a href=#3041--45124-an-end-to-end-deep-learning-generative-framework-for-refinable-shape-matching-and-generation-soodeh-kalaie-et-al-2024>(30/41 | 45/124) An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation (Soodeh Kalaie et al., 2024)</a></li><li><a href=#3141--46124-clear-cross-transformers-with-pre-trained-language-model-is-all-you-need-for-person-attribute-recognition-and-retrieval-doanh-c-bui-et-al-2024>(31/41 | 46/124) CLEAR: Cross-Transformers with Pre-trained Language Model is All you need for Person Attribute Recognition and Retrieval (Doanh C. Bui et al., 2024)</a></li><li><a href=#3241--47124-is-vanilla-mlp-in-neural-radiance-field-enough-for-few-shot-view-synthesis-hanxin-zhu-et-al-2024>(32/41 | 47/124) Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis? (Hanxin Zhu et al., 2024)</a></li><li><a href=#3341--48124-text-guided-variational-image-generation-for-industrial-anomaly-detection-and-segmentation-mingyu-lee-et-al-2024>(33/41 | 48/124) Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation (Mingyu Lee et al., 2024)</a></li><li><a href=#3441--49124-blazebvd-make-scale-time-equalization-great-again-for-blind-video-deflickering-xinmin-qiu-et-al-2024>(34/41 | 49/124) BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video Deflickering (Xinmin Qiu et al., 2024)</a></li><li><a href=#3541--50124-finding-visual-saliency-in-continuous-spike-stream-lin-zhu-et-al-2024>(35/41 | 50/124) Finding Visual Saliency in Continuous Spike Stream (Lin Zhu et al., 2024)</a></li><li><a href=#3641--51124-s-dyrf-reference-based-stylized-radiance-fields-for-dynamic-scenes-xingyi-li-et-al-2024>(36/41 | 51/124) S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes (Xingyi Li et al., 2024)</a></li><li><a href=#3741--52124-diffumatting-synthesizing-arbitrary-objects-with-matting-level-annotation-xiaobin-hu-et-al-2024>(37/41 | 52/124) DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation (Xiaobin Hu et al., 2024)</a></li><li><a href=#3841--53124-cross-cluster-shifting-for-efficient-and-effective-3d-object-detection-in-autonomous-driving-zhili-chen-et-al-2024>(38/41 | 53/124) Cross-Cluster Shifting for Efficient and Effective 3D Object Detection in Autonomous Driving (Zhili Chen et al., 2024)</a></li><li><a href=#3941--54124-all-in-one-platform-for-ai-rd-in-medical-imaging-encompassing-data-collection-selection-annotation-and-pre-processing-changhee-han-et-al-2024>(39/41 | 54/124) All-in-one platform for AI R&amp;D in medical imaging, encompassing data collection, selection, annotation, and pre-processing (Changhee Han et al., 2024)</a></li><li><a href=#4041--55124-bayesian-random-semantic-data-augmentation-for-medical-image-classification-yaoyao-zhu-et-al-2024>(40/41 | 55/124) Bayesian Random Semantic Data Augmentation for Medical Image Classification (Yaoyao Zhu et al., 2024)</a></li><li><a href=#4141--56124-style-blind-domain-generalized-semantic-segmentation-via-covariance-alignment-and-semantic-consistence-contrastive-learning-woo-jin-ahn-et-al-2024>(41/41 | 56/124) Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning (Woo-Jin Ahn et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--57124-llms-still-cant-avoid-instanceof-an-investigation-into-gpt-35-gpt-4-and-bards-capacity-to-handle-object-oriented-programming-assignments-bruno-pereira-cipriano-et-al-2024>(1/2 | 57/124) LLMs Still Can&rsquo;t Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard&rsquo;s Capacity to Handle Object-Oriented Programming Assignments (Bruno Pereira Cipriano et al., 2024)</a></li><li><a href=#22--58124-repohyper-better-context-retrieval-is-all-you-need-for-repository-level-code-completion-huy-n-phan-et-al-2024>(2/2 | 58/124) RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion (Huy N. Phan et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--59124-simulating-family-conversations-using-llms-demonstration-of-parenting-styles-frank-tian-fang-ye-et-al-2024>(1/1 | 59/124) Simulating Family Conversations using LLMs: Demonstration of Parenting Styles (Frank Tian-fang Ye et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#14--60124-fedpit-towards-privacy-preserving-and-few-shot-federated-instruction-tuning-zhuo-zhang-et-al-2024>(1/4 | 60/124) FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning (Zhuo Zhang et al., 2024)</a></li><li><a href=#24--61124-attacking-transformers-with-feature-diversity-adversarial-perturbation-chenxing-gao-et-al-2024>(2/4 | 61/124) Attacking Transformers with Feature Diversity Adversarial Perturbation (Chenxing Gao et al., 2024)</a></li><li><a href=#34--62124-fluent-round-efficient-secure-aggregation-for-private-federated-learning-xincheng-li-et-al-2024>(3/4 | 62/124) Fluent: Round-efficient Secure Aggregation for Private Federated Learning (Xincheng Li et al., 2024)</a></li><li><a href=#44--63124-federated-learning-attacks-defenses-opportunities-and-challenges-ghazaleh-shirvani-et-al-2024>(4/4 | 63/124) Federated Learning: Attacks, Defenses, Opportunities, and Challenges (Ghazaleh Shirvani et al., 2024)</a></li></ul></li><li><a href=#cslg-15>cs.LG (15)</a><ul><li><a href=#115--64124-cooperative-classification-and-rationalization-for-graph-generalization-linan-yue-et-al-2024>(1/15 | 64/124) Cooperative Classification and Rationalization for Graph Generalization (Linan Yue et al., 2024)</a></li><li><a href=#215--65124-l2gc-lorentzian-linear-graph-convolutional-networks-for-node-classification-qiuyu-liang-et-al-2024>(2/15 | 65/124) L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification (Qiuyu Liang et al., 2024)</a></li><li><a href=#315--66124-framequant-flexible-low-bit-quantization-for-transformers-harshavardhan-adepu-et-al-2024>(3/15 | 66/124) FrameQuant: Flexible Low-Bit Quantization for Transformers (Harshavardhan Adepu et al., 2024)</a></li><li><a href=#415--67124-revisiting-edge-perturbation-for-graph-neural-network-in-graph-data-augmentation-and-attack-xin-liu-et-al-2024>(4/15 | 67/124) Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack (Xin Liu et al., 2024)</a></li><li><a href=#515--68124-generalization-of-graph-neural-networks-through-the-lens-of-homomorphism-shouheng-li-et-al-2024>(5/15 | 68/124) Generalization of Graph Neural Networks through the Lens of Homomorphism (Shouheng Li et al., 2024)</a></li><li><a href=#615--69124-risk-sensitive-rl-with-optimized-certainty-equivalents-via-reduction-to-standard-rl-kaiwen-wang-et-al-2024>(6/15 | 69/124) Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL (Kaiwen Wang et al., 2024)</a></li><li><a href=#715--70124-optimal-policy-sparsification-and-low-rank-decomposition-for-deep-reinforcement-learning-vikram-goddla-2024>(7/15 | 70/124) Optimal Policy Sparsification and Low Rank Decomposition for Deep Reinforcement Learning (Vikram Goddla, 2024)</a></li><li><a href=#815--71124-local-vertex-colouring-graph-neural-networks-shouheng-li-et-al-2024>(8/15 | 71/124) Local Vertex Colouring Graph Neural Networks (Shouheng Li et al., 2024)</a></li><li><a href=#915--72124-transferable-reinforcement-learning-via-generalized-occupancy-models-chuning-zhu-et-al-2024>(9/15 | 72/124) Transferable Reinforcement Learning via Generalized Occupancy Models (Chuning Zhu et al., 2024)</a></li><li><a href=#1015--73124-analysis-of-total-variation-minimization-for-clustered-federated-learning-a-jung-2024>(10/15 | 73/124) Analysis of Total Variation Minimization for Clustered Federated Learning (A. Jung, 2024)</a></li><li><a href=#1115--74124-fake-or-compromised-making-sense-of-malicious-clients-in-federated-learning-hamid-mozaffari-et-al-2024>(11/15 | 74/124) Fake or Compromised? Making Sense of Malicious Clients in Federated Learning (Hamid Mozaffari et al., 2024)</a></li><li><a href=#1215--75124-fwin-transformer-for-dengue-prediction-under-climate-and-ocean-influence-nhat-thanh-tran-et-al-2024>(12/15 | 75/124) FWin transformer for dengue prediction under climate and ocean influence (Nhat Thanh Tran et al., 2024)</a></li><li><a href=#1315--76124-probabilistic-neural-circuits-pedro-zuidberg-dos-martires-2024>(13/15 | 76/124) Probabilistic Neural Circuits (Pedro Zuidberg Dos Martires, 2024)</a></li><li><a href=#1415--77124-linearapt-an-adaptive-algorithm-for-the-fixed-budget-thresholding-linear-bandit-problem-yun-ang-wu-et-al-2024>(14/15 | 77/124) LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding Linear Bandit Problem (Yun-Ang Wu et al., 2024)</a></li><li><a href=#1515--78124-domain-adversarial-active-learning-for-domain-generalization-classification-jianting-chen-et-al-2024>(15/15 | 78/124) Domain Adversarial Active Learning for Domain Generalization Classification (Jianting Chen et al., 2024)</a></li></ul></li><li><a href=#csai-3>cs.AI (3)</a><ul><li><a href=#13--79124-trad-enhancing-llm-agents-with-step-wise-thought-retrieval-and-aligned-decision-ruiwen-zhou-et-al-2024>(1/3 | 79/124) TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision (Ruiwen Zhou et al., 2024)</a></li><li><a href=#23--80124-argmed-agents-explainable-clinical-decision-reasoning-with-large-language-models-via-argumentation-schemes-shengxin-hong-et-al-2024>(2/3 | 80/124) ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes (Shengxin Hong et al., 2024)</a></li><li><a href=#33--81124-towards-generalizable-and-interpretable-motion-prediction-a-deep-variational-bayes-approach-juanwu-lu-et-al-2024>(3/3 | 81/124) Towards Generalizable and Interpretable Motion Prediction: A Deep Variational Bayes Approach (Juanwu Lu et al., 2024)</a></li></ul></li><li><a href=#csce-3>cs.CE (3)</a><ul><li><a href=#13--82124-generative-lstm-models-and-asset-hierarchy-creation-in-industrial-facilities-morgen-pronk-2024>(1/3 | 82/124) Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities (Morgen Pronk, 2024)</a></li><li><a href=#23--83124-no-language-is-an-island-unifying-chinese-and-english-in-financial-large-language-models-instruction-data-and-benchmarks-gang-hu-et-al-2024>(2/3 | 83/124) No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks (Gang Hu et al., 2024)</a></li><li><a href=#33--84124-rads--restricted-anisotropic-diffusion-spectrum-model-for-axonal-health-quantification-in-multiple-sclerosis-nand-sharma-2024>(3/3 | 84/124) RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis (Nand Sharma, 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--85124-are-llms-ready-for-visualization-pere-pau-vázquez-2024>(1/4 | 85/124) Are LLMs ready for Visualization? (Pere-Pau Vázquez, 2024)</a></li><li><a href=#24--86124-explaining-code-with-a-purpose-an-integrated-approach-for-developing-code-comprehension-and-prompting-skills-paul-denny-et-al-2024>(2/4 | 86/124) Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills (Paul Denny et al., 2024)</a></li><li><a href=#34--87124-developing-an-ai-based-psychometric-system-for-assessing-learning-difficulties-and-adaptive-system-to-overcome-a-qualitative-and-conceptual-framework-aaron-hu-2024>(3/4 | 87/124) Developing an AI-Based Psychometric System for Assessing Learning Difficulties and Adaptive System to Overcome: A Qualitative and Conceptual Framework (Aaron Hu, 2024)</a></li><li><a href=#44--88124-understanding-parents-perceptions-and-practices-toward-childrens-security-and-privacy-in-virtual-reality-jiaxun-cao-et-al-2024>(4/4 | 88/124) Understanding Parents&rsquo; Perceptions and Practices Toward Children&rsquo;s Security and Privacy in Virtual Reality (Jiaxun Cao et al., 2024)</a></li></ul></li><li><a href=#eessiv-6>eess.IV (6)</a><ul><li><a href=#16--89124-low-dose-ct-denoising-with-language-engaged-dual-space-alignment-zhihao-chen-et-al-2024>(1/6 | 89/124) Low-dose CT Denoising with Language-engaged Dual-space Alignment (Zhihao Chen et al., 2024)</a></li><li><a href=#26--90124-implicit-image-to-image-schrodinger-bridge-for-ct-super-resolution-and-denoising-yuang-wang-et-al-2024>(2/6 | 90/124) Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising (Yuang Wang et al., 2024)</a></li><li><a href=#36--91124-causalcellsegmenter-causal-inference-inspired-diversified-aggregation-convolution-for-pathology-image-segmentation-dawei-fan-et-al-2024>(3/6 | 91/124) CausalCellSegmenter: Causal Inference inspired Diversified Aggregation Convolution for Pathology Image Segmentation (Dawei Fan et al., 2024)</a></li><li><a href=#46--92124-decoupled-data-consistency-with-diffusion-purification-for-image-restoration-xiang-li-et-al-2024>(4/6 | 92/124) Decoupled Data Consistency with Diffusion Purification for Image Restoration (Xiang Li et al., 2024)</a></li><li><a href=#56--93124-pepsi-pathology-enhanced-pulse-sequence-invariant-representations-for-brain-mri-peirong-liu-et-al-2024>(5/6 | 93/124) PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for Brain MRI (Peirong Liu et al., 2024)</a></li><li><a href=#66--94124-drfuse-learning-disentangled-representation-for-clinical-multi-modal-fusion-with-missing-modality-and-modal-inconsistency-wenfang-yao-et-al-2024>(6/6 | 94/124) DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency (Wenfang Yao et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--95124-acm-mmsys-2024-bandwidth-estimation-in-real-time-communications-challenge-sami-khairy-et-al-2024>(1/1 | 95/124) ACM MMSys 2024 Bandwidth Estimation in Real Time Communications Challenge (Sami Khairy et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#13--96124-fine-tuning-of-diffusion-models-via-stochastic-control-entropy-regularization-and-beyond-wenpin-tang-2024>(1/3 | 96/124) Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond (Wenpin Tang, 2024)</a></li><li><a href=#23--97124-whiteness-based-bilevel-learning-of-regularization-parameters-in-imaging-carlo-santambrogio-et-al-2024>(2/3 | 97/124) Whiteness-based bilevel learning of regularization parameters in imaging (Carlo Santambrogio et al., 2024)</a></li><li><a href=#33--98124-absence-of-spurious-solutions-far-from-ground-truth-a-low-rank-analysis-with-high-order-losses-ziye-ma-et-al-2024>(3/3 | 98/124) Absence of spurious solutions far from ground truth: A low-rank analysis with high-order losses (Ziye Ma et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#14--99124-quantized-constant-envelope-waveform-design-for-massive-mimo-dfrc-systems-zheyu-wu-et-al-2024>(1/4 | 99/124) Quantized Constant-Envelope Waveform Design for Massive MIMO DFRC Systems (Zheyu Wu et al., 2024)</a></li><li><a href=#24--100124-stochastic-geometry-analysis-for-distributed-riss-assisted-mmwave-communications-yuan-xu-et-al-2024>(2/4 | 100/124) Stochastic Geometry Analysis for Distributed RISs-Assisted mmWave Communications (Yuan Xu et al., 2024)</a></li><li><a href=#34--101124-hashing-beam-training-for-near-field-communications-yuan-xu-et-al-2024>(3/4 | 101/124) Hashing Beam Training for Near-Field Communications (Yuan Xu et al., 2024)</a></li><li><a href=#44--102124-channel-estimation-considerate-precoder-design-for-multi-user-massive-mimo-ofdm-systems-the-concept-and-fast-algorithms-liu-junkai-et-al-2024>(4/4 | 102/124) Channel Estimation Considerate Precoder Design for Multi-user Massive MIMO-OFDM Systems: The Concept and Fast Algorithms (Liu Junkai et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--103124-control-strategies-for-recommendation-systems-in-social-networks-ben-sprenger-et-al-2024>(1/3 | 103/124) Control Strategies for Recommendation Systems in Social Networks (Ben Sprenger et al., 2024)</a></li><li><a href=#23--104124-direct-shooting-method-for-numerical-optimal-control-a-modified-transcription-approach-jiawei-tang-et-al-2024>(2/3 | 104/124) Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach (Jiawei Tang et al., 2024)</a></li><li><a href=#33--105124-multi-gated-perimeter-flow-control-for-monocentric-cities-efficiency-and-equity-ruzanna-mat-jusoh-et-al-2024>(3/3 | 105/124) Multi-gated perimeter flow control for monocentric cities: Efficiency and equity (Ruzanna Mat Jusoh et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--106124-disentangling-shared-and-private-latent-factors-in-multimodal-variational-autoencoders-kaspar-märtens-et-al-2024>(1/3 | 106/124) Disentangling shared and private latent factors in multimodal Variational Autoencoders (Kaspar Märtens et al., 2024)</a></li><li><a href=#23--107124-nonparametric-automatic-differentiation-variational-inference-with-spline-approximation-yuda-shao-et-al-2024>(2/3 | 107/124) Nonparametric Automatic Differentiation Variational Inference with Spline Approximation (Yuda Shao et al., 2024)</a></li><li><a href=#33--108124-the-alell_0core-tensor-decomposition-for-sparse-count-data-john-hood-et-al-2024>(3/3 | 108/124) The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data (John Hood et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--109124-vertex-block-descent-anka-he-chen-et-al-2024>(1/1 | 109/124) Vertex Block Descent (Anka He Chen et al., 2024)</a></li></ul></li><li><a href=#csro-4>cs.RO (4)</a><ul><li><a href=#14--110124-a-study-on-domain-generalization-for-failure-detection-through-human-reactions-in-hri-maria-teresa-parreira-et-al-2024>(1/4 | 110/124) A Study on Domain Generalization for Failure Detection through Human Reactions in HRI (Maria Teresa Parreira et al., 2024)</a></li><li><a href=#24--111124-robust-predictive-motion-planning-by-learning-obstacle-uncertainty-jian-zhou-et-al-2024>(2/4 | 111/124) Robust Predictive Motion Planning by Learning Obstacle Uncertainty (Jian Zhou et al., 2024)</a></li><li><a href=#34--112124-speeding-up-6-dof-grasp-sampling-with-quality-diversity-johann-huber-et-al-2024>(3/4 | 112/124) Speeding up 6-DoF Grasp Sampling with Quality-Diversity (Johann Huber et al., 2024)</a></li><li><a href=#44--113124-mind-meets-robots-a-review-of-eeg-based-brain-robot-interaction-systems-yuchong-zhang-et-al-2024>(4/4 | 113/124) Mind Meets Robots: A Review of EEG-Based Brain-Robot Interaction Systems (Yuchong Zhang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--114124-enhancing-quantum-variational-algorithms-with-zero-noise-extrapolation-via-neural-networks-subhasree-bhattacharjee-et-al-2024>(1/1 | 114/124) Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation via Neural Networks (Subhasree Bhattacharjee et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--115124-ideas-information-driven-ev-admission-in-charging-station-considering-user-impatience-to-improve-qos-and-station-utilization-animesh-chattopadhyay-et-al-2024>(1/2 | 115/124) IDEAS: Information-Driven EV Admission in Charging Station Considering User Impatience to Improve QoS and Station Utilization (Animesh Chattopadhyay et al., 2024)</a></li><li><a href=#22--116124-dynamics-of-polarization-under-normative-institutions-and-opinion-expression-stewarding-atrisha-sarkar-et-al-2024>(2/2 | 116/124) Dynamics of Polarization Under Normative Institutions and Opinion Expression Stewarding (Atrisha Sarkar et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--117124-an-approach-using-the-null-space-to-implement-dirichlet-and-constraint-boundary-conditions-into-fem-stefan-schoder-2024>(1/1 | 117/124) An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM (Stefan Schoder, 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--118124-fish-inspired-tracking-of-underwater-turbulent-plumes-peter-gunnarson-et-al-2024>(1/1 | 118/124) Fish-inspired tracking of underwater turbulent plumes (Peter Gunnarson et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--119124-solution-hashing-search-based-on-layout-graph-transformation-for-unequal-circle-packing-jianrong-zhou-et-al-2024>(1/1 | 119/124) Solution-Hashing Search Based on Layout-Graph Transformation for Unequal Circle Packing (Jianrong Zhou et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--120124-spectral-lower-bounds-for-local-search-simina-brânzei-et-al-2024>(1/1 | 120/124) Spectral Lower Bounds for Local Search (Simina Brânzei et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--121124-online-multi-spectral-neuron-tracing-bin-duan-et-al-2024>(1/1 | 121/124) Online Multi-spectral Neuron Tracing (Bin Duan et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--122124-improved-fpt-approximation-scheme-and-approximate-kernel-for-biclique-free-max-k-weight-sat-greedy-strikes-back-pasin-manurangsi-2024>(1/2 | 122/124) Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back (Pasin Manurangsi, 2024)</a></li><li><a href=#22--123124-revisiting-path-contraction-and-cycle-contraction-r-krithika-et-al-2024>(2/2 | 123/124) Revisiting Path Contraction and Cycle Contraction (R. Krithika et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--124124-a-two-level-thermal-cycling-aware-task-mapping-technique-for-reliability-management-in-manycore-systems-fatemeh-hossein-khani-et-al-2024>(1/1 | 124/124) A Two-Level Thermal Cycling-aware Task Mapping Technique for Reliability Management in Manycore Systems (Fatemeh Hossein Khani et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>